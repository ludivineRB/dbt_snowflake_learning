## üéØ Objectifs d'apprentissage

- Comprendre l'architecture et les concepts d'Apache Spark
- Ma√Ætriser les DataFrames et Datasets
- Effectuer des transformations et actions sur les donn√©es
- Utiliser Spark SQL pour requ√™ter les donn√©es
- Optimiser les performances des requ√™tes

## 1. Architecture Apache Spark

### Concepts fondamentaux

```bash
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ARCHITECTURE SPARK                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ  ‚îÇ   Driver     ‚îÇ   (Master Node)                            ‚îÇ
‚îÇ  ‚îÇ   Program    ‚îÇ   ‚Ä¢ SparkContext                           ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ   ‚Ä¢ Planification des t√¢ches               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚Ä¢ Distribution du code                   ‚îÇ
‚îÇ         ‚îÇ                                                     ‚îÇ
‚îÇ         ‚îÇ                                                     ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ    ‚îÇ      Cluster Manager                 ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ  (YARN / Mesos / Kubernetes / Local) ‚îÇ                  ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ                                      ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ    ‚îÇ Worker  ‚îÇ    ‚îÇ Worker  ‚îÇ    ‚îÇ Worker  ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ Node 1  ‚îÇ    ‚îÇ Node 2  ‚îÇ    ‚îÇ Node 3  ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ    ‚îÇ ‚îÇTask ‚îÇ ‚îÇ                ‚îÇ
‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| Composant | R√¥le | Responsabilit√©s |
| --- | --- | --- |
| **Driver** | N≈ìud ma√Ætre | ‚Ä¢ Ex√©cute le code utilisateur  ‚Ä¢ Cr√©e le SparkContext  ‚Ä¢ Planifie les t√¢ches |
| **Cluster Manager** | Gestionnaire de ressources | ‚Ä¢ Alloue les ressources  ‚Ä¢ G√®re les workers  ‚Ä¢ Monitore la sant√© du cluster |
| **Workers** | N≈ìuds de calcul | ‚Ä¢ Ex√©cutent les t√¢ches  ‚Ä¢ Stockent les donn√©es en cache  ‚Ä¢ Renvoient les r√©sultats |
| **Executors** | Processus sur workers | ‚Ä¢ Ex√©cutent le code  ‚Ä¢ G√®rent le cache  ‚Ä¢ Communiquent avec driver |

## 2. DataFrames et Datasets

### Qu'est-ce qu'un DataFrame ?

Un DataFrame est une collection distribu√©e de donn√©es organis√©e en colonnes nomm√©es, similaire √† une table SQL ou un DataFrame pandas, mais optimis√© pour le traitement distribu√©.

### Cr√©ation de DataFrames

```bash
# 1. Depuis une collection Python
data = [
    ("Alice", 34, "Engineering"),
    ("Bob", 45, "Sales"),
    ("Charlie", 29, "Marketing")
]
df = spark.createDataFrame(data, ["name", "age", "department"])
df.show()

# 2. Depuis un fichier CSV
df_csv = spark.read.csv(
    "/path/to/data.csv",
    header=True,
    inferSchema=True
)

# 3. Depuis un fichier Parquet
df_parquet = spark.read.parquet("/path/to/data.parquet")

# 4. Depuis une table Delta
df_delta = spark.read.format("delta").load("/path/to/delta-table")

# 5. Depuis JSON
df_json = spark.read.json("/path/to/data.json")

# 6. Depuis une requ√™te SQL
df_sql = spark.sql("SELECT * FROM my_table WHERE age > 30")
```

### Sch√©ma des DataFrames

```bash
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# D√©finir un sch√©ma explicitement
schema = StructType([
    StructField("name", StringType(), nullable=False),
    StructField("age", IntegerType(), nullable=True),
    StructField("department", StringType(), nullable=True)
])

# Cr√©er DataFrame avec sch√©ma
df = spark.createDataFrame(data, schema=schema)

# Afficher le sch√©ma
df.printSchema()
# root
#  |-- name: string (nullable = false)
#  |-- age: integer (nullable = true)
#  |-- department: string (nullable = true)

# Obtenir le sch√©ma sous forme de DDL
print(df.schema.simpleString())
```

## 3. Transformations et Actions

Spark utilise deux types d'op√©rations :

#### Transformations (Lazy)

Cr√©ent un nouveau DataFrame sans ex√©cution imm√©diate

- select(), filter(), groupBy()
- join(), orderBy(), distinct()
- withColumn(), drop()

#### Actions (Eager)

D√©clenchent l'ex√©cution et retournent un r√©sultat

- show(), count(), collect()
- take(), first(), head()
- write.save(), foreach()

### Transformations courantes

```bash
from pyspark.sql.functions import col, lit, when, avg, sum, count

# SELECT : S√©lectionner des colonnes
df_select = df.select("name", "age")
df_select = df.select(col("name"), col("age") + 1)

# FILTER / WHERE : Filtrer les lignes
df_filtered = df.filter(col("age") > 30)
df_filtered = df.where("age > 30 AND department = 'Engineering'")

# WITH COLUMN : Ajouter ou modifier une colonne
df_with_senior = df.withColumn(
    "is_senior",
    when(col("age") >= 40, lit("Yes")).otherwise(lit("No"))
)

df_with_salary = df.withColumn("estimated_salary", col("age") * 1000)

# DROP : Supprimer des colonnes
df_dropped = df.drop("department")

# DISTINCT : Valeurs uniques
df_unique = df.select("department").distinct()

# ORDER BY : Trier
df_sorted = df.orderBy(col("age").desc())

# LIMIT : Limiter le nombre de lignes
df_limited = df.limit(10)
```

### Agr√©gations

```bash
from pyspark.sql.functions import avg, sum, count, min, max, stddev

# Agr√©gations simples
df.select(
    avg("age").alias("avg_age"),
    min("age").alias("min_age"),
    max("age").alias("max_age")
).show()

# GROUP BY
df_grouped = df.groupBy("department").agg(
    count("*").alias("employee_count"),
    avg("age").alias("avg_age"),
    min("age").alias("youngest"),
    max("age").alias("oldest")
)
df_grouped.show()

# Multiple groupBy
df.groupBy("department", "is_senior").agg(
    count("*").alias("count")
).show()
```

### Jointures

```bash
# Cr√©er deux DataFrames
employees = spark.createDataFrame([
    (1, "Alice", "Engineering"),
    (2, "Bob", "Sales"),
    (3, "Charlie", "Engineering")
], ["emp_id", "name", "dept"])

salaries = spark.createDataFrame([
    (1, 95000),
    (2, 85000),
    (3, 78000)
], ["emp_id", "salary"])

# INNER JOIN
df_inner = employees.join(salaries, on="emp_id", how="inner")
df_inner.show()

# LEFT JOIN
df_left = employees.join(salaries, on="emp_id", how="left")

# RIGHT JOIN
df_right = employees.join(salaries, on="emp_id", how="right")

# FULL OUTER JOIN
df_full = employees.join(salaries, on="emp_id", how="outer")

# JOIN avec conditions multiples
departments = spark.createDataFrame([
    ("Engineering", "Building A"),
    ("Sales", "Building B")
], ["dept_name", "location"])

df_complex_join = employees.join(
    departments,
    employees.dept == departments.dept_name,
    how="left"
).drop("dept_name")
df_complex_join.show()
```

## 4. Spark SQL

### Cr√©er des vues temporaires

```bash
# Cr√©er une vue temporaire globale
df.createOrReplaceGlobalTempView("employees_global")

# Cr√©er une vue temporaire locale (session)
df.createOrReplaceTempView("employees")

# Requ√™ter avec SQL
result = spark.sql("""
    SELECT
        department,
        COUNT(*) as employee_count,
        AVG(age) as avg_age,
        MIN(age) as youngest,
        MAX(age) as oldest
    FROM employees
    GROUP BY department
    ORDER BY employee_count DESC
""")
result.show()
```

### Requ√™tes SQL avanc√©es

```bash
%sql
-- Window functions
SELECT
  name,
  age,
  department,
  AVG(age) OVER (PARTITION BY department) as dept_avg_age,
  RANK() OVER (PARTITION BY department ORDER BY age DESC) as age_rank
FROM employees;

-- CTEs (Common Table Expressions)
WITH dept_stats AS (
  SELECT
    department,
    AVG(age) as avg_age,
    COUNT(*) as emp_count
  FROM employees
  GROUP BY department
)
SELECT
  e.name,
  e.age,
  e.department,
  ds.avg_age,
  ds.emp_count
FROM employees e
JOIN dept_stats ds ON e.department = ds.department
WHERE e.age > ds.avg_age;

-- Sous-requ√™tes
SELECT *
FROM employees
WHERE age > (SELECT AVG(age) FROM employees);
```

### Fonctions SQL utiles

```bash
from pyspark.sql.functions import *

df_advanced = df.select(
# Fonctions de cha√Æne
    upper(col("name")).alias("name_upper"),
    lower(col("name")).alias("name_lower"),
    length(col("name")).alias("name_length"),
    concat(col("name"), lit(" - "), col("department")).alias("full_desc"),

# Fonctions de date
    current_date().alias("today"),
    current_timestamp().alias("now"),
    date_add(current_date(), 7).alias("next_week"),

# Fonctions conditionnelles
    when(col("age") >= 40, "Senior")
    .when(col("age") >= 30, "Mid")
    .otherwise("Junior").alias("seniority"),

# Fonctions math√©matiques
    round(col("age") / 10, 2).alias("age_decades"),
    abs(col("age") - 35).alias("distance_from_35")
)
df_advanced.show(truncate=False)
```

## 5. Optimisation des performances

### Catalyst Optimizer

Spark utilise Catalyst, un optimiseur de requ√™tes qui r√©organise automatiquement vos op√©rations pour de meilleures performances.

```bash
# Voir le plan d'ex√©cution logique
df.explain(mode="simple")

# Plan d'ex√©cution d√©taill√©
df.explain(mode="extended")

# Plan d'ex√©cution format√©
df.explain(mode="formatted")

# Co√ªt estim√©
df.explain(mode="cost")
```

### Partitionnement

```bash
# V√©rifier le nombre de partitions
print(f"Nombre de partitions : {df.rdd.getNumPartitions()}")

# Repartitionner (shuffle)
df_repartitioned = df.repartition(10)
df_by_dept = df.repartition(10, "department")

# Coalesce (pas de shuffle, r√©duction uniquement)
df_coalesced = df.coalesce(5)

# Optimal : partition par colonne fr√©quemment filtr√©e
df_optimized = df.repartition("department")
```

### Mise en cache

```bash
# Cache en m√©moire
df.cache()  # ou df.persist()

# Utiliser le DataFrame mis en cache
df.count()  # Premier acc√®s : calcul et mise en cache
df.filter(col("age") > 30).count()  # R√©utilise le cache

# Retirer du cache
df.unpersist()

# Cache avec niveau de stockage personnalis√©
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_AND_DISK)
```

### Bonnes pratiques d'optimisation

#### Filtrage pr√©coce

Filtrez les donn√©es le plus t√¥t possible pour r√©duire le volume trait√©

```bash
# Bon
df.filter(col("age") > 30) \
  .select("name", "dept") \
  .groupBy("dept").count()

# Moins bon
df.groupBy("dept").count() \
  .filter(col("count") > 10)
```

#### √âviter collect()

collect() ram√®ne toutes les donn√©es au driver, risque d'OutOfMemory

```bash
# Dangereux sur big data
# all_data = df.collect()

# Pr√©f√©rer
df.show(20)
df.take(10)
df.limit(100).toPandas()
```

#### Broadcast Join

Pour joindre une petite table avec une grande

```bash
from pyspark.sql.functions import broadcast

# Broadcast automatique si < 10MB
df_large.join(
    broadcast(df_small),
    on="key"
)
```

#### Adaptive Query Execution

Active par d√©faut dans Spark 3.x, optimise dynamiquement

```bash
spark.conf.set(
  "spark.sql.adaptive.enabled",
  "true"
)
```

#### Fonctionnalit√©s sp√©cifiques Databricks

- **Photon Engine :** Moteur vectoris√© C++ jusqu'√† 4x plus rapide (Premium tier)
- **Auto Optimize :** Optimisation automatique des tables Delta
- **Adaptive Query Execution :** Activ√© par d√©faut
- **Dynamic File Pruning :** R√©duit les fichiers lus lors des jointures

## 6. Exemple pratique complet

```bash
# Sc√©nario : Analyse de ventes e-commerce

# 1. Charger les donn√©es
orders = spark.read.parquet("/mnt/data/orders/")
customers = spark.read.parquet("/mnt/data/customers/")
products = spark.read.parquet("/mnt/data/products/")

# 2. Cr√©er des vues pour SQL
orders.createOrReplaceTempView("orders")
customers.createOrReplaceTempView("customers")
products.createOrReplaceTempView("products")

# 3. Analyse avec PySpark
from pyspark.sql.functions import *

# Jointure enrichie
enriched_orders = orders \
    .join(customers, on="customer_id", how="left") \
    .join(products, on="product_id", how="left") \
    .withColumn("order_date", to_date(col("timestamp"))) \
    .withColumn("revenue", col("quantity") * col("unit_price"))

# Agr√©gation par produit et mois
monthly_sales = enriched_orders \
    .withColumn("month", date_trunc("month", col("order_date"))) \
    .groupBy("month", "product_name", "category") \
    .agg(
        sum("revenue").alias("total_revenue"),
        sum("quantity").alias("total_quantity"),
        count("order_id").alias("order_count"),
        countDistinct("customer_id").alias("unique_customers")
    ) \
    .orderBy(col("month").desc(), col("total_revenue").desc())

# 4. Ou avec SQL
result = spark.sql("""
    SELECT
        DATE_TRUNC('month', o.order_date) as month,
        p.product_name,
        p.category,
        SUM(o.quantity * o.unit_price) as total_revenue,
        SUM(o.quantity) as total_quantity,
        COUNT(o.order_id) as order_count,
        COUNT(DISTINCT o.customer_id) as unique_customers
    FROM orders o
    LEFT JOIN products p ON o.product_id = p.product_id
    GROUP BY month, p.product_name, p.category
    ORDER BY month DESC, total_revenue DESC
""")

# 5. Sauvegarder les r√©sultats
result.write \
    .format("delta") \
    .mode("overwrite") \
    .partitionBy("month") \
    .save("/mnt/analytics/monthly_sales")

# 6. Afficher les insights
display(result.limit(20))
```

### üìå Points cl√©s √† retenir

- Spark distribue le calcul sur plusieurs n≈ìuds (Driver + Workers)
- DataFrames sont des collections distribu√©es avec sch√©ma
- Transformations (lazy) vs Actions (eager) - comprenez la diff√©rence
- Spark SQL permet de requ√™ter avec syntaxe SQL standard
- Catalyst optimizer optimise automatiquement vos requ√™tes
- Cache les DataFrames r√©utilis√©s, partitionnez intelligemment
- √âvitez collect() sur de gros volumes, privil√©giez show()/take()

#### Prochaine √©tape

Vous ma√Ætrisez Spark ! Dans la **Partie 5**, d√©couvrez Delta Lake pour des donn√©es fiables et performantes.