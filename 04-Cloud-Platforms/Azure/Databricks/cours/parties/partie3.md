## üéØ Objectifs d'apprentissage

- Cr√©er et organiser des notebooks Databricks
- Utiliser Python, SQL, Scala et R dans les notebooks
- Ma√Ætriser les magic commands
- Cr√©er des visualisations de donn√©es interactives
- Collaborer efficacement avec les widgets et le partage

## 1. Introduction aux notebooks Databricks

Les notebooks Databricks sont des documents interactifs qui combinent code, visualisations et texte narratif. Ils supportent plusieurs langages dans un m√™me notebook.

### Cr√©ation d'un notebook

#### Cr√©er votre premier notebook

1. Dans la barre lat√©rale, cliquez sur `Workspace`
2. Naviguez vers votre dossier utilisateur
3. Clic droit ‚Üí `Create` ‚Üí `Notebook`
4. Nommez-le "Mon Premier Notebook"
5. Choisissez le langage par d√©faut : **Python**
6. S√©lectionnez un cluster (ou cr√©ez-en un)

### Structure d'un notebook

#### Cellules de code

Ex√©cutent du code Python, SQL, Scala ou R

#### Cellules Markdown

Documentation format√©e avec titres, listes, images

#### Visualisations

Graphiques int√©gr√©s g√©n√©r√©s √† partir des r√©sultats

#### Widgets

Param√®tres interactifs pour l'utilisateur

## 2. Langages support√©s

### Python (PySpark)

Le langage le plus populaire pour Data Science et Machine Learning sur Databricks.

```bash
# Exemple Python dans un notebook Databricks
# Cr√©er un DataFrame depuis une liste
data = [
    ("Alice", 34, "Data Engineer"),
    ("Bob", 45, "Data Scientist"),
    ("Charlie", 29, "ML Engineer")
]

df = spark.createDataFrame(data, ["name", "age", "role"])

# Afficher le contenu
display(df)

# Op√©rations sur le DataFrame
df_filtered = df.filter(df.age > 30)
display(df_filtered)

# Statistiques descriptives
df.describe().show()
```

### SQL

Pour les requ√™tes et analyses de donn√©es avec une syntaxe SQL famili√®re.

```bash
-- Utiliser SQL dans une cellule
-- Cr√©er une table temporaire
CREATE OR REPLACE TEMP VIEW employees AS
SELECT * FROM VALUES
  ('Alice', 34, 'Data Engineer', 95000),
  ('Bob', 45, 'Data Scientist', 105000),
  ('Charlie', 29, 'ML Engineer', 98000)
AS (name, age, role, salary);

-- Requ√™te d'analyse
SELECT
  role,
  AVG(salary) as avg_salary,
  COUNT(*) as count
FROM employees
GROUP BY role
ORDER BY avg_salary DESC;
```

### Scala

Langage natif de Spark, offrant les meilleures performances.

```bash
// Exemple Scala
val data = Seq(
  ("Alice", 34, "Data Engineer"),
  ("Bob", 45, "Data Scientist"),
  ("Charlie", 29, "ML Engineer")
)

val df = data.toDF("name", "age", "role")
display(df)

// Transformations typ√©es
case class Employee(name: String, age: Int, role: String)
val ds = df.as[Employee]
val seniors = ds.filter(_.age > 30)
display(seniors)
```

### R

Pour les statisticiens et analystes pr√©f√©rant R.

```bash
# Exemple R avec SparkR
library(SparkR)

df <- createDataFrame(data.frame(
  name = c("Alice", "Bob", "Charlie"),
  age = c(34, 45, 29),
  role = c("Data Engineer", "Data Scientist", "ML Engineer")
))

display(df)

# Utiliser ggplot2 pour visualisation
library(ggplot2)
local_df <- collect(df)
ggplot(local_df, aes(x=name, y=age)) +
  geom_bar(stat="identity", fill="steelblue")
```

## 3. Magic Commands

Les magic commands permettent de m√©langer plusieurs langages dans un m√™me notebook.

| Magic Command | Description | Exemple |
| --- | --- | --- |
| `%python` | Ex√©cuter du code Python | Notebook par d√©faut SQL |
| `%sql` | Ex√©cuter une requ√™te SQL | Requ√™tes dans notebook Python |
| `%scala` | Ex√©cuter du code Scala | Librairies Scala sp√©cifiques |
| `%r` | Ex√©cuter du code R | Analyses statistiques R |
| `%md` | Cellule Markdown | Documentation |
| `%sh` | Ex√©cuter des commandes shell | V√©rifier l'environnement |
| `%fs` | Commandes filesystem (DBFS) | Lister/copier des fichiers |
| `%run` | Ex√©cuter un autre notebook | Modularisation du code |

### Exemples d'utilisation

```bash
# Cellule 1 : Python par d√©faut
data = [("Paris", 2.2), ("Lyon", 0.5), ("Marseille", 0.9)]
cities_df = spark.createDataFrame(data, ["city", "population_millions"])
cities_df.createOrReplaceTempView("cities")
```

```bash
-- Cellule 2 : Utiliser SQL sur les donn√©es Python
%sql
SELECT
  city,
  population_millions,
  ROUND(population_millions * 1000000) as population
FROM cities
ORDER BY population_millions DESC
```

```bash
%md
### Cellule 3 : Documentation Markdown

Les **trois plus grandes villes** de France :
1. Paris
2. Marseille
3. Lyon

*Donn√©es de population en millions d'habitants*
```

```bash
%sh
# Cellule 4 : Commandes shell
echo "Python version:"
python --version
echo "Spark version:"
spark-submit --version | head -1
```

```bash
%fs
# Cellule 5 : Op√©rations filesystem
ls /databricks-datasets/
```

## 4. Visualisations de donn√©es

Databricks offre des visualisations int√©gr√©es puissantes avec la fonction `display()`.

### Visualisations automatiques

```bash
# Cr√©er des donn√©es de ventes
sales_data = [
    ("2024-01", "Produit A", 15000),
    ("2024-01", "Produit B", 12000),
    ("2024-02", "Produit A", 18000),
    ("2024-02", "Produit B", 14000),
    ("2024-03", "Produit A", 22000),
    ("2024-03", "Produit B", 16000)
]

sales_df = spark.createDataFrame(sales_data, ["month", "product", "revenue"])

# La fonction display() g√©n√®re automatiquement des visualisations
display(sales_df)
```

#### Types de graphiques disponibles

- **Bar Chart :** Comparaisons cat√©gorielles
- **Line Chart :** √âvolutions temporelles
- **Pie Chart :** Proportions
- **Scatter Plot :** Corr√©lations
- **Map :** Donn√©es g√©ospatiales
- **Box Plot :** Distributions statistiques

### Visualisations avec biblioth√®ques Python

```bash
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convertir en Pandas pour visualisation
pandas_df = sales_df.toPandas()

# Cr√©er un graphique avec matplotlib
plt.figure(figsize=(10, 6))
for product in pandas_df['product'].unique():
    data = pandas_df[pandas_df['product'] == product]
    plt.plot(data['month'], data['revenue'], marker='o', label=product)

plt.xlabel('Mois')
plt.ylabel('Revenu (‚Ç¨)')
plt.title('√âvolution des revenus par produit')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

# Afficher dans Databricks
display(plt.gcf())
```

```bash
# Visualisation Seaborn
fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(data=pandas_df, x='month', y='revenue', hue='product', ax=ax)
ax.set_title('Comparaison des revenus')
ax.set_xlabel('Mois')
ax.set_ylabel('Revenu (‚Ç¨)')
plt.xticks(rotation=45)
display(fig)
```

## 5. Widgets et param√©trage

Les widgets permettent de cr√©er des notebooks param√©trables et interactifs.

### Types de widgets

| Type | Fonction | Cas d'usage |
| --- | --- | --- |
| `text` | Champ de texte | Saisie de chemins, noms |
| `dropdown` | Liste d√©roulante | S√©lection parmi options |
| `combobox` | Combinaison dropdown + texte | Options + saisie libre |
| `multiselect` | S√©lection multiple | Filtres multiples |

### Cr√©ation et utilisation de widgets

```bash
# Cr√©er un widget dropdown pour s√©lectionner un pays
dbutils.widgets.dropdown("country", "France", ["France", "Allemagne", "Espagne", "Italie"])

# Cr√©er un widget texte pour la date
dbutils.widgets.text("start_date", "2024-01-01")

# Cr√©er un widget multiselect pour les cat√©gories
dbutils.widgets.multiselect("categories", "Electronics", ["Electronics", "Clothing", "Food", "Books"])

# R√©cup√©rer les valeurs des widgets
selected_country = dbutils.widgets.get("country")
start_date = dbutils.widgets.get("start_date")
selected_categories = dbutils.widgets.get("categories")

print(f"Analyse pour {selected_country} √† partir du {start_date}")
print(f"Cat√©gories : {selected_categories}")

# Utiliser dans une requ√™te
df = spark.sql(f"""
    SELECT * FROM sales
    WHERE country = '{selected_country}'
    AND date >= '{start_date}'
""")

display(df)

# Supprimer un widget
# dbutils.widgets.remove("country")

# Supprimer tous les widgets
# dbutils.widgets.removeAll()
```

#### Exercice pratique : Dashboard param√©tr√©

Cr√©ez un notebook qui :

1. Cr√©e un widget dropdown pour s√©lectionner un produit
2. Cr√©e un widget texte pour sp√©cifier un seuil de revenu
3. Filtre les donn√©es selon ces param√®tres
4. Affiche un graphique des r√©sultats

## 6. Collaboration et partage

### Fonctionnalit√©s collaboratives

#### √âdition multi-utilisateurs

Plusieurs personnes peuvent travailler simultan√©ment sur un notebook

#### Commentaires

Ajoutez des commentaires sur des cellules sp√©cifiques pour discussion

#### Contr√¥le de version

Historique des r√©visions int√©gr√© avec Git integration

#### Permissions

Contr√¥le d'acc√®s granulaire (lecture, √©dition, ex√©cution)

### Partage d'un notebook

#### Partager avec votre √©quipe

1. Ouvrez le notebook √† partager
2. Cliquez sur `Share` en haut √† droite
3. Ajoutez des utilisateurs ou groupes
4. D√©finissez les permissions :
   - **Can Read :** Lecture seule
   - **Can Run :** Lecture + ex√©cution
   - **Can Edit :** Lecture + √©dition + ex√©cution
5. Cliquez sur `Add`

### Int√©gration Git

```bash
# Configuration Git dans Databricks
# 1. Dans User Settings ‚Üí Git Integration
# 2. Connectez votre compte GitHub/GitLab/Azure DevOps

# Cloner un repository
# Workspace ‚Üí Add ‚Üí Repo ‚Üí Clone from Git
# URL: https://github.com/your-org/your-repo.git

# Les notebooks sont synchronis√©s avec le repo
# Commits et push depuis l'interface Databricks
```

### Export et partage de r√©sultats

```bash
# Exporter un notebook en diff√©rents formats
# File ‚Üí Export ‚Üí DBC Archive (Databricks format)
# File ‚Üí Export ‚Üí Source File (.py, .scala, .r, .sql)
# File ‚Üí Export ‚Üí HTML
# File ‚Üí Export ‚Üí Jupyter Notebook (.ipynb)

# Partager les r√©sultats via un dashboard
# Dans une cellule de visualisation :
# Cliquez sur "Add to Dashboard"
# Cr√©ez un nouveau dashboard ou ajoutez √† un existant
```

## 7. Bonnes pratiques

### üí° Recommandations

- **Organisation :** Structurez vos notebooks avec des sections Markdown claires
- **Nommage :** Utilisez des noms descriptifs (ex: "ETL\_Sales\_Daily" pas "Notebook1")
- **Modularit√© :** Utilisez %run pour r√©utiliser du code commun
- **Documentation :** Commentez votre code et utilisez des cellules Markdown
- **Performance :** √âvitez de charger trop de donn√©es avec display(), limitez avec .limit()
- **Widgets :** Rendez vos notebooks param√©trables pour la r√©utilisation
- **Version Control :** Int√©grez Git pour historique et collaboration
- **Nettoyage :** Supprimez les widgets avec removeAll() √† la fin si n√©cessaire

#### Erreurs courantes √† √©viter

- Oublier de d√©tacher le notebook avant de supprimer un cluster
- Utiliser `collect()` sur de tr√®s grandes DataFrames (risque OutOfMemory)
- Ne pas nettoyer les tables temporaires (cr√©ent du clutter)
- Hardcoder des chemins au lieu d'utiliser des widgets

### üìå Points cl√©s √† retenir

- Notebooks supportent Python, SQL, Scala et R dans un m√™me document
- Magic commands (%sql, %python, %md, %fs, %run) pour m√©langer les langages
- display() g√©n√®re des visualisations interactives automatiques
- Widgets cr√©ent des notebooks param√©trables et r√©utilisables
- Collaboration en temps r√©el avec commentaires et permissions
- Int√©gration Git pour version control professionnel

#### Prochaine √©tape

Vous ma√Ætrisez les notebooks ! Dans la **Partie 4**, plongez dans Apache Spark et le traitement distribu√© de donn√©es.