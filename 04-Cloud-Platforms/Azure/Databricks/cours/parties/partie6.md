## ğŸ¯ Objectifs d'apprentissage

- Comprendre Databricks Workflows (anciennement Jobs)
- CrÃ©er des Jobs avec plusieurs tÃ¢ches
- GÃ©rer les dÃ©pendances entre tÃ¢ches
- Planifier l'exÃ©cution automatique
- Monitorer et gÃ©rer les alertes
- IntÃ©grer avec Azure Data Factory

## 1. Introduction aux Databricks Workflows

Databricks Workflows est la plateforme d'orchestration native pour automatiser et orchestrer vos pipelines de donnÃ©es et ML.

### Composants principaux

#### Jobs

Un job est un workflow automatisÃ© composÃ© d'une ou plusieurs tÃ¢ches

#### Tasks

Une tÃ¢che est une unitÃ© d'exÃ©cution (notebook, script, JAR, etc.)

#### Triggers

DÃ©clenchement par planification (cron) ou Ã©vÃ©nements

#### Runs

Une exÃ©cution d'un job avec son historique et statut

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABRICKS WORKFLOW                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         Trigger (Schedule/Manual)           â”‚
â”‚  â”‚    Job     â”‚                    â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â–¼                         â”‚
â”‚        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚        â”‚              â”‚   Task 1: Ingestion  â”‚               â”‚
â”‚        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚        â”‚                         â”‚                           â”‚
â”‚        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚        â”‚          â–¼                           â–¼              â”‚
â”‚        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚        â”‚   â”‚Task 2: ETL  â”‚           â”‚Task 3: Checkâ”‚        â”‚
â”‚        â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚        â”‚          â”‚                          â”‚               â”‚
â”‚        â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚        â”‚                     â–¼                               â”‚
â”‚        â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚        â”‚            â”‚Task 4: Analyticsâ”‚                      â”‚
â”‚        â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚        â”‚                                                     â”‚
â”‚        â”‚  Results â†’ Notifications (Email, Slack, etc.)      â”‚
â”‚        â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. CrÃ©er un Job via l'interface

#### CrÃ©er votre premier Job

1. Dans la barre latÃ©rale, cliquez sur `Workflows`
2. Cliquez sur `Create Job`
3. Nommez le job : "ETL Pipeline Daily"
4. CrÃ©ez la premiÃ¨re tÃ¢che :
   - **Task name :** "ingest\_raw\_data"
   - **Type :** Notebook
   - **Source :** SÃ©lectionnez votre notebook d'ingestion
   - **Cluster :** SÃ©lectionnez un job cluster
   - **Parameters :** Ajoutez des paramÃ¨tres si nÃ©cessaire
5. Ajoutez d'autres tÃ¢ches avec `+ Add task`
6. DÃ©finissez les dÃ©pendances en liant les tÃ¢ches
7. Configurez le dÃ©clenchement (schedule)
8. Cliquez sur `Create`

### Types de tÃ¢ches

| Type | Description | Cas d'usage |
| --- | --- | --- |
| **Notebook** | ExÃ©cute un notebook Databricks | ETL, analyse, ML training |
| **Python script** | ExÃ©cute un fichier .py | Scripts standalone |
| **JAR** | Application Scala/Java | Jobs Spark Scala |
| **Python wheel** | Package Python (.whl) | Applications Python packagees |
| **SQL** | ExÃ©cute des requÃªtes SQL | Data transformations SQL |
| **dbt** | ExÃ©cute des projets dbt | Transformations avec dbt |
| **Delta Live Tables** | Pipeline DLT | Pipelines dÃ©claratifs |

## 3. CrÃ©er un Job via l'API

```bash
import requests
import json

DATABRICKS_HOST = "https://<workspace-url>"
DATABRICKS_TOKEN = "<your-token>"

# Configuration du job
job_config = {
    "name": "ETL Pipeline Daily",
    "email_notifications": {
        "on_failure": ["data-team@company.com"],
        "on_success": ["data-team@company.com"]
    },
    "timeout_seconds": 7200,  # 2 heures
    "max_concurrent_runs": 1,
    "tasks": [
        {
            "task_key": "ingest_raw_data",
            "description": "Ingestion des donnÃ©es brutes",
            "notebook_task": {
                "notebook_path": "/Workspace/ETL/01_Ingestion",
                "base_parameters": {
                    "date": "{{job.start_date}}",
                    "environment": "production"
                }
            },
            "new_cluster": {
                "spark_version": "13.3.x-scala2.12",
                "node_type_id": "Standard_DS3_v2",
                "num_workers": 2,
                "autoscale": {
                    "min_workers": 2,
                    "max_workers": 8
                }
            },
            "timeout_seconds": 3600,
            "max_retries": 2,
            "retry_on_timeout": True
        },
        {
            "task_key": "transform_data",
            "description": "Transformation des donnÃ©es",
            "depends_on": [
                {"task_key": "ingest_raw_data"}
            ],
            "notebook_task": {
                "notebook_path": "/Workspace/ETL/02_Transform",
                "base_parameters": {
                    "date": "{{job.start_date}}"
                }
            },
            "new_cluster": {
                "spark_version": "13.3.x-scala2.12",
                "node_type_id": "Standard_DS3_v2",
                "num_workers": 4
            }
        },
        {
            "task_key": "data_quality_checks",
            "description": "VÃ©rifications qualitÃ©",
            "depends_on": [
                {"task_key": "transform_data"}
            ],
            "python_wheel_task": {
                "package_name": "data_quality",
                "entry_point": "run_checks",
                "parameters": ["--date", "{{job.start_date}}"]
            },
            "libraries": [
                {"pypi": {"package": "great-expectations==0.18.0"}}
            ],
            "existing_cluster_id": "<cluster-id>"
        },
        {
            "task_key": "publish_analytics",
            "description": "Publication analytics",
            "depends_on": [
                {"task_key": "data_quality_checks"}
            ],
            "sql_task": {
                "query": {
                    "query_id": "<query-id>"
                },
                "warehouse_id": "<warehouse-id>"
            }
        }
    ],
    "schedule": {
        "quartz_cron_expression": "0 0 2 * * ?",  # 2h du matin
        "timezone_id": "Europe/Paris",
        "pause_status": "UNPAUSED"
    },
    "tags": {
        "environment": "production",
        "team": "data-engineering"
    }
}

# CrÃ©er le job
headers = {
    "Authorization": f"Bearer {DATABRICKS_TOKEN}",
    "Content-Type": "application/json"
}

response = requests.post(
    f"{DATABRICKS_HOST}/api/2.1/jobs/create",
    headers=headers,
    json=job_config
)

job_id = response.json()["job_id"]
print(f"Job crÃ©Ã© avec l'ID : {job_id}")

# DÃ©clencher une exÃ©cution immÃ©diate
run_response = requests.post(
    f"{DATABRICKS_HOST}/api/2.1/jobs/run-now",
    headers=headers,
    json={"job_id": job_id}
)

run_id = run_response.json()["run_id"]
print(f"Run dÃ©marrÃ© avec l'ID : {run_id}")
```

## 4. Gestion des dÃ©pendances

### Types de dÃ©pendances

#### Sequential (SÃ©quentielle)

TÃ¢che B attend que A se termine

```bash
A â†’ B
```

#### Parallel (ParallÃ¨le)

B et C s'exÃ©cutent en parallÃ¨le aprÃ¨s A

```bash
   â”Œâ†’ B
A â”€â”¤
   â””â†’ C
```

#### Fan-in (Convergence)

D attend B ET C

```bash
B â”€â”
   â”œâ†’ D
C â”€â”˜
```

#### Conditional

ExÃ©cution conditionnelle basÃ©e sur le rÃ©sultat

```bash
A â†’ IF success â†’ B
  â†’ IF failure â†’ C
```

### Exemple de workflow complexe

```bash
# Pipeline ETL complet avec branches conditionnelles
workflow_config = {
    "name": "Complex ETL Pipeline",
    "tasks": [
# TÃ¢che initiale
        {
            "task_key": "validate_source",
            "description": "Valider la source de donnÃ©es",
            "notebook_task": {
                "notebook_path": "/ETL/00_Validate"
            }
        },
# Branches parallÃ¨les pour diffÃ©rentes sources
        {
            "task_key": "ingest_crm",
            "depends_on": [{"task_key": "validate_source"}],
            "notebook_task": {
                "notebook_path": "/ETL/Ingest_CRM"
            }
        },
        {
            "task_key": "ingest_erp",
            "depends_on": [{"task_key": "validate_source"}],
            "notebook_task": {
                "notebook_path": "/ETL/Ingest_ERP"
            }
        },
        {
            "task_key": "ingest_web",
            "depends_on": [{"task_key": "validate_source"}],
            "notebook_task": {
                "notebook_path": "/ETL/Ingest_Web"
            }
        },
# Convergence : transformation aprÃ¨s toutes les ingestions
        {
            "task_key": "transform_join",
            "depends_on": [
                {"task_key": "ingest_crm"},
                {"task_key": "ingest_erp"},
                {"task_key": "ingest_web"}
            ],
            "notebook_task": {
                "notebook_path": "/ETL/Transform_Join"
            }
        },
# Branches parallÃ¨les analytics
        {
            "task_key": "analytics_sales",
            "depends_on": [{"task_key": "transform_join"}],
            "notebook_task": {
                "notebook_path": "/Analytics/Sales"
            }
        },
        {
            "task_key": "analytics_customer",
            "depends_on": [{"task_key": "transform_join"}],
            "notebook_task": {
                "notebook_path": "/Analytics/Customer"
            }
        },
# Notification finale
        {
            "task_key": "send_report",
            "depends_on": [
                {"task_key": "analytics_sales"},
                {"task_key": "analytics_customer"}
            ],
            "python_wheel_task": {
                "package_name": "reporting",
                "entry_point": "send_daily_report"
            }
        }
    ]
}
```

## 5. Planification et dÃ©clenchement

### Expressions Cron

| Expression | Description | Cas d'usage |
| --- | --- | --- |
| `0 0 2 * * ?` | Tous les jours Ã  2h du matin | ETL quotidien |
| `0 */4 * * * ?` | Toutes les 4 heures | Synchronisation rÃ©guliÃ¨re |
| `0 0 0 * * MON` | Tous les lundis Ã  minuit | Rapport hebdomadaire |
| `0 0 9 1 * ?` | Le 1er de chaque mois Ã  9h | Rapport mensuel |
| `0 30 8-17 * * MON-FRI` | Toutes les heures de 8h30 Ã  17h30 en semaine | Monitoring business hours |

### DÃ©clenchement par Ã©vÃ©nement

```bash
# DÃ©clencher un job lorsqu'un fichier arrive
# Via Azure Event Grid + Logic App + Databricks API

# 1. Event Grid dÃ©tecte nouveau fichier dans Blob Storage
# 2. Logic App reÃ§oit l'Ã©vÃ©nement
# 3. Logic App appelle l'API Databricks

# Exemple de dÃ©clenchement programmatique
import requests

def trigger_job_on_file_arrival(file_path):
    job_id = "12345"

    response = requests.post(
        f"{DATABRICKS_HOST}/api/2.1/jobs/run-now",
        headers=headers,
        json={
            "job_id": job_id,
            "notebook_params": {
                "file_path": file_path,
                "triggered_by": "event"
            }
        }
    )

    return response.json()["run_id"]
```

## 6. Monitoring et alertes

### Types de notifications

```bash
# Configuration des notifications
notification_config = {
    "email_notifications": {
        "on_start": ["team@company.com"],
        "on_success": ["team@company.com"],
        "on_failure": ["team@company.com", "oncall@company.com"],
        "on_duration_warning_threshold_exceeded": ["team@company.com"]
    },
    "webhook_notifications": {
        "on_failure": [{
            "id": "slack-webhook-id"
        }]
    }
}
```

### Monitoring des runs

```bash
# Obtenir le statut d'un run
run_status = requests.get(
    f"{DATABRICKS_HOST}/api/2.1/jobs/runs/get",
    headers=headers,
    params={"run_id": run_id}
).json()

print(f"State: {run_status['state']['life_cycle_state']}")
print(f"Result: {run_status['state'].get('result_state', 'N/A')}")

# Lister tous les runs d'un job
runs_list = requests.get(
    f"{DATABRICKS_HOST}/api/2.1/jobs/runs/list",
    headers=headers,
    params={"job_id": job_id, "limit": 25}
).json()

for run in runs_list.get("runs", []):
    print(f"Run {run['run_id']}: {run['state']['result_state']}")

# Annuler un run en cours
requests.post(
    f"{DATABRICKS_HOST}/api/2.1/jobs/runs/cancel",
    headers=headers,
    json={"run_id": run_id}
)
```

### MÃ©triques et dashboards

#### MÃ©triques clÃ©s Ã  surveiller

- **Success Rate :** Taux de rÃ©ussite des runs
- **Duration :** Temps d'exÃ©cution (dÃ©tection de dÃ©gradation)
- **Cost :** CoÃ»t DBU par job
- **Failures :** Nombre et type d'Ã©checs
- **SLA Compliance :** Respect des SLA de temps

## 7. IntÃ©gration Azure Data Factory

Vous pouvez orchestrer Databricks depuis Azure Data Factory pour une intÃ©gration complÃ¨te avec l'Ã©cosystÃ¨me Azure.

```bash
{
  "name": "DatabricksPipeline",
  "properties": {
    "activities": [
      {
        "name": "RunDatabricksNotebook",
        "type": "DatabricksNotebook",
        "linkedServiceName": {
          "referenceName": "DatabricksLinkedService",
          "type": "LinkedServiceReference"
        },
        "typeProperties": {
          "notebookPath": "/Workspace/ETL/Transform",
          "baseParameters": {
            "date": "@formatDateTime(pipeline().TriggerTime, 'yyyy-MM-dd')",
            "environment": "production"
          }
        }
      },
      {
        "name": "RunSparkJob",
        "type": "DatabricksSparkJar",
        "dependsOn": [
          {
            "activity": "RunDatabricksNotebook",
            "dependencyConditions": ["Succeeded"]
          }
        ],
        "typeProperties": {
          "mainClassName": "com.company.etl.MainApp",
          "parameters": ["--date", "@formatDateTime(pipeline().TriggerTime, 'yyyy-MM-dd')"]
        }
      }
    ]
  }
}
```

### Avantages ADF vs Databricks Workflows

| CritÃ¨re | Databricks Workflows | Azure Data Factory |
| --- | --- | --- |
| **SimplicitÃ©** | âœ… Natif, intÃ©grÃ© | Interface visuelle ADF |
| **IntÃ©gration Azure** | Via API/connectors | âœ… Natif (Blob, SQL, etc.) |
| **CoÃ»t** | Inclus dans Databricks | Facturation ADF sÃ©parÃ©e |
| **Monitoring** | Databricks UI | âœ… Azure Monitor intÃ©grÃ© |
| **Orchestration hybride** | Databricks uniquement | âœ… Multi-services Azure |

### ğŸ“Œ Points clÃ©s Ã  retenir

- Workflows orchestre vos pipelines avec tÃ¢ches et dÃ©pendances
- SupportÃ© plusieurs types de tÃ¢ches : notebooks, scripts, SQL, dbt, DLT
- Planification flexible avec expressions Cron
- Gestion avancÃ©e des dÃ©pendances (sÃ©quentiel, parallÃ¨le, conditionnel)
- Monitoring complet avec notifications multi-canaux
- IntÃ©gration Azure Data Factory pour orchestration hybride
- API complÃ¨te pour automatisation et CI/CD

#### Prochaine Ã©tape

Vos workflows sont automatisÃ©s ! Dans la **Partie 7**, dÃ©couvrez le Machine Learning avec MLflow.