## ğŸ¯ Objectifs d'Apprentissage

- Comprendre l'architecture de YARN
- Distinguer ResourceManager et NodeManager
- MaÃ®triser les concepts d'ApplicationMaster et Container
- DÃ©couvrir les schedulers YARN

## ğŸ“š 1. Qu'est-ce que YARN ?

**YARN** (Yet Another Resource Negotiator) est le gestionnaire de ressources de Hadoop 2.x et versions supÃ©rieures.
Il sÃ©pare la gestion des ressources du traitement des donnÃ©es.

### Pourquoi YARN ?

#### ProblÃ¨me dans Hadoop 1.x

Dans Hadoop 1.x, le **JobTracker** gÃ©rait Ã  la fois :

- L'allocation des ressources du cluster
- La planification et le monitoring des jobs MapReduce

**Limites :** Goulot d'Ã©tranglement (scalabilitÃ© limitÃ©e Ã  ~4000 nÅ“uds),
support uniquement de MapReduce, pas d'autres frameworks.

#### Solution : YARN (Hadoop 2.x)

YARN sÃ©pare les responsabilitÃ©s :

- **Gestion des ressources** : ResourceManager + NodeManagers
- **Gestion des applications** : ApplicationMaster par application

**Avantages :** ScalabilitÃ© > 10 000 nÅ“uds, support multi-framework
(MapReduce, Spark, Tez, Storm, etc.)

### Hadoop 1.x vs Hadoop 2.x

| Aspect | Hadoop 1.x | Hadoop 2.x (YARN) |
| --- | --- | --- |
| Gestion de ressources | JobTracker | ResourceManager |
| ExÃ©cution locale | TaskTracker | NodeManager |
| Frameworks supportÃ©s | MapReduce uniquement | MapReduce, Spark, Tez, etc. |
| ScalabilitÃ© | ~4 000 nÅ“uds | >10 000 nÅ“uds |
| Utilisation des ressources | Slots fixes (map/reduce) | Conteneurs flexibles |

## ğŸ—ï¸ 2. Architecture de YARN

### Composants Principaux

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT                               â”‚
â”‚                   (Soumet l'application)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RESOURCE MANAGER                          â”‚
â”‚                      (Master Global)                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Scheduler   â”‚              â”‚ ApplicationsManager â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                          â”‚
         â†“                                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NODE MANAGER 1    â”‚                  â”‚  NODE MANAGER N      â”‚
â”‚                    â”‚                  â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Container 1  â”‚  â”‚                  â”‚  â”‚ Container 1  â”‚    â”‚
â”‚  â”‚ AppMaster    â”‚  â”‚                  â”‚  â”‚ Task         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Container 2  â”‚  â”‚                  â”‚  â”‚ Container 2  â”‚    â”‚
â”‚  â”‚ Task         â”‚  â”‚                  â”‚  â”‚ Task         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ResourceManager (RM)

Le **ResourceManager** est le maÃ®tre global qui gÃ¨re toutes les ressources du cluster.

#### ResponsabilitÃ©s :

- Allouer les ressources aux applications
- Maintenir un inventaire des ressources disponibles
- Planifier les applications (via le Scheduler)
- GÃ©rer le cycle de vie des applications

#### Sous-composants :

- **Scheduler** : Alloue les ressources (CPU, mÃ©moire) aux applications
- **ApplicationsManager** : Accepte les soumissions de jobs, nÃ©gocie le premier conteneur pour l'ApplicationMaster

### NodeManager (NM)

Le **NodeManager** est l'agent qui s'exÃ©cute sur chaque nÅ“ud worker du cluster.

#### ResponsabilitÃ©s :

- GÃ©rer les conteneurs sur son nÅ“ud
- Monitorer l'utilisation des ressources (CPU, mÃ©moire, disque, rÃ©seau)
- Envoyer des heartbeats au ResourceManager
- Reporter l'Ã©tat des conteneurs
- GÃ©rer les logs des applications

### ApplicationMaster (AM)

L'**ApplicationMaster** est un processus spÃ©cifique Ã  chaque application.

#### ResponsabilitÃ©s :

- NÃ©gocier les ressources avec le ResourceManager
- Travailler avec les NodeManagers pour exÃ©cuter et monitorer les tÃ¢ches
- GÃ©rer le cycle de vie de l'application
- GÃ©rer les Ã©checs de tÃ¢ches et relancer si nÃ©cessaire

*Note : Chaque application (job MapReduce, application Spark, etc.) a son propre ApplicationMaster.*

### Container

Un **Container** est une unitÃ© d'allocation de ressources.

#### CaractÃ©ristiques :

- Encapsule des ressources : CPU (vcores) et mÃ©moire (RAM)
- S'exÃ©cute sur un NodeManager
- Peut contenir un ApplicationMaster ou une tÃ¢che (Map, Reduce, Spark executor, etc.)

**Exemple :** Container avec 2 GB RAM et 1 vcore

## ğŸ”„ 3. Cycle de Vie d'une Application YARN

```bash
1. Client soumet une application au ResourceManager
   â†“
2. ResourceManager alloue un conteneur pour l'ApplicationMaster
   â†“
3. NodeManager lance l'ApplicationMaster dans ce conteneur
   â†“
4. ApplicationMaster s'enregistre auprÃ¨s du ResourceManager
   â†“
5. ApplicationMaster demande des conteneurs pour les tÃ¢ches
   â†“
6. ResourceManager (Scheduler) alloue les conteneurs
   â†“
7. ApplicationMaster contacte les NodeManagers pour lancer les conteneurs
   â†“
8. NodeManagers lancent les conteneurs et exÃ©cutent les tÃ¢ches
   â†“
9. TÃ¢ches reportent leur statut Ã  l'ApplicationMaster
   â†“
10. ApplicationMaster reporte le progrÃ¨s au ResourceManager
   â†“
11. Une fois terminÃ©, ApplicationMaster se dÃ©senregistre
   â†“
12. NodeManagers nettoient les conteneurs
```

#### Exemple Concret : Job MapReduce

1. Client soumet un job MapReduce
2. RM alloue un conteneur pour le MR ApplicationMaster
3. MR AppMaster dÃ©marre et calcule les splits d'entrÃ©e
4. MR AppMaster demande des conteneurs pour les mappers et reducers
5. RM alloue les conteneurs demandÃ©s
6. NMs lancent les tÃ¢ches Map et Reduce
7. MR AppMaster monitore le progrÃ¨s
8. Ã€ la fin, MR AppMaster se dÃ©senregistre

## ğŸ“… 4. Schedulers YARN

Le **Scheduler** dÃ©termine quelle application reÃ§oit des ressources et quand.

### Types de Schedulers

#### 1. FIFO Scheduler

**First In, First Out** - Le plus simple.

- Les applications sont servies dans l'ordre de soumission
- Une application monopolise toutes les ressources jusqu'Ã  sa fin

**Avantage :** Simple

**InconvÃ©nient :** Pas de multitÃ¢che, petites applications attendent longtemps

*Rarement utilisÃ© en production.*

#### 2. Capacity Scheduler

Divise le cluster en **queues** (files d'attente) avec des capacitÃ©s garanties.

- Chaque queue a un pourcentage minimum de ressources
- Les ressources inutilisÃ©es peuvent Ãªtre partagÃ©es (Ã©lasticitÃ©)
- HiÃ©rarchie de queues possible
- ACLs (contrÃ´les d'accÃ¨s) par queue

**Exemple :**

- Queue "production" : 70% des ressources
- Queue "dev" : 20%
- Queue "test" : 10%

*Scheduler par dÃ©faut dans la plupart des distributions Hadoop.*

#### 3. Fair Scheduler

Partage Ã©quitablement les ressources entre toutes les applications actives.

- Chaque application reÃ§oit environ la mÃªme quantitÃ© de ressources
- Support des pools (similaire aux queues)
- PossibilitÃ© de dÃ©finir des poids et des prioritÃ©s
- PrÃ©emption : peut tuer des conteneurs pour Ã©quilibrer

**Avantage :** Ã‰quitÃ© entre utilisateurs et applications

*UtilisÃ© par dÃ©faut dans Cloudera (CDH).*

### Comparaison des Schedulers

| CaractÃ©ristique | FIFO | Capacity | Fair |
| --- | --- | --- | --- |
| MultitÃ¢che | âŒ | âœ… | âœ… |
| Partage de ressources | âŒ | Par queue | Ã‰quitable |
| Ã‰lasticitÃ© | âŒ | âœ… | âœ… |
| PrÃ©emption | âŒ | Optionnel | âœ… |
| ComplexitÃ© configuration | Faible | Moyenne | Moyenne |
| Cas d'usage | Test/Dev | Multi-tenant avec SLA | Partage Ã©quitable |

## ğŸ–¥ï¸ 5. Monitoring YARN

### YARN Web UI

Interface web pour monitorer le cluster YARN.

```bash
# AccÃ©der Ã  l'interface Web du ResourceManager
http://<resourcemanager-host>:8088
```

#### Informations Disponibles :

#### ğŸ“Š Cluster Metrics

- MÃ©moire totale/utilisÃ©e/disponible
- VCores totaux/utilisÃ©s/disponibles
- Nombre de NodeManagers actifs

#### ğŸ“± Applications

- Applications en cours, terminÃ©es, Ã©chouÃ©es
- ProgrÃ¨s de chaque application
- Logs et diagnostics

#### ğŸ“‚ Queues

- Utilisation par queue
- Applications en attente par queue
- CapacitÃ© utilisÃ©e vs disponible

#### ğŸ–¥ï¸ Nodes

- Ã‰tat de chaque NodeManager
- Ressources utilisÃ©es par nÅ“ud
- Conteneurs actifs par nÅ“ud

### Commandes CLI

| Commande | Description |
| --- | --- |
| `yarn node -list` | Lister tous les NodeManagers |
| `yarn application -list` | Lister les applications en cours |
| `yarn application -status <app-id>` | Voir le statut d'une application |
| `yarn application -kill <app-id>` | Tuer une application |
| `yarn logs -applicationId <app-id>` | Voir les logs d'une application |
| `yarn queue -status <queue-name>` | Voir le statut d'une queue |

## âš™ï¸ 6. Configuration YARN

Fichier principal : `yarn-site.xml`

### ParamÃ¨tres Importants

```bash
<configuration>
    <!-- Adresse du ResourceManager -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master.example.com</value>
    </property>

    <!-- MÃ©moire totale par NodeManager -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>8192</value>
    </property>

    <!-- VCores totaux par NodeManager -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>4</value>
    </property>

    <!-- Scheduler Ã  utiliser -->
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>

    <!-- MÃ©moire minimum par conteneur -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>

    <!-- MÃ©moire maximum par conteneur -->
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>8192</value>
    </property>
</configuration>
```

## ğŸ“ RÃ©sumÃ© de la Partie 4

### Points ClÃ©s Ã  Retenir

- YARN sÃ©pare la gestion des ressources du traitement des donnÃ©es
- Architecture : ResourceManager (maÃ®tre) + NodeManagers (workers)
- Chaque application a son propre ApplicationMaster
- Les Containers sont les unitÃ©s d'allocation de ressources
- 3 schedulers : FIFO (simple), Capacity (queues), Fair (Ã©quitable)
- YARN permet de faire cohabiter plusieurs frameworks (MapReduce, Spark, etc.)
- Monitoring via Web UI (port 8088) et commandes yarn CLI

#### âœ… PrÃªt pour la Suite ?

Vous maÃ®trisez maintenant YARN ! Dans la partie suivante, nous explorerons **l'Ã©cosystÃ¨me Hadoop** avec des outils comme Hive, Pig, HBase et plus encore.