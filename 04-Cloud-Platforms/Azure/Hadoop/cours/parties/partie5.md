## üéØ Objectifs d'Apprentissage

- D√©couvrir Apache Hive pour requ√™ter avec SQL
- Utiliser Apache Pig pour le scripting de donn√©es
- Comprendre HBase comme base NoSQL
- Ma√Ætriser Sqoop pour l'import/export de donn√©es
- Explorer Flume pour la collecte de logs

## üó∫Ô∏è 1. Vue d'Ensemble de l'√âcosyst√®me

```bash
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  √âCOSYST√àME HADOOP                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  INGESTION          STOCKAGE        TRAITEMENT    ANALYSE  ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
‚îÇ  Flume              HDFS            MapReduce     Hive     ‚îÇ
‚îÇ  Sqoop              HBase           Spark         Pig      ‚îÇ
‚îÇ  Kafka                              Tez           Impala   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ORCHESTRATION      COORDINATION    MONITORING             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             ‚îÇ
‚îÇ  Oozie              ZooKeeper       Ambari                 ‚îÇ
‚îÇ                                     Ganglia                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

| Outil | Cat√©gorie | Description |
| --- | --- | --- |
| **Hive** | SQL/Analyse | Entrep√¥t de donn√©es avec interface SQL (HiveQL) |
| **Pig** | Scripting | Langage de haut niveau pour flux de donn√©es |
| **HBase** | Base NoSQL | Base de donn√©es distribu√©e orient√©e colonnes |
| **Sqoop** | Import/Export | Transfert entre Hadoop et SGBD relationnels |
| **Flume** | Ingestion | Collecte et agr√©gation de logs streaming |
| **Oozie** | Orchestration | Planificateur de workflows Hadoop |
| **ZooKeeper** | Coordination | Service de coordination distribu√©e |
| **Spark** | Traitement | Moteur de traitement in-memory rapide |

## üêù 2. Apache Hive : SQL sur Hadoop

### Qu'est-ce que Hive ?

**Apache Hive** est un entrep√¥t de donn√©es construit sur Hadoop qui permet d'interroger
et d'analyser de grandes quantit√©s de donn√©es avec **HiveQL**, un langage similaire √† SQL.

#### ‚úÖ Avantages

- Syntaxe SQL famili√®re
- Pas besoin d'√©crire du MapReduce
- Support de gros volumes
- Optimiseur de requ√™tes

#### ‚ùå Inconv√©nients

- Latence √©lev√©e (batch, pas temps r√©el)
- Pas de modification en place (INSERT only)
- Pas id√©al pour les petites requ√™tes

### Architecture Hive

```bash
Client (Hive CLI / Beeline / JDBC)
         ‚Üì
    Hive Server 2
         ‚Üì
    Metastore (MySQL/PostgreSQL) ‚Üê Stocke les sch√©mas
         ‚Üì
    Driver (Query Compiler, Optimizer, Executor)
         ‚Üì
    Execution Engine (MapReduce / Tez / Spark)
         ‚Üì
    HDFS (Donn√©es)
```

### Exemple de Requ√™tes HiveQL

#### Cr√©er une Table

```bash
CREATE TABLE IF NOT EXISTS employees (
    id INT,
    name STRING,
    department STRING,
    salary DOUBLE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
```

#### Charger des Donn√©es

```bash
-- Charger depuis un fichier local
LOAD DATA LOCAL INPATH '/tmp/employees.csv' INTO TABLE employees;

-- Charger depuis HDFS
LOAD DATA INPATH '/user/data/employees.csv' INTO TABLE employees;
```

#### Requ√™tes SELECT

```bash
-- S√©lection simple
SELECT * FROM employees WHERE salary > 50000;

-- Agr√©gation
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department
HAVING avg_salary > 60000;

-- Jointure
SELECT e.name, d.department_name
FROM employees e
JOIN departments d ON e.department = d.dept_id;
```

#### Partitionnement

```bash
-- Table partitionn√©e par ann√©e
CREATE TABLE logs (
    timestamp STRING,
    level STRING,
    message STRING
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET;

-- Ins√©rer dans une partition
INSERT INTO TABLE logs PARTITION (year=2025, month=1)
SELECT timestamp, level, message FROM raw_logs
WHERE year(timestamp) = 2025 AND month(timestamp) = 1;
```

#### Bonnes Pratiques Hive

- Utiliser le partitionnement pour les grandes tables
- Pr√©f√©rer les formats colonnaires (Parquet, ORC) pour les performances
- Utiliser Tez ou Spark au lieu de MapReduce comme moteur
- Analyser les tables pour optimiser les requ√™tes (`ANALYZE TABLE`)

## üê∑ 3. Apache Pig : Scripting de Donn√©es

### Qu'est-ce que Pig ?

**Apache Pig** est une plateforme pour analyser de grandes donn√©es avec
**Pig Latin**, un langage de haut niveau orient√© flux de donn√©es.

#### Quand utiliser Pig plut√¥t que Hive ?

- **Pig :** Transformations complexes, ETL, traitement semi-structur√©
- **Hive :** Requ√™tes SQL classiques, reporting, analyses SQL

### Exemple de Script Pig Latin

#### WordCount en Pig

```bash
-- Charger les donn√©es
lines = LOAD '/user/data/input.txt' AS (line:chararray);

-- D√©couper en mots
words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;

-- Grouper par mot
grouped = GROUP words BY word;

-- Compter
word_counts = FOREACH grouped GENERATE group AS word, COUNT(words) AS count;

-- Trier par compte d√©croissant
sorted = ORDER word_counts BY count DESC;

-- Stocker le r√©sultat
STORE sorted INTO '/user/data/output' USING PigStorage(',');
```

#### Filtrage et Transformation

```bash
-- Charger les logs
logs = LOAD '/logs/access.log' USING PigStorage(' ')
       AS (ip:chararray, timestamp:chararray, method:chararray,
           url:chararray, status:int, bytes:int);

-- Filtrer les erreurs 404
errors = FILTER logs BY status == 404;

-- Extraire l'IP et l'URL
result = FOREACH errors GENERATE ip, url;

-- Grouper par URL
grouped = GROUP result BY url;

-- Compter les erreurs par URL
error_counts = FOREACH grouped GENERATE group AS url, COUNT(result) AS count;

-- Stocker
STORE error_counts INTO '/output/404_errors';
```

#### Jointure en Pig

```bash
-- Charger deux datasets
users = LOAD '/data/users' AS (user_id:int, name:chararray);
orders = LOAD '/data/orders' AS (order_id:int, user_id:int, amount:double);

-- Jointure
joined = JOIN users BY user_id, orders BY user_id;

-- Stocker
STORE joined INTO '/output/user_orders';
```

### Ex√©cuter un Script Pig

```bash
# Mode local (pour tester)
pig -x local script.pig

# Mode MapReduce
pig -x mapreduce script.pig

# Mode interactif (Grunt shell)
pig
```

## üóÑÔ∏è 4. Apache HBase : Base de Donn√©es NoSQL

### Qu'est-ce que HBase ?

**Apache HBase** est une base de donn√©es NoSQL distribu√©e, orient√©e colonnes,
construite sur HDFS. Inspir√©e de Google BigTable.

### Caract√©ristiques

#### üìà Scalabilit√©

Supporte des milliards de lignes et millions de colonnes

#### ‚ö° Acc√®s Rapide

Lecture/√©criture en temps r√©el (millisecondes)

#### üîë Cl√©-Valeur

Acc√®s par cl√© primaire (row key)

#### üìä Orient√© Colonnes

Stockage en familles de colonnes

### Mod√®le de Donn√©es

```bash
Table
  ‚îú‚îÄ Row Key (cl√© unique)
  ‚îî‚îÄ Column Families
       ‚îú‚îÄ Family 1
       ‚îÇ    ‚îú‚îÄ Column 1:1 (avec timestamp)
       ‚îÇ    ‚îî‚îÄ Column 1:2
       ‚îî‚îÄ Family 2
            ‚îî‚îÄ Column 2:1
```

### Commandes HBase Shell

#### Cr√©er et G√©rer des Tables

```bash
# Lancer HBase shell
hbase shell

# Cr√©er une table avec deux familles de colonnes
create 'users', 'profile', 'contacts'

# Lister les tables
list

# D√©crire une table
describe 'users'

# D√©sactiver et supprimer une table
disable 'users'
drop 'users'
```

#### Ins√©rer et Lire des Donn√©es

```bash
# Ins√©rer des donn√©es
put 'users', 'user1', 'profile:name', 'Alice'
put 'users', 'user1', 'profile:age', '30'
put 'users', 'user1', 'contacts:email', 'alice@example.com'

# Lire une ligne
get 'users', 'user1'

# Lire une colonne sp√©cifique
get 'users', 'user1', 'profile:name'

# Scanner toute la table
scan 'users'

# Scanner avec filtre
scan 'users', {FILTER => "ValueFilter(=, 'binary:Alice')"}

# Supprimer une cellule
delete 'users', 'user1', 'profile:age'
```

### Cas d'Usage HBase

| Domaine | Cas d'Usage |
| --- | --- |
| R√©seaux Sociaux | Profils utilisateurs, fils d'actualit√©, messages |
| IoT | Stockage de s√©ries temporelles de capteurs |
| Finance | Historique de transactions en temps r√©el |
| E-commerce | Catalogue produits, historique des commandes |

## üîÑ 5. Apache Sqoop : Import/Export de Donn√©es

### Qu'est-ce que Sqoop ?

**Apache Sqoop** (SQL to Hadoop) est un outil pour transf√©rer des donn√©es
entre Hadoop et des bases de donn√©es relationnelles (MySQL, PostgreSQL, Oracle, etc.).

### Commandes Principales

#### Import depuis SGBD vers HDFS

```bash
# Importer une table enti√®re
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --target-dir /user/hadoop/employees

# Importer avec une requ√™te WHERE
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --where "department = 'IT'" \
  --target-dir /user/hadoop/it_employees

# Importer toutes les tables d'une base
sqoop import-all-tables \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --warehouse-dir /user/hadoop/warehouse
```

#### Export depuis HDFS vers SGBD

```bash
# Exporter vers une table MySQL
sqoop export \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees_export \
  --export-dir /user/hadoop/employees_processed
```

#### Import Incr√©mental

```bash
# Import incr√©mental bas√© sur une colonne d'ID
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table orders \
  --incremental append \
  --check-column order_id \
  --last-value 1000 \
  --target-dir /user/hadoop/orders
```

#### Import Direct vers Hive

```bash
sqoop import \
  --connect jdbc:mysql://localhost/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --hive-import \
  --hive-table employees \
  --create-hive-table
```

## üì° 6. Apache Flume : Collecte de Logs

### Qu'est-ce que Flume ?

**Apache Flume** est un service distribu√© pour collecter, agr√©ger et d√©placer
efficacement de grandes quantit√©s de donn√©es de logs vers HDFS.

### Architecture Flume

```bash
Source ‚Üí Channel ‚Üí Sink

Exemples:
- Source : Netcat, Exec, Avro, Kafka
- Channel : Memory, File, JDBC
- Sink : HDFS, HBase, Logger, Avro
```

### Exemple de Configuration

```bash
# flume-conf.properties

# D√©finir les composants
agent1.sources = source1
agent1.channels = channel1
agent1.sinks = sink1

# Configurer la source (√©coute sur un port)
agent1.sources.source1.type = netcat
agent1.sources.source1.bind = localhost
agent1.sources.source1.port = 44444

# Configurer le channel (m√©moire)
agent1.channels.channel1.type = memory
agent1.channels.channel1.capacity = 1000
agent1.channels.channel1.transactionCapacity = 100

# Configurer le sink (HDFS)
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path = /user/flume/events/%Y-%m-%d
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.rollInterval = 60
agent1.sinks.sink1.hdfs.rollSize = 0
agent1.sinks.sink1.hdfs.rollCount = 0

# Relier les composants
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1
```

#### Lancer Flume

```bash
flume-ng agent \
  --conf-file flume-conf.properties \
  --name agent1 \
  -Dflume.root.logger=INFO,console
```

## üìù R√©sum√© de la Partie 5

### Points Cl√©s √† Retenir

- **Hive** : SQL sur Hadoop, id√©al pour analyses et reporting
- **Pig** : Scripting pour ETL et transformations complexes
- **HBase** : Base NoSQL temps r√©el sur HDFS
- **Sqoop** : Import/Export entre Hadoop et SGBD relationnels
- **Flume** : Ingestion de logs en streaming vers HDFS
- L'√©cosyst√®me Hadoop est riche et chaque outil a son cas d'usage sp√©cifique
- Ces outils peuvent √™tre combin√©s pour cr√©er des pipelines Big Data complets

#### ‚úÖ Pr√™t pour la Suite ?

Vous connaissez maintenant l'√©cosyst√®me Hadoop ! Dans la derni√®re partie, nous verrons comment **installer et configurer** un cluster Hadoop.