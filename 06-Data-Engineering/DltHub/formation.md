## Objectifs de la formation

- Comprendre DltHub et ses avantages
- CrÃ©er des pipelines de donnÃ©es simples
- Utiliser les sources et destinations
- ImplÃ©menter des transformations
- GÃ©rer le schema evolution
- DÃ©ployer en production

## 1. Introduction Ã  DltHub

### Qu'est-ce que DltHub ?

**DltHub (Data Load Tool Hub)** est un framework Python open-source qui simplifie la crÃ©ation
de pipelines de donnÃ©es (ELT). Il permet d'extraire des donnÃ©es de diverses sources et de les charger dans
des destinations (data warehouses, data lakes) avec un minimum de code.

#### Philosophie de DltHub

DltHub suit une approche **ELT** (Extract, Load, Transform) plutÃ´t que ETL.
Les transformations lourdes sont effectuÃ©es dans la destination (warehouse) plutÃ´t que pendant le transit.

### Pourquoi DltHub ?

#### ğŸš€ SimplicitÃ©

CrÃ©ez des pipelines en quelques lignes de Python, sans infrastructure complexe

#### ğŸ”„ Schema Evolution

Gestion automatique des changements de schÃ©ma, pas de migrations manuelles

#### ğŸ¯ Python-First

100% Python, intÃ©gration naturelle avec pandas, requests, etc.

#### ğŸ”Œ Connecteurs

Nombreux connecteurs vers BigQuery, Snowflake, PostgreSQL, DuckDB...

#### ğŸ“Š Data Quality

Validation automatique, dÃ©tection d'anomalies, data contracts

#### âš¡ IncrÃ©mental

Support natif du loading incrÃ©mental pour Ã©conomiser ressources et temps

### DltHub vs Alternatives

| Aspect | DltHub | Airbyte | Fivetran |
| --- | --- | --- | --- |
| **Type** | Librairie Python | Platform (UI + API) | SaaS |
| **Code** | Python natif | Configuration YAML | No-code |
| **Hosting** | Votre infra | Self-hosted ou Cloud | Cloud seulement |
| **ComplexitÃ©** | Simple (Python) | Moyenne (Docker) | Simple (UI) |
| **CoÃ»t** | Gratuit (open source) | Gratuit + Paid | Payant |
| **FlexibilitÃ©** | TrÃ¨s haute (code) | Moyenne | LimitÃ©e |

### Concepts clÃ©s

| Concept | Description |
| --- | --- |
| **Source** | Origine des donnÃ©es (API, base de donnÃ©es, fichiers) |
| **Resource** | UnitÃ© de donnÃ©es Ã  charger (table, endpoint API) |
| **Destination** | OÃ¹ les donnÃ©es sont chargÃ©es (BigQuery, PostgreSQL...) |
| **Pipeline** | Configuration du flux source â†’ destination |
| **Schema** | Structure des donnÃ©es, Ã©volutif automatiquement |
| **State** | Sauvegarde de l'Ã©tat pour loading incrÃ©mental |

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SOURCE    â”‚       â”‚   DLTHUB     â”‚       â”‚ DESTINATION â”‚
â”‚             â”‚       â”‚   PIPELINE   â”‚       â”‚             â”‚
â”‚  API/DB/    â”‚  â”€â”€â”€â†’ â”‚              â”‚  â”€â”€â”€â†’ â”‚  BigQuery   â”‚
â”‚  Files      â”‚       â”‚  Extract     â”‚       â”‚  Snowflake  â”‚
â”‚             â”‚       â”‚  Validate    â”‚       â”‚  PostgreSQL â”‚
â”‚             â”‚       â”‚  Transform   â”‚       â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚    STATE     â”‚
                      â”‚ (incremental)â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. Installation et premier pipeline

### Installation

```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # macOS/Linux
# ou
venv\Scripts\activate  # Windows

# Installer dlt
pip install dlt

# Installer des destinations spÃ©cifiques
pip install "dlt[duckdb]"      # Pour DuckDB
pip install "dlt[postgres]"    # Pour PostgreSQL
pip install "dlt[bigquery]"    # Pour BigQuery
pip install "dlt[snowflake]"   # Pour Snowflake

# VÃ©rifier l'installation
dlt --version
```

### Premier pipeline : API â†’ DuckDB

CrÃ©ons un pipeline simple qui charge des donnÃ©es depuis une API vers DuckDB :

```bash
"""
Premier pipeline DLT : API â†’ DuckDB
"""
import dlt
import requests


# 1. DÃ©finir une source (function qui gÃ©nÃ¨re des donnÃ©es)
@dlt.resource(name="users", write_disposition="replace")
def get_users():
    """RÃ©cupÃ©rer des utilisateurs depuis JSONPlaceholder API"""
    response = requests.get("https://jsonplaceholder.typicode.com/users")
    yield response.json()


# 2. CrÃ©er le pipeline
def run_pipeline():
# Configurer le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="api_to_duckdb",
        destination="duckdb",
        dataset_name="demo_data"
    )

# Charger les donnÃ©es
    load_info = pipeline.run(get_users())

# Afficher les rÃ©sultats
    print(f"âœ… Pipeline exÃ©cutÃ© avec succÃ¨s!")
    print(f"ğŸ“Š Lignes chargÃ©es: {load_info}")


if __name__ == "__main__":
    run_pipeline()
```

#### ExÃ©cuter le pipeline

```bash
python first_pipeline.py

# RÃ©sultat
âœ… Pipeline exÃ©cutÃ© avec succÃ¨s!
ğŸ“Š Lignes chargÃ©es: LoadInfo(...)

# VÃ©rifier les donnÃ©es dans DuckDB
duckdb demo_data.duckdb

# Dans DuckDB:
SELECT * FROM users LIMIT 5;
.exit
```

#### FÃ©licitations !

Vous venez de crÃ©er votre premier pipeline DLT en quelques lignes de code.
Les donnÃ©es de l'API sont maintenant stockÃ©es dans DuckDB et prÃªtes Ã  Ãªtre analysÃ©es.

### Structure d'un projet DLT

```bash
my_dlt_project/
â”œâ”€â”€ .dlt/
â”‚   â”œâ”€â”€ config.toml          # Configuration (credentials)
â”‚   â””â”€â”€ secrets.toml         # Secrets (API keys, passwords)
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ api_pipeline.py
â”‚   â””â”€â”€ database_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

#### Configuration (.dlt/config.toml)

```bash
# Configuration du pipeline
[runtime]
log_level = "INFO"

# Configuration de la destination
[destination.duckdb]
credentials = "demo_data.duckdb"

[destination.postgres]
credentials = "postgresql://user:password@localhost:5432/db"
```

#### Secrets (.dlt/secrets.toml)

```bash
# Ne JAMAIS commiter ce fichier !
[sources.api]
api_key = "your-secret-api-key"

[destination.bigquery.credentials]
project_id = "my-project"
private_key = "-----BEGIN PRIVATE KEY-----\n..."
```

#### SÃ©curitÃ©

Ajoutez `.dlt/secrets.toml` dans votre `.gitignore` pour ne pas
commiter vos credentials par erreur !

## 3. Sources et Resources

### Qu'est-ce qu'une Resource ?

Une **resource** est une unitÃ© de donnÃ©es que DLT peut charger.
Elle peut Ãªtre une fonction, un gÃ©nÃ©rateur ou un itÃ©rateur qui produit des donnÃ©es.

#### Resource simple

```bash
import dlt

@dlt.resource
def my_data():
    """Resource simple qui retourne une liste"""
    return [
        {"id": 1, "name": "Alice", "age": 28},
        {"id": 2, "name": "Bob", "age": 32},
        {"id": 3, "name": "Charlie", "age": 25}
    ]

# Utilisation
pipeline = dlt.pipeline(
    pipeline_name="simple",
    destination="duckdb",
    dataset_name="demo"
)

pipeline.run(my_data())
```

#### Resource avec gÃ©nÃ©rateur (lazy loading)

```bash
@dlt.resource
def paginated_api():
    """Resource avec pagination (lazy loading)"""
    page = 1
    while True:
        response = requests.get(f"https://api.example.com/data?page={page}")
        data = response.json()

        if not data:
            break

        yield data  # Yield permet de traiter les donnÃ©es par batch
        page += 1
```

### Write Disposition

Le **write\_disposition** dÃ©finit comment les donnÃ©es sont Ã©crites dans la destination :

| Mode | Description | Cas d'usage |
| --- | --- | --- |
| `replace` | Remplace toutes les donnÃ©es | Snapshots complets, petites tables |
| `append` | Ajoute les nouvelles donnÃ©es | Logs, Ã©vÃ©nements, time-series |
| `merge` | Upsert (update ou insert) | Dimension tables, CDC |

```bash
# Replace : Ã©crase les donnÃ©es
@dlt.resource(write_disposition="replace")
def full_snapshot():
    return get_all_users()

# Append : ajoute aux donnÃ©es existantes
@dlt.resource(write_disposition="append")
def events_log():
    return get_new_events()

# Merge : upsert basÃ© sur primary_key
@dlt.resource(
    write_disposition="merge",
    primary_key="user_id"
)
def user_updates():
    return get_updated_users()
```

### Sources : Regrouper plusieurs Resources

Une **source** regroupe plusieurs resources liÃ©es :

```bash
import dlt

@dlt.source
def my_api_source(api_key: str):
    """Source qui regroupe plusieurs endpoints"""

    @dlt.resource(write_disposition="replace")
    def users():
        """RÃ©cupÃ©rer les utilisateurs"""
        response = requests.get(
            "https://api.example.com/users",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

    @dlt.resource(write_disposition="append")
    def orders():
        """RÃ©cupÃ©rer les commandes"""
        response = requests.get(
            "https://api.example.com/orders",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

    @dlt.resource(write_disposition="merge", primary_key="product_id")
    def products():
        """RÃ©cupÃ©rer les produits"""
        response = requests.get(
            "https://api.example.com/products",
            headers={"Authorization": f"Bearer {api_key}"}
        )
        yield response.json()

# Retourner toutes les resources
    return users(), orders(), products()


# Utilisation
pipeline = dlt.pipeline(
    pipeline_name="ecommerce",
    destination="duckdb",
    dataset_name="ecommerce_data"
)

# Charger toutes les resources de la source
load_info = pipeline.run(my_api_source(api_key="secret-key"))

# Ou charger une seule resource
source = my_api_source(api_key="secret-key")
load_info = pipeline.run(source.users)
```

### Loading incrÃ©mental

Le **loading incrÃ©mental** permet de ne charger que les nouvelles donnÃ©es :

```bash
from datetime import datetime, timedelta
import dlt

@dlt.resource(
    write_disposition="append",
    primary_key="order_id"
)
def orders_incremental(
    last_timestamp=dlt.sources.incremental("created_at")
):
    """
    Charger seulement les commandes rÃ©centes
    DLT sauvegarde automatiquement le dernier timestamp
    """

# Si c'est la premiÃ¨re exÃ©cution, prendre les 7 derniers jours
    if last_timestamp.start_value is None:
        last_timestamp.start_value = datetime.now() - timedelta(days=7)

# RequÃªter l'API avec le timestamp
    response = requests.get(
        "https://api.example.com/orders",
        params={"since": last_timestamp.start_value.isoformat()}
    )

    orders = response.json()

# DLT met Ã  jour automatiquement le timestamp
# basÃ© sur la colonne "created_at"
    yield orders


# PremiÃ¨re exÃ©cution : charge les 7 derniers jours
pipeline.run(orders_incremental())

# DeuxiÃ¨me exÃ©cution : charge seulement les nouvelles donnÃ©es
pipeline.run(orders_incremental())
```

#### State Management

DLT sauvegarde automatiquement l'Ã©tat (timestamps, cursors) dans la destination.
Vous n'avez pas besoin de gÃ©rer manuellement le tracking des donnÃ©es dÃ©jÃ  chargÃ©es.

### Transformer les donnÃ©es

```bash
@dlt.resource
def users_transformed():
    """Resource avec transformation des donnÃ©es"""
    raw_data = requests.get("https://api.example.com/users").json()

# Transformer chaque enregistrement
    for user in raw_data:
        yield {
            "user_id": user["id"],
            "full_name": f"{user['first_name']} {user['last_name']}",
            "email": user["email"].lower(),
            "age": user["age"],
            "is_active": user.get("status") == "active",
            "created_at": datetime.fromisoformat(user["created_at"]),
# Ajouter des champs calculÃ©s
            "age_group": "young" if user["age"] < 30 else "senior",
            "loaded_at": datetime.now()
        }
```

### Best Practices pour les Resources

- Utilisez des gÃ©nÃ©rateurs (yield) pour les grandes quantitÃ©s de donnÃ©es
- Ajoutez un primary\_key pour les merge operations
- Utilisez le loading incrÃ©mental pour Ã©conomiser API calls
- Ajoutez des champs de mÃ©tadonnÃ©es (loaded\_at, source, etc.)
- GÃ©rez les erreurs avec try-except
- Loggez les opÃ©rations importantes

## 4. Destinations et configuration

### Destinations supportÃ©es

#### ğŸ¦† DuckDB

Base de donnÃ©es analytique locale, parfait pour le dev et tests

#### ğŸ˜ PostgreSQL

Base de donnÃ©es relationnelle classique

#### â˜ï¸ BigQuery

Data warehouse Google Cloud, serverless et scalable

#### â„ï¸ Snowflake

Data warehouse cloud leader du marchÃ©

#### ğŸ”· Azure Synapse

Data warehouse Microsoft Azure

#### ğŸª£ Parquet/CSV

Export vers fichiers pour data lakes

### Configuration DuckDB

```bash
import dlt

# MÃ©thode 1 : Configuration inline
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="duckdb",
    dataset_name="my_dataset"
)

# MÃ©thode 2 : Configuration via credentials
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.duckdb("my_data.duckdb"),
    dataset_name="my_dataset"
)
```

### Configuration PostgreSQL

```bash
import dlt

# Configuration PostgreSQL
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.postgres(
        "postgresql://user:password@localhost:5432/mydb"
    ),
    dataset_name="analytics"
)

# Ou via .dlt/secrets.toml:
# [destination.postgres.credentials]
# database = "mydb"
# username = "user"
# password = "password"
# host = "localhost"
# port = 5432

pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="postgres",
    dataset_name="analytics"
)
```

### Configuration BigQuery

```bash
import dlt

# Configuration BigQuery
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="bigquery",
    dataset_name="analytics"
)

# Fichier .dlt/secrets.toml :
# [destination.bigquery.credentials]
# project_id = "my-gcp-project"
# private_key = "-----BEGIN PRIVATE KEY-----\n..."
# client_email = "service-account@project.iam.gserviceaccount.com"

# Ou utiliser GOOGLE_APPLICATION_CREDENTIALS
# export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
```

### Configuration Snowflake

```bash
import dlt

# Fichier .dlt/secrets.toml :
# [destination.snowflake.credentials]
# database = "MY_DATABASE"
# username = "MY_USER"
# password = "MY_PASSWORD"
# host = "account.snowflakecomputing.com"
# warehouse = "MY_WAREHOUSE"
# role = "MY_ROLE"

pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination="snowflake",
    dataset_name="analytics"
)
```

### Export vers fichiers (Data Lake)

```bash
import dlt

# Export vers Parquet
pipeline = dlt.pipeline(
    pipeline_name="my_pipeline",
    destination=dlt.destinations.filesystem("/data/lake"),
    dataset_name="raw_data"
)

# Configuration dans .dlt/config.toml :
# [destination.filesystem]
# bucket_url = "s3://my-bucket/data"  # S3
# # ou
# bucket_url = "gs://my-bucket/data"  # GCS
# # ou
# bucket_url = "az://my-container/data"  # Azure Blob

# Layout des fichiers :
# /data/lake/
#   â”œâ”€â”€ users/
#   â”‚   â”œâ”€â”€ 2024-01-15_001.parquet
#   â”‚   â””â”€â”€ 2024-01-16_001.parquet
#   â””â”€â”€ orders/
#       â””â”€â”€ 2024-01-15_001.parquet
```

#### Environnements multiples

Utilisez diffÃ©rents fichiers de configuration pour dev, staging et production :

- `.dlt/config.dev.toml`
- `.dlt/config.staging.toml`
- `.dlt/config.prod.toml`

## 5. SchÃ©mas et validation des donnÃ©es

### Schema Evolution automatique

DLT gÃ¨re automatiquement l'Ã©volution du schÃ©ma. Si de nouvelles colonnes apparaissent,
elles sont ajoutÃ©es automatiquement sans casser le pipeline.

```bash
# PremiÃ¨re exÃ©cution : 3 colonnes
data_v1 = [
    {"id": 1, "name": "Alice", "age": 28}
]
pipeline.run(data_v1, table_name="users")

# DeuxiÃ¨me exÃ©cution : nouvelle colonne "email"
# DLT ajoute automatiquement la colonne
data_v2 = [
    {"id": 2, "name": "Bob", "age": 32, "email": "bob@example.com"}
]
pipeline.run(data_v2, table_name="users")

# RÃ©sultat dans la table :
# id | name  | age | email
# 1  | Alice | 28  | NULL
# 2  | Bob   | 32  | bob@example.com
```

### DÃ©finir un schÃ©ma explicite

```bash
from dlt.common.schema import TColumnSchema

@dlt.resource(
    columns={
        "user_id": {
            "data_type": "bigint",
            "nullable": False,
            "primary_key": True
        },
        "email": {
            "data_type": "text",
            "nullable": False,
            "unique": True
        },
        "age": {
            "data_type": "bigint",
            "nullable": True
        },
        "created_at": {
            "data_type": "timestamp",
            "nullable": False
        }
    }
)
def users_with_schema():
    """Resource avec schÃ©ma explicite"""
    return get_users_data()
```

### Validation des donnÃ©es

```bash
import dlt
from dlt.common.typing import TDataItem

@dlt.resource
def validated_users():
    """Resource avec validation custom"""
    raw_data = get_raw_users()

    for user in raw_data:
# Validation manuelle
        if not user.get("email") or "@" not in user["email"]:
            print(f"âš ï¸ Email invalide pour user {user.get('id')}")
            continue  # Skip cet enregistrement

        if user.get("age") and user["age"] < 0:
            print(f"âš ï¸ Age nÃ©gatif pour user {user.get('id')}")
            user["age"] = None  # Corriger la donnÃ©e

        yield user


# Ou utiliser Pydantic pour la validation
from pydantic import BaseModel, EmailStr, validator

class UserModel(BaseModel):
    """ModÃ¨le Pydantic pour validation"""
    user_id: int
    email: EmailStr
    age: int

    @validator('age')
    def age_must_be_positive(cls, v):
        if v < 0:
            raise ValueError('Age must be positive')
        return v


@dlt.resource
def users_pydantic():
    """Validation avec Pydantic"""
    raw_data = get_raw_users()

    for user in raw_data:
        try:
# Valider avec Pydantic
            validated = UserModel(**user)
            yield validated.dict()
        except Exception as e:
            print(f"âŒ Validation error: {e}")
# Logger ou rejeter
```

### Data Contracts

Les **data contracts** permettent de dÃ©finir des rÃ¨gles strictes sur les donnÃ©es :

```bash
@dlt.resource(
    columns={
        "user_id": {"data_type": "bigint", "nullable": False},
        "email": {"data_type": "text", "nullable": False}
    },
    schema_contract={
        "tables": "evolve",      # Nouvelles tables autorisÃ©es
        "columns": "freeze",     # Nouvelles colonnes interdites
        "data_type": "freeze"    # Changement de type interdit
    }
)
def strict_users():
    """
    Schema contract strict :
    - Nouvelles colonnes = erreur
    - Changement de type = erreur
    """
    return get_users()
```

#### Attention

Les data contracts stricts peuvent casser vos pipelines si l'API source change.
Utilisez-les seulement quand vous contrÃ´lez la source ou avez des SLAs stricts.

## 6. Patterns avancÃ©s et production

### Pipeline complet production-ready

```bash
"""
Pipeline production-ready avec gestion d'erreurs et monitoring
"""
import dlt
import requests
from datetime import datetime
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dlt.source
def ecommerce_source(api_key: str, base_url: str):
    """Source e-commerce complÃ¨te"""

    @dlt.resource(
        write_disposition="merge",
        primary_key="user_id",
        merge_key="user_id"
    )
    def users():
        """Utilisateurs avec upsert"""
        try:
            logger.info("ğŸ“¥ Fetching users...")
            response = requests.get(
                f"{base_url}/users",
                headers={"Authorization": f"Bearer {api_key}"},
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            logger.info(f"âœ… Fetched {len(data)} users")

# Enrichir avec mÃ©tadonnÃ©es
            for user in data:
                user["_loaded_at"] = datetime.now()
                user["_source"] = "api"
                yield user

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching users: {e}")
            raise

    @dlt.resource(
        write_disposition="append",
        primary_key="order_id"
    )
    def orders(last_date=dlt.sources.incremental("created_at")):
        """Commandes avec loading incrÃ©mental"""
        try:
            logger.info(f"ğŸ“¥ Fetching orders since {last_date.start_value}...")

            params = {}
            if last_date.start_value:
                params["since"] = last_date.start_value.isoformat()

            response = requests.get(
                f"{base_url}/orders",
                headers={"Authorization": f"Bearer {api_key}"},
                params=params,
                timeout=30
            )
            response.raise_for_status()

            data = response.json()
            logger.info(f"âœ… Fetched {len(data)} orders")

            for order in data:
                order["_loaded_at"] = datetime.now()
                yield order

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching orders: {e}")
            raise

    return users(), orders()


def run_pipeline():
    """ExÃ©cuter le pipeline avec monitoring"""
    logger.info("ğŸš€ Starting pipeline...")

# CrÃ©er le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="ecommerce_prod",
        destination="bigquery",
        dataset_name="analytics"
    )

    try:
# RÃ©cupÃ©rer credentials depuis les secrets
        api_key = dlt.secrets.value["sources.ecommerce.api_key"]
        base_url = dlt.config.value["sources.ecommerce.base_url"]

# ExÃ©cuter le pipeline
        source = ecommerce_source(api_key=api_key, base_url=base_url)
        load_info = pipeline.run(source)

# Logger les rÃ©sultats
        logger.info("âœ… Pipeline completed successfully!")
        logger.info(f"ğŸ“Š Load info: {load_info}")

# VÃ©rifier les erreurs
        if load_info.has_failed_jobs:
            logger.error("âŒ Some jobs failed!")
            for job in load_info.load_packages[0].jobs["failed_jobs"]:
                logger.error(f"Failed job: {job}")
            raise Exception("Pipeline had failed jobs")

        return load_info

    except Exception as e:
        logger.error(f"âŒ Pipeline failed: {e}")
# Envoyer une alerte (Slack, email, etc.)
        send_alert(f"Pipeline failed: {e}")
        raise


def send_alert(message: str):
    """Envoyer une alerte en cas d'erreur"""
# Slack webhook
    webhook_url = dlt.secrets.value.get("alerts.slack_webhook")
    if webhook_url:
        requests.post(webhook_url, json={"text": message})


if __name__ == "__main__":
    run_pipeline()
```

### DÃ©ploiement avec Airflow

```bash
"""
DAG Airflow pour exÃ©cuter un pipeline DLT
"""
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import dlt


def run_dlt_pipeline():
    """Fonction appelÃ©e par Airflow"""
    pipeline = dlt.pipeline(
        pipeline_name="ecommerce",
        destination="bigquery",
        dataset_name="analytics"
    )

# Importer votre source
    from pipelines.ecommerce import ecommerce_source

    api_key = dlt.secrets.value["sources.ecommerce.api_key"]
    source = ecommerce_source(api_key=api_key)

    load_info = pipeline.run(source)

    if load_info.has_failed_jobs:
        raise Exception("Pipeline had failed jobs")


# DÃ©finir le DAG
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['alerts@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'dlt_ecommerce_pipeline',
    default_args=default_args,
    description='Load ecommerce data with DLT',
    schedule_interval='@hourly',  # Toutes les heures
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['dlt', 'ecommerce']
) as dag:

    run_pipeline_task = PythonOperator(
        task_id='run_dlt_pipeline',
        python_callable=run_dlt_pipeline
    )
```

### Tests unitaires

```bash
"""
Tests pour les pipelines DLT
"""
import pytest
import dlt


def test_users_resource():
    """Tester la resource users"""
    from pipelines.ecommerce import ecommerce_source

# Mock API
    api_key = "test-key"
    base_url = "https://api.test.com"

    source = ecommerce_source(api_key=api_key, base_url=base_url)

# Tester avec DuckDB (destination de test)
    pipeline = dlt.pipeline(
        pipeline_name="test_pipeline",
        destination="duckdb",
        dataset_name="test_data",
        full_refresh=True  # Nettoyer entre chaque test
    )

    load_info = pipeline.run(source.users)

    assert not load_info.has_failed_jobs, "Pipeline should not have failed jobs"

# VÃ©rifier les donnÃ©es chargÃ©es
    with pipeline.sql_client() as client:
        result = client.execute_sql("SELECT COUNT(*) as count FROM users")
        count = result[0][0]
        assert count > 0, "Should have loaded users"


def test_schema_validation():
    """Tester la validation du schÃ©ma"""
    from pipelines.ecommerce import users_resource

# DonnÃ©es de test
    test_data = [
        {"user_id": 1, "email": "test@example.com", "age": 25},
        {"user_id": 2, "email": "invalid-email", "age": -5}  # Invalide
    ]

# VÃ©rifier que les donnÃ©es invalides sont rejetÃ©es
# ... votre logique de test
```

### Checklist Production

- Gestion d'erreurs complÃ¨te (try-except)
- Logging dÃ©taillÃ© de toutes les opÃ©rations
- Alertes en cas d'Ã©chec (Slack, email)
- Loading incrÃ©mental pour Ã©conomiser ressources
- Monitoring des mÃ©triques (lignes chargÃ©es, durÃ©e)
- Tests unitaires et d'intÃ©gration
- Documentation du pipeline
- Secrets sÃ©curisÃ©s (jamais en dur dans le code)
- Retries automatiques sur erreurs transitoires
- CI/CD pour dÃ©ploiement automatique

## ğŸ“š Ressources et liens utiles

[**Documentation officielle DLT**

Documentation complÃ¨te, guides et rÃ©fÃ©rences](https://dlthub.com/docs)
[**DLT sur GitHub**

Code source, issues et contributions](https://github.com/dlt-hub/dlt)
[**Exemples de pipelines**

Exemples prÃªts Ã  l'emploi pour diffÃ©rentes sources](https://dlthub.com/docs/examples)
[**API Reference**

Documentation dÃ©taillÃ©e de l'API DLT](https://dlthub.com/docs/reference)
[**Blog DLT**

Articles, tutorials et best practices](https://dlthub.com/docs/blog)
[**Discord Community**

Aide, discussions et support communautaire](https://discord.gg/dlthub)

#### Prochaines Ã©tapes

Maintenant que vous maÃ®trisez DLT, explorez :

- **Sources vÃ©rifiÃ©es** : Utilisez les sources prÃ©-construites (GitHub, Stripe, etc.)
- **dbt integration** : Transformez vos donnÃ©es avec dbt aprÃ¨s le load
- **Orchestration** : DÃ©ployez avec Airflow, Dagster ou Prefect
- **Monitoring** : IntÃ©grez avec DataDog, Prometheus pour le monitoring
- **Data Quality** : Ajoutez Great Expectations pour la validation
- **Reverse ETL** : Chargez des donnÃ©es depuis votre warehouse vers des apps