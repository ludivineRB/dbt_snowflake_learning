# ğŸ³ Brief Pratique Docker Complet - Data Engineering

**DurÃ©e estimÃ©e :** 6 heures
**Niveau :** DÃ©butant Ã  AvancÃ©
**ModalitÃ© :** Pratique individuelle ou binÃ´me

---

## ğŸ¯ Objectifs du Brief

Ã€ l'issue de ce brief, vous serez capable de :
- CrÃ©er des Dockerfiles optimisÃ©s pour des applications Data Engineering
- Orchestrer une infrastructure complÃ¨te multi-conteneurs
- GÃ©rer les volumes, rÃ©seaux et variables d'environnement
- DÃ©ployer un pipeline de donnÃ©es end-to-end conteneurisÃ©
- Troubleshooter et optimiser des conteneurs Docker
- Appliquer les bonnes pratiques de sÃ©curitÃ© et performance

---

## ğŸ“‹ Contexte

Vous Ãªtes Data Engineer chez **DataStream Analytics**, une startup qui collecte et analyse des donnÃ©es en temps rÃ©el provenant de multiples sources (APIs, fichiers, streams). Votre mission est de conteneuriser l'ensemble de l'infrastructure de traitement de donnÃ©es pour :
- Faciliter le dÃ©ploiement sur diffÃ©rents environnements
- Garantir la reproductibilitÃ© des traitements
- Simplifier la scalabilitÃ© de la plateforme
- Permettre aux Data Scientists de travailler dans des environnements isolÃ©s

---

## ğŸš€ Partie 1 : Pipeline ETL Basique (1h30)

### ğŸ¯ Objectif
CrÃ©er un pipeline ETL Python conteneurisÃ© qui extrait des donnÃ©es depuis une API, les transforme et les charge dans PostgreSQL.

### TÃ¢che 1.1 : PrÃ©parer les fichiers sources

CrÃ©ez la structure de projet suivante :
```
docker-etl-project/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â””â”€â”€ logs/
```

### TÃ¢che 1.2 : CrÃ©er le script ETL

**Fichier : `requirements.txt`**
```
pandas==2.1.3
requests==2.31.0
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pyyaml==6.0.1
python-dotenv==1.0.0
```

**Fichier : `etl_pipeline.py`**
```python
import pandas as pd
import requests
import psycopg2
from sqlalchemy import create_engine
import os
import logging
from datetime import datetime
import yaml

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/etl.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ETLPipeline:
    def __init__(self):
        self.db_url = os.getenv('DATABASE_URL')
        self.engine = create_engine(self.db_url)
        logger.info("ETL Pipeline initialisÃ©")

    def extract(self):
        """Extraction des donnÃ©es depuis une API publique"""
        logger.info("DÃ©but de l'extraction des donnÃ©es")
        try:
            # Utilisation de l'API JSONPlaceholder comme exemple
            response = requests.get('https://jsonplaceholder.typicode.com/users')
            response.raise_for_status()
            data = response.json()
            df = pd.DataFrame(data)
            logger.info(f"{len(df)} enregistrements extraits")
            return df
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction : {e}")
            raise

    def transform(self, df):
        """Transformation des donnÃ©es"""
        logger.info("DÃ©but de la transformation")
        try:
            # Nettoyage et transformation
            df['extracted_at'] = datetime.now()

            # Normalisation des colonnes imbriquÃ©es
            if 'address' in df.columns:
                df['city'] = df['address'].apply(lambda x: x.get('city', ''))
                df['zipcode'] = df['address'].apply(lambda x: x.get('zipcode', ''))
                df['lat'] = df['address'].apply(lambda x: float(x.get('geo', {}).get('lat', 0)))
                df['lng'] = df['address'].apply(lambda x: float(x.get('geo', {}).get('lng', 0)))

            # SÃ©lection des colonnes importantes
            columns_to_keep = ['id', 'name', 'username', 'email', 'phone',
                             'city', 'zipcode', 'lat', 'lng', 'extracted_at']
            df_clean = df[columns_to_keep].copy()

            logger.info(f"Transformation terminÃ©e : {len(df_clean)} lignes")
            return df_clean
        except Exception as e:
            logger.error(f"Erreur lors de la transformation : {e}")
            raise

    def load(self, df, table_name='users_data'):
        """Chargement des donnÃ©es dans PostgreSQL"""
        logger.info(f"DÃ©but du chargement dans la table {table_name}")
        try:
            df.to_sql(table_name, self.engine, if_exists='replace', index=False)
            logger.info(f"âœ… {len(df)} lignes insÃ©rÃ©es dans {table_name}")
        except Exception as e:
            logger.error(f"Erreur lors du chargement : {e}")
            raise

    def run(self):
        """ExÃ©cution complÃ¨te du pipeline"""
        logger.info("ğŸš€ DÃ©marrage du pipeline ETL")
        try:
            # ETL
            raw_data = self.extract()
            transformed_data = self.transform(raw_data)
            self.load(transformed_data)

            logger.info("âœ… Pipeline terminÃ© avec succÃ¨s")
            return True
        except Exception as e:
            logger.error(f"âŒ Erreur dans le pipeline : {e}")
            return False

if __name__ == "__main__":
    pipeline = ETLPipeline()
    success = pipeline.run()
    exit(0 if success else 1)
```

### TÃ¢che 1.3 : CrÃ©er un Dockerfile optimisÃ©

CrÃ©ez un `Dockerfile` qui respecte les bonnes pratiques :
- Utilisez une image Python slim
- Optimisez le cache Docker
- CrÃ©ez un utilisateur non-root
- Ajoutez des health checks
- Minimisez la taille de l'image

**Exemple de structure attendue :**
```dockerfile
# Utilisez python:3.11-slim comme base
# Configurez les variables d'environnement Python
# Installez les dÃ©pendances systÃ¨me nÃ©cessaires (gcc, postgresql-client)
# Copiez requirements.txt et installez les dÃ©pendances Python
# Copiez le code source
# CrÃ©ez un utilisateur non-root
# DÃ©finissez le point d'entrÃ©e
```

### TÃ¢che 1.4 : CrÃ©er le docker-compose.yml

CrÃ©ez un fichier `docker-compose.yml` orchestrant :

1. **Service PostgreSQL**
   - Image : `postgres:15-alpine`
   - Variables d'environnement pour la base de donnÃ©es
   - Volume persistant pour les donnÃ©es
   - Health check pour vÃ©rifier la disponibilitÃ©

2. **Service ETL**
   - Build depuis votre Dockerfile
   - DÃ©pend de PostgreSQL (avec condition de health)
   - Monte les volumes pour logs et donnÃ©es
   - Variables d'environnement pour la connexion DB

3. **Service PgAdmin** (optionnel pour visualisation)
   - Image : `dpage/pgadmin4`
   - Port exposÃ© pour l'interface web

**CritÃ¨res de rÃ©ussite :**
- âœ… `docker-compose up -d` dÃ©marre tous les services
- âœ… L'ETL se connecte automatiquement Ã  PostgreSQL
- âœ… Les donnÃ©es sont visibles dans la table `users_data`
- âœ… Les logs sont persistÃ©s dans le dossier `logs/`

### TÃ¢che 1.5 : Tester et valider

```bash
# Construire et dÃ©marrer
docker-compose up -d

# VÃ©rifier les logs
docker-compose logs -f etl

# Se connecter Ã  PostgreSQL et vÃ©rifier les donnÃ©es
docker-compose exec postgres psql -U dataeng -d warehouse -c "SELECT * FROM users_data;"

# VÃ©rifier les statistiques
docker stats
```

---

## ğŸ—„ï¸ Partie 2 : Infrastructure Multi-Services (2h)

### ğŸ¯ Objectif
Ã‰tendre l'infrastructure avec des services additionnels : MongoDB, Redis, et une API REST.

### TÃ¢che 2.1 : Ajouter MongoDB pour les donnÃ©es non structurÃ©es

Ajoutez un service MongoDB Ã  votre `docker-compose.yml` :
- Image : `mongo:7`
- Authentification configurÃ©e
- Volume persistant
- ExposÃ© sur le port 27017

Modifiez votre pipeline ETL pour Ã©crire Ã©galement les donnÃ©es brutes dans MongoDB :

```python
from pymongo import MongoClient

def load_to_mongo(self, data, collection_name='raw_data'):
    """Charge les donnÃ©es brutes dans MongoDB"""
    mongo_url = os.getenv('MONGO_URL', 'mongodb://admin:secret@mongodb:27017/')
    client = MongoClient(mongo_url)
    db = client['datawarehouse']
    collection = db[collection_name]

    # InsÃ©rer les donnÃ©es
    if isinstance(data, pd.DataFrame):
        records = data.to_dict('records')
    else:
        records = data

    result = collection.insert_many(records)
    logger.info(f"âœ… {len(result.inserted_ids)} documents insÃ©rÃ©s dans MongoDB")
```

### TÃ¢che 2.2 : Ajouter Redis pour le cache

Ajoutez Redis pour mettre en cache les rÃ©sultats :
- Image : `redis:7-alpine`
- Volume pour la persistence
- Port 6379

ImplÃ©mentez un systÃ¨me de cache :

```python
import redis
import json
import hashlib

class CacheManager:
    def __init__(self):
        self.redis_client = redis.Redis(
            host=os.getenv('REDIS_HOST', 'redis'),
            port=6379,
            decode_responses=True
        )

    def get_cached(self, key):
        """RÃ©cupÃ¨re une valeur du cache"""
        return self.redis_client.get(key)

    def set_cache(self, key, value, ttl=3600):
        """Met en cache une valeur"""
        self.redis_client.setex(key, ttl, json.dumps(value))

    def generate_cache_key(self, *args):
        """GÃ©nÃ¨re une clÃ© de cache unique"""
        key_str = "_".join(str(arg) for arg in args)
        return hashlib.md5(key_str.encode()).hexdigest()
```

### TÃ¢che 2.3 : CrÃ©er une API REST avec FastAPI

CrÃ©ez un service API REST pour exposer les donnÃ©es.

**Fichier : `api/main.py`**
```python
from fastapi import FastAPI, HTTPException
from sqlalchemy import create_engine, text
import os
from typing import List, Dict

app = FastAPI(title="DataStream API", version="1.0.0")

DATABASE_URL = os.getenv('DATABASE_URL')
engine = create_engine(DATABASE_URL)

@app.get("/")
def read_root():
    return {"message": "DataStream API", "status": "running"}

@app.get("/health")
def health_check():
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        return {"status": "healthy", "database": "connected"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/users")
def get_users():
    try:
        with engine.connect() as conn:
            result = conn.execute(text("SELECT * FROM users_data LIMIT 100"))
            users = [dict(row._mapping) for row in result]
        return {"count": len(users), "data": users}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/users/{user_id}")
def get_user(user_id: int):
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text("SELECT * FROM users_data WHERE id = :id"),
                {"id": user_id}
            )
            user = result.fetchone()
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            return dict(user._mapping)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
def get_stats():
    try:
        with engine.connect() as conn:
            count = conn.execute(text("SELECT COUNT(*) FROM users_data")).scalar()
            cities = conn.execute(
                text("SELECT city, COUNT(*) as count FROM users_data GROUP BY city")
            ).fetchall()
        return {
            "total_users": count,
            "cities": [{"city": c[0], "count": c[1]} for c in cities]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Fichier : `api/requirements.txt`**
```
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
```

**Fichier : `api/Dockerfile`**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

Ajoutez le service API Ã  votre `docker-compose.yml` :
```yaml
api:
  build: ./api
  container_name: datastream-api
  ports:
    - "8000:8000"
  environment:
    DATABASE_URL: postgresql://dataeng:${POSTGRES_PASSWORD}@postgres:5432/warehouse
  networks:
    - data-network
  depends_on:
    postgres:
      condition: service_healthy
```

**CritÃ¨res de rÃ©ussite :**
- âœ… L'API est accessible sur http://localhost:8000
- âœ… `/docs` affiche la documentation Swagger automatique
- âœ… `/health` retourne le statut de santÃ©
- âœ… `/users` retourne les donnÃ©es depuis PostgreSQL
- âœ… Les donnÃ©es sont cachÃ©es dans Redis

### TÃ¢che 2.4 : CrÃ©er un rÃ©seau personnalisÃ©

CrÃ©ez un rÃ©seau Docker personnalisÃ© avec des sous-rÃ©seaux :

```yaml
networks:
  data-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
          gateway: 172.28.0.1

  monitoring-network:
    driver: bridge
```

---

## ğŸ“Š Partie 3 : Orchestration avec Airflow (1h30)

### ğŸ¯ Objectif
Orchestrer les pipelines avec Apache Airflow et crÃ©er des DAGs pour automatiser les traitements.

### TÃ¢che 3.1 : Configurer Airflow

CrÃ©ez une configuration Airflow complÃ¨te dans votre `docker-compose.yml`.

**Services Ã  ajouter :**
1. **Airflow Webserver**
2. **Airflow Scheduler**
3. **Airflow Initdb** (pour l'initialisation)

**Structure pour Airflow :**
```yaml
x-airflow-common: &airflow-common
  image: apache/airflow:2.7.3-python3.11
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dataeng:${POSTGRES_PASSWORD}@postgres/airflow
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
  networks:
    - data-network
  depends_on:
    postgres:
      condition: service_healthy

services:
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $${HOSTNAME}"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-init:
    <<: *airflow-common
    command: >
      bash -c "
        airflow db init &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin
      "
    restart: on-failure
```

### TÃ¢che 3.2 : CrÃ©er un DAG ETL

CrÃ©ez un DAG qui orchestre votre pipeline ETL.

**Fichier : `dags/etl_dag.py`**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
from sqlalchemy import create_engine
import os

default_args = {
    'owner': 'dataeng',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'datastream_etl_pipeline',
    default_args=default_args,
    description='Pipeline ETL DataStream',
    schedule_interval='@daily',
    catchup=False,
    tags=['etl', 'datastream'],
)

def extract_data(**context):
    """Extraction des donnÃ©es"""
    response = requests.get('https://jsonplaceholder.typicode.com/users')
    data = response.json()
    context['ti'].xcom_push(key='raw_data', value=data)
    return f"âœ… {len(data)} enregistrements extraits"

def transform_data(**context):
    """Transformation des donnÃ©es"""
    data = context['ti'].xcom_pull(key='raw_data', task_ids='extract')
    df = pd.DataFrame(data)

    # Transformations
    df['extracted_at'] = datetime.now().isoformat()
    df['city'] = df['address'].apply(lambda x: x.get('city', ''))

    transformed = df.to_dict('records')
    context['ti'].xcom_push(key='transformed_data', value=transformed)
    return f"âœ… {len(transformed)} lignes transformÃ©es"

def load_data(**context):
    """Chargement des donnÃ©es"""
    data = context['ti'].xcom_pull(key='transformed_data', task_ids='transform')
    df = pd.DataFrame(data)

    db_url = os.getenv('DATABASE_URL', 'postgresql://dataeng:secret@postgres:5432/warehouse')
    engine = create_engine(db_url)

    df.to_sql('users_data_airflow', engine, if_exists='replace', index=False)
    return f"âœ… {len(df)} lignes chargÃ©es"

def check_data_quality(**context):
    """VÃ©rification de la qualitÃ© des donnÃ©es"""
    db_url = os.getenv('DATABASE_URL', 'postgresql://dataeng:secret@postgres:5432/warehouse')
    engine = create_engine(db_url)

    with engine.connect() as conn:
        count = conn.execute("SELECT COUNT(*) FROM users_data_airflow").scalar()

    if count == 0:
        raise ValueError("âŒ Aucune donnÃ©e trouvÃ©e !")

    return f"âœ… {count} enregistrements vÃ©rifiÃ©s"

# DÃ©finir les tÃ¢ches
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load_data,
    dag=dag,
)

quality_check_task = PythonOperator(
    task_id='quality_check',
    python_callable=check_data_quality,
    dag=dag,
)

notify_task = BashOperator(
    task_id='notify',
    bash_command='echo "âœ… Pipeline ETL terminÃ© avec succÃ¨s !"',
    dag=dag,
)

# DÃ©finir les dÃ©pendances
extract_task >> transform_task >> load_task >> quality_check_task >> notify_task
```

### TÃ¢che 3.3 : CrÃ©er un DAG de monitoring

**Fichier : `dags/monitoring_dag.py`**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from sqlalchemy import create_engine
import os

default_args = {
    'owner': 'dataeng',
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
}

dag = DAG(
    'data_monitoring',
    default_args=default_args,
    description='Monitoring des donnÃ©es',
    schedule_interval='@hourly',
    catchup=False,
    tags=['monitoring'],
)

def check_database_size(**context):
    """VÃ©rifier la taille de la base"""
    db_url = os.getenv('DATABASE_URL')
    engine = create_engine(db_url)

    with engine.connect() as conn:
        result = conn.execute("""
            SELECT
                pg_size_pretty(pg_database_size('warehouse')) as db_size
        """)
        size = result.scalar()

    print(f"ğŸ“Š Taille de la base : {size}")
    return size

def check_table_counts(**context):
    """VÃ©rifier le nombre d'enregistrements"""
    db_url = os.getenv('DATABASE_URL')
    engine = create_engine(db_url)

    tables = ['users_data', 'users_data_airflow']
    counts = {}

    with engine.connect() as conn:
        for table in tables:
            try:
                count = conn.execute(f"SELECT COUNT(*) FROM {table}").scalar()
                counts[table] = count
            except:
                counts[table] = 0

    print(f"ğŸ“Š Nombre d'enregistrements : {counts}")
    return counts

check_size = PythonOperator(
    task_id='check_database_size',
    python_callable=check_database_size,
    dag=dag,
)

check_counts = PythonOperator(
    task_id='check_table_counts',
    python_callable=check_table_counts,
    dag=dag,
)

check_size >> check_counts
```

**CritÃ¨res de rÃ©ussite :**
- âœ… Airflow UI accessible sur http://localhost:8080
- âœ… Les DAGs apparaissent dans l'interface
- âœ… Le DAG ETL s'exÃ©cute manuellement avec succÃ¨s
- âœ… Toutes les tÃ¢ches passent au vert
- âœ… Les donnÃ©es sont correctement insÃ©rÃ©es

---

## ğŸ”§ Partie 4 : Optimisation et SÃ©curitÃ© (1h)

### ğŸ¯ Objectif
Optimiser les images Docker et sÃ©curiser l'infrastructure.

### TÃ¢che 4.1 : Optimiser les Dockerfiles avec Multi-Stage Build

Refactorisez votre Dockerfile ETL avec un multi-stage build :

```dockerfile
# ============================================
# STAGE 1: Builder
# ============================================
FROM python:3.11 AS builder

WORKDIR /build

# Installer les dÃ©pendances de compilation
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc g++ make postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copier et installer les dÃ©pendances Python
COPY requirements.txt .
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /build/wheels -r requirements.txt

# ============================================
# STAGE 2: Runtime
# ============================================
FROM python:3.11-slim

WORKDIR /app

# Copier uniquement les wheels compilÃ©s
COPY --from=builder /build/wheels /wheels
COPY --from=builder /build/requirements.txt .

# Installer depuis les wheels (plus rapide et plus petit)
RUN pip install --no-cache /wheels/*

# Copier le code
COPY etl_pipeline.py .
COPY src/ ./src/
COPY config/ ./config/

# CrÃ©er les rÃ©pertoires
RUN mkdir -p /data/raw /data/processed /app/logs

# Utilisateur non-root
RUN useradd -m -u 1000 dataeng && \
    chown -R dataeng:dataeng /app /data
USER dataeng

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
    CMD python -c "import sys; sys.exit(0)" || exit 1

CMD ["python", "etl_pipeline.py"]
```

### TÃ¢che 4.2 : CrÃ©er un .dockerignore complet

```
# Git
.git
.gitignore
.gitattributes

# IDE
.vscode
.idea
*.swp
*.swo

# Python
__pycache__
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/

# Environment
.env
.env.local
.env.*.local
venv/
ENV/

# Logs
*.log
logs/

# Data
data/raw/*
data/processed/*
!data/raw/.gitkeep
!data/processed/.gitkeep

# Docker
docker-compose.yml
docker-compose.*.yml
Dockerfile*
.dockerignore

# Documentation
README.md
docs/
*.md
```

### TÃ¢che 4.3 : SÃ©curiser avec des secrets

CrÃ©ez un fichier `.env` pour les secrets (Ã  ne JAMAIS commiter) :

```env
# Database
POSTGRES_USER=dataeng
POSTGRES_PASSWORD=VotreSuperMotDePasseSecure123!
POSTGRES_DB=warehouse

# MongoDB
MONGO_ROOT_USERNAME=admin
MONGO_ROOT_PASSWORD=MongoSecure456!

# Airflow
AIRFLOW_FERNET_KEY=VotreCleFernet789==
AIRFLOW_SECRET_KEY=VotreCleSecrete012==

# Redis
REDIS_PASSWORD=RedisSecure345!
```

Modifiez votre `docker-compose.yml` pour utiliser les secrets :

```yaml
services:
  postgres:
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    # Ne jamais exposer directement en production
    # ports:
    #   - "5432:5432"
```

### TÃ¢che 4.4 : Ajouter des Health Checks partout

Ajoutez des health checks Ã  tous les services :

```yaml
postgres:
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
    interval: 10s
    timeout: 5s
    retries: 5
    start_period: 10s

mongodb:
  healthcheck:
    test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
    interval: 10s
    timeout: 5s
    retries: 5

redis:
  healthcheck:
    test: ["CMD", "redis-cli", "ping"]
    interval: 10s
    timeout: 3s
    retries: 5

api:
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 5s
    retries: 3
```

### TÃ¢che 4.5 : Limiter les ressources

Limitez les ressources pour chaque conteneur :

```yaml
services:
  postgres:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  etl:
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
```

**CritÃ¨res de rÃ©ussite :**
- âœ… Les images finales sont 50% plus petites
- âœ… Aucun secret n'est hardcodÃ© dans les fichiers
- âœ… Tous les services ont des health checks fonctionnels
- âœ… Les ressources sont limitÃ©es et contrÃ´lÃ©es

---

## ğŸ Partie 5 : Monitoring et Production (Bonus - 1h)

### TÃ¢che 5.1 : Ajouter Prometheus et Grafana

Ajoutez le monitoring avec Prometheus et Grafana.

**Service Prometheus :**
```yaml
prometheus:
  image: prom/prometheus:latest
  container_name: prometheus
  volumes:
    - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  ports:
    - "9090:9090"
  networks:
    - monitoring-network
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
```

**Fichier : `monitoring/prometheus.yml`**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'docker'
    static_configs:
      - targets: ['cadvisor:8080']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'api'
    static_configs:
      - targets: ['api:8000']
```

**Service Grafana :**
```yaml
grafana:
  image: grafana/grafana:latest
  container_name: grafana
  ports:
    - "3000:3000"
  environment:
    - GF_SECURITY_ADMIN_USER=admin
    - GF_SECURITY_ADMIN_PASSWORD=admin
  volumes:
    - grafana_data:/var/lib/grafana
  networks:
    - monitoring-network
```

### TÃ¢che 5.2 : Configurer les logs centralisÃ©s

Configurez un driver de logs :

```yaml
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    labels: "service"

services:
  etl:
    logging: *default-logging
```

### TÃ¢che 5.3 : CrÃ©er un script de backup

**Fichier : `scripts/backup.sh`**
```bash
#!/bin/bash

# Backup PostgreSQL
docker-compose exec -T postgres pg_dump -U dataeng warehouse > backups/db_$(date +%Y%m%d_%H%M%S).sql

# Backup MongoDB
docker-compose exec -T mongodb mongodump --out=/backup

# Backup volumes
docker run --rm -v docker-etl-project_postgres_data:/data -v $(pwd)/backups:/backup alpine tar czf /backup/postgres_volume_$(date +%Y%m%d_%H%M%S).tar.gz /data

echo "âœ… Backup terminÃ©"
```

---

## ğŸ“¤ Livrables Finaux

Ã€ la fin du brief, vous devez fournir :

### 1. Code Source Complet
```
docker-etl-project/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_dag.py
â”‚   â””â”€â”€ monitoring_dag.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ backup.sh
â”‚   â””â”€â”€ restore.sh
â””â”€â”€ README.md
```

### 2. Documentation (README.md)

Votre README doit contenir :
- Architecture globale de la solution
- PrÃ©requis et installation
- Guide de dÃ©marrage rapide
- Variables d'environnement
- Commandes utiles
- AccÃ¨s aux services (ports, URLs)
- Troubleshooting
- SchÃ©ma d'architecture

### 3. Preuves de Fonctionnement

- Captures d'Ã©cran de l'interface Airflow avec DAGs rÃ©ussis
- Captures de PgAdmin montrant les donnÃ©es
- Captures de l'API (Swagger UI)
- Logs du pipeline ETL
- RÃ©sultats des health checks
- MÃ©triques de performance (docker stats)

---

## âœ… CritÃ¨res d'Ã‰valuation

| CritÃ¨re | Points | Description |
|---------|--------|-------------|
| **Infrastructure Docker** | 20 | docker-compose.yml complet et fonctionnel |
| **Dockerfile optimisÃ©** | 15 | Multi-stage, sÃ©curisÃ©, minimal |
| **Pipeline ETL** | 20 | Extract, Transform, Load fonctionnels |
| **Orchestration Airflow** | 15 | DAGs fonctionnels et bien structurÃ©s |
| **API REST** | 10 | FastAPI avec endpoints fonctionnels |
| **Volumes & RÃ©seaux** | 10 | Persistance et isolation correctes |
| **SÃ©curitÃ©** | 10 | Secrets, non-root, health checks |
| **Documentation** | 10 | README complet et clair |
| **Bonus** | 10 | Monitoring, backups, optimisations |

**Total : 120 points (100 + 20 bonus)**

### BarÃ¨me de notation :
- **100-120 pts** : Excellent - Production ready
- **80-99 pts** : TrÃ¨s bien - Quelques amÃ©liorations mineures
- **60-79 pts** : Bien - Fonctionnel mais perfectible
- **< 60 pts** : Ã€ revoir - ProblÃ¨mes majeurs

---

## ğŸ’¡ Conseils et Astuces

### Debugging Docker

```bash
# Voir tous les conteneurs
docker ps -a

# Logs dÃ©taillÃ©s
docker-compose logs --tail=100 -f [service]

# Se connecter Ã  un conteneur
docker-compose exec [service] bash

# Inspecter un conteneur
docker inspect [container_id]

# Voir l'utilisation des ressources
docker stats

# Nettoyer
docker system prune -a --volumes
```

### VÃ©rifier la santÃ©

```bash
# Health check de tous les services
docker-compose ps

# Tester la connexion PostgreSQL
docker-compose exec postgres psql -U dataeng -d warehouse -c "SELECT version();"

# Tester MongoDB
docker-compose exec mongodb mongosh --eval "db.runCommand({ ping: 1 })"

# Tester Redis
docker-compose exec redis redis-cli ping

# Tester l'API
curl http://localhost:8000/health
```

### Optimisation

```bash
# Voir la taille des images
docker images

# Analyser les layers d'une image
docker history mon-image:tag

# Scanner les vulnÃ©rabilitÃ©s
docker scan mon-image:tag
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Documentation Officielle
- [Docker Documentation](https://docs.docker.com/)
- [Docker Compose Reference](https://docs.docker.com/compose/compose-file/)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [Dockerfile Best Practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)

### Outils Data Engineering
- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [PostgreSQL Docker Hub](https://hub.docker.com/_/postgres)
- [MongoDB Docker Hub](https://hub.docker.com/_/mongo)

### SÃ©curitÃ©
- [Docker Security Best Practices](https://docs.docker.com/engine/security/)
- [CIS Docker Benchmark](https://www.cisecurity.org/benchmark/docker)
- [Trivy - Scanner de vulnÃ©rabilitÃ©s](https://github.com/aquasecurity/trivy)

---

## ğŸ†˜ Troubleshooting

### ProblÃ¨me : Le conteneur PostgreSQL ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose logs postgres

# VÃ©rifier les permissions du volume
docker volume inspect docker-etl-project_postgres_data

# RecrÃ©er le volume
docker-compose down -v
docker-compose up -d postgres
```

### ProblÃ¨me : L'ETL ne peut pas se connecter Ã  PostgreSQL
```bash
# VÃ©rifier que PostgreSQL est prÃªt
docker-compose exec postgres pg_isready -U dataeng

# VÃ©rifier les variables d'environnement
docker-compose exec etl env | grep DATABASE

# Tester la connexion rÃ©seau
docker-compose exec etl ping postgres
```

### ProblÃ¨me : Airflow ne dÃ©marre pas
```bash
# RÃ©initialiser la base Airflow
docker-compose run --rm airflow-init

# VÃ©rifier la configuration
docker-compose config

# VÃ©rifier les logs du scheduler
docker-compose logs airflow-scheduler
```

### ProblÃ¨me : Manque d'espace disque
```bash
# Voir l'utilisation
docker system df

# Nettoyer
docker system prune -a --volumes

# Supprimer les images non utilisÃ©es
docker image prune -a
```

---

## ğŸ¯ Checklist Finale

Avant de soumettre votre travail, vÃ©rifiez que :

- [ ] `docker-compose up -d` dÃ©marre tous les services sans erreur
- [ ] PostgreSQL est accessible et contient les donnÃ©es
- [ ] MongoDB fonctionne et stocke les donnÃ©es brutes
- [ ] Redis cache les rÃ©sultats correctement
- [ ] L'API REST rÃ©pond sur http://localhost:8000
- [ ] Airflow UI est accessible sur http://localhost:8080
- [ ] Les DAGs s'exÃ©cutent avec succÃ¨s
- [ ] Les logs sont persistÃ©s
- [ ] Les volumes conservent les donnÃ©es aprÃ¨s un redÃ©marrage
- [ ] Aucun secret n'est hardcodÃ©
- [ ] Tous les services ont des health checks
- [ ] Les ressources sont limitÃ©es
- [ ] Le README est complet
- [ ] Les captures d'Ã©cran sont fournies
- [ ] Le code est commentÃ©

---

**Bon courage ! ğŸš€**

Ce brief vous permettra de maÃ®triser Docker pour le Data Engineering de A Ã  Z. N'hÃ©sitez pas Ã  expÃ©rimenter et Ã  aller au-delÃ  des exigences !
