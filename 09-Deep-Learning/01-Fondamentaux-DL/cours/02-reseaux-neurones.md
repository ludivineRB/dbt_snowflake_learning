# Chapitre 2 : RÃ©seaux de Neurones â€“ Le Moteur du Deep Learning

## ğŸ¯ Objectifs

- Comprendre la forward propagation (flux de donnÃ©es dans le rÃ©seau)
- MaÃ®triser les fonctions d'activation et savoir quand les utiliser
- Comprendre les fonctions de coÃ»t (loss functions) et leur importance
- Comprendre la backpropagation et le gradient descent
- Savoir choisir un optimizer et monitorer l'entraÃ®nement
- Diagnostiquer les problÃ¨mes d'entraÃ®nement (overfitting, underfitting, oscillations)

---

## 1. ğŸ”„ Forward Propagation

### 1.1 Le flux de donnÃ©es

La forward propagation est le processus par lequel les donnÃ©es traversent le rÃ©seau de l'entrÃ©e vers la sortie :

```
EntrÃ©e (X)     Couche 1        Couche 2        Sortie
  [x1]    â†’   [h1]   â†’       [h1']   â†’       [y1]
  [x2]    â†’   [h2]   â†’       [h2']   â†’       [y2]
  [x3]    â†’   [h3]   â†’       [h3']
  [x4]    â†’   [h4]

         Z1 = W1Â·X + b1    Z2 = W2Â·A1 + b2
         A1 = f(Z1)        A2 = f(Z2)         Å· = A2
```

### 1.2 Calcul matriciel

Pour chaque couche, le calcul est :

```
Z = W Â· X + b       â† combinaison linÃ©aire
A = activation(Z)   â† transformation non-linÃ©aire
```

OÃ¹ :
- **W** : matrice de poids (shape: [neurones_sortie, neurones_entrÃ©e])
- **X** : vecteur d'entrÃ©e (ou matrice pour un batch)
- **b** : vecteur de biais
- **Z** : prÃ©-activation (avant la fonction d'activation)
- **A** : activation (sortie de la couche)

### 1.3 ImplÃ©mentation from scratch

```python
import numpy as np

class ReseauSimple:
    """RÃ©seau de neurones Ã  2 couches, implÃ©mentÃ© from scratch"""

    def __init__(self, n_entrees, n_cachees, n_sorties):
        # Initialisation des poids (Xavier initialization)
        self.W1 = np.random.randn(n_cachees, n_entrees) * np.sqrt(2.0 / n_entrees)
        self.b1 = np.zeros((n_cachees, 1))
        self.W2 = np.random.randn(n_sorties, n_cachees) * np.sqrt(2.0 / n_cachees)
        self.b2 = np.zeros((n_sorties, 1))

    def relu(self, z):
        """Fonction d'activation ReLU"""
        return np.maximum(0, z)

    def sigmoid(self, z):
        """Fonction d'activation Sigmoid"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    def forward(self, X):
        """Forward propagation Ã  travers le rÃ©seau"""
        # Couche 1 : entrÃ©e â†’ couche cachÃ©e
        self.Z1 = self.W1 @ X + self.b1      # Combinaison linÃ©aire
        self.A1 = self.relu(self.Z1)           # Activation ReLU

        # Couche 2 : couche cachÃ©e â†’ sortie
        self.Z2 = self.W2 @ self.A1 + self.b2  # Combinaison linÃ©aire
        self.A2 = self.sigmoid(self.Z2)         # Activation Sigmoid (sortie)

        return self.A2

# Exemple : rÃ©seau pour classification binaire
reseau = ReseauSimple(n_entrees=4, n_cachees=8, n_sorties=1)

# DonnÃ©es d'entrÃ©e (4 features, 1 sample)
X = np.array([[0.5], [0.3], [-0.1], [0.8]])
prediction = reseau.forward(X)
print(f"PrÃ©diction : {prediction[0, 0]:.4f}")  # ProbabilitÃ© entre 0 et 1
```

> ğŸ’¡ **Conseil** : Ce code est pÃ©dagogique. En pratique, PyTorch fait tout cela en quelques lignes. Mais comprendre le calcul matriciel sous-jacent est essentiel pour diagnostiquer les problÃ¨mes.

---

## 2. âš¡ Fonctions d'activation

Les fonctions d'activation introduisent la **non-linÃ©aritÃ©** dans le rÃ©seau. Sans elles, un rÃ©seau multicouche serait Ã©quivalent Ã  un seul neurone linÃ©aire (composition de fonctions linÃ©aires = fonction linÃ©aire).

### 2.1 Sigmoid (Ïƒ)

```
Ïƒ(z) = 1 / (1 + e^(-z))

Sortie : [0, 1]

        1 |          ___________
          |        /
     0.5 |------/
          |    /
        0 |___/
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           -6    0    6
```

**PropriÃ©tÃ©s :**
- Sortie bornÃ©e entre 0 et 1 â†’ interprÃ©table comme une probabilitÃ©
- Gradient max = 0.25 (en z=0)
- UtilisÃ©e principalement en **couche de sortie** pour la classification binaire

**ProblÃ¨me majeur : Vanishing Gradient**
- Pour |z| > 5, le gradient est quasiment nul (~0)
- Les couches profondes reÃ§oivent un signal d'apprentissage trÃ¨s faible
- Le rÃ©seau arrÃªte d'apprendre dans ses premiÃ¨res couches

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivee(z):
    s = sigmoid(z)
    return s * (1 - s)

# DÃ©monstration du vanishing gradient
print(f"Gradient en z=0  : {sigmoid_derivee(0):.4f}")   # 0.25
print(f"Gradient en z=5  : {sigmoid_derivee(5):.6f}")   # 0.0066
print(f"Gradient en z=10 : {sigmoid_derivee(10):.8f}")  # 0.0000454
# â†’ Le gradient "disparaÃ®t" pour les grandes valeurs
```

### 2.2 Tanh (Tangente hyperbolique)

```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))

Sortie : [-1, 1]

        1 |          ___________
          |        /
        0 |------/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          |    /
       -1 |___/
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           -6    0    6
```

**PropriÃ©tÃ©s :**
- Sortie centrÃ©e sur 0 (contrairement Ã  sigmoid)
- Gradient max = 1 (en z=0), meilleur que sigmoid
- Souffre aussi du vanishing gradient mais moins que sigmoid
- UtilisÃ©e dans certaines architectures (LSTM, normalisation)

### 2.3 ReLU (Rectified Linear Unit)

```
ReLU(z) = max(0, z)

Sortie : [0, +âˆ)

          |       /
          |      /
          |     /
          |    /
        0 |___/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           -4    0    4
```

**PropriÃ©tÃ©s :**
- ExtrÃªmement simple et rapide Ã  calculer
- Pas de vanishing gradient pour z > 0
- **Standard de facto** pour les couches cachÃ©es depuis 2012
- Gradient = 1 pour z > 0, gradient = 0 pour z < 0

**ProblÃ¨me : Dying ReLU**
- Si z < 0, le gradient est exactement 0
- Un neurone "mort" ne peut plus jamais se rÃ©activer
- Peut arriver avec un learning rate trop Ã©levÃ©

```python
import numpy as np

def relu(z):
    return np.maximum(0, z)

def relu_derivee(z):
    return (z > 0).astype(float)

# DÃ©monstration
z = np.array([-3, -1, 0, 1, 3])
print(f"ReLU({z})    = {relu(z)}")          # [0, 0, 0, 1, 3]
print(f"ReLU'({z})   = {relu_derivee(z)}")  # [0, 0, 0, 1, 1]
```

### 2.4 Leaky ReLU

```
LeakyReLU(z) = z     si z > 0
               Î±Â·z   si z â‰¤ 0   (Î± = 0.01 typiquement)

Sortie : (-âˆ, +âˆ)

          |       /
          |      /
          |     /
          |    /
        0 |___/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          â””â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          (pente trÃ¨s faible pour z < 0)
```

**PropriÃ©tÃ©s :**
- RÃ©sout le problÃ¨me du dying ReLU
- Permet un petit gradient mÃªme pour z < 0
- Variante : **Parametric ReLU (PReLU)** oÃ¹ Î± est appris

### 2.5 Softmax

```
Softmax(zi) = e^zi / Î£(e^zj)

EntrÃ©e : vecteur de scores â†’ Sortie : vecteur de probabilitÃ©s (somme = 1)
```

**PropriÃ©tÃ©s :**
- Transforme des scores en **distribution de probabilitÃ©s**
- UtilisÃ©e exclusivement en **couche de sortie** pour la classification multi-classes
- Chaque sortie âˆˆ [0, 1] et la somme = 1

```python
import numpy as np

def softmax(z):
    # Trick de stabilitÃ© numÃ©rique : soustraire le max
    z_stable = z - np.max(z)
    exp_z = np.exp(z_stable)
    return exp_z / np.sum(exp_z)

# Exemple : classification en 3 classes
scores = np.array([2.0, 1.0, 0.1])
probabilites = softmax(scores)
print(f"Scores  : {scores}")
print(f"Probas  : {probabilites.round(4)}")
# [0.6590, 0.2424, 0.0986] â†’ somme = 1.0
print(f"Somme   : {probabilites.sum():.4f}")
print(f"Classe prÃ©dite : {np.argmax(probabilites)}")  # Classe 0
```

### 2.6 Table rÃ©capitulative

| Activation | Range | Usage | Avantages | InconvÃ©nients |
|-----------|-------|-------|-----------|---------------|
| **Sigmoid** | [0, 1] | Sortie (binaire) | ProbabilitÃ© | Vanishing gradient |
| **Tanh** | [-1, 1] | Couches cachÃ©es (rare) | CentrÃ©e sur 0 | Vanishing gradient |
| **ReLU** | [0, +âˆ) | Couches cachÃ©es | Simple, rapide, pas de vanishing | Dying ReLU |
| **Leaky ReLU** | (-âˆ, +âˆ) | Couches cachÃ©es | Pas de dying ReLU | Un hyperparamÃ¨tre (Î±) |
| **Softmax** | [0, 1] (somme=1) | Sortie (multi-classes) | ProbabilitÃ©s | Seulement en sortie |

### 2.7 RÃ¨gles de choix

```
Couches cachÃ©es â†’ ReLU (par dÃ©faut)
                  Leaky ReLU (si problÃ¨me de dying ReLU)

Couche de sortie â†’ Sigmoid    (classification binaire)
                   Softmax    (classification multi-classes)
                   LinÃ©aire   (rÃ©gression, pas d'activation)
                   Tanh       (sortie bornÃ©e [-1, 1])
```

> ğŸ’¡ **Conseil de pro** : En 2024, utilisez **ReLU par dÃ©faut** pour les couches cachÃ©es et **Softmax** pour la sortie en classification multi-classes. Ne cherchez pas Ã  optimiser la fonction d'activation avant d'avoir optimisÃ© le learning rate et l'architecture.

---

## 3. ğŸ“Š Fonctions de coÃ»t (Loss Functions)

La fonction de coÃ»t (loss function) mesure l'**Ã©cart entre la prÃ©diction du modÃ¨le et la rÃ©alitÃ©**. C'est la mÃ©trique que le modÃ¨le cherche Ã  minimiser pendant l'entraÃ®nement.

> ğŸ’¡ **Conseil de pro** : La loss function est votre **boussole**. Si elle ne reflÃ¨te pas le problÃ¨me mÃ©tier, le modÃ¨le apprendra la mauvaise chose. Choisissez-la avec soin.

### 3.1 MSE â€“ Mean Squared Error (RÃ©gression)

```
MSE = (1/n) Ã— Î£(yi - Å·i)Â²

yi = valeur rÃ©elle
Å·i = prÃ©diction du modÃ¨le
```

**PropriÃ©tÃ©s :**
- PÃ©nalise fortement les grandes erreurs (erreur au carrÃ©)
- Toujours positive, minimum = 0 (prÃ©diction parfaite)
- Sensible aux outliers (une erreur de 10 â†’ pÃ©nalitÃ© de 100)

```python
import numpy as np

def mse(y_vrai, y_pred):
    """Mean Squared Error"""
    return np.mean((y_vrai - y_pred) ** 2)

# Exemple
y_vrai = np.array([3.0, 5.0, 2.5, 7.0])
y_pred = np.array([2.8, 5.2, 2.3, 6.5])
print(f"MSE : {mse(y_vrai, y_pred):.4f}")  # 0.0850

# Impact d'un outlier
y_pred_outlier = np.array([2.8, 5.2, 2.3, 2.0])  # Erreur de 5 sur la derniÃ¨re
print(f"MSE avec outlier : {mse(y_vrai, y_pred_outlier):.4f}")  # 6.3325
# â†’ L'outlier fait exploser la MSE !
```

**Variantes :**
- **MAE** (Mean Absolute Error) : `(1/n) Ã— Î£|yi - Å·i|` â€” moins sensible aux outliers
- **Huber Loss** : combinaison MSE/MAE, robuste aux outliers
- **RMSE** : racine de la MSE, mÃªme unitÃ© que les donnÃ©es

### 3.2 Binary Cross-Entropy (Classification binaire)

```
BCE = -(1/n) Ã— Î£[yiÂ·log(Å·i) + (1-yi)Â·log(1-Å·i)]

yi âˆˆ {0, 1}     â†’ classe rÃ©elle
Å·i âˆˆ (0, 1)     â†’ probabilitÃ© prÃ©dite
```

**Intuition :**
- Si la classe rÃ©elle est 1, on veut que Å· soit proche de 1 â†’ `log(Å·)` est maximisÃ©
- Si la classe rÃ©elle est 0, on veut que Å· soit proche de 0 â†’ `log(1-Å·)` est maximisÃ©
- La loss pÃ©nalise exponentiellement les **prÃ©dictions confiantes mais fausses**

```python
import numpy as np

def binary_cross_entropy(y_vrai, y_pred):
    """Binary Cross-Entropy Loss"""
    # Clip pour Ã©viter log(0)
    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
    return -np.mean(
        y_vrai * np.log(y_pred) + (1 - y_vrai) * np.log(1 - y_pred)
    )

# Bonnes prÃ©dictions
y_vrai = np.array([1, 0, 1, 1])
y_pred_bon = np.array([0.9, 0.1, 0.8, 0.95])
print(f"BCE (bon modÃ¨le)    : {binary_cross_entropy(y_vrai, y_pred_bon):.4f}")

# Mauvaises prÃ©dictions
y_pred_mauvais = np.array([0.2, 0.8, 0.3, 0.4])
print(f"BCE (mauvais modÃ¨le): {binary_cross_entropy(y_vrai, y_pred_mauvais):.4f}")

# PrÃ©diction confiante et FAUSSE (la pire situation)
y_pred_confiant_faux = np.array([0.01, 0.99, 0.01, 0.01])
print(f"BCE (confiant faux) : {binary_cross_entropy(y_vrai, y_pred_confiant_faux):.4f}")
# â†’ Loss trÃ¨s Ã©levÃ©e ! Le modÃ¨le est pÃ©nalisÃ© pour Ãªtre confiant et faux
```

### 3.3 Categorical Cross-Entropy (Classification multi-classes)

```
CCE = -(1/n) Ã— Î£ Î£(yij Â· log(Å·ij))

yij = 1 si le sample i appartient Ã  la classe j (one-hot)
Å·ij = probabilitÃ© prÃ©dite pour la classe j (sortie softmax)
```

```python
import numpy as np

def categorical_cross_entropy(y_vrai_onehot, y_pred):
    """Categorical Cross-Entropy Loss"""
    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
    return -np.mean(np.sum(y_vrai_onehot * np.log(y_pred), axis=1))

# 3 classes, 2 samples
y_vrai = np.array([[1, 0, 0],   # Classe 0
                    [0, 0, 1]])  # Classe 2

y_pred_bon = np.array([[0.8, 0.1, 0.1],   # Bonne prÃ©diction classe 0
                        [0.1, 0.1, 0.8]])  # Bonne prÃ©diction classe 2

y_pred_mauvais = np.array([[0.2, 0.5, 0.3],   # Mauvaise prÃ©diction
                            [0.5, 0.3, 0.2]])  # Mauvaise prÃ©diction

print(f"CCE (bon)     : {categorical_cross_entropy(y_vrai, y_pred_bon):.4f}")
print(f"CCE (mauvais) : {categorical_cross_entropy(y_vrai, y_pred_mauvais):.4f}")
```

### 3.4 Comment choisir sa loss function

| ProblÃ¨me | Loss Function | Activation de sortie | PyTorch |
|----------|---------------|---------------------|---------|
| **RÃ©gression** | MSE | LinÃ©aire (aucune) | `nn.MSELoss()` |
| **RÃ©gression robuste** | Huber / L1 | LinÃ©aire | `nn.HuberLoss()` |
| **Classification binaire** | Binary Cross-Entropy | Sigmoid | `nn.BCEWithLogitsLoss()` |
| **Classification multi-classes** | Categorical Cross-Entropy | Softmax | `nn.CrossEntropyLoss()` |
| **Multi-label** | Binary Cross-Entropy | Sigmoid (par label) | `nn.BCEWithLogitsLoss()` |
| **Segmentation** | Dice Loss + CE | Softmax | Custom |

> âš ï¸ **Attention** : En PyTorch, `nn.CrossEntropyLoss()` **inclut dÃ©jÃ  le Softmax**. Ne mettez PAS de Softmax dans votre rÃ©seau si vous utilisez cette loss. C'est une erreur trÃ¨s courante chez les dÃ©butants.

---

## 4. ğŸ”™ Backpropagation

### 4.1 Le principe

La backpropagation (rÃ©tropropagation) est l'algorithme qui permet au rÃ©seau d'**apprendre**. Il propage l'erreur de la sortie vers l'entrÃ©e pour ajuster les poids :

```
Forward (â†’) : DonnÃ©es â†’ Couche 1 â†’ Couche 2 â†’ ... â†’ PrÃ©diction â†’ Loss
Backward (â†): âˆ‚Loss/âˆ‚W â† Couche 1 â† Couche 2 â† ... â† âˆ‚Loss/âˆ‚Å· â† Loss

1. Forward : calculer la prÃ©diction
2. Calculer la loss (Ã©cart prÃ©diction vs rÃ©alitÃ©)
3. Backward : calculer les gradients (âˆ‚Loss/âˆ‚Wi pour chaque poids)
4. Mettre Ã  jour les poids : Wi = Wi - lr Ã— âˆ‚Loss/âˆ‚Wi
```

### 4.2 La rÃ¨gle de la chaÃ®ne (Chain Rule)

Le coeur mathÃ©matique de la backpropagation est la **rÃ¨gle de la chaÃ®ne** :

```
Si y = f(g(x)), alors dy/dx = f'(g(x)) Ã— g'(x)

Pour un rÃ©seau :
âˆ‚Loss/âˆ‚W1 = âˆ‚Loss/âˆ‚A2 Ã— âˆ‚A2/âˆ‚Z2 Ã— âˆ‚Z2/âˆ‚A1 Ã— âˆ‚A1/âˆ‚Z1 Ã— âˆ‚Z1/âˆ‚W1
```

Chaque couche multiplie son gradient local par le gradient reÃ§u de la couche suivante.

### 4.3 Exemple pas Ã  pas

Prenons un rÃ©seau minimal : 1 neurone, 1 entrÃ©e, MSE loss.

```python
import numpy as np

# RÃ©seau : 1 neurone (1 poids, 1 biais, sigmoid)
# Forward : Å· = sigmoid(wÂ·x + b)
# Loss : L = (y - Å·)Â²

# DonnÃ©es
x = 1.0       # EntrÃ©e
y = 0.0       # Cible (classe 0)
w = 0.5       # Poids initial
b = 0.1       # Biais initial
lr = 0.1      # Learning rate

print("=== EntraÃ®nement pas Ã  pas ===")
for step in range(5):
    # --- FORWARD ---
    z = w * x + b                        # PrÃ©-activation
    y_pred = 1 / (1 + np.exp(-z))       # Sigmoid
    loss = (y - y_pred) ** 2             # MSE

    # --- BACKWARD (calcul des gradients) ---
    # âˆ‚L/âˆ‚Å· = 2(Å· - y)
    dL_dy_pred = 2 * (y_pred - y)

    # âˆ‚Å·/âˆ‚z = sigmoid(z) Ã— (1 - sigmoid(z))
    dy_pred_dz = y_pred * (1 - y_pred)

    # âˆ‚z/âˆ‚w = x,  âˆ‚z/âˆ‚b = 1
    dz_dw = x
    dz_db = 1

    # Chain rule : âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚Å· Ã— âˆ‚Å·/âˆ‚z Ã— âˆ‚z/âˆ‚w
    dL_dw = dL_dy_pred * dy_pred_dz * dz_dw
    dL_db = dL_dy_pred * dy_pred_dz * dz_db

    # --- MISE A JOUR ---
    w = w - lr * dL_dw
    b = b - lr * dL_db

    print(f"Step {step}: loss={loss:.4f}, Å·={y_pred:.4f}, w={w:.4f}, b={b:.4f}")
```

### 4.4 SchÃ©ma du flux Forward / Backward

```
         FORWARD â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’â†’
         â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
  X â”€â”€â”€â†’ â”‚ WÂ·X â”‚ â”€â”€â†’ â”‚  + bias â”‚ â”€â”€â†’ â”‚  ReLU   â”‚ â”€â”€â†’ â”‚ Loss â”‚
         â”‚ +b  â”‚     â”‚         â”‚     â”‚         â”‚     â”‚      â”‚
         â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
         â”‚ âˆ‚Z/âˆ‚Wâ”‚     â”‚         â”‚     â”‚ âˆ‚A/âˆ‚Z   â”‚     â”‚âˆ‚L/âˆ‚A â”‚
         â†â†â†â†â†â†â†     â†â†â†â†â†â†â†â†â†â†     â†â†â†â†â†â†â†â†â†â†     â†â†â†â†â†â†â†
         â†â†â†â†â†â† BACKWARD â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†
```

> ğŸ’¡ **Conseil** : Vous n'avez **pas besoin** de calculer les gradients Ã  la main. PyTorch le fait automatiquement avec **autograd**. Mais comprendre le principe est essentiel pour diagnostiquer les problÃ¨mes (vanishing gradients, exploding gradients).

---

## 5. ğŸ“‰ Gradient Descent et variantes

### 5.1 Le principe du Gradient Descent

Le gradient descent (descente de gradient) est l'algorithme d'optimisation qui met Ã  jour les poids :

```
Nouveau poids = Ancien poids - learning_rate Ã— gradient

W = W - Î· Ã— âˆ‚Loss/âˆ‚W
```

**Analogie :** Imaginez descendre une montagne dans le brouillard. Le gradient vous indique la direction de la pente la plus raide. Le learning rate est la taille de vos pas.

```
        Loss
     â•±â•²
    â•±  â•²         â† lr trop grand : on oscille
   â•±    â•²  â•±â•²
  â•±      â•²â•±  â•²
 â•±    â—â†’â†’â†’â†’â†’â†’â†’â—  â† lr correct : on converge
â•±               â•²
                 minimum
```

### 5.2 Batch GD vs SGD vs Mini-batch

| Variante | DonnÃ©es par mise Ã  jour | Avantages | InconvÃ©nients |
|----------|------------------------|-----------|---------------|
| **Batch GD** | Toutes les donnÃ©es | Stable, convergence garantie | Lent, mÃ©moire Ã©norme |
| **SGD** | 1 sample | Rapide, peut sortir des minima locaux | TrÃ¨s bruitÃ© |
| **Mini-batch SGD** | 32-256 samples | Bon compromis | HyperparamÃ¨tre (batch_size) |

> ğŸ’¡ **Conseil** : En pratique, on utilise toujours le **mini-batch SGD** avec un batch_size entre 32 et 256. Le terme "SGD" dans les frameworks dÃ©signe en rÃ©alitÃ© le mini-batch SGD.

### 5.3 SGD avec Momentum

Le momentum accÃ©lÃ¨re la convergence en ajoutant une "inertie" au mouvement :

```
v = Î² Ã— v_precedent + âˆ‚Loss/âˆ‚W    (accumulation du moment)
W = W - Î· Ã— v                      (mise Ã  jour avec moment)
```

Comme une bille qui roule : elle accumule de la vitesse dans les directions cohÃ©rentes et amortit les oscillations.

```python
# SGD avec Momentum en PyTorch
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

### 5.4 Adam (Adaptive Moment Estimation)

Adam est le **standard actuel** pour l'entraÃ®nement de rÃ©seaux de neurones. Il combine :
- **Momentum** (moyenne mobile des gradients)
- **RMSprop** (adaptation du learning rate par paramÃ¨tre)

```
m = Î²1 Ã— m + (1-Î²1) Ã— gradient          â† Moment 1 (direction)
v = Î²2 Ã— v + (1-Î²2) Ã— gradientÂ²         â† Moment 2 (magnitude)
W = W - Î· Ã— m / (âˆšv + Îµ)                â† Mise Ã  jour adaptative
```

**Pourquoi Adam est populaire :**
- Learning rate adaptatif pour chaque paramÃ¨tre
- Fonctionne bien "out of the box" avec les hyperparamÃ¨tres par dÃ©faut
- Converge gÃ©nÃ©ralement plus vite que SGD

```python
# Adam en PyTorch â€” le standard
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# Variante : AdamW (Adam avec Weight Decay correct)
optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
```

> ğŸ’¡ **Conseil de pro** : Commencez **TOUJOURS** avec Adam et `lr=1e-3`. C'est la baseline universelle. Ne passez Ã  SGD+Momentum que si vous avez le temps de tuner et que vous visez les derniers % de performance.

### 5.5 Learning Rate Scheduling

Le learning rate peut varier pendant l'entraÃ®nement pour amÃ©liorer la convergence :

| Scheduler | Principe | PyTorch | Quand l'utiliser |
|-----------|----------|---------|------------------|
| **StepLR** | Divise le lr tous les N epochs | `StepLR(optimizer, step_size=30, gamma=0.1)` | Baseline simple |
| **CosineAnnealing** | DÃ©croissance en cosinus | `CosineAnnealingLR(optimizer, T_max=100)` | EntraÃ®nement long |
| **ReduceLROnPlateau** | RÃ©duit si la mÃ©trique stagne | `ReduceLROnPlateau(optimizer, patience=10)` | Choix le plus sÃ»r |
| **OneCycleLR** | Augmente puis diminue | `OneCycleLR(optimizer, max_lr=0.01)` | Performance maximale |
| **Warmup** | Commence petit puis augmente | Custom ou transformers | Transformers |

```python
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

optimizer = optim.Adam(model.parameters(), lr=1e-3)

# RÃ©duire le lr si la val_loss ne s'amÃ©liore pas pendant 5 epochs
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',        # On minimise la loss
    factor=0.5,        # Diviser le lr par 2
    patience=5,        # Attendre 5 epochs sans amÃ©lioration
    verbose=True       # Afficher quand le lr change
)

# Dans la boucle d'entraÃ®nement :
# scheduler.step(val_loss)
```

### 5.6 Table comparative des optimizers

| Optimizer | Learning Rate | Convergence | MÃ©moire | Usage recommandÃ© |
|-----------|--------------|-------------|---------|-------------------|
| **SGD** | Fixe | Lent mais prÃ©cis | Faible | Fine-tuning final |
| **SGD + Momentum** | Fixe | Plus rapide | Faible | CNN classiques |
| **Adam** | Adaptatif | Rapide | 2Ã— SGD | Baseline universelle |
| **AdamW** | Adaptatif + WD | Rapide | 2Ã— SGD | Standard actuel |
| **LAMB** | Adaptatif | TrÃ¨s rapide | 2Ã— SGD | Grands batch sizes |

---

## 6. ğŸ“Š MÃ©triques et monitoring de l'entraÃ®nement

### 6.1 Les courbes de loss : votre tableau de bord

Les courbes de loss (train vs validation) sont le **diagnostic principal** de votre entraÃ®nement.

### 6.2 Cas 1 : Underfitting

```
Loss
  â”‚
  â”‚ â”€â”€â”€â”€ Train loss (haute, ne descend plus)
  â”‚ â”€â”€â”€â”€ Val loss (haute, ne descend plus)
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs

Diagnostic : le modÃ¨le est trop simple ou le lr trop faible
Solutions :
  â†’ Augmenter la capacitÃ© du rÃ©seau (plus de couches/neurones)
  â†’ Augmenter le learning rate
  â†’ EntraÃ®ner plus longtemps
  â†’ VÃ©rifier les donnÃ©es
```

### 6.3 Cas 2 : Bon fit

```
Loss
  â”‚â•²
  â”‚ â•²â”€â”€â”€â”€ Train loss (descend)
  â”‚  â•²â”€â”€â”€ Val loss (descend, proche du train)
  â”‚   â•²â”€â”€
  â”‚    â•²â”€â”€â”€ Les deux convergent vers un plateau bas
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs

Diagnostic : le modÃ¨le apprend bien et gÃ©nÃ©ralise
Action : tout va bien ! Continuez l'entraÃ®nement jusqu'au plateau
```

### 6.4 Cas 3 : Overfitting

```
Loss
  â”‚
  â”‚     â•±â”€â”€ Val loss (remonte !)
  â”‚   â•±
  â”‚  â•±â”€â”€â”€â”€â”€â”€ Val loss plateau
  â”‚ â•±
  â”‚â•²â”€â”€â”€â”€â”€â”€â”€â”€ Train loss (continue de baisser)
  â”‚ â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs

Diagnostic : le modÃ¨le mÃ©morise les donnÃ©es d'entraÃ®nement
Solutions :
  â†’ Dropout, Weight Decay (rÃ©gularisation)
  â†’ Data Augmentation
  â†’ Early Stopping
  â†’ RÃ©duire la capacitÃ© du rÃ©seau
  â†’ Plus de donnÃ©es
```

### 6.5 Cas 4 : Oscillations / InstabilitÃ©

```
Loss
  â”‚
  â”‚  â•±â•²  â•±â•²  â•±â•²
  â”‚ â•±  â•²â•±  â•²â•±  â•²  â† Oscillations violentes
  â”‚â•±              â•²
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs

Diagnostic : learning rate trop Ã©levÃ©
Solutions :
  â†’ Diviser le learning rate par 10
  â†’ Utiliser un scheduler (ReduceLROnPlateau)
  â†’ Gradient clipping
```

### 6.6 Cas 5 : Loss explose (NaN)

```
Loss
  â”‚           â•±
  â”‚          â•± â†’ NaN !
  â”‚         â•±
  â”‚        â•±
  â”‚â”€â”€â”€â”€â”€â”€â”€â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs

Diagnostic : exploding gradients
Solutions :
  â†’ RÃ©duire drastiquement le learning rate
  â†’ Gradient clipping : torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
  â†’ VÃ©rifier les donnÃ©es (valeurs aberrantes ?)
  â†’ VÃ©rifier l'initialisation des poids
```

> ğŸ’¡ **Conseil** : Si la loss de validation remonte alors que la loss d'entraÃ®nement continue de baisser, c'est de l'**overfitting**. Il est temps d'appliquer de la rÃ©gularisation (dropout, weight decay, early stopping).

### 6.7 MÃ©triques complÃ©mentaires Ã  surveiller

| MÃ©trique | Ce qu'elle mesure | Quand la surveiller |
|----------|-------------------|---------------------|
| **Train Loss** | CapacitÃ© d'apprentissage | Toujours |
| **Val Loss** | CapacitÃ© de gÃ©nÃ©ralisation | Toujours |
| **Train Accuracy** | Performance sur le train | Classification |
| **Val Accuracy** | Performance rÃ©elle | Classification |
| **Learning Rate** | Vitesse d'apprentissage actuelle | Si scheduler actif |
| **Gradient Norm** | Magnitude des gradients | Si instabilitÃ© |
| **Poids** | Distribution des paramÃ¨tres | Debug avancÃ© |

```python
# Template de logging minimal pour chaque epoch
def afficher_metriques(epoch, train_loss, val_loss, train_acc, val_acc):
    ecart = val_loss - train_loss
    status = ""
    if ecart > 0.5:
        status = "âš ï¸ OVERFITTING"
    elif train_loss > 1.0 and epoch > 20:
        status = "âš ï¸ UNDERFITTING"
    else:
        status = "âœ… OK"

    print(f"Epoch {epoch:3d} | "
          f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | "
          f"Train Acc: {train_acc:.2%} | Val Acc: {val_acc:.2%} | "
          f"Ã‰cart: {ecart:.4f} | {status}")
```

---

## ğŸ“ Points clÃ©s Ã  retenir

- La **forward propagation** calcule la sortie : Z = WÂ·X + b, A = activation(Z)
- Les **fonctions d'activation** apportent la non-linÃ©aritÃ© : ReLU (couches cachÃ©es), Sigmoid/Softmax (sortie)
- La **loss function** mesure l'erreur : MSE (rÃ©gression), Cross-Entropy (classification)
- La **backpropagation** calcule les gradients via la rÃ¨gle de la chaÃ®ne
- Le **gradient descent** met Ã  jour les poids : W = W - lr Ã— gradient
- **Adam** est l'optimizer standard (lr=1e-3 comme point de dÃ©part)
- Les **courbes de loss** (train vs val) sont votre outil de diagnostic principal

## âœ… Checklist de validation

- [ ] Je peux expliquer le flux forward propagation (Z â†’ A)
- [ ] Je sais choisir la bonne activation pour chaque couche
- [ ] Je connais le problÃ¨me du vanishing gradient et comment l'Ã©viter
- [ ] Je sais choisir la bonne loss function selon le type de problÃ¨me
- [ ] Je comprends la backpropagation (intuitivement, pas le calcul exact)
- [ ] Je connais la diffÃ©rence entre SGD, Momentum et Adam
- [ ] Je sais diagnostiquer overfitting / underfitting sur les courbes de loss
- [ ] `nn.CrossEntropyLoss()` en PyTorch inclut dÃ©jÃ  le Softmax

---

**Chapitre prÃ©cÃ©dent :** [01 - Introduction au Deep Learning](./01-introduction-deep-learning.md)
**Prochain chapitre :** [03 - PyTorch](./03-frameworks-pytorch.md)

[Retour au sommaire](../README.md)
