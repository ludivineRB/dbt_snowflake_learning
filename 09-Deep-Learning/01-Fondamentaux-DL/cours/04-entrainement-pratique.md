# Chapitre 4 : Entra√Ænement Pratique ‚Äì Les Secrets d'un Bon Mod√®le

## üéØ Objectifs

- Ma√Ætriser les hyperparam√®tres d'entra√Ænement (learning rate, batch size, epochs)
- D√©tecter et combattre l'overfitting (Dropout, Weight Decay, Data Augmentation, Early Stopping)
- Utiliser le Transfer Learning pour des r√©sultats rapides avec peu de donn√©es
- Monitorer les gradients et diagnostiquer les probl√®mes avanc√©s
- D√©velopper une m√©thodologie rigoureuse et reproductible
- √âviter les erreurs courantes des d√©butants

---

## 1. ‚öôÔ∏è Hyperparam√®tres d'entra√Ænement

### 1.1 Learning Rate : l'hyperparam√®tre #1

Le learning rate (taux d'apprentissage) contr√¥le la **taille des pas** lors de la descente de gradient. C'est de loin l'hyperparam√®tre le plus important.

```
Trop grand (lr=0.1)         Correct (lr=0.001)        Trop petit (lr=0.000001)

Loss                        Loss                       Loss
 ‚îÇ ‚ï±‚ï≤  ‚ï±‚ï≤  ‚ï±‚ï≤              ‚îÇ‚ï≤                          ‚îÇ‚ï≤
 ‚îÇ‚ï±  ‚ï≤‚ï±  ‚ï≤‚ï±  ‚ï≤             ‚îÇ ‚ï≤                         ‚îÇ ‚ï≤
 ‚îÇ         ‚ï≤  ‚Üí NaN!        ‚îÇ  ‚ï≤                        ‚îÇ  ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 ‚îÇ                          ‚îÇ   ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minimum           ‚îÇ   (descente ultra lente)
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs

 Diverge ou oscille         Converge bien              Converge mais beaucoup
                                                       trop lentement
```

**R√®gles pratiques :**

| Learning Rate | Quand l'utiliser |
|--------------|------------------|
| `1e-2` (0.01) | SGD sur mod√®les simples |
| `1e-3` (0.001) | Adam, baseline universelle |
| `1e-4` (0.0001) | Fine-tuning de mod√®les pr√©-entra√Æn√©s |
| `1e-5` (0.00001) | Fine-tuning de grands Transformers (BERT, GPT) |

> üí° **Conseil de pro** : Le learning rate est l'hyperparam√®tre #1. Passez plus de temps √† le tuner qu'√† changer l'architecture. Un bon lr peut transformer un mod√®le m√©diocre en bon mod√®le.

### 1.2 La technique du Learning Rate Finder

Plut√¥t que de deviner le learning rate, on peut le trouver automatiquement :

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

def trouver_learning_rate(model, train_loader, device,
                           lr_min=1e-7, lr_max=1.0, num_steps=100):
    """
    Technique du Learning Rate Finder (Smith, 2017).
    Augmente progressivement le lr et observe la loss.
    Le meilleur lr est juste avant que la loss remonte.
    """
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=lr_min)

    # Augmentation exponentielle du lr
    facteur = (lr_max / lr_min) ** (1 / num_steps)

    lrs = []
    losses = []
    lr_actuel = lr_min

    for step, (batch_X, batch_y) in enumerate(train_loader):
        if step >= num_steps:
            break

        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)

        # Forward + backward
        optimizer.zero_grad()
        pred = model(batch_X)
        loss = criterion(pred, batch_y)
        loss.backward()
        optimizer.step()

        # Logger
        lrs.append(lr_actuel)
        losses.append(loss.item())

        # Augmenter le lr
        lr_actuel *= facteur
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_actuel

        # Arr√™ter si la loss explose
        if loss.item() > losses[0] * 10:
            break

    # Tracer la courbe
    plt.figure(figsize=(10, 6))
    plt.semilogx(lrs, losses)
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Finder')
    plt.grid(True, alpha=0.3)

    # Trouver le lr optimal (point de descente la plus raide)
    gradients = np.gradient(losses)
    lr_optimal = lrs[np.argmin(gradients)]
    plt.axvline(x=lr_optimal, color='red', linestyle='--',
                label=f'LR optimal ‚âà {lr_optimal:.2e}')
    plt.legend()
    plt.savefig('lr_finder.png', dpi=150, bbox_inches='tight')
    plt.show()

    print(f"Learning Rate recommand√© : {lr_optimal:.2e}")
    return lr_optimal
```

> üí° **Conseil** : Le learning rate optimal se situe g√©n√©ralement l√† o√π la loss **descend le plus vite**, pas au minimum. Prenez un lr l√©g√®rement inf√©rieur au point de descente maximale.

### 1.3 Batch Size : impact sur m√©moire et convergence

| Batch Size | M√©moire GPU | Convergence | R√©gularisation |
|------------|-------------|-------------|----------------|
| **8-16** (petit) | Faible | Bruit√©e mais explore bien | Implicite (bruit agit comme r√©gularisation) |
| **32-64** (moyen) | Mod√©r√©e | Bon compromis | Mod√©r√©e |
| **128-256** (grand) | √âlev√©e | Stable mais risque de minima plats | Faible |
| **512+** (tr√®s grand) | Tr√®s √©lev√©e | Tr√®s stable, converge vite | Quasi nulle |

```python
# Adapter le batch_size selon votre GPU
# Technique : commencer grand, r√©duire si "CUDA Out of Memory"

batch_sizes_a_tester = [256, 128, 64, 32]

for bs in batch_sizes_a_tester:
    try:
        loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)
        batch_X, batch_y = next(iter(loader))
        batch_X = batch_X.to(device)
        output = model(batch_X)
        loss = criterion(output, batch_y.to(device))
        loss.backward()
        print(f"‚úÖ batch_size={bs} fonctionne")
        break
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"‚ùå batch_size={bs} : VRAM insuffisante")
            torch.cuda.empty_cache()
        else:
            raise e
```

> üí° **Conseil** : 32 ou 64 est un bon `batch_size` de d√©part. Augmentez si vous avez assez de VRAM. Si vous augmentez le batch_size, pensez √† augmenter proportionnellement le learning rate (r√®gle du scaling lin√©aire).

### 1.4 Nombre d'Epochs et Early Stopping

Le nombre d'epochs d√©termine **combien de fois** le mod√®le voit l'int√©gralit√© des donn√©es.

**Le probl√®me :** Trop peu d'epochs ‚Üí underfitting. Trop d'epochs ‚Üí overfitting.

**La solution :** Early Stopping ‚Äî arr√™ter automatiquement quand le mod√®le ne s'am√©liore plus.

```python
class EarlyStopping:
    """
    Arr√™te l'entra√Ænement si la val_loss ne s'am√©liore pas
    pendant 'patience' epochs cons√©cutives.
    """
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience       # Nombre d'epochs sans am√©lioration
        self.min_delta = min_delta     # Am√©lioration minimale consid√©r√©e
        self.compteur = 0
        self.meilleure_loss = float('inf')
        self.stop = False

    def __call__(self, val_loss):
        if val_loss < self.meilleure_loss - self.min_delta:
            # Am√©lioration ‚Üí reset du compteur
            self.meilleure_loss = val_loss
            self.compteur = 0
        else:
            # Pas d'am√©lioration
            self.compteur += 1
            if self.compteur >= self.patience:
                self.stop = True
                print(f"\n‚õî Early Stopping ! Pas d'am√©lioration depuis "
                      f"{self.patience} epochs.")
                print(f"   Meilleure val_loss : {self.meilleure_loss:.4f}")

# Utilisation dans la boucle d'entra√Ænement
early_stopping = EarlyStopping(patience=10, min_delta=0.001)

for epoch in range(max_epochs):
    # ... entra√Ænement + validation ...

    early_stopping(val_loss)
    if early_stopping.stop:
        break
```

---

## 2. üìä Overfitting : le d√©tecter et le combattre

L'overfitting est le **probl√®me #1** en Deep Learning. Le mod√®le m√©morise les donn√©es d'entra√Ænement au lieu d'apprendre des patterns g√©n√©ralisables.

### 2.1 Diagnostic par les courbes d'apprentissage

```
CAS 1 : UNDERFITTING               CAS 2 : BON FIT                CAS 3 : OVERFITTING

Loss                               Loss                            Loss
 ‚îÇ                                  ‚îÇ‚ï≤                              ‚îÇ
 ‚îÇ‚îÄ‚îÄ Train (haute)                  ‚îÇ ‚ï≤‚îÄ‚îÄ Train                     ‚îÇ     ‚ï±‚îÄ‚îÄ Val (remonte)
 ‚îÇ‚îÄ‚îÄ Val (haute)                    ‚îÇ  ‚ï≤‚îÄ‚îÄ Val (proche)             ‚îÇ   ‚ï±
 ‚îÇ                                  ‚îÇ   ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ                       ‚îÇ  ‚ï±
 ‚îÇ Les deux stagnent               ‚îÇ    ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ convergent           ‚îÇ‚ï≤‚ï±
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs                  ‚îÇ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ Train (baisse)
                                                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs
Solutions :                         Parfait !
‚Üí R√©seau plus grand                                                Solutions :
‚Üí Plus d'epochs                                                    ‚Üí Dropout
‚Üí LR plus √©lev√©                                                    ‚Üí Weight Decay
‚Üí V√©rifier les donn√©es                                             ‚Üí Data Augmentation
                                                                   ‚Üí Early Stopping
                                                                   ‚Üí Moins de param√®tres
```

**Indicateurs quantitatifs d'overfitting :**

| M√©trique | Valeur seuil | Action |
|----------|-------------|--------|
| `val_loss - train_loss` | > 0.5 | Alerte overfitting |
| `train_acc - val_acc` | > 10% | Alerte overfitting |
| `val_loss` remonte pendant 5+ epochs | - | Early stopping |
| `train_loss` = 0 | - | Overfitting s√©v√®re |

### 2.2 Dropout

Le Dropout d√©sactive **al√©atoirement** des neurones pendant l'entra√Ænement. Cela force le r√©seau √† ne pas d√©pendre d'un seul neurone et √† apprendre des repr√©sentations redondantes.

```
R√©seau normal :                  Avec Dropout (p=0.5) :

[n1] ‚îÄ‚îÄ‚îÄ [h1] ‚îÄ‚îÄ‚îÄ [o1]         [n1] ‚îÄ‚îÄ‚îÄ [h1] ‚îÄ‚îÄ‚îÄ [o1]
[n2] ‚îÄ‚îÄ‚îÄ [h2] ‚îÄ‚îÄ‚îÄ [o2]         [n2] ‚îÄ‚îÄ‚îÄ [  ] ‚îÄ‚îÄ‚îÄ [o2]  ‚Üê h2 d√©sactiv√©
[n3] ‚îÄ‚îÄ‚îÄ [h3]                   [  ] ‚îÄ‚îÄ‚îÄ [h3]           ‚Üê n3 d√©sactiv√©
[n4] ‚îÄ‚îÄ‚îÄ [h4]                   [n4] ‚îÄ‚îÄ‚îÄ [h4]

Chaque neurone est               Des neurones diff√©rents
toujours actif                   sont d√©sactiv√©s √† chaque batch
```

```python
import torch.nn as nn

class ReseauAvecDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 512)
        self.dropout1 = nn.Dropout(p=0.3)   # 30% des neurones d√©sactiv√©s
        self.fc2 = nn.Linear(512, 256)
        self.dropout2 = nn.Dropout(p=0.5)   # 50% des neurones d√©sactiv√©s
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)          # Dropout apr√®s l'activation
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)          # Plus de dropout dans les couches profondes
        x = self.fc3(x)               # PAS de dropout en sortie
        return x
```

**R√®gles pratiques pour le Dropout :**

| Position | Taux recommand√© | Notes |
|----------|----------------|-------|
| Apr√®s les premi√®res couches | 0.2 - 0.3 | Garder la plupart des features d'entr√©e |
| Couches interm√©diaires | 0.3 - 0.5 | Standard |
| Avant la derni√®re couche | 0.5 | R√©gularisation maximale |
| Couche de sortie | 0 (jamais) | Ne jamais mettre de dropout en sortie |

> üí° **Conseil** : Dropout 0.2-0.5 est standard. Plus le r√©seau est grand et plus vous avez peu de donn√©es, plus le dropout doit √™tre √©lev√©.

### 2.3 R√©gularisation L2 (Weight Decay)

Le Weight Decay p√©nalise les poids trop grands, for√ßant le r√©seau √† trouver des solutions simples.

```python
import torch.optim as optim

# Weight Decay int√©gr√© √† l'optimizer
# Ajoute une p√©nalit√© L2 : Loss_totale = Loss_data + Œª √ó Œ£(wi¬≤)
optimizer = optim.Adam(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4    # Œª = 0.0001 (valeur standard)
)

# Ou avec AdamW (impl√©mentation plus correcte du weight decay)
optimizer = optim.AdamW(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-2    # AdamW utilise des valeurs plus √©lev√©es (0.01)
)
```

**Valeurs de Weight Decay recommand√©es :**

| Optimizer | Weight Decay | Notes |
|-----------|-------------|-------|
| Adam | `1e-4` √† `1e-3` | Valeurs classiques |
| AdamW | `1e-2` √† `1e-1` | D√©corr√©l√© du lr, valeurs plus √©lev√©es |
| SGD | `1e-4` √† `5e-4` | Standard pour CNN |

### 2.4 Data Augmentation

La Data Augmentation cr√©e des **versions modifi√©es** des donn√©es d'entra√Ænement. C'est la technique anti-overfitting la plus puissante quand vous avez peu de donn√©es.

```python
from torchvision import transforms

# Data Augmentation pour les images
train_transform = transforms.Compose([
    # --- Augmentations g√©om√©triques ---
    transforms.RandomHorizontalFlip(p=0.5),        # Miroir horizontal
    transforms.RandomVerticalFlip(p=0.1),           # Miroir vertical (rare)
    transforms.RandomRotation(degrees=15),           # Rotation ¬±15¬∞
    transforms.RandomResizedCrop(                    # Crop + resize al√©atoire
        size=224,
        scale=(0.8, 1.0),
        ratio=(0.9, 1.1)
    ),
    transforms.RandomAffine(                         # Transformations affines
        degrees=10,
        translate=(0.1, 0.1),
        shear=5
    ),

    # --- Augmentations photom√©triques ---
    transforms.ColorJitter(
        brightness=0.2,     # Variation de luminosit√© ¬±20%
        contrast=0.2,       # Variation de contraste ¬±20%
        saturation=0.2,     # Variation de saturation ¬±20%
        hue=0.05            # Variation de teinte ¬±5%
    ),
    transforms.RandomGrayscale(p=0.1),              # Passage en gris (10%)

    # --- Conversion et normalisation ---
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],                 # Moyenne ImageNet
        std=[0.229, 0.224, 0.225]                    # √âcart-type ImageNet
    ),

    # --- Augmentation avanc√©e ---
    transforms.RandomErasing(p=0.1),                # Masquer une zone al√©atoire
])

# ‚ö†Ô∏è PAS de data augmentation pour la validation/test
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
])
```

> üí° **Conseil de pro** : La Data Augmentation est la technique anti-overfitting **la plus puissante** quand vous avez peu de donn√©es. Elle est gratuite en termes de collecte de donn√©es et tr√®s efficace.

### 2.5 Batch Normalization

La Batch Normalization normalise les activations entre les couches, ce qui :
- **Acc√©l√®re** l'entra√Ænement (gradient plus stable)
- **R√©gularise** l√©g√®rement (bruit d√ª aux statistiques du batch)
- Permet des **learning rates plus √©lev√©s**

```python
import torch.nn as nn

class ReseauAvecBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)    # BatchNorm apr√®s la couche lin√©aire
        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.fc3 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)

        # Pattern : Linear ‚Üí BatchNorm ‚Üí Activation
        x = self.fc1(x)
        x = self.bn1(x)          # Normalise les activations
        x = torch.relu(x)

        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)

        x = self.fc3(x)          # Pas de BatchNorm en sortie
        return x
```

> üí° **Conseil** : L'ordre standard est **Linear ‚Üí BatchNorm ‚Üí Activation**. Certains placent BatchNorm apr√®s l'activation, les deux fonctionnent. L'important est d'√™tre coh√©rent.

### 2.6 R√©sum√© des techniques anti-overfitting

| Technique | Efficacit√© | Co√ªt | Quand l'utiliser |
|-----------|-----------|------|------------------|
| **Data Augmentation** | Tr√®s √©lev√©e | Nul (gratuit) | Toujours (images) |
| **Dropout** | √âlev√©e | Nul | Toujours |
| **Weight Decay** | Moyenne | Nul | Toujours (1e-4 par d√©faut) |
| **Early Stopping** | √âlev√©e | Nul | Toujours |
| **Batch Normalization** | Moyenne | Faible | R√©seaux profonds |
| **Plus de donn√©es** | Tr√®s √©lev√©e | √âlev√© | Si possible |
| **R√©duire le r√©seau** | Moyenne | Nul | Dernier recours |

---

## 3. üöÄ Transfer Learning

### 3.1 Le principe

Le Transfer Learning consiste √† **r√©utiliser un mod√®le pr√©-entra√Æn√©** sur un grand dataset (ex: ImageNet, 14M d'images) et √† l'adapter √† votre t√¢che sp√©cifique.

```
Mod√®le pr√©-entra√Æn√© (ImageNet)       Votre t√¢che (ex: chats vs chiens)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Conv1 : d√©tecte les bords     ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ Garde tel quel (gel√©)    ‚îÇ
‚îÇ Conv2 : d√©tecte les textures  ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ Garde tel quel (gel√©)    ‚îÇ
‚îÇ Conv3 : d√©tecte les formes    ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ Garde tel quel (gel√©)    ‚îÇ
‚îÇ Conv4 : d√©tecte les objets    ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ Optionnel : fine-tune    ‚îÇ
‚îÇ FC : classifie (1000 classes) ‚îÇ ‚îÄ‚îÄ‚Üí ‚îÇ REMPLACER (2 classes)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pourquoi c'est si puissant :**
- Les premi√®res couches apprennent des features **universelles** (bords, textures)
- Seules les derni√®res couches sont sp√©cifiques au dataset
- Vous r√©utilisez des millions de param√®tres d√©j√† optimis√©s

### 3.2 Feature Extraction vs Fine-Tuning

| Approche | Couches gel√©es | Couches entra√Æn√©es | Donn√©es n√©cessaires | Quand |
|----------|---------------|-------------------|---------------------|-------|
| **Feature Extraction** | Toutes sauf la derni√®re | Derni√®re couche seulement | Tr√®s peu (100-1000) | Peu de donn√©es |
| **Fine-tuning partiel** | Premi√®res couches | Derni√®res couches + nouvelle t√™te | Moyen (1000-10000) | Cas standard |
| **Fine-tuning complet** | Aucune | Tout le r√©seau | Beaucoup (10000+) | Beaucoup de donn√©es |

### 3.3 Code complet : Transfer Learning avec ResNet

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

def creer_modele_transfer(n_classes, mode='fine_tuning'):
    """
    Cr√©e un mod√®le bas√© sur ResNet-18 pr√©-entra√Æn√©.

    Args:
        n_classes: nombre de classes de votre probl√®me
        mode: 'feature_extraction' ou 'fine_tuning'
    """
    # 1. Charger le mod√®le pr√©-entra√Æn√©
    model = models.resnet18(weights='IMAGENET1K_V1')
    print(f"ResNet-18 charg√© ({sum(p.numel() for p in model.parameters()):,} param√®tres)")

    # 2. Geler les couches selon le mode
    if mode == 'feature_extraction':
        # Geler TOUTES les couches
        for param in model.parameters():
            param.requires_grad = False
        print("Mode Feature Extraction : toutes les couches gel√©es")

    elif mode == 'fine_tuning':
        # Geler les premi√®res couches, laisser les derni√®res entra√Ænables
        for name, param in model.named_parameters():
            if 'layer4' not in name and 'fc' not in name:
                param.requires_grad = False
        print("Mode Fine-tuning : seuls layer4 et fc sont entra√Ænables")

    # 3. Remplacer la couche de classification
    n_features = model.fc.in_features  # 512 pour ResNet-18
    model.fc = nn.Sequential(
        nn.Dropout(0.3),
        nn.Linear(n_features, n_classes)
    )

    # Compter les param√®tres entra√Ænables
    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    n_total = sum(p.numel() for p in model.parameters())
    print(f"Param√®tres entra√Ænables : {n_trainable:,} / {n_total:,} "
          f"({n_trainable/n_total:.1%})")

    return model

# Exemple : classification chats vs chiens (2 classes)
model = creer_modele_transfer(n_classes=2, mode='fine_tuning')

# Optimizer avec learning rate diff√©renci√©
# (lr plus faible pour les couches pr√©-entra√Æn√©es)
optimizer = optim.Adam([
    {'params': model.layer4.parameters(), 'lr': 1e-4},    # Couches pr√©-entra√Æn√©es
    {'params': model.fc.parameters(), 'lr': 1e-3},        # Nouvelle couche
])
```

### 3.4 Mod√®les pr√©-entra√Æn√©s courants

| Mod√®le | Param√®tres | Top-1 ImageNet | Usage |
|--------|-----------|----------------|-------|
| **ResNet-18** | 11M | 69.8% | Prototypage rapide, petits datasets |
| **ResNet-50** | 25M | 76.1% | Standard industriel |
| **EfficientNet-B0** | 5M | 77.1% | Meilleur ratio performance/taille |
| **EfficientNet-B4** | 19M | 82.9% | Haute performance |
| **ViT-B/16** | 86M | 81.8% | Vision Transformer |
| **ConvNeXt-Base** | 89M | 83.8% | CNN moderne (comparable aux ViT) |

```python
# Charger diff√©rents mod√®les pr√©-entra√Æn√©s
from torchvision import models

resnet50 = models.resnet50(weights='IMAGENET1K_V2')
efficientnet = models.efficientnet_b0(weights='IMAGENET1K_V1')
vit = models.vit_b_16(weights='IMAGENET1K_V1')
```

> üí° **Conseil de pro** : En 2024, commencez **TOUJOURS** par le Transfer Learning. Entra√Æner from scratch est rarement n√©cessaire et presque toujours moins performant. M√™me avec 500 images, un ResNet fine-tun√© battrait un r√©seau entra√Æn√© de z√©ro.

---

## 4. üìä M√©triques avanc√©es pour le Deep Learning

### 4.1 Analyse d√©taill√©e des Loss Curves

| Observation | Diagnostic | Action |
|-------------|-----------|--------|
| Train loss descend, val loss **stable** | D√©but d'overfitting | Commencer √† r√©gulariser |
| Train loss descend, val loss **remonte** | Overfitting confirm√© | Dropout, WD, Data Aug, Early Stop |
| Les deux **stagnent** haut | Underfitting | Plus de capacit√©, meilleur lr |
| Les deux **oscillent** | lr trop √©lev√© | R√©duire le lr |
| Train loss ‚Üí **NaN** | Exploding gradients | lr √∑ 10, gradient clipping |
| Train loss **ne bouge pas** | lr trop faible ou bug | V√©rifier le code, augmenter lr |
| Val loss **beaucoup plus haute** d√®s le d√©but | Erreur de preprocessing | V√©rifier train_transform vs val_transform |

### 4.2 Gradient Monitoring

```python
import torch

def surveiller_gradients(model):
    """
    Surveille les gradients pendant l'entra√Ænement.
    D√©tecte vanishing et exploding gradients.
    """
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_mean = param.grad.mean().item()
            grad_max = param.grad.max().item()

            # Alerte vanishing gradient
            if grad_norm < 1e-7:
                print(f"‚ö†Ô∏è  VANISHING gradient dans {name} "
                      f"(norm={grad_norm:.2e})")

            # Alerte exploding gradient
            if grad_norm > 100:
                print(f"‚ö†Ô∏è  EXPLODING gradient dans {name} "
                      f"(norm={grad_norm:.2e})")

# Gradient Clipping : limiter la norme des gradients
def entrainer_avec_clipping(model, train_loader, criterion, optimizer,
                             device, max_grad_norm=1.0):
    """Entra√Ænement avec gradient clipping"""
    model.train()
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        output = model(batch_X)
        loss = criterion(output, batch_y)
        loss.backward()

        # Clipper les gradients AVANT optimizer.step()
        torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_norm=max_grad_norm
        )

        optimizer.step()
```

### 4.3 Vanishing vs Exploding Gradients

```
Vanishing Gradients                    Exploding Gradients
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Sympt√¥mes :                            Sympt√¥mes :
- Premi√®res couches n'apprennent pas   - Loss explose ou NaN
- Poids restent proches de l'init      - Poids deviennent tr√®s grands
- Loss stagne apr√®s quelques epochs    - Instabilit√© d'entra√Ænement

Causes :                               Causes :
- Sigmoid/Tanh en couches cach√©es      - Learning rate trop √©lev√©
- R√©seau trop profond sans skip conn.  - Mauvaise initialisation
- Mauvaise initialisation              - Absence de normalisation

Solutions :                            Solutions :
- ReLU / Leaky ReLU                    - Gradient Clipping
- Skip connections (ResNet)            - R√©duire le learning rate
- Batch Normalization                  - Batch Normalization
- Xavier/He initialisation             - Gradient Clipping (max_norm=1.0)
```

### 4.4 Top-K Accuracy

Pour la classification multi-classes, le Top-K Accuracy mesure si la bonne classe est parmi les K pr√©dictions les plus probables.

```python
import torch

def topk_accuracy(output, target, topk=(1, 5)):
    """
    Calcule Top-1 et Top-5 accuracy.

    Args:
        output: logits du mod√®le (batch_size, n_classes)
        target: vrais labels (batch_size,)
        topk: tuple des K √† calculer
    """
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        # Top-K pr√©dictions
        _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)
        pred = pred.t()  # Transposer pour faciliter la comparaison

        correct = pred.eq(target.view(1, -1).expand_as(pred))

        resultats = {}
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0)
            resultats[f'top{k}'] = (correct_k / batch_size).item()

        return resultats

# Utilisation
# output = model(images)           # (batch, 1000) pour ImageNet
# acc = topk_accuracy(output, labels, topk=(1, 5))
# print(f"Top-1: {acc['top1']:.2%}, Top-5: {acc['top5']:.2%}")
```

### 4.5 Matrice de confusion

```python
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import torch

def evaluer_modele(model, test_loader, device, noms_classes):
    """√âvaluation compl√®te avec matrice de confusion et rapport"""
    model.eval()
    toutes_predictions = []
    tous_labels = []

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X = batch_X.to(device)
            output = model(batch_X)
            _, predicted = output.max(1)
            toutes_predictions.extend(predicted.cpu().numpy())
            tous_labels.extend(batch_y.numpy())

    # Matrice de confusion
    cm = confusion_matrix(tous_labels, toutes_predictions)
    print("Matrice de confusion :")
    print(cm)

    # Rapport de classification
    print("\nRapport de classification :")
    print(classification_report(
        tous_labels,
        toutes_predictions,
        target_names=noms_classes
    ))
```

---

## 5. üìà M√©thodologie compl√®te

### 5.1 Checklist pour un projet Deep Learning

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            M√âTHODOLOGIE DEEP LEARNING                       ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  1. üîç BASELINE                                            ‚îÇ
‚îÇ     ‚Üí Commencer par un mod√®le SIMPLE (MLP, petit CNN)      ‚îÇ
‚îÇ     ‚Üí √âtablir une performance de r√©f√©rence                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  2. üêõ SANITY CHECK                                        ‚îÇ
‚îÇ     ‚Üí Overfitter un petit batch (10-100 samples)            ‚îÇ
‚îÇ     ‚Üí Si train_loss ‚Üí 0, le code est correct                ‚îÇ
‚îÇ     ‚Üí Si non, il y a un BUG !                               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  3. üìä DONN√âES COMPL√àTES                                   ‚îÇ
‚îÇ     ‚Üí Charger toutes les donn√©es                            ‚îÇ
‚îÇ     ‚Üí Train/Val/Test split                                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  4. üéØ LEARNING RATE                                       ‚îÇ
‚îÇ     ‚Üí LR Finder ou commencer √† 1e-3 (Adam)                 ‚îÇ
‚îÇ     ‚Üí Tuner avant tout le reste                             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  5. üõ°Ô∏è R√âGULARISATION                                      ‚îÇ
‚îÇ     ‚Üí Dropout (0.2-0.5)                                     ‚îÇ
‚îÇ     ‚Üí Weight Decay (1e-4)                                   ‚îÇ
‚îÇ     ‚Üí Data Augmentation                                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  6. ‚èπÔ∏è EARLY STOPPING                                      ‚îÇ
‚îÇ     ‚Üí patience=10-20 epochs                                 ‚îÇ
‚îÇ     ‚Üí Sauvegarder le meilleur mod√®le                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  7. üöÄ TRANSFER LEARNING                                   ‚îÇ
‚îÇ     ‚Üí Si applicable (images, NLP)                           ‚îÇ
‚îÇ     ‚Üí ResNet, EfficientNet, BERT...                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  8. üîß HYPERPARAMETER SEARCH                               ‚îÇ
‚îÇ     ‚Üí Grid Search ou Optuna                                 ‚îÇ
‚îÇ     ‚Üí lr, batch_size, architecture, dropout                 ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 Le Sanity Check : l'√©tape CRUCIALE

```python
import torch

def sanity_check(model, train_loader, criterion, optimizer, device, n_steps=100):
    """
    V√©rifie que le mod√®le peut overfitter un petit batch.
    Si oui ‚Üí le code est correct.
    Si non ‚Üí il y a un bug !
    """
    model.train()
    model.to(device)

    # Prendre un seul batch
    batch_X, batch_y = next(iter(train_loader))
    batch_X, batch_y = batch_X.to(device), batch_y.to(device)

    print(f"Sanity check sur {batch_X.size(0)} samples...")

    for step in range(n_steps):
        optimizer.zero_grad()
        output = model(batch_X)
        loss = criterion(output, batch_y)
        loss.backward()
        optimizer.step()

        if step % 20 == 0:
            _, predicted = output.max(1)
            acc = (predicted == batch_y).float().mean()
            print(f"  Step {step:3d} | Loss: {loss.item():.4f} | Acc: {acc:.2%}")

    # V√©rification finale
    _, predicted = output.max(1)
    acc_finale = (predicted == batch_y).float().mean()

    if acc_finale > 0.95:
        print(f"\n‚úÖ SANITY CHECK PASS√â : le mod√®le peut overfitter "
              f"(acc={acc_finale:.2%})")
    else:
        print(f"\n‚ùå SANITY CHECK √âCHOU√â : le mod√®le n'arrive pas √† overfitter "
              f"(acc={acc_finale:.2%})")
        print("   ‚Üí V√©rifiez : architecture, loss function, donn√©es, lr")
```

> üí° **Conseil de pro** : L'√©tape 2 (Sanity Check) est **CRUCIALE**. Si votre mod√®le ne peut pas overfitter 10-100 samples, il y a un bug dans votre code. Inutile de continuer avant d'avoir r√©solu ce probl√®me.

### 5.3 Hyperparameter Search avec Optuna

```python
# uv add optuna
import optuna
import torch
import torch.nn as nn
import torch.optim as optim

def objectif(trial):
    """Fonction objectif pour Optuna"""
    # Sugg√©rer des hyperparam√®tres
    n_couches = trial.suggest_int('n_couches', 1, 4)
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])

    # Construire le mod√®le dynamiquement
    couches = []
    taille_entree = 784
    for i in range(n_couches):
        taille_sortie = trial.suggest_int(f'n_units_{i}', 64, 512, step=64)
        couches.extend([
            nn.Linear(taille_entree, taille_sortie),
            nn.ReLU(),
            nn.Dropout(dropout),
        ])
        taille_entree = taille_sortie
    couches.append(nn.Linear(taille_entree, 10))

    model = nn.Sequential(*couches).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Entra√Æner et √©valuer (simplifi√©)
    # ... (boucle d'entra√Ænement standard)

    return val_accuracy  # Optuna maximise cette valeur

# Lancer la recherche
etude = optuna.create_study(direction='maximize')
etude.optimize(objectif, n_trials=50)

print(f"Meilleurs hyperparam√®tres : {etude.best_params}")
print(f"Meilleure val_accuracy : {etude.best_value:.2%}")
```

---

## 6. üö´ Les erreurs courantes en Deep Learning

### 6.1 Table des erreurs et solutions

| # | Erreur | Sympt√¥me | Solution |
|---|--------|----------|----------|
| 1 | **Pas de normalisation des donn√©es** | Loss ne descend pas ou tr√®s lentement | `transforms.Normalize()` (mean=0, std=1) |
| 2 | **Learning rate trop √©lev√©** | Loss explose ou NaN | Diviser le lr par 10 |
| 3 | **Learning rate trop faible** | Loss descend tr√®s lentement | Multiplier le lr par 10 |
| 4 | **Pas de shuffle** | Convergence bizarre, biais | `DataLoader(shuffle=True)` sur le train |
| 5 | **Mauvaise loss function** | Mod√®le ne converge pas | V√©rifier le match loss ‚Üî activation |
| 6 | **Softmax + CrossEntropyLoss** | Performance d√©grad√©e | Enlever le Softmax (CE l'inclut) |
| 7 | **GPU pas utilis√©** | Entra√Ænement tr√®s lent | `.to(device)` sur mod√®le ET donn√©es |
| 8 | **Oublier zero_grad()** | Gradients accumulent, instabilit√© | `optimizer.zero_grad()` √† chaque batch |
| 9 | **Oublier model.eval()** | M√©triques val instables | `model.eval()` + `torch.no_grad()` |
| 10 | **Train/Val transforms diff√©rents** | Val loss tr√®s haute d√®s le d√©but | M√™me normalisation, pas d'augmentation en val |
| 11 | **Data leakage** | Val performance trop bonne | V√©rifier le split train/val/test |
| 12 | **Mauvais dtype** | Erreur de type | `torch.FloatTensor` pour les features |
| 13 | **Tensors sur devices diff√©rents** | RuntimeError | Tout sur le m√™me device |
| 14 | **Pas de seed** | R√©sultats non reproductibles | `torch.manual_seed(42)` |

### 6.2 Debugging rapide

```python
import torch

def debug_modele(model, train_loader, device):
    """Script de diagnostic rapide pour un mod√®le PyTorch"""
    print("=" * 60)
    print("DIAGNOSTIC DU MOD√àLE")
    print("=" * 60)

    # 1. Architecture
    n_params = sum(p.numel() for p in model.parameters())
    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nüìê Architecture :")
    print(f"   Param√®tres totaux     : {n_params:,}")
    print(f"   Param√®tres entra√Ænables : {n_trainable:,}")

    # 2. Donn√©es
    batch_X, batch_y = next(iter(train_loader))
    print(f"\nüìä Donn√©es :")
    print(f"   Batch X shape : {batch_X.shape}")
    print(f"   Batch Y shape : {batch_y.shape}")
    print(f"   X dtype       : {batch_X.dtype}")
    print(f"   Y dtype       : {batch_y.dtype}")
    print(f"   X range       : [{batch_X.min():.4f}, {batch_X.max():.4f}]")
    print(f"   Y unique      : {batch_y.unique().tolist()}")

    # 3. Forward pass
    model.to(device)
    model.eval()
    with torch.no_grad():
        try:
            output = model(batch_X.to(device))
            print(f"\n‚úÖ Forward pass OK :")
            print(f"   Output shape : {output.shape}")
            print(f"   Output range : [{output.min():.4f}, {output.max():.4f}]")
        except Exception as e:
            print(f"\n‚ùå Forward pass ERREUR : {e}")

    # 4. V√©rifier les NaN
    has_nan = False
    for name, param in model.named_parameters():
        if torch.isnan(param).any():
            print(f"‚ùå NaN d√©tect√© dans {name}")
            has_nan = True
    if not has_nan:
        print(f"\n‚úÖ Pas de NaN dans les poids")

    print("=" * 60)

# Utilisation :
# debug_modele(model, train_loader, device)
```

### 6.3 Reproductibilit√©

```python
import torch
import numpy as np
import random

def fixer_seed(seed=42):
    """Fixer toutes les sources d'al√©atoire pour la reproductibilit√©"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"Seed fix√©e √† {seed} pour reproductibilit√©")

# Appeler au d√©but de chaque script
fixer_seed(42)
```

> ‚ö†Ô∏è **Attention** : `torch.backends.cudnn.deterministic = True` ralentit l'entra√Ænement de ~10-20%. Utilisez-le pour le debugging et les r√©sultats finaux, pas pendant l'exploration.

---

## üìù Points cl√©s √† retenir

- Le **Learning Rate** est l'hyperparam√®tre #1 (commencer avec lr=1e-3 pour Adam)
- L'**overfitting** se d√©tecte quand `val_loss` remonte alors que `train_loss` baisse
- Les techniques anti-overfitting : **Dropout**, **Weight Decay**, **Data Augmentation**, **Early Stopping**
- Le **Transfer Learning** est presque toujours meilleur que d'entra√Æner from scratch
- Le **Sanity Check** (overfitter un petit batch) est l'√©tape de debug la plus importante
- Surveillez les **gradients** (vanishing, exploding) pour les r√©seaux profonds
- La **m√©thodologie** est plus importante que l'architecture : baseline ‚Üí sanity check ‚Üí tune lr ‚Üí r√©gulariser

## ‚úÖ Checklist de validation

- [ ] Je sais utiliser le Learning Rate Finder
- [ ] Je connais l'impact du batch_size sur la convergence et la m√©moire
- [ ] Je sais impl√©menter l'Early Stopping
- [ ] Je ma√Ætrise les 5 techniques anti-overfitting (Dropout, WD, DA, BN, ES)
- [ ] Je sais faire du Transfer Learning avec ResNet/EfficientNet
- [ ] Je peux diagnostiquer vanishing/exploding gradients
- [ ] J'applique la m√©thodologie en 8 √©tapes pour tout nouveau projet
- [ ] Je fais TOUJOURS un sanity check avant d'entra√Æner sur les donn√©es compl√®tes
- [ ] Je sais debugger un mod√®le qui ne converge pas

---

**Chapitre pr√©c√©dent :** [03 - PyTorch](./03-frameworks-pytorch.md)

[Retour au sommaire](../README.md)
