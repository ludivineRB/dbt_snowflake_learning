{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting et Regularisation\n",
    "\n",
    "> Ce notebook est un exemple pratique clef en main pour comprendre visuellement l'overfitting et maitriser les techniques de regularisation.\n",
    "\n",
    "Ce notebook demontre visuellement l'overfitting et les techniques de regularisation.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre ce qu'est l'**overfitting** (surapprentissage) et le detecter\n",
    "- Appliquer le **Dropout** comme technique de regularisation\n",
    "- Utiliser le **Weight Decay** (regularisation L2)\n",
    "- Implementer l'**Early Stopping**\n",
    "- Decouvrir la **Data Augmentation**\n",
    "- Comparer toutes les approches\n",
    "\n",
    "### Pre-requis\n",
    "\n",
    "```bash\n",
    "uv add torch torchvision matplotlib numpy scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "\n",
    "# Configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilise : {device}\")\n",
    "\n",
    "# Graine aleatoire pour la reproductibilite\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 - Generer un Dataset Synthetique\n",
    "\n",
    "Nous utilisons `make_moons` de scikit-learn pour creer un dataset de classification 2D en forme de croissants de lune. Ce dataset est volontairement **petit** (300 points) et **bruite** pour provoquer facilement l'overfitting.\n",
    "\n",
    "> **Pourquoi un petit dataset ?** L'overfitting se produit lorsque le modele a trop de parametres par rapport a la quantite de donnees. Avec un petit dataset, c'est plus facile a observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generation du dataset ===\n",
    "\n",
    "NB_ECHANTILLONS = 300\n",
    "BRUIT = 0.25  # Quantite de bruit (rend la classification plus difficile)\n",
    "\n",
    "# Creer les donnees en forme de croissants de lune\n",
    "X, y = make_moons(n_samples=NB_ECHANTILLONS, noise=BRUIT, random_state=42)\n",
    "\n",
    "# Separer en ensembles d'entrainement (60%), validation (20%), test (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entrainement : {len(X_train)}\")\n",
    "print(f\"Taille de l'ensemble de validation  : {len(X_val)}\")\n",
    "print(f\"Taille de l'ensemble de test        : {len(X_test)}\")\n",
    "\n",
    "# Convertir en tensors PyTorch\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "y_train_t = torch.LongTensor(y_train).to(device)\n",
    "X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "y_val_t = torch.LongTensor(y_val).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "y_test_t = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualisation du dataset ===\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, X_set, y_set, titre in [\n",
    "    (axes[0], X_train, y_train, f'Entrainement (n={len(X_train)})'),\n",
    "    (axes[1], X_val, y_val, f'Validation (n={len(X_val)})'),\n",
    "    (axes[2], X_test, y_test, f'Test (n={len(X_test)})'),\n",
    "]:\n",
    "    scatter = ax.scatter(X_set[:, 0], X_set[:, 1], c=y_set, cmap='RdYlBu',\n",
    "                         edgecolors='black', s=50, alpha=0.8)\n",
    "    ax.set_title(titre, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Dataset \"Make Moons\" - Classification binaire 2D', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 - Un Modele qui Overfit\n",
    "\n",
    "Nous allons maintenant creer un reseau **beaucoup trop gros** pour nos 180 points d'entrainement. Avec des centaines de milliers de parametres pour si peu de donnees, le modele va **memoriser** les donnees d'entrainement au lieu d'apprendre le pattern general.\n",
    "\n",
    "> **Signe d'overfitting** : La perte d'entrainement diminue, mais la perte de validation **augmente** apres un certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fonctions utilitaires ===\n",
    "\n",
    "def entrainer_modele(modele, optimiseur, nb_epochs=200, verbose=True):\n",
    "    \"\"\"Entraine un modele et retourne l'historique des pertes.\"\"\"\n",
    "    critere = nn.CrossEntropyLoss()\n",
    "    historique = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        # --- Entrainement ---\n",
    "        modele.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            predictions = modele(X_batch)\n",
    "            loss = critere(predictions, y_batch)\n",
    "            optimiseur.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiseur.step()\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # --- Validation ---\n",
    "        modele.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = modele(X_val_t)\n",
    "            val_loss = critere(val_preds, y_val_t).item()\n",
    "            _, val_predicted = torch.max(val_preds, 1)\n",
    "            val_acc = 100 * (val_predicted == y_val_t).sum().item() / len(y_val_t)\n",
    "\n",
    "        historique['train_loss'].append(train_loss)\n",
    "        historique['val_loss'].append(val_loss)\n",
    "        historique['train_acc'].append(train_acc)\n",
    "        historique['val_acc'].append(val_acc)\n",
    "\n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch+1:>3}/{nb_epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.1f}% | Val Acc: {val_acc:.1f}%\")\n",
    "\n",
    "    return historique\n",
    "\n",
    "\n",
    "def tracer_courbes(historique, titre=\"\"):\n",
    "    \"\"\"Trace les courbes de perte et precision (train vs validation).\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    epochs = range(1, len(historique['train_loss']) + 1)\n",
    "\n",
    "    # Courbe de perte\n",
    "    ax1.plot(epochs, historique['train_loss'], 'b-', label='Train', linewidth=2, alpha=0.8)\n",
    "    ax1.plot(epochs, historique['val_loss'], 'r-', label='Validation', linewidth=2, alpha=0.8)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Perte (Loss)')\n",
    "    ax1.set_title('Perte')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Courbe de precision\n",
    "    ax2.plot(epochs, historique['train_acc'], 'b-', label='Train', linewidth=2, alpha=0.8)\n",
    "    ax2.plot(epochs, historique['val_acc'], 'r-', label='Validation', linewidth=2, alpha=0.8)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Precision (%)')\n",
    "    ax2.set_title('Precision')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(titre, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def tracer_frontiere_decision(modele, X, y, titre=\"\"):\n",
    "    \"\"\"Trace la frontiere de decision du modele sur les donnees.\"\"\"\n",
    "    modele.eval()\n",
    "\n",
    "    # Creer une grille de points\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    # Predire sur toute la grille\n",
    "    grille = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]).to(device)\n",
    "    with torch.no_grad():\n",
    "        Z = modele(grille)\n",
    "        Z = torch.softmax(Z, dim=1)[:, 1]  # Probabilite de la classe 1\n",
    "    Z = Z.cpu().numpy().reshape(xx.shape)\n",
    "\n",
    "    # Tracer\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    contour = ax.contourf(xx, yy, Z, levels=50, cmap='RdYlBu', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)  # Frontiere\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black', s=60)\n",
    "    plt.colorbar(contour, ax=ax, label='Probabilite classe 1')\n",
    "    ax.set_title(titre, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modele TROP GRAND (provoque l'overfitting) ===\n",
    "\n",
    "class GrosModele(nn.Module):\n",
    "    \"\"\"Un reseau beaucoup trop gros pour 180 points d'entrainement.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GrosModele, self).__init__()\n",
    "        self.couches = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.couches(x)\n",
    "\n",
    "\n",
    "# Entrainer le gros modele\n",
    "modele_overfit = GrosModele().to(device)\n",
    "nb_params = sum(p.numel() for p in modele_overfit.parameters())\n",
    "print(f\"Nombre de parametres : {nb_params:,} (pour seulement {len(X_train)} points !)\")\n",
    "print(f\"Ratio parametres/donnees : {nb_params/len(X_train):.0f}:1 (devrait etre < 10:1)\")\n",
    "print()\n",
    "\n",
    "optimiseur_overfit = optim.Adam(modele_overfit.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Entrainement du modele (sans regularisation) :\")\n",
    "historique_overfit = entrainer_modele(modele_overfit, optimiseur_overfit, nb_epochs=200)\n",
    "\n",
    "# Visualiser les courbes\n",
    "tracer_courbes(historique_overfit, 'SANS regularisation - Overfitting visible')\n",
    "\n",
    "# Visualiser la frontiere de decision\n",
    "tracer_frontiere_decision(modele_overfit, X_train, y_train,\n",
    "                          'Frontiere de decision - Modele surappris (overfitted)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 - Regularisation par Dropout\n",
    "\n",
    "Le **Dropout** est une technique de regularisation qui consiste a **desactiver aleatoirement** un pourcentage de neurones a chaque etape d'entrainement.\n",
    "\n",
    "**Comment ca fonctionne :**\n",
    "- Pendant l'entrainement : chaque neurone a une probabilite `p` d'etre mis a zero\n",
    "- Pendant l'evaluation : tous les neurones sont actifs, mais les poids sont multiplies par `(1-p)` pour compenser\n",
    "\n",
    "**Pourquoi ca marche :** Le reseau ne peut pas compter sur un seul chemin neural. Il doit apprendre des representations **redondantes** et **robustes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modele avec Dropout ===\n",
    "\n",
    "class GrosModeleDropout(nn.Module):\n",
    "    \"\"\"Meme architecture que GrosModele, mais avec Dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        super(GrosModeleDropout, self).__init__()\n",
    "        self.couches = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),    # 40% des neurones desactives\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.couches(x)\n",
    "\n",
    "\n",
    "modele_dropout = GrosModeleDropout(dropout_rate=0.4).to(device)\n",
    "optimiseur_dropout = optim.Adam(modele_dropout.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Entrainement du modele avec Dropout (p=0.4) :\")\n",
    "historique_dropout = entrainer_modele(modele_dropout, optimiseur_dropout, nb_epochs=200)\n",
    "\n",
    "# Visualiser\n",
    "tracer_courbes(historique_dropout, 'AVEC Dropout (p=0.4) - Meilleure generalisation')\n",
    "tracer_frontiere_decision(modele_dropout, X_train, y_train,\n",
    "                          'Frontiere de decision - Avec Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 - Regularisation par Weight Decay\n",
    "\n",
    "Le **Weight Decay** (ou regularisation L2) ajoute une penalite sur la taille des poids dans la fonction de perte :\n",
    "\n",
    "$$\\text{Loss}_{\\text{total}} = \\text{Loss}_{\\text{original}} + \\lambda \\sum_{i} w_i^2$$\n",
    "\n",
    "Cela force les poids a rester **petits**, ce qui produit des modeles plus simples et mieux generalises.\n",
    "\n",
    "En PyTorch, il suffit d'ajouter le parametre `weight_decay` a l'optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modele avec Weight Decay ===\n",
    "\n",
    "modele_wd = GrosModele().to(device)  # Meme architecture sans Dropout\n",
    "\n",
    "# Le weight_decay est le parametre lambda de la regularisation L2\n",
    "WEIGHT_DECAY = 0.01  # Valeur typique : entre 0.0001 et 0.1\n",
    "\n",
    "optimiseur_wd = optim.Adam(modele_wd.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"Entrainement du modele avec Weight Decay (lambda={WEIGHT_DECAY}) :\")\n",
    "historique_wd = entrainer_modele(modele_wd, optimiseur_wd, nb_epochs=200)\n",
    "\n",
    "# Visualiser\n",
    "tracer_courbes(historique_wd, f'AVEC Weight Decay (lambda={WEIGHT_DECAY})')\n",
    "tracer_frontiere_decision(modele_wd, X_train, y_train,\n",
    "                          'Frontiere de decision - Avec Weight Decay')\n",
    "\n",
    "# Comparer la norme des poids\n",
    "norme_overfit = sum(p.data.norm().item()**2 for p in modele_overfit.parameters())**0.5\n",
    "norme_wd = sum(p.data.norm().item()**2 for p in modele_wd.parameters())**0.5\n",
    "print(f\"\\nNorme des poids (sans regularisation) : {norme_overfit:.2f}\")\n",
    "print(f\"Norme des poids (avec Weight Decay)   : {norme_wd:.2f}\")\n",
    "print(f\"Reduction : {(1 - norme_wd/norme_overfit)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 - Early Stopping\n",
    "\n",
    "L'**Early Stopping** est la technique la plus simple et souvent la plus efficace pour eviter l'overfitting.\n",
    "\n",
    "**Principe :** On surveille la perte de validation pendant l'entrainement. Si elle ne s'ameliore plus pendant un certain nombre d'epochs (**patience**), on arrete et on revient au meilleur modele.\n",
    "\n",
    "```\n",
    "Perte de validation :\n",
    "  |\\        /\n",
    "  | \\      /  <-- Overfitting commence ici\n",
    "  |  \\    /\n",
    "  |   \\__/    <-- On s'arrete au minimum\n",
    "  +---------> Epochs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Entrainement avec Early Stopping ===\n",
    "\n",
    "def entrainer_avec_early_stopping(modele, optimiseur, patience=15, nb_epochs_max=300):\n",
    "    \"\"\"Entraine un modele avec Early Stopping.\n",
    "\n",
    "    Args:\n",
    "        patience: nombre d'epochs sans amelioration avant d'arreter\n",
    "        nb_epochs_max: nombre maximal d'epochs\n",
    "    \"\"\"\n",
    "    critere = nn.CrossEntropyLoss()\n",
    "    historique = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    meilleure_val_loss = float('inf')\n",
    "    meilleur_modele = None\n",
    "    compteur_patience = 0\n",
    "    epoch_arret = nb_epochs_max\n",
    "\n",
    "    for epoch in range(nb_epochs_max):\n",
    "        # --- Entrainement ---\n",
    "        modele.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            predictions = modele(X_batch)\n",
    "            loss = critere(predictions, y_batch)\n",
    "            optimiseur.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiseur.step()\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # --- Validation ---\n",
    "        modele.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = modele(X_val_t)\n",
    "            val_loss = critere(val_preds, y_val_t).item()\n",
    "            _, val_predicted = torch.max(val_preds, 1)\n",
    "            val_acc = 100 * (val_predicted == y_val_t).sum().item() / len(y_val_t)\n",
    "\n",
    "        historique['train_loss'].append(train_loss)\n",
    "        historique['val_loss'].append(val_loss)\n",
    "        historique['train_acc'].append(train_acc)\n",
    "        historique['val_acc'].append(val_acc)\n",
    "\n",
    "        # --- Logique Early Stopping ---\n",
    "        if val_loss < meilleure_val_loss:\n",
    "            meilleure_val_loss = val_loss\n",
    "            meilleur_modele = deepcopy(modele.state_dict())  # Sauvegarder le meilleur modele\n",
    "            compteur_patience = 0\n",
    "        else:\n",
    "            compteur_patience += 1\n",
    "\n",
    "        if compteur_patience >= patience:\n",
    "            epoch_arret = epoch + 1\n",
    "            print(f\"  Early Stopping declenchee a l'epoch {epoch_arret} !\")\n",
    "            print(f\"  Meilleure val loss : {meilleure_val_loss:.4f} (a l'epoch {epoch_arret - patience})\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch+1:>3}/{nb_epochs_max} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Patience: {compteur_patience}/{patience}\")\n",
    "\n",
    "    # Restaurer le meilleur modele\n",
    "    if meilleur_modele is not None:\n",
    "        modele.load_state_dict(meilleur_modele)\n",
    "\n",
    "    return historique, epoch_arret\n",
    "\n",
    "\n",
    "# Entrainer avec Early Stopping\n",
    "modele_es = GrosModele().to(device)\n",
    "optimiseur_es = optim.Adam(modele_es.parameters(), lr=0.001)\n",
    "\n",
    "PATIENCE = 15\n",
    "print(f\"Entrainement avec Early Stopping (patience={PATIENCE}) :\")\n",
    "historique_es, epoch_arret = entrainer_avec_early_stopping(\n",
    "    modele_es, optimiseur_es, patience=PATIENCE, nb_epochs_max=300\n",
    ")\n",
    "\n",
    "# Visualiser avec la ligne d'arret\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, len(historique_es['train_loss']) + 1)\n",
    "\n",
    "ax1.plot(epochs, historique_es['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(epochs, historique_es['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "meilleure_epoch = epoch_arret - PATIENCE\n",
    "if meilleure_epoch > 0:\n",
    "    ax1.axvline(x=meilleure_epoch, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Meilleur modele (epoch {meilleure_epoch})')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Perte')\n",
    "ax1.set_title('Perte avec Early Stopping')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs, historique_es['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "ax2.plot(epochs, historique_es['val_acc'], 'r-', label='Validation', linewidth=2)\n",
    "if meilleure_epoch > 0:\n",
    "    ax2.axvline(x=meilleure_epoch, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Meilleur modele (epoch {meilleure_epoch})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Precision (%)')\n",
    "ax2.set_title('Precision avec Early Stopping')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Early Stopping (patience={PATIENCE}, arret epoch {epoch_arret})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tracer_frontiere_decision(modele_es, X_train, y_train,\n",
    "                          f'Frontiere de decision - Early Stopping (epoch {meilleure_epoch})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 - Data Augmentation (concept)\n",
    "\n",
    "La **Data Augmentation** consiste a creer de nouvelles donnees d'entrainement en appliquant des **transformations** aux donnees existantes. C'est particulierement utile pour les images.\n",
    "\n",
    "Transformations courantes :\n",
    "- **Rotation** aleatoire\n",
    "- **Flip** horizontal/vertical\n",
    "- **Crop** (recadrage) aleatoire\n",
    "- **Changement de luminosite/contraste**\n",
    "- **Ajout de bruit**\n",
    "\n",
    "> **Principe :** Plus on a de donnees (meme synthetiques), moins le modele risque d'overfitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Demonstration de la Data Augmentation sur des images ===\n",
    "\n",
    "# Charger quelques images MNIST pour la demonstration\n",
    "mnist_demo = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Definir differentes transformations\n",
    "augmentations = {\n",
    "    'Original': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Rotation\\n(+/- 30 deg)': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Translation\\naleatoire': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Mise a\\nl\\'echelle': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomAffine(degrees=0, scale=(0.7, 1.3)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Deformation\\nperspective': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomPerspective(distortion_scale=0.4, p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'Effacement\\naleatoire': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=1.0, scale=(0.05, 0.2)),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Afficher les augmentations pour 3 images differentes\n",
    "nb_images = 3\n",
    "nb_transforms = len(augmentations)\n",
    "\n",
    "fig, axes = plt.subplots(nb_images, nb_transforms, figsize=(16, 8))\n",
    "fig.suptitle('Exemples de Data Augmentation sur MNIST', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(nb_images):\n",
    "    image_originale = mnist_demo[i][0]  # Tensor de l'image\n",
    "\n",
    "    for j, (nom, transform) in enumerate(augmentations.items()):\n",
    "        img_augmentee = transform(image_originale)\n",
    "        axes[i, j].imshow(img_augmentee.squeeze().numpy(), cmap='gray')\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(nom, fontsize=10, fontweight='bold')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLa Data Augmentation multiplie artificiellement la taille du dataset.\")\n",
    "print(\"Chaque epoch, le modele voit des versions legerement differentes des images,\")\n",
    "print(\"ce qui l'empeche de memoriser les exemples exacts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 - Comparaison Finale\n",
    "\n",
    "Comparons toutes les approches de regularisation sur notre dataset synthetique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation finale de tous les modeles sur le jeu de TEST ===\n",
    "\n",
    "def evaluer_sur_test(modele, nom):\n",
    "    \"\"\"Evalue un modele sur le jeu de test et retourne les resultats.\"\"\"\n",
    "    modele.eval()\n",
    "    critere = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = modele(X_test_t)\n",
    "        loss = critere(preds, y_test_t).item()\n",
    "        _, predicted = torch.max(preds, 1)\n",
    "        acc = 100 * (predicted == y_test_t).sum().item() / len(y_test_t)\n",
    "\n",
    "    return {'nom': nom, 'test_loss': loss, 'test_acc': acc}\n",
    "\n",
    "\n",
    "# Evaluer chaque modele\n",
    "resultats = [\n",
    "    evaluer_sur_test(modele_overfit, 'Sans regularisation'),\n",
    "    evaluer_sur_test(modele_dropout, 'Dropout (p=0.4)'),\n",
    "    evaluer_sur_test(modele_wd, f'Weight Decay ({WEIGHT_DECAY})'),\n",
    "    evaluer_sur_test(modele_es, f'Early Stopping (p={PATIENCE})'),\n",
    "]\n",
    "\n",
    "# Tableau recapitulatif\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Methode':<30} | {'Test Loss':>12} | {'Test Accuracy':>14}\")\n",
    "print(\"=\" * 65)\n",
    "for r in resultats:\n",
    "    print(f\"{r['nom']:<30} | {r['test_loss']:>12.4f} | {r['test_acc']:>13.1f}%\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# === Graphique de comparaison ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "noms = [r['nom'] for r in resultats]\n",
    "couleurs = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "# Precision\n",
    "accs = [r['test_acc'] for r in resultats]\n",
    "bars1 = ax1.bar(range(len(noms)), accs, color=couleurs, edgecolor='black', width=0.6)\n",
    "ax1.set_ylabel('Precision sur le test (%)')\n",
    "ax1.set_title('Precision sur le jeu de test', fontweight='bold')\n",
    "ax1.set_xticks(range(len(noms)))\n",
    "ax1.set_xticklabels(noms, rotation=20, ha='right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars1, accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Perte\n",
    "losses = [r['test_loss'] for r in resultats]\n",
    "bars2 = ax2.bar(range(len(noms)), losses, color=couleurs, edgecolor='black', width=0.6)\n",
    "ax2.set_ylabel('Perte sur le test')\n",
    "ax2.set_title('Perte sur le jeu de test', fontweight='bold')\n",
    "ax2.set_xticks(range(len(noms)))\n",
    "ax2.set_xticklabels(noms, rotation=20, ha='right', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, loss in zip(bars2, losses):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "             f'{loss:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comparaison des techniques de regularisation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Comparaison des courbes de validation ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "historiques = [\n",
    "    (historique_overfit, 'Sans regularisation', '#e74c3c'),\n",
    "    (historique_dropout, 'Dropout', '#3498db'),\n",
    "    (historique_wd, 'Weight Decay', '#2ecc71'),\n",
    "    (historique_es, 'Early Stopping', '#f39c12'),\n",
    "]\n",
    "\n",
    "for hist, nom, couleur in historiques:\n",
    "    epochs = range(1, len(hist['val_loss']) + 1)\n",
    "    ax1.plot(epochs, hist['val_loss'], label=nom, linewidth=2, color=couleur, alpha=0.8)\n",
    "    ax2.plot(epochs, hist['val_acc'], label=nom, linewidth=2, color=couleur, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Perte de validation')\n",
    "ax1.set_title('Evolution de la perte de validation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Precision de validation (%)')\n",
    "ax2.set_title('Evolution de la precision de validation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Courbes de validation - Toutes les approches', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Comparaison des frontieres de decision ===\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n",
    "modeles = [\n",
    "    (modele_overfit, 'Sans regularisation'),\n",
    "    (modele_dropout, 'Dropout'),\n",
    "    (modele_wd, 'Weight Decay'),\n",
    "    (modele_es, 'Early Stopping'),\n",
    "]\n",
    "\n",
    "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "grille = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]).to(device)\n",
    "\n",
    "for ax, (mod, nom) in zip(axes, modeles):\n",
    "    mod.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = mod(grille)\n",
    "        Z = torch.softmax(Z, dim=1)[:, 1].cpu().numpy().reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdYlBu', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu',\n",
    "               edgecolors='black', s=40)\n",
    "    ax.set_title(nom, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "\n",
    "plt.suptitle('Frontieres de decision comparees (sur donnees de TEST)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "### Resume des techniques de regularisation\n",
    "\n",
    "| Technique | Principe | Quand l'utiliser |\n",
    "|---|---|---|\n",
    "| **Dropout** | Desactive aleatoirement des neurones | Reseaux denses (MLP), facile a ajouter |\n",
    "| **Weight Decay (L2)** | Penalise les grands poids | Toujours, en premiere intention |\n",
    "| **Early Stopping** | Arrete l'entrainement au bon moment | Toujours, indispensable en pratique |\n",
    "| **Data Augmentation** | Augmente artificiellement le dataset | Vision par ordinateur surtout |\n",
    "| **Batch Normalization** | Normalise les activations | Reseaux profonds (> 5 couches) |\n",
    "\n",
    "---\n",
    "\n",
    "### Conseil de pro : Comment detecter l'overfitting ?\n",
    "\n",
    "> **Toujours tracer les courbes train vs validation !** C'est le moyen le plus fiable.\n",
    "> - Si les courbes **divergent** (train descend, val remonte) = overfitting\n",
    "> - Si les courbes **restent proches** = bonne generalisation\n",
    "> - Si les deux courbes sont **hautes** = underfitting (modele trop simple)\n",
    "\n",
    "### Conseil de pro : Quelle technique choisir ?\n",
    "\n",
    "> En pratique, on **combine** souvent plusieurs techniques :\n",
    "> 1. **Toujours** utiliser Early Stopping\n",
    "> 2. **Toujours** essayer un peu de Weight Decay (0.0001 a 0.01)\n",
    "> 3. Ajouter du Dropout si necessaire (0.1 a 0.5)\n",
    "> 4. Utiliser la Data Augmentation si possible (surtout en vision)\n",
    "> 5. Si rien ne marche : **collecter plus de donnees** !\n",
    "\n",
    "### Conseil de pro : Le biais-variance tradeoff\n",
    "\n",
    "> - **Biais eleve** (underfitting) : le modele est trop simple -> augmenter la capacite\n",
    "> - **Variance elevee** (overfitting) : le modele est trop complexe -> regulariser\n",
    "> - L'objectif est de trouver le **juste milieu** entre les deux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}