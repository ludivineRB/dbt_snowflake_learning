{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premier Reseau de Neurones avec PyTorch\n",
    "\n",
    "> Ce notebook est un exemple pratique clef en main pour construire votre premier reseau de neurones avec PyTorch.\n",
    "> Nous allons partir de zero et construire pas a pas un classificateur d'images capable de reconnaitre des chiffres manuscrits.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre les **tensors PyTorch** et leurs operations\n",
    "- Charger et preparer un dataset d'images (**MNIST**)\n",
    "- Construire un **Perceptron Multi-Couches** (MLP)\n",
    "- Implementer une **boucle d'entrainement** complete\n",
    "- **Evaluer** les performances du modele\n",
    "- **Ameliorer** l'architecture avec Dropout et BatchNorm\n",
    "\n",
    "### Pre-requis\n",
    "\n",
    "```bash\n",
    "# Installation des dependances (si necessaire)\n",
    "uv add torch torchvision matplotlib numpy scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Configuration de matplotlib pour de jolis graphiques\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Detection automatique du device (GPU si disponible, sinon CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilise : {device}\")\n",
    "print(f\"Version de PyTorch : {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 - Les Tensors PyTorch\n",
    "\n",
    "Les **tensors** sont la structure de donnees fondamentale de PyTorch. Ce sont des tableaux multidimensionnels similaires aux arrays NumPy, mais avec deux avantages majeurs :\n",
    "- Ils peuvent etre transferes sur **GPU** pour des calculs acceleres\n",
    "- Ils supportent la **differentiation automatique** (autograd) pour la retropropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Creation de tensors ===\n",
    "\n",
    "# A partir d'une liste Python\n",
    "tensor_liste = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"Tensor depuis une liste : {tensor_liste}\")\n",
    "print(f\"  Shape : {tensor_liste.shape}\")\n",
    "print(f\"  Dtype : {tensor_liste.dtype}\")\n",
    "print()\n",
    "\n",
    "# Tensors speciaux\n",
    "zeros = torch.zeros(3, 4)        # Matrice 3x4 de zeros\n",
    "ones = torch.ones(2, 3)          # Matrice 2x3 de uns\n",
    "aleatoire = torch.randn(3, 3)    # Matrice 3x3 aleatoire (distribution normale)\n",
    "\n",
    "print(f\"Matrice de zeros (3x4) :\\n{zeros}\\n\")\n",
    "print(f\"Matrice aleatoire (3x3) :\\n{aleatoire}\\n\")\n",
    "\n",
    "# === Operations sur les tensors ===\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "# Operations element par element\n",
    "print(f\"Addition : \\n{a + b}\\n\")\n",
    "print(f\"Multiplication element par element : \\n{a * b}\\n\")\n",
    "\n",
    "# Produit matriciel (tres utilise dans les reseaux de neurones)\n",
    "produit = torch.matmul(a, b)  # ou a @ b\n",
    "print(f\"Produit matriciel (a @ b) : \\n{produit}\\n\")\n",
    "\n",
    "# === Reshape : redimensionner un tensor ===\n",
    "# C'est crucial : une image 28x28 doit etre \"aplatie\" en vecteur de 784\n",
    "image_simulee = torch.randn(28, 28)\n",
    "vecteur = image_simulee.view(-1)  # -1 = calcul automatique de la taille\n",
    "print(f\"Image 28x28 -> Vecteur : {image_simulee.shape} -> {vecteur.shape}\")\n",
    "\n",
    "# === Device : transfert CPU <-> GPU ===\n",
    "tensor_cpu = torch.randn(3, 3)\n",
    "tensor_device = tensor_cpu.to(device)\n",
    "print(f\"\\nTensor sur {tensor_device.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 - Le Dataset MNIST\n",
    "\n",
    "**MNIST** est le \"Hello World\" du deep learning. Il contient **70 000 images** de chiffres manuscrits (0-9) en niveaux de gris, de taille 28x28 pixels.\n",
    "\n",
    "- **60 000** images d'entrainement\n",
    "- **10 000** images de test\n",
    "\n",
    "Le dataset est telecharge automatiquement par `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Chargement du dataset MNIST ===\n",
    "\n",
    "# Transformation : convertir les images en tensors et normaliser les pixels [0,1] -> [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                          # Convertit l'image PIL en tensor [0, 1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))      # Normalise avec la moyenne et ecart-type de MNIST\n",
    "])\n",
    "\n",
    "# Telechargement et chargement des donnees\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoaders : chargent les donnees par lots (batches)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Nombre d'images d'entrainement : {len(train_dataset)}\")\n",
    "print(f\"Nombre d'images de test : {len(test_dataset)}\")\n",
    "print(f\"Taille d'un batch : {BATCH_SIZE}\")\n",
    "print(f\"Nombre de batches d'entrainement : {len(train_loader)}\")\n",
    "\n",
    "# Inspectons un batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nShape d'un batch d'images : {images.shape}\")\n",
    "print(f\"  -> {images.shape[0]} images, {images.shape[1]} canal, {images.shape[2]}x{images.shape[3]} pixels\")\n",
    "print(f\"Shape des labels : {labels.shape}\")\n",
    "\n",
    "# === Visualisation d'echantillons ===\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
    "fig.suptitle('Echantillons du dataset MNIST', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # On \"denormalise\" pour l'affichage\n",
    "    img = images[i].squeeze()  # Retirer la dimension du canal (1, 28, 28) -> (28, 28)\n",
    "    ax.imshow(img.numpy(), cmap='gray')\n",
    "    ax.set_title(f'Label : {labels[i].item()}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des classes\n",
    "all_labels = [label for _, label in train_dataset]\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(all_labels, bins=range(11), edgecolor='black', align='left', color='steelblue')\n",
    "ax.set_xlabel('Chiffre')\n",
    "ax.set_ylabel('Nombre d\\'images')\n",
    "ax.set_title('Distribution des classes dans MNIST (entrainement)')\n",
    "ax.set_xticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 - Construire un Reseau de Neurones\n",
    "\n",
    "Nous allons construire un **Perceptron Multi-Couches** (MLP) avec l'architecture suivante :\n",
    "\n",
    "```\n",
    "Image (28x28 = 784 pixels)\n",
    "    |  Couche Lineaire (784 -> 128) + ReLU\n",
    "    v\n",
    "128 neurones\n",
    "    |  Couche Lineaire (128 -> 64) + ReLU\n",
    "    v\n",
    "64 neurones\n",
    "    |  Couche Lineaire (64 -> 10)\n",
    "    v\n",
    "10 sorties (une par chiffre 0-9)\n",
    "```\n",
    "\n",
    "**ReLU** (Rectified Linear Unit) est la fonction d'activation la plus courante : `f(x) = max(0, x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Definition du modele MLP ===\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Perceptron Multi-Couches pour la classification de chiffres MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Definir les couches du reseau\n",
    "        self.couches = nn.Sequential(\n",
    "            nn.Flatten(),           # (batch, 1, 28, 28) -> (batch, 784)\n",
    "            nn.Linear(784, 128),    # Couche 1 : 784 entrees -> 128 neurones\n",
    "            nn.ReLU(),              # Fonction d'activation\n",
    "            nn.Linear(128, 64),     # Couche 2 : 128 -> 64 neurones\n",
    "            nn.ReLU(),              # Fonction d'activation\n",
    "            nn.Linear(64, 10),      # Couche de sortie : 64 -> 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagation avant : passe les donnees a travers le reseau.\"\"\"\n",
    "        return self.couches(x)\n",
    "\n",
    "\n",
    "# Instancier le modele et le deplacer sur le device\n",
    "modele = MLP().to(device)\n",
    "print(modele)\n",
    "\n",
    "# Compter le nombre de parametres\n",
    "nb_params = sum(p.numel() for p in modele.parameters())\n",
    "nb_params_entrainables = sum(p.numel() for p in modele.parameters() if p.requires_grad)\n",
    "print(f\"\\nNombre total de parametres : {nb_params:,}\")\n",
    "print(f\"Nombre de parametres entrainables : {nb_params_entrainables:,}\")\n",
    "\n",
    "# Verification avec un batch factice\n",
    "batch_test = torch.randn(4, 1, 28, 28).to(device)  # 4 images factices\n",
    "sortie = modele(batch_test)\n",
    "print(f\"\\nShape de la sortie : {sortie.shape}\")\n",
    "print(f\"Sortie brute (logits) pour la 1ere image : {sortie[0].detach().cpu().numpy().round(3)}\")\n",
    "print(f\"Prediction : chiffre {sortie[0].argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 - La Boucle d'Entrainement\n",
    "\n",
    "L'entrainement d'un reseau de neurones suit toujours le meme schema :\n",
    "\n",
    "1. **Propagation avant** (forward pass) : passer les donnees dans le reseau\n",
    "2. **Calcul de la perte** (loss) : mesurer l'erreur entre la prediction et la verite\n",
    "3. **Retropropagation** (backward pass) : calculer les gradients\n",
    "4. **Mise a jour** des poids : l'optimiseur ajuste les parametres\n",
    "\n",
    "Nous utilisons :\n",
    "- **CrossEntropyLoss** : fonction de perte standard pour la classification multi-classes\n",
    "- **Adam** : optimiseur adaptatif, tres utilise en pratique\n",
    "\n",
    "> L'entrainement sur 5 epochs prend environ 1-2 minutes sur CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration de l'entrainement ===\n",
    "\n",
    "# Reinitialiser le modele pour un entrainement propre\n",
    "modele = MLP().to(device)\n",
    "\n",
    "# Fonction de perte : CrossEntropyLoss combine LogSoftmax + NLLLoss\n",
    "critere = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimiseur : Adam avec un taux d'apprentissage de 0.001\n",
    "optimiseur = optim.Adam(modele.parameters(), lr=0.001)\n",
    "\n",
    "# Nombre d'epochs (passages complets sur le dataset)\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "# Historique pour les graphiques\n",
    "historique = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': []\n",
    "}\n",
    "\n",
    "# === Boucle d'entrainement ===\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Epoch':>6} | {'Train Loss':>12} | {'Train Acc':>10} | {'Test Loss':>12} | {'Test Acc':>10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    # --- Phase d'entrainement ---\n",
    "    modele.train()  # Mode entrainement\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Envoyer les donnees sur le device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # 1. Propagation avant\n",
    "        predictions = modele(images)\n",
    "\n",
    "        # 2. Calcul de la perte\n",
    "        loss = critere(predictions, labels)\n",
    "\n",
    "        # 3. Retropropagation\n",
    "        optimiseur.zero_grad()  # Remettre les gradients a zero\n",
    "        loss.backward()         # Calculer les gradients\n",
    "\n",
    "        # 4. Mise a jour des poids\n",
    "        optimiseur.step()\n",
    "\n",
    "        # Statistiques\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # --- Phase d'evaluation ---\n",
    "    modele.eval()  # Mode evaluation (desactive Dropout, etc.)\n",
    "    total_loss_test = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():  # Pas besoin de calculer les gradients\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            predictions = modele(images)\n",
    "            loss = critere(predictions, labels)\n",
    "\n",
    "            total_loss_test += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = total_loss_test / total_test\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "\n",
    "    # Sauvegarder l'historique\n",
    "    historique['train_loss'].append(train_loss)\n",
    "    historique['train_acc'].append(train_acc)\n",
    "    historique['test_loss'].append(test_loss)\n",
    "    historique['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f\"{epoch+1:>6} | {train_loss:>12.4f} | {train_acc:>9.2f}% | {test_loss:>12.4f} | {test_acc:>9.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Entrainement termine !\")\n",
    "\n",
    "# === Visualiser les courbes d'entrainement ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot(range(1, NB_EPOCHS+1), historique['train_loss'], 'b-o', label='Train', linewidth=2)\n",
    "ax1.plot(range(1, NB_EPOCHS+1), historique['test_loss'], 'r-o', label='Test', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Perte (Loss)')\n",
    "ax1.set_title('Evolution de la perte')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Courbe de precision\n",
    "ax2.plot(range(1, NB_EPOCHS+1), historique['train_acc'], 'b-o', label='Train', linewidth=2)\n",
    "ax2.plot(range(1, NB_EPOCHS+1), historique['test_acc'], 'r-o', label='Test', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Precision (%)')\n",
    "ax2.set_title('Evolution de la precision')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Courbes d\\'entrainement du MLP sur MNIST', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 - Evaluation\n",
    "\n",
    "Evaluons notre modele en detail :\n",
    "- **Precision globale** sur le jeu de test\n",
    "- **Matrice de confusion** pour voir quels chiffres sont confondus\n",
    "- **Visualisation** de predictions correctes et incorrectes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation detaillee sur le jeu de test ===\n",
    "\n",
    "modele.eval()\n",
    "toutes_predictions = []\n",
    "tous_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        sorties = modele(images)\n",
    "        _, predictions = torch.max(sorties, 1)\n",
    "        toutes_predictions.extend(predictions.cpu().numpy())\n",
    "        tous_labels.extend(labels.numpy())\n",
    "\n",
    "toutes_predictions = np.array(toutes_predictions)\n",
    "tous_labels = np.array(tous_labels)\n",
    "\n",
    "# Precision globale\n",
    "precision_globale = 100 * (toutes_predictions == tous_labels).sum() / len(tous_labels)\n",
    "print(f\"Precision globale sur le jeu de test : {precision_globale:.2f}%\")\n",
    "\n",
    "# Precision par classe\n",
    "print(f\"\\n{'Chiffre':>8} | {'Precision':>10} | {'Nb correct':>11} / {'Total':>6}\")\n",
    "print(\"-\" * 50)\n",
    "for chiffre in range(10):\n",
    "    masque = tous_labels == chiffre\n",
    "    nb_correct = (toutes_predictions[masque] == chiffre).sum()\n",
    "    nb_total = masque.sum()\n",
    "    precision = 100 * nb_correct / nb_total\n",
    "    print(f\"{chiffre:>8} | {precision:>9.1f}% | {nb_correct:>11} / {nb_total:>6}\")\n",
    "\n",
    "# === Matrice de confusion ===\n",
    "cm = confusion_matrix(tous_labels, toutes_predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "ax.set_title('Matrice de confusion - MLP sur MNIST', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Verite')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualisation de predictions ===\n",
    "\n",
    "# Recuperer un batch d'images de test\n",
    "images_test, labels_test = next(iter(test_loader))\n",
    "images_test_device = images_test.to(device)\n",
    "\n",
    "modele.eval()\n",
    "with torch.no_grad():\n",
    "    sorties = modele(images_test_device)\n",
    "    probas = torch.softmax(sorties, dim=1)  # Convertir les logits en probabilites\n",
    "    _, preds = torch.max(sorties, 1)\n",
    "\n",
    "preds = preds.cpu()\n",
    "probas = probas.cpu()\n",
    "\n",
    "# Separer les predictions correctes et incorrectes\n",
    "corrects = (preds == labels_test).nonzero(as_tuple=True)[0]\n",
    "incorrects = (preds != labels_test).nonzero(as_tuple=True)[0]\n",
    "\n",
    "# --- Predictions CORRECTES ---\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "fig.suptitle('Predictions CORRECTES', fontsize=16, fontweight='bold', color='green')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(corrects):\n",
    "        idx = corrects[i].item()\n",
    "        img = images_test[idx].squeeze()\n",
    "        ax.imshow(img.numpy(), cmap='gray')\n",
    "        confiance = probas[idx][preds[idx]].item() * 100\n",
    "        ax.set_title(f'Pred: {preds[idx].item()} ({confiance:.0f}%)', fontsize=11, color='green')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Predictions INCORRECTES ---\n",
    "if len(incorrects) > 0:\n",
    "    nb_incorrects_affichees = min(10, len(incorrects))\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "    fig.suptitle('Predictions INCORRECTES', fontsize=16, fontweight='bold', color='red')\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(incorrects):\n",
    "            idx = incorrects[i].item()\n",
    "            img = images_test[idx].squeeze()\n",
    "            ax.imshow(img.numpy(), cmap='gray')\n",
    "            confiance = probas[idx][preds[idx]].item() * 100\n",
    "            ax.set_title(\n",
    "                f'Pred: {preds[idx].item()} ({confiance:.0f}%)\\nVrai: {labels_test[idx].item()}',\n",
    "                fontsize=10, color='red'\n",
    "            )\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Aucune prediction incorrecte dans ce batch ! Le modele est performant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 - Ameliorations\n",
    "\n",
    "Ameliorons notre modele avec deux techniques courantes :\n",
    "\n",
    "### Dropout\n",
    "Le **Dropout** desactive aleatoirement des neurones pendant l'entrainement (typiquement 20-50% des neurones). Cela force le reseau a ne pas dependre d'un seul neurone et ameliore la **generalisation**.\n",
    "\n",
    "### Batch Normalization\n",
    "La **Batch Normalization** normalise les activations de chaque couche, ce qui :\n",
    "- Stabilise et accelere l'entrainement\n",
    "- Permet d'utiliser des taux d'apprentissage plus eleves\n",
    "- A un leger effet de regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MLP Ameliore avec Dropout et BatchNorm ===\n",
    "\n",
    "class MLPAmeliore(nn.Module):\n",
    "    \"\"\"MLP ameliore avec Dropout et Batch Normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(MLPAmeliore, self).__init__()\n",
    "\n",
    "        self.couches = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(784, 128),\n",
    "            nn.BatchNorm1d(128),     # Normalisation par batch\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate), # Dropout : desactive 30% des neurones\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.couches(x)\n",
    "\n",
    "\n",
    "# Instancier le modele ameliore\n",
    "modele_ameliore = MLPAmeliore(dropout_rate=0.3).to(device)\n",
    "print(modele_ameliore)\n",
    "\n",
    "nb_params_ameliore = sum(p.numel() for p in modele_ameliore.parameters())\n",
    "print(f\"\\nNombre de parametres : {nb_params_ameliore:,} (vs {nb_params:,} pour le MLP simple)\")\n",
    "\n",
    "# === Entrainement du modele ameliore ===\n",
    "critere_ameliore = nn.CrossEntropyLoss()\n",
    "optimiseur_ameliore = optim.Adam(modele_ameliore.parameters(), lr=0.001)\n",
    "\n",
    "historique_ameliore = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'Epoch':>6} | {'Train Loss':>12} | {'Train Acc':>10} | {'Test Loss':>12} | {'Test Acc':>10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    # Entrainement\n",
    "    modele_ameliore.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = modele_ameliore(images)\n",
    "        loss = critere_ameliore(predictions, labels)\n",
    "        optimiseur_ameliore.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiseur_ameliore.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Evaluation\n",
    "    modele_ameliore.eval()\n",
    "    total_loss_test, correct_test, total_test = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            predictions = modele_ameliore(images)\n",
    "            loss = critere_ameliore(predictions, labels)\n",
    "            total_loss_test += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = total_loss_test / total_test\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "\n",
    "    historique_ameliore['train_loss'].append(train_loss)\n",
    "    historique_ameliore['train_acc'].append(train_acc)\n",
    "    historique_ameliore['test_loss'].append(test_loss)\n",
    "    historique_ameliore['test_acc'].append(test_acc)\n",
    "\n",
    "    print(f\"{epoch+1:>6} | {train_loss:>12.4f} | {train_acc:>9.2f}% | {test_loss:>12.4f} | {test_acc:>9.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === Comparaison des deux modeles ===\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Perte\n",
    "ax1.plot(range(1, NB_EPOCHS+1), historique['test_loss'], 'r-o', label='MLP Simple (test)', linewidth=2)\n",
    "ax1.plot(range(1, NB_EPOCHS+1), historique_ameliore['test_loss'], 'g-s', label='MLP Ameliore (test)', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Perte (Loss)')\n",
    "ax1.set_title('Comparaison de la perte sur le test')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision\n",
    "ax2.plot(range(1, NB_EPOCHS+1), historique['test_acc'], 'r-o', label='MLP Simple (test)', linewidth=2)\n",
    "ax2.plot(range(1, NB_EPOCHS+1), historique_ameliore['test_acc'], 'g-s', label='MLP Ameliore (test)', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Precision (%)')\n",
    "ax2.set_title('Comparaison de la precision sur le test')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MLP Simple vs MLP Ameliore (Dropout + BatchNorm)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resume\n",
    "print(f\"\\n{'Modele':<20} | {'Precision test finale':>22}\")\n",
    "print(\"-\" * 46)\n",
    "print(f\"{'MLP Simple':<20} | {historique['test_acc'][-1]:>21.2f}%\")\n",
    "print(f\"{'MLP Ameliore':<20} | {historique_ameliore['test_acc'][-1]:>21.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "### Points cles a retenir\n",
    "\n",
    "| Concept | Description |\n",
    "|---|---|\n",
    "| **Tensor** | Structure de donnees fondamentale de PyTorch, similaire a un array NumPy mais compatible GPU |\n",
    "| **nn.Module** | Classe de base pour definir un reseau de neurones en PyTorch |\n",
    "| **Forward pass** | Propagation des donnees a travers le reseau |\n",
    "| **Backward pass** | Calcul des gradients par retropropagation |\n",
    "| **CrossEntropyLoss** | Fonction de perte standard pour la classification multi-classes |\n",
    "| **Adam** | Optimiseur adaptatif, bon choix par defaut |\n",
    "| **Dropout** | Regularisation par desactivation aleatoire de neurones |\n",
    "| **BatchNorm** | Normalisation des activations pour stabiliser l'entrainement |\n",
    "\n",
    "### Pour aller plus loin\n",
    "\n",
    "- Essayez de modifier l'architecture (plus de couches, plus de neurones)\n",
    "- Testez differents taux d'apprentissage (lr)\n",
    "- Remplacez le MLP par un **CNN** (Convolutional Neural Network) pour de meilleures performances\n",
    "- Explorez d'autres datasets : Fashion-MNIST, CIFAR-10\n",
    "\n",
    "> **Prochain notebook** : Nous verrons en detail l'**overfitting** et les techniques de **regularisation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}