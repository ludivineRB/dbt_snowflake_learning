# 05 - RÃ©seaux de Neurones RÃ©currents (RNN)

[â† 04 - Embeddings](04-word-embeddings.md) | [ğŸ  Accueil](README.md) | [06 - Transformers â†’](06-attention-transformers.md)

---

## ğŸ”„ GÃ©rer la SÃ©quentialitÃ©

### ğŸ§  Pourquoi les RNN ?
Les rÃ©seaux classiques ne se souviennent pas du passÃ©. Les RNN possÃ¨dent une boucle de rÃ©troaction qui sert de mÃ©moire pour traiter les phrases mot par mot.

### âš™ï¸ LSTM et GRU
Architectures Ã©voluÃ©es qui rÃ©solvent le problÃ¨me de la disparition du gradient (oubli des infos lointaines) grÃ¢ce Ã  des "portes" mathÃ©matiques.
- **LSTM** : MÃ©moire Ã  long et court terme.
- **GRU** : Version simplifiÃ©e et plus rapide du LSTM.

---

[â† 04 - Embeddings](04-word-embeddings.md) | [ğŸ  Accueil](README.md) | [06 - Transformers â†’](06-attention-transformers.md)
