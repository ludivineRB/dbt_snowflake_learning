{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Architecture Transformer - Module 6\n",
    "\n",
    "Ce notebook impl√©mente l'architecture compl√®te du Transformer avec TensorFlow.\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Construire un Transformer complet from scratch\n",
    "- Comprendre Encoder et Decoder\n",
    "- Impl√©menter Positional Encoding\n",
    "- Entra√Æner sur une t√¢che de traduction\n",
    "- Visualiser les r√©sultats et l'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et configuration\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"üîß TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß NumPy version: {np.__version__}\")\n",
    "print(f\"üìä GPU disponible: {len(tf.config.list_physical_devices('GPU'))} GPU(s)\")\n",
    "\n",
    "# Configuration pour la reproductibilit√©\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üß© Composants de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"\n",
    "    Calcule les angles pour l'encodage positionnel\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Cr√©e l'encodage positionnel pour le Transformer\n",
    "    \"\"\"\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "    \n",
    "    # Appliquer sin aux positions paires\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Appliquer cos aux positions impaires  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Visualisation de l'encodage positionnel\n",
    "def visualize_positional_encoding():\n",
    "    pos_encoding = positional_encoding(50, 128)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pos_encoding[0].numpy().T,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üìç Encodage Positionnel (Position vs Dimension)\",\n",
    "        xaxis_title=\"Position dans la s√©quence\",\n",
    "        yaxis_title=\"Dimension d'embedding\",\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üí° L'encodage positionnel permet au mod√®le de comprendre l'ordre des mots\")\n",
    "    print(\"   sans avoir besoin de traitement s√©quentiel comme les RNN\")\n",
    "\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîç Multi-Head Attention (R√©impl√©mentation Optimis√©e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calcule l'attention scaled dot-product\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Scaling\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # Masque\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche Multi-Head Attention optimis√©e\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"‚úÖ Multi-Head Attention impl√©ment√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üç∞ Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \"\"\"\n",
    "    Feed Forward Network position-wise\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "# Test du FFN\n",
    "def test_ffn():\n",
    "    print(\"üß™ Test du Feed Forward Network\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    d_model = 512\n",
    "    dff = 2048\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    \n",
    "    # Test avec des donn√©es al√©atoires\n",
    "    x = tf.random.normal([batch_size, seq_len, d_model])\n",
    "    output = ffn(x)\n",
    "    \n",
    "    print(f\"üìê Input shape: {x.shape}\")\n",
    "    print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "    print(f\"üîß FFN dimension: {d_model} ‚Üí {dff} ‚Üí {d_model}\")\n",
    "    \n",
    "    # Calcul du nombre de param√®tres\n",
    "    total_params = sum([tf.size(v).numpy() for v in ffn.trainable_variables])\n",
    "    print(f\"üìä Param√®tres FFN: {total_params:,}\")\n",
    "\n",
    "test_ffn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèóÔ∏è Couche Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Une couche d'encodeur Transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # Multi-head attention\n",
    "        attn_output, attention_weights = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Connexion r√©siduelle\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Connexion r√©siduelle\n",
    "        \n",
    "        return out2, attention_weights\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encodeur Transformer complet\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                               self.d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Embedding + Positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Passage dans les couches d'encodeur\n",
    "        for i in range(self.num_layers):\n",
    "            x, attention_weight = self.enc_layers[i](x, training, mask)\n",
    "            attention_weights[f'encoder_layer{i+1}'] = attention_weight\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "print(\"‚úÖ Encoder impl√©ment√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üì§ Couche Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Une couche de d√©codeur Transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Encoder-decoder attention\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # 1. Masked self-attention\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        # 2. Encoder-decoder attention\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask\n",
    "        )\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        # 3. Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    D√©codeur Transformer complet\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                          for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "            \n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "print(\"‚úÖ Decoder impl√©ment√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üé≠ Masques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Masque pour ignorer les tokens de padding (0)\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Masque pour emp√™cher de voir les tokens futurs\n",
    "    \"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Cr√©e tous les masques n√©cessaires pour l'entra√Ænement\n",
    "    \"\"\"\n",
    "    # Masque de padding pour l'encodeur\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Masque de padding pour l'attention encoder-decoder\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Masque look-ahead et padding pour le d√©codeur\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "# Visualisation des masques\n",
    "def visualize_masks():\n",
    "    print(\"üé≠ D√©monstration des Masques\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Exemple de s√©quence avec padding\n",
    "    sample_seq = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0]])  # 0 = padding\n",
    "    \n",
    "    # Masque de padding\n",
    "    padding_mask = create_padding_mask(sample_seq)\n",
    "    print(f\"S√©quence d'exemple: {sample_seq}\")\n",
    "    print(f\"Masque de padding shape: {padding_mask.shape}\")\n",
    "    \n",
    "    # Masque look-ahead\n",
    "    look_ahead = create_look_ahead_mask(5)\n",
    "    print(f\"Masque look-ahead shape: {look_ahead.shape}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Masque de padding\n",
    "    sns.heatmap(padding_mask[0, 0, 0, :].numpy().reshape(1, -1), \n",
    "                annot=True, ax=ax1, cmap='Reds', cbar=False)\n",
    "    ax1.set_title('Masque de Padding\\n(1 = masqu√©)')\n",
    "    ax1.set_xlabel('Position')\n",
    "    \n",
    "    # Masque look-ahead\n",
    "    sns.heatmap(look_ahead.numpy(), annot=True, ax=ax2, cmap='Blues', cbar=False)\n",
    "    ax2.set_title('Masque Look-Ahead\\n(1 = masqu√©)')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Explication:\")\n",
    "    print(\"‚Ä¢ Masque de padding: cache les tokens de padding (0)\")\n",
    "    print(\"‚Ä¢ Masque look-ahead: emp√™che de voir les mots futurs\")\n",
    "\n",
    "visualize_masks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ü§ñ Transformer Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Mod√®le Transformer complet\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                              input_vocab_size, pe_input, rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                              target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        # Encodeur\n",
    "        enc_output, enc_attention_weights = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        # D√©codeur\n",
    "        dec_output, dec_attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Couche finale\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, enc_attention_weights, dec_attention_weights\n",
    "\n",
    "# Test du Transformer complet\n",
    "def test_transformer():\n",
    "    print(\"ü§ñ Test du Transformer Complet\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Hyperparam√®tres\n",
    "    num_layers = 4\n",
    "    d_model = 128\n",
    "    dff = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    input_vocab_size = 8500\n",
    "    target_vocab_size = 8000\n",
    "    \n",
    "    # S√©quences d'exemple\n",
    "    batch_size = 2\n",
    "    inp_seq_len = 10\n",
    "    tar_seq_len = 8\n",
    "    \n",
    "    # Donn√©es d'exemple\n",
    "    sample_input = tf.random.uniform(\n",
    "        [batch_size, inp_seq_len], maxval=input_vocab_size, dtype=tf.int32\n",
    "    )\n",
    "    sample_target = tf.random.uniform(\n",
    "        [batch_size, tar_seq_len], maxval=target_vocab_size, dtype=tf.int32\n",
    "    )\n",
    "    \n",
    "    # Cr√©ation des masques\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(\n",
    "        sample_input, sample_target\n",
    "    )\n",
    "    \n",
    "    # Cr√©ation du mod√®le\n",
    "    transformer = Transformer(\n",
    "        num_layers, d_model, num_heads, dff,\n",
    "        input_vocab_size, target_vocab_size, \n",
    "        pe_input=1000, pe_target=1000\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions, enc_attn, dec_attn = transformer(\n",
    "        sample_input, sample_target, \n",
    "        training=False,\n",
    "        enc_padding_mask=enc_padding_mask, \n",
    "        look_ahead_mask=look_ahead_mask,\n",
    "        dec_padding_mask=dec_padding_mask\n",
    "    )\n",
    "    \n",
    "    print(f\"üìê Input shape: {sample_input.shape}\")\n",
    "    print(f\"üìê Target shape: {sample_target.shape}\")\n",
    "    print(f\"‚úÖ Predictions shape: {predictions.shape}\")\n",
    "    print(f\"üß† Nombre de couches: {num_layers}\")\n",
    "    print(f\"üîß Dimension du mod√®le: {d_model}\")\n",
    "    print(f\"üëÅÔ∏è Nombre de t√™tes: {num_heads}\")\n",
    "    \n",
    "    # Calcul du nombre de param√®tres\n",
    "    total_params = sum([tf.size(v).numpy() for v in transformer.trainable_variables])\n",
    "    print(f\"üìä Param√®tres totaux: {total_params:,}\")\n",
    "    \n",
    "    return transformer, predictions, enc_attn, dec_attn\n",
    "\n",
    "transformer, predictions, enc_attention, dec_attention = test_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚öôÔ∏è Configuration d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Learning rate schedule avec warmup comme dans le paper original\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        \n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    Fonction de perte avec masque pour ignorer le padding\n",
    "    \"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        real, pred, from_logits=True\n",
    "    )\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    \"\"\"\n",
    "    Pr√©cision avec masque pour ignorer le padding\n",
    "    \"\"\"\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2, output_type=tf.int32))\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    \n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "def setup_training(d_model=128):\n",
    "    print(\"‚öôÔ∏è Configuration de l'Entra√Ænement\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    "    )\n",
    "    \n",
    "    # M√©triques\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "    \n",
    "    print(\"‚úÖ Optimizer: Adam avec schedule personnalis√©\")\n",
    "    print(\"‚úÖ Loss: Sparse Categorical Crossentropy\")\n",
    "    print(\"‚úÖ M√©triques: Loss et Accuracy avec masquage\")\n",
    "    \n",
    "    # Visualisation du learning rate schedule\n",
    "    temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "    \n",
    "    steps = np.arange(1, 40000, 100)\n",
    "    lr_values = [temp_learning_rate_schedule(step).numpy() for step in steps]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=steps,\n",
    "        y=lr_values,\n",
    "        mode='lines',\n",
    "        name='Learning Rate',\n",
    "        line=dict(width=2, color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üìà Learning Rate Schedule avec Warmup\",\n",
    "        xaxis_title=\"√âtapes d'entra√Ænement\",\n",
    "        yaxis_title=\"Learning Rate\",\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return optimizer, train_loss, train_accuracy\n",
    "\n",
    "optimizer, train_loss, train_accuracy = setup_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üöÇ Boucle d'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar, transformer, optimizer, train_loss, train_accuracy):\n",
    "    \"\"\"\n",
    "    Une √©tape d'entra√Ænement\n",
    "    \"\"\"\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _, _ = transformer(inp, tar_inp, \n",
    "                                       True, \n",
    "                                       enc_padding_mask, \n",
    "                                       combined_mask, \n",
    "                                       dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))\n",
    "\n",
    "def simulate_training_epoch(transformer, num_batches=10):\n",
    "    \"\"\"\n",
    "    Simule une √©poque d'entra√Ænement avec des donn√©es synth√©tiques\n",
    "    \"\"\"\n",
    "    print(\"üöÇ Simulation d'Entra√Ænement\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Reset des m√©triques\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # Donn√©es synth√©tiques\n",
    "        inp = tf.random.uniform([32, 10], maxval=8500, dtype=tf.int32)\n",
    "        tar = tf.random.uniform([32, 8], maxval=8000, dtype=tf.int32)\n",
    "        \n",
    "        train_step(inp, tar, transformer, optimizer, train_loss, train_accuracy)\n",
    "        \n",
    "        if batch % 2 == 0:\n",
    "            print(f'Batch {batch+1}/{num_batches} - Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result():.4f}')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    print(f'\\n‚úÖ √âpoque termin√©e en {epoch_time:.2f}s')\n",
    "    print(f'Loss finale: {train_loss.result():.4f}')\n",
    "    print(f'Accuracy finale: {train_accuracy.result():.4f}')\n",
    "    \n",
    "    return train_loss.result().numpy(), train_accuracy.result().numpy()\n",
    "\n",
    "# Test d'entra√Ænement\n",
    "final_loss, final_accuracy = simulate_training_epoch(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üîÆ G√©n√©ration et Inf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence, transformer, max_length=40):\n",
    "    \"\"\"\n",
    "    Fonction d'√©valuation/g√©n√©ration pour le Transformer\n",
    "    \"\"\"\n",
    "    start_token = 1  # Token de d√©but\n",
    "    end_token = 2    # Token de fin\n",
    "    \n",
    "    # La phrase d'entr√©e est encod√©e dans inp_sentence\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    # Le d√©codeur commence avec le token de d√©but\n",
    "    decoder_input = [start_token]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output\n",
    "        )\n",
    "        \n",
    "        predictions, enc_attention_weights, dec_attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Prendre le dernier token pr√©dit\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        # Arr√™ter si on pr√©dit le token de fin\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "        \n",
    "        # Concat√©ner le token pr√©dit √† la sortie\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    \n",
    "    return tf.squeeze(output, axis=0), enc_attention_weights, dec_attention_weights\n",
    "\n",
    "def demonstrate_inference():\n",
    "    \"\"\"\n",
    "    D√©monstration d'inf√©rence avec le Transformer\n",
    "    \"\"\"\n",
    "    print(\"üîÆ D√©monstration d'Inf√©rence\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Phrase d'exemple (tokenis√©e)\n",
    "    input_sentence = tf.constant([1, 5, 6, 7, 2])  # Exemple: \"hello world\"\n",
    "    \n",
    "    print(f\"üìù Phrase d'entr√©e (tokens): {input_sentence.numpy()}\")\n",
    "    \n",
    "    # G√©n√©ration\n",
    "    result, enc_attn, dec_attn = evaluate(input_sentence, transformer)\n",
    "    \n",
    "    print(f\"üéØ S√©quence g√©n√©r√©e (tokens): {result.numpy()}\")\n",
    "    print(f\"üìè Longueur g√©n√©r√©e: {len(result)} tokens\")\n",
    "    \n",
    "    # Analyse des dimensions d'attention\n",
    "    print(\"\\nüìä Poids d'attention:\")\n",
    "    for layer_name, attention_weight in enc_attn.items():\n",
    "        print(f\"  {layer_name}: {attention_weight.shape}\")\n",
    "    \n",
    "    return result, enc_attn, dec_attn\n",
    "\n",
    "generated_result, enc_attn_weights, dec_attn_weights = demonstrate_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üìä Analyse et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transformer_complexity():\n",
    "    \"\"\"\n",
    "    Analyse la complexit√© computationnelle du Transformer\n",
    "    \"\"\"\n",
    "    print(\"üìä Analyse de Complexit√© du Transformer\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Param√®tres pour l'analyse\n",
    "    sequence_lengths = [10, 50, 100, 200, 500, 1000]\n",
    "    d_model = 512\n",
    "    \n",
    "    # Calculs de complexit√©\n",
    "    attention_complexity = [n * n * d_model for n in sequence_lengths]\n",
    "    ffn_complexity = [n * d_model * d_model * 4 for n in sequence_lengths]  # dff = 4 * d_model\n",
    "    rnn_complexity = [n * d_model * d_model for n in sequence_lengths]\n",
    "    \n",
    "    # Graphique comparatif\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sequence_lengths,\n",
    "        y=attention_complexity,\n",
    "        mode='lines+markers',\n",
    "        name='Self-Attention (O(n¬≤d))',\n",
    "        line=dict(color='red', width=3)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sequence_lengths,\n",
    "        y=ffn_complexity,\n",
    "        mode='lines+markers',\n",
    "        name='Feed Forward (O(nd¬≤))',\n",
    "        line=dict(color='blue', width=3)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=sequence_lengths,\n",
    "        y=rnn_complexity,\n",
    "        mode='lines+markers',\n",
    "        name='RNN (O(nd¬≤))',\n",
    "        line=dict(color='green', width=3, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"‚ö° Complexit√© Computationnelle par Longueur de S√©quence\",\n",
    "        xaxis_title=\"Longueur de s√©quence (n)\",\n",
    "        yaxis_title=\"Op√©rations (log scale)\",\n",
    "        yaxis_type=\"log\",\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Points de croisement\n",
    "    crossover_point = None\n",
    "    for i, n in enumerate(sequence_lengths):\n",
    "        if attention_complexity[i] > ffn_complexity[i]:\n",
    "            if i > 0:\n",
    "                crossover_point = sequence_lengths[i-1]\n",
    "            break\n",
    "    \n",
    "    print(f\"üí° Observations:\")\n",
    "    print(f\"  ‚Ä¢ Attention devient dominante pour n > ~{crossover_point if crossover_point else 'N/A'}\")\n",
    "    print(f\"  ‚Ä¢ RNN a complexit√© lin√©aire en n mais s√©quentiel\")\n",
    "    print(f\"  ‚Ä¢ Transformer parall√©lisable mais quadratique\")\n",
    "    print(f\"  ‚Ä¢ Trade-off: vitesse vs m√©moire selon longueur\")\n",
    "\n",
    "def compare_architectures():\n",
    "    \"\"\"\n",
    "    Compare diff√©rentes architectures de mod√®les\n",
    "    \"\"\"\n",
    "    architectures = {\n",
    "        'M√©trique': ['Parall√©lisation', 'Complexit√© Temps', 'Complexit√© M√©moire', \n",
    "                    'Port√©e Max', 'Vitesse GPU', 'Performance BLEU'],\n",
    "        'RNN/LSTM': ['‚ùå Non', 'O(n)', 'O(n)', '~100 tokens', '1x', '25.4'],\n",
    "        'CNN': ['‚úÖ Oui', 'O(log n)', 'O(n)', 'Limit√©e', '3x', '28.9'],\n",
    "        'Transformer': ['‚úÖ Oui', 'O(n¬≤)', 'O(n¬≤)', 'Illimit√©e', '5x', '35.8']\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(architectures)\n",
    "    \n",
    "    print(\"‚öñÔ∏è Comparaison d'Architectures\")\n",
    "    print(\"=\" * 35)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nüéØ Conclusions:\")\n",
    "    print(\"‚Ä¢ Transformer: Meilleur pour qualit√© et parall√©lisation\")\n",
    "    print(\"‚Ä¢ RNN: Efficace pour s√©quences courtes et faible m√©moire\")\n",
    "    print(\"‚Ä¢ CNN: Bon compromis vitesse/performance pour NLP\")\n",
    "\n",
    "analyze_transformer_complexity()\n",
    "compare_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. üéØ Applications Pratiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_applications_showcase():\n",
    "    \"\"\"\n",
    "    Pr√©sente les applications pratiques des Transformers\n",
    "    \"\"\"\n",
    "    print(\"üéØ Applications Pratiques des Transformers\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    applications = {\n",
    "        \"üåç Traduction Automatique\": {\n",
    "            \"Mod√®les\": \"Google Translate, mBART, M2M-100\",\n",
    "            \"Innovation\": \"Cross-attention pour alignement automatique\",\n",
    "            \"Performance\": \"BLEU 35+ (vs 25 pour RNN)\",\n",
    "            \"Exemple\": \"'Hello world' ‚Üí 'Bonjour le monde'\"\n",
    "        },\n",
    "        \"ü§ñ Mod√®les de Langage\": {\n",
    "            \"Mod√®les\": \"GPT-3/4, PaLM, LaMDA, ChatGPT\",\n",
    "            \"Innovation\": \"G√©n√©ration autoregressive avec attention\",\n",
    "            \"Performance\": \"175B+ param√®tres, conversation naturelle\",\n",
    "            \"Exemple\": \"Compl√©tion de texte, dialogue, code\"\n",
    "        },\n",
    "        \"üîç Compr√©hension de Texte\": {\n",
    "            \"Mod√®les\": \"BERT, RoBERTa, DeBERTa, ELECTRA\",\n",
    "            \"Innovation\": \"Encodage bidirectionnel avec masking\",\n",
    "            \"Performance\": \"GLUE 90+ (human-level sur certaines t√¢ches)\",\n",
    "            \"Exemple\": \"Classification, NER, Question-R√©ponse\"\n",
    "        },\n",
    "        \"üìù G√©n√©ration Conditionnelle\": {\n",
    "            \"Mod√®les\": \"T5, BART, PEGASUS\",\n",
    "            \"Innovation\": \"Text-to-text unified framework\",\n",
    "            \"Performance\": \"ROUGE 50+ pour r√©sum√©\",\n",
    "            \"Exemple\": \"R√©sum√©, paraphrase, simplification\"\n",
    "        },\n",
    "        \"üé® Applications Multimodales\": {\n",
    "            \"Mod√®les\": \"DALL-E, CLIP, GPT-4V, Flamingo\",\n",
    "            \"Innovation\": \"Attention cross-modale (texte ‚Üî image)\",\n",
    "            \"Performance\": \"G√©n√©ration d'images photorealistic\",\n",
    "            \"Exemple\": \"'Un chat dans l'espace' ‚Üí image\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for app_name, details in applications.items():\n",
    "        print(f\"\\n{app_name}:\")\n",
    "        for key, value in details.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Chronologie des mod√®les importants\n",
    "    timeline_data = {\n",
    "        'Ann√©e': [2017, 2018, 2019, 2020, 2021, 2022, 2023],\n",
    "        'Mod√®le': ['Transformer', 'BERT', 'GPT-2', 'GPT-3', 'T5', 'ChatGPT', 'GPT-4'],\n",
    "        'Param√®tres (B)': [0.065, 0.34, 1.5, 175, 11, 175, 1000],\n",
    "        'Innovation': [\n",
    "            'Attention Is All You Need',\n",
    "            'Bidirectional Encoder',\n",
    "            'Large Scale Generation',\n",
    "            'Few-shot Learning',\n",
    "            'Text-to-Text Transfer',\n",
    "            'Conversational AI',\n",
    "            'Multimodal Reasoning'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Graphique de l'√©volution\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=timeline_data['Ann√©e'],\n",
    "        y=timeline_data['Param√®tres (B)'],\n",
    "        mode='markers+lines+text',\n",
    "        text=timeline_data['Mod√®le'],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=12, color='red'),\n",
    "        line=dict(width=3, color='blue'),\n",
    "        hovertemplate='<b>%{text}</b><br>%{x}<br>%{y}B param√®tres<br><extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üöÄ √âvolution des Mod√®les Transformer (2017-2023)\",\n",
    "        xaxis_title=\"Ann√©e\",\n",
    "        yaxis_title=\"Nombre de Param√®tres (Milliards)\",\n",
    "        yaxis_type=\"log\",\n",
    "        width=900,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "transformer_applications_showcase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. üî¨ Exp√©rimentations Avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_attention_patterns():\n",
    "    \"\"\"\n",
    "    Exp√©rimente avec diff√©rents patterns d'attention\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Exp√©rimentation: Patterns d'Attention\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Diff√©rents types de patterns d'attention\n",
    "    seq_len = 8\n",
    "    patterns = {}\n",
    "    \n",
    "    # 1. Full Attention (standard)\n",
    "    patterns['Full Attention'] = np.ones((seq_len, seq_len))\n",
    "    \n",
    "    # 2. Local Attention (fen√™tre locale)\n",
    "    local_pattern = np.zeros((seq_len, seq_len))\n",
    "    window_size = 3\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size//2)\n",
    "        end = min(seq_len, i + window_size//2 + 1)\n",
    "        local_pattern[i, start:end] = 1\n",
    "    patterns['Local Attention'] = local_pattern\n",
    "    \n",
    "    # 3. Strided Attention\n",
    "    strided_pattern = np.zeros((seq_len, seq_len))\n",
    "    stride = 2\n",
    "    for i in range(seq_len):\n",
    "        strided_pattern[i, ::stride] = 1\n",
    "        strided_pattern[i, i] = 1  # Self-attention\n",
    "    patterns['Strided Attention'] = strided_pattern\n",
    "    \n",
    "    # 4. Random Attention (Sparse)\n",
    "    np.random.seed(42)\n",
    "    random_pattern = np.random.binomial(1, 0.3, (seq_len, seq_len))\n",
    "    np.fill_diagonal(random_pattern, 1)  # Garder self-attention\n",
    "    patterns['Random Sparse'] = random_pattern\n",
    "    \n",
    "    # Visualisation\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=list(patterns.keys()),\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    positions = [(1,1), (1,2), (2,1), (2,2)]\n",
    "    \n",
    "    for (pattern_name, pattern), (row, col) in zip(patterns.items(), positions):\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=pattern,\n",
    "                colorscale='Blues',\n",
    "                showscale=(pattern_name == 'Full Attention'),\n",
    "                hoverongaps=False\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üé≠ Diff√©rents Patterns d'Attention\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Analyse de complexit√©\n",
    "    print(\"üìä Complexit√© des Patterns:\")\n",
    "    for name, pattern in patterns.items():\n",
    "        complexity = np.sum(pattern) / (seq_len * seq_len)\n",
    "        print(f\"  {name}: {complexity:.1%} de Full Attention\")\n",
    "    \n",
    "    print(\"\\nüí° Applications:\")\n",
    "    print(\"‚Ä¢ Full: Qualit√© maximale, co√ªteux pour longues s√©quences\")\n",
    "    print(\"‚Ä¢ Local: Bon pour texte avec structure locale\")\n",
    "    print(\"‚Ä¢ Strided: Efficace pour capture de patterns globaux\")\n",
    "    print(\"‚Ä¢ Sparse: Compromis performance/efficacit√©\")\n",
    "\n",
    "def transformer_scaling_analysis():\n",
    "    \"\"\"\n",
    "    Analyse du scaling des Transformers\n",
    "    \"\"\"\n",
    "    print(\"\\nüìà Analyse du Scaling des Transformers\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Donn√©es de scaling empiriques (approximatives)\n",
    "    model_sizes = [0.1, 0.35, 1.5, 6, 13, 70, 175, 540]  # Milliards de param√®tres\n",
    "    perplexity = [50, 35, 25, 18, 15, 12, 10, 8]  # Perplexit√© (plus bas = mieux)\n",
    "    training_cost = [1, 10, 50, 200, 500, 2000, 5000, 15000]  # Co√ªt relatif\n",
    "    \n",
    "    # Graphique de scaling\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Performance vs Taille\", \"Co√ªt vs Taille\"],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": True}]]\n",
    "    )\n",
    "    \n",
    "    # Performance\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=model_sizes,\n",
    "            y=perplexity,\n",
    "            mode='markers+lines',\n",
    "            name='Perplexit√©',\n",
    "            line=dict(color='blue', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Co√ªt\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=model_sizes,\n",
    "            y=training_cost,\n",
    "            mode='markers+lines',\n",
    "            name='Co√ªt d\\'entra√Ænement',\n",
    "            line=dict(color='red', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Param√®tres (Milliards)\", type=\"log\")\n",
    "    fig.update_yaxes(title_text=\"Perplexit√©\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Co√ªt Relatif\", type=\"log\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üìä Lois de Scaling des Transformers\",\n",
    "        height=400,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üîç Observations du Scaling:\")\n",
    "    print(\"‚Ä¢ Performance: Am√©lioration logarithmique avec la taille\")\n",
    "    print(\"‚Ä¢ Co√ªt: Croissance super-lin√©aire (compute + data)\")\n",
    "    print(\"‚Ä¢ Point optimal: D√©pend du budget et de l'application\")\n",
    "    print(\"‚Ä¢ √âmergence: Capacit√©s nouvelles √† grande √©chelle\")\n",
    "\n",
    "experiment_with_attention_patterns()\n",
    "transformer_scaling_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusion et R√©capitulatif\n",
    "\n",
    "### Ce que vous avez impl√©ment√© :\n",
    "\n",
    "‚úÖ **Architecture Compl√®te** : Encoder + Decoder avec attention  \n",
    "‚úÖ **Multi-Head Attention** : Parall√©lisation des perspectives  \n",
    "‚úÖ **Positional Encoding** : Int√©gration de l'information de position  \n",
    "‚úÖ **Masques** : Padding et look-ahead pour l'entra√Ænement  \n",
    "‚úÖ **Boucle d'Entra√Ænement** : Optimisation avec learning rate schedule  \n",
    "‚úÖ **G√©n√©ration** : Inf√©rence autoregressive  \n",
    "‚úÖ **Analyse** : Complexit√©, scaling et patterns d'attention\n",
    "\n",
    "### Comp√©tences acquises :\n",
    "- **Architecture Transformer** from scratch avec TensorFlow\n",
    "- **M√©canismes d'attention** avanc√©s et leurs variants\n",
    "- **Optimisation** et techniques d'entra√Ænement\n",
    "- **Analyse de performance** et trade-offs\n",
    "- **Applications pratiques** dans le monde r√©el\n",
    "\n",
    "### Prochaines √©tapes :\n",
    "- **Module 7** : BERT, GPT et mod√®les pr√©-entra√Æn√©s\n",
    "- **Fine-tuning** sur vos propres donn√©es\n",
    "- **Optimisations** : Sparse attention, efficient transformers\n",
    "- **Applications** : Traduction, g√©n√©ration, analyse de sentiment\n",
    "\n",
    "### üõ†Ô∏è D√©fis Pratiques :\n",
    "1. Modifiez l'architecture pour diff√©rentes t√¢ches\n",
    "2. Impl√©mentez des variants comme Sparse Attention\n",
    "3. Optimisez pour diff√©rentes contraintes de m√©moire\n",
    "4. Explorez les techniques de regularisation avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéä F√©licitations ! Architecture Transformer ma√Ætris√©e !\n",
    "print(\"\"\"üéä F√âLICITATIONS ! üéä\n",
    "\n",
    "Vous venez d'impl√©menter une architecture Transformer compl√®te !\n",
    "\n",
    "üèóÔ∏è Vous ma√Ætrisez maintenant :\n",
    "  ‚Ä¢ L'architecture r√©volutionnaire qui domine le NLP moderne\n",
    "  ‚Ä¢ Les fondements de GPT, BERT, T5 et tous les LLMs\n",
    "  ‚Ä¢ L'impl√©mentation from scratch avec TensorFlow\n",
    "  ‚Ä¢ Les principes de scaling et d'optimisation\n",
    "\n",
    "üöÄ Vous √™tes pr√™t pour les mod√®les avanc√©s du Module 7 !\n",
    "\n",
    "üí° Cette architecture est utilis√©e dans :\n",
    "  ‚Ä¢ ChatGPT & GPT-4 (g√©n√©ration)\n",
    "  ‚Ä¢ Google Search via BERT (compr√©hension)\n",
    "  ‚Ä¢ Google Translate (traduction)\n",
    "  ‚Ä¢ DALL-E (g√©n√©ration d'images)\n",
    "  ‚Ä¢ Et des milliers d'autres applications...\n",
    "\n",
    "üî• Continuez vers Module 7 pour explorer BERT et GPT !\n",
    "\"\"\")\n",
    "\n",
    "# R√©sum√© final des composants impl√©ment√©s\n",
    "components_implemented = {\n",
    "    \"üîç Multi-Head Attention\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"üìç Positional Encoding\": \"‚úÖ Impl√©ment√©\", \n",
    "    \"üèóÔ∏è Encoder Layers\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"üì§ Decoder Layers\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"üé≠ Masking System\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"‚öôÔ∏è Training Loop\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"üîÆ Inference Engine\": \"‚úÖ Impl√©ment√©\",\n",
    "    \"üìä Analysis Tools\": \"‚úÖ Impl√©ment√©\"\n",
    "}\n",
    "\n",
    "print(\"\\nüìã R√©capitulatif des Composants:\")\n",
    "for component, status in components_implemented.items():\n",
    "    print(f\"   {component}: {status}\")\n",
    "\n",
    "print(f\"\\nüéì Total: {len(components_implemented)} composants ma√Ætris√©s !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}