{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëÅÔ∏è M√©canismes d'Attention - Module 6\n",
    "\n",
    "Ce notebook explore en d√©tail les m√©canismes d'attention qui sont au c≈ìur des Transformers.\n",
    "\n",
    "## üéØ Objectifs\n",
    "- Comprendre le concept d'attention\n",
    "- Impl√©menter Self-Attention avec TensorFlow\n",
    "- Explorer Multi-Head Attention\n",
    "- Visualiser les matrices d'attention\n",
    "- Comparer avec les approches RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"üîß TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üîß NumPy version: {np.__version__}\")\n",
    "print(f\"üìä GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üß† Intuition de l'Attention\n",
    "\n",
    "L'attention permet √† un mod√®le de se concentrer sur les parties importantes de l'input lors de chaque pr√©diction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_concept():\n",
    "    \"\"\"\n",
    "    Visualise le concept d'attention sur une phrase exemple\n",
    "    \"\"\"\n",
    "    sentence = [\"Le\", \"chat\", \"noir\", \"mange\", \"des\", \"croquettes\"]\n",
    "    \n",
    "    # Simulation d'une matrice d'attention\n",
    "    attention_weights = np.array([\n",
    "        [0.1, 0.7, 0.1, 0.05, 0.025, 0.025],  # \"Le\" fait attention √† \"chat\"\n",
    "        [0.1, 0.5, 0.3, 0.05, 0.025, 0.025],  # \"chat\" √† lui-m√™me et \"noir\"\n",
    "        [0.05, 0.6, 0.25, 0.05, 0.025, 0.025], # \"noir\" fait attention √† \"chat\"\n",
    "        [0.05, 0.3, 0.1, 0.4, 0.1, 0.05],     # \"mange\" √©quilibr√©\n",
    "        [0.05, 0.1, 0.05, 0.3, 0.3, 0.2],     # \"des\" vers \"croquettes\"\n",
    "        [0.05, 0.1, 0.05, 0.2, 0.2, 0.4]      # \"croquettes\" vers lui-m√™me\n",
    "    ])\n",
    "    \n",
    "    # Cr√©ation de la heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attention_weights,\n",
    "        x=sentence,\n",
    "        y=sentence,\n",
    "        colorscale='Reds',\n",
    "        text=np.round(attention_weights, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\":10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üéØ Matrice d'Attention : Qui regarde qui ?\",\n",
    "        xaxis_title=\"Mots de la phrase (cl√©s)\",\n",
    "        yaxis_title=\"Mots de la phrase (requ√™tes)\",\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"üí° Interpr√©tation :\")\n",
    "    print(\"- Les couleurs intenses montrent o√π chaque mot 'fait attention'\")\n",
    "    print(\"- 'Le' fait attention √† 'chat' (relation d√©terminant-nom)\")\n",
    "    print(\"- 'noir' fait attention √† 'chat' (adjectif-nom)\")\n",
    "    print(\"- 'mange' regarde √† la fois le sujet et l'objet\")\n",
    "    \n",
    "visualize_attention_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîç Impl√©mentation de l'Attention Scaled Dot-Product\n",
    "\n",
    "La formule fondamentale : **Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Impl√©mentation de l'attention Scaled Dot-Product\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=None, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def call(self, queries, keys, values, mask=None, training=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: [batch_size, seq_len, d_k]\n",
    "            keys: [batch_size, seq_len, d_k]\n",
    "            values: [batch_size, seq_len, d_v]\n",
    "            mask: Masque optionnel\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_v]\n",
    "            attention_weights: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # √âtape 1: Calcul des scores QK^T\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        \n",
    "        # √âtape 2: Normalisation par ‚àöd_k\n",
    "        d_k = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "        if self.temperature is not None:\n",
    "            scores = scores / self.temperature\n",
    "        else:\n",
    "            scores = scores / tf.math.sqrt(d_k)\n",
    "        \n",
    "        # √âtape 3: Application du masque (si fourni)\n",
    "        if mask is not None:\n",
    "            scores += (mask * -1e9)\n",
    "        \n",
    "        # √âtape 4: Softmax pour obtenir les poids d'attention\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # √âtape 5: Application des poids aux valeurs\n",
    "        output = tf.matmul(attention_weights, values)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test de l'impl√©mentation\n",
    "def test_attention():\n",
    "    print(\"üß™ Test de l'Attention Scaled Dot-Product\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Param√®tres\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 64\n",
    "    \n",
    "    # Cr√©ation de donn√©es d'exemple\n",
    "    tf.random.set_seed(42)\n",
    "    queries = tf.random.normal([batch_size, seq_len, d_model])\n",
    "    keys = tf.random.normal([batch_size, seq_len, d_model])\n",
    "    values = tf.random.normal([batch_size, seq_len, d_model])\n",
    "    \n",
    "    # Instanciation de la couche d'attention\n",
    "    attention_layer = ScaledDotProductAttention()\n",
    "    \n",
    "    # Calcul de l'attention\n",
    "    output, attention_weights = attention_layer(queries, keys, values)\n",
    "    \n",
    "    print(f\"üìê Shape des queries: {queries.shape}\")\n",
    "    print(f\"üìê Shape des keys: {keys.shape}\")\n",
    "    print(f\"üìê Shape des values: {values.shape}\")\n",
    "    print(f\"‚úÖ Shape de l'output: {output.shape}\")\n",
    "    print(f\"üìä Shape des poids d'attention: {attention_weights.shape}\")\n",
    "    \n",
    "    # V√©rification que les poids somment √† 1\n",
    "    sum_weights = tf.reduce_sum(attention_weights, axis=-1)\n",
    "    print(f\"üîç Somme des poids (doit √™tre ~1.0): {tf.reduce_mean(sum_weights):.6f}\")\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "output, attention_weights = test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üé® Visualisation des Poids d'Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(attention_weights, sentence=None):\n",
    "    \"\"\"\n",
    "    Visualise les poids d'attention avec Plotly\n",
    "    \"\"\"\n",
    "    # Prendre le premier exemple du batch\n",
    "    weights = attention_weights[0].numpy()\n",
    "    \n",
    "    if sentence is None:\n",
    "        sentence = [f\"Token_{i}\" for i in range(weights.shape[0])]\n",
    "    \n",
    "    # Cr√©ation de la heatmap interactive\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=weights,\n",
    "        x=sentence,\n",
    "        y=sentence,\n",
    "        colorscale='Viridis',\n",
    "        text=np.round(weights, 3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\":8},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üî• Heatmap des Poids d'Attention\",\n",
    "        xaxis_title=\"Keys (ce qui est regard√©)\",\n",
    "        yaxis_title=\"Queries (qui regarde)\",\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Visualisation avec notre exemple\n",
    "sentence = [\"Je\", \"mange\", \"une\", \"pomme\", \"rouge\"]\n",
    "visualize_attention_weights(attention_weights, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß† Multi-Head Attention\n",
    "\n",
    "Le Multi-Head Attention permet d'avoir plusieurs \"perspectives\" d'attention en parall√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Impl√©mentation du Multi-Head Attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Couches lin√©aires pour Q, K, V\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # Couche de sortie\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # Couche d'attention\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Divise la derni√®re dimension en (num_heads, depth)\n",
    "        Transpose pour avoir shape (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask=None, training=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 1. Transformation lin√©aire et division en t√™tes\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # 2. Attention scaled dot-product\n",
    "        scaled_attention, attention_weights = self.attention(\n",
    "            q, k, v, mask=mask, training=training\n",
    "        )\n",
    "        \n",
    "        # 3. Concat√©nation des t√™tes\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 4. Projection finale\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test du Multi-Head Attention\n",
    "def test_multihead_attention():\n",
    "    print(\"üß™ Test du Multi-Head Attention\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Param√®tres\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    \n",
    "    # Donn√©es d'exemple\n",
    "    tf.random.set_seed(42)\n",
    "    x = tf.random.normal([batch_size, seq_len, d_model])\n",
    "    \n",
    "    # Cr√©ation de la couche Multi-Head Attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # Self-attention (Q, K, V sont identiques)\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "    \n",
    "    print(f\"üìê Input shape: {x.shape}\")\n",
    "    print(f\"‚úÖ Output shape: {output.shape}\")\n",
    "    print(f\"üìä Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"üß† Nombre de t√™tes: {num_heads}\")\n",
    "    print(f\"üîß Dimension par t√™te: {d_model // num_heads}\")\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "mha_output, mha_attention_weights = test_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üé≠ Visualisation Multi-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multihead_attention(attention_weights, sentence=None, max_heads=4):\n",
    "    \"\"\"\n",
    "    Visualise les diff√©rentes t√™tes d'attention\n",
    "    \"\"\"\n",
    "    # Prendre le premier exemple du batch\n",
    "    weights = attention_weights[0].numpy()  # Shape: [num_heads, seq_len, seq_len]\n",
    "    \n",
    "    if sentence is None:\n",
    "        seq_len = weights.shape[1]\n",
    "        sentence = [f\"T{i}\" for i in range(seq_len)]\n",
    "    \n",
    "    num_heads = min(weights.shape[0], max_heads)\n",
    "    \n",
    "    # Cr√©ation des sous-graphiques\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[f\"T√™te {i+1}\" for i in range(num_heads)],\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        row = (i // 2) + 1\n",
    "        col = (i % 2) + 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=weights[i],\n",
    "                x=sentence,\n",
    "                y=sentence,\n",
    "                colorscale='Viridis',\n",
    "                showscale=(i == 0),\n",
    "                text=np.round(weights[i], 2),\n",
    "                texttemplate=\"%{text}\",\n",
    "                textfont={\"size\":8}\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üß† Multi-Head Attention - Diff√©rentes Perspectives\",\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Analyse des patterns\n",
    "    print(\"üîç Analyse des patterns d'attention:\")\n",
    "    for i in range(num_heads):\n",
    "        # Calcul de la concentration (entropie inverse)\n",
    "        entropy = -np.sum(weights[i] * np.log(weights[i] + 1e-10), axis=-1)\n",
    "        avg_entropy = np.mean(entropy)\n",
    "        \n",
    "        if avg_entropy < 1.0:\n",
    "            pattern = \"Tr√®s focalis√©e\"\n",
    "        elif avg_entropy < 1.5:\n",
    "            pattern = \"Mod√©r√©ment focalis√©e\"\n",
    "        else:\n",
    "            pattern = \"Diffuse\"\n",
    "            \n",
    "        print(f\"  T√™te {i+1}: {pattern} (entropie: {avg_entropy:.2f})\")\n",
    "\n",
    "# Test avec une phrase exemple\n",
    "sentence_example = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Cr√©er un exemple plus petit pour la visualisation\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = len(sentence_example)\n",
    "batch_size = 1\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "x_small = tf.random.normal([batch_size, seq_len, d_model])\n",
    "mha_small = MultiHeadAttention(d_model, num_heads)\n",
    "_, small_attention_weights = mha_small(x_small, x_small, x_small)\n",
    "\n",
    "visualize_multihead_attention(small_attention_weights, sentence_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîÑ Self-Attention vs Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_attention_types():\n",
    "    \"\"\"\n",
    "    D√©montre la diff√©rence entre Self-Attention et Cross-Attention\n",
    "    \"\"\"\n",
    "    print(\"üîÑ D√©monstration: Self-Attention vs Cross-Attention\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Param√®tres\n",
    "    d_model = 256\n",
    "    num_heads = 4\n",
    "    batch_size = 1\n",
    "    \n",
    "    # S√©quences d'exemple\n",
    "    source_len = 5  # \"Hello how are you ?\"\n",
    "    target_len = 4  # \"Bonjour comment √ßa\"\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    source_seq = tf.random.normal([batch_size, source_len, d_model])\n",
    "    target_seq = tf.random.normal([batch_size, target_len, d_model])\n",
    "    \n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # 1. Self-Attention sur la s√©quence source\n",
    "    print(\"1Ô∏è‚É£ Self-Attention (source regarde source):\")\n",
    "    self_output, self_weights = mha(source_seq, source_seq, source_seq)\n",
    "    print(f\"   Input: {source_seq.shape}\")\n",
    "    print(f\"   Output: {self_output.shape}\")\n",
    "    print(f\"   Attention weights: {self_weights.shape}\")\n",
    "    \n",
    "    # 2. Cross-Attention (target regarde source)\n",
    "    print(\"\\n2Ô∏è‚É£ Cross-Attention (target regarde source):\")\n",
    "    cross_output, cross_weights = mha(\n",
    "        v=source_seq,  # Values viennent de la source\n",
    "        k=source_seq,  # Keys viennent de la source\n",
    "        q=target_seq   # Queries viennent de la target\n",
    "    )\n",
    "    print(f\"   Queries (target): {target_seq.shape}\")\n",
    "    print(f\"   Keys/Values (source): {source_seq.shape}\")\n",
    "    print(f\"   Output: {cross_output.shape}\")\n",
    "    print(f\"   Attention weights: {cross_weights.shape}\")\n",
    "    \n",
    "    return self_weights, cross_weights\n",
    "\n",
    "self_attn_weights, cross_attn_weights = demonstrate_attention_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ Application Pratique: Analyse de Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSentimentAnalyzer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Mod√®le d'analyse de sentiment utilisant l'attention\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, max_length=100, **kwargs):\n",
    "        super(AttentionSentimentAnalyzer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Couches d'embedding\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(max_length, d_model)\n",
    "        \n",
    "        # Couche d'attention\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed Forward\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model * 4, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Classification\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        \"\"\"\n",
    "        Cr√©e l'encodage positionnel\n",
    "        \"\"\"\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(d_model)[np.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "        \n",
    "        # Sinus pour les positions paires\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        # Cosinus pour les positions impaires\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def call(self, x, training=None, return_attention=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Self-Attention\n",
    "        attn_output, attention_weights = self.mha(x, x, x, training=training)\n",
    "        x = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed Forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorm2(x + ffn_output)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "# Test du mod√®le\n",
    "def test_sentiment_model():\n",
    "    print(\"üé≠ Test du Mod√®le de Sentiment avec Attention\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Param√®tres\n",
    "    vocab_size = 10000\n",
    "    d_model = 128\n",
    "    num_heads = 4\n",
    "    max_length = 50\n",
    "    \n",
    "    # Donn√©es d'exemple\n",
    "    batch_size = 4\n",
    "    seq_len = 20\n",
    "    \n",
    "    # S√©quences al√©atoires (simulent des phrases tokenis√©es)\n",
    "    x = tf.random.uniform([batch_size, seq_len], maxval=vocab_size, dtype=tf.int32)\n",
    "    \n",
    "    # Cr√©ation du mod√®le\n",
    "    model = AttentionSentimentAnalyzer(vocab_size, d_model, num_heads, max_length)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    predictions, attention_weights = model(x, return_attention=True)\n",
    "    \n",
    "    print(f\"üìê Input shape: {x.shape}\")\n",
    "    print(f\"‚úÖ Predictions shape: {predictions.shape}\")\n",
    "    print(f\"üìä Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"üéØ Exemple de pr√©dictions: {predictions.numpy().flatten()[:4]}\")\n",
    "    \n",
    "    # Statistiques du mod√®le\n",
    "    total_params = sum([tf.size(v).numpy() for v in model.trainable_variables])\n",
    "    print(f\"üîß Nombre total de param√®tres: {total_params:,}\")\n",
    "    \n",
    "    return model, attention_weights\n",
    "\n",
    "sentiment_model, sentiment_attention = test_sentiment_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìä Comparaison RNN vs Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rnn_vs_attention():\n",
    "    \"\"\"\n",
    "    Compare les performances et caract√©ristiques RNN vs Attention\n",
    "    \"\"\"\n",
    "    print(\"‚öñÔ∏è Comparaison RNN vs Attention\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simulation de benchmarks\n",
    "    metrics = {\n",
    "        'Architecture': ['RNN/LSTM', 'Self-Attention', 'Multi-Head Attention'],\n",
    "        'Parall√©lisation': ['‚ùå S√©quentiel', '‚úÖ Parall√®le', '‚úÖ Parall√®le'],\n",
    "        'Complexit√©': ['O(n)', 'O(n¬≤)', 'O(n¬≤)'],\n",
    "        'M√©moire √† long terme': ['Limit√©e', '‚úÖ Illimit√©e', '‚úÖ Illimit√©e'],\n",
    "        'Vitesse d\\'entra√Ænement': ['Lent', 'Rapide', 'Tr√®s rapide'],\n",
    "        'Performance (BLEU)': [25.4, 32.1, 35.8],\n",
    "        'Param√®tres (M)': [2.5, 8.2, 12.1]\n",
    "    }\n",
    "    \n",
    "    # Cr√©ation du tableau comparatif\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Affichage styl√©\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Graphique de performance\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=metrics['Param√®tres (M)'],\n",
    "        y=metrics['Performance (BLEU)'],\n",
    "        mode='markers+lines+text',\n",
    "        text=metrics['Architecture'],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(size=15, color=['red', 'blue', 'green']),\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üìà Performance vs Complexit√©\",\n",
    "        xaxis_title=\"Nombre de Param√®tres (Millions)\",\n",
    "        yaxis_title=\"Score BLEU (Traduction)\",\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nüí° Observations cl√©s:\")\n",
    "    print(\"‚Ä¢ L'attention permet la parall√©lisation ‚Üí entra√Ænement plus rapide\")\n",
    "    print(\"‚Ä¢ Pas de limite de distance ‚Üí meilleure m√©moire √† long terme\")\n",
    "    print(\"‚Ä¢ Plus de param√®tres mais performance sup√©rieure\")\n",
    "    print(\"‚Ä¢ Complexit√© O(n¬≤) peut √™tre probl√©matique pour tr√®s longues s√©quences\")\n",
    "\n",
    "compare_rnn_vs_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéÆ Attention Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_attention_demo():\n",
    "    \"\"\"\n",
    "    D√©monstration interactive de l'attention\n",
    "    \"\"\"\n",
    "    print(\"üéÆ D√©monstration Interactive de l'Attention\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Phrases d'exemple avec diff√©rents patterns\n",
    "    examples = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"Despite the rain, the game continued\",\n",
    "        \"She loves him but he doesn't care\",\n",
    "        \"The quick brown fox jumps over the lazy dog\"\n",
    "    ]\n",
    "    \n",
    "    for i, sentence in enumerate(examples):\n",
    "        print(f\"\\n{i+1}. Phrase: '{sentence}'\")\n",
    "        \n",
    "        # Tokenisation simple\n",
    "        tokens = sentence.lower().split()\n",
    "        vocab = {word: idx for idx, word in enumerate(set(tokens))}\n",
    "        token_ids = [vocab[word] for word in tokens]\n",
    "        \n",
    "        # Cr√©ation d'embeddings al√©atoires mais coh√©rents\n",
    "        tf.random.set_seed(42 + i)  # Seed diff√©rent pour chaque phrase\n",
    "        embeddings = tf.random.normal([len(tokens), 64])\n",
    "        \n",
    "        # Calcul de l'attention\n",
    "        attention_layer = ScaledDotProductAttention()\n",
    "        _, attention_weights = attention_layer(\n",
    "            embeddings[None, ...], \n",
    "            embeddings[None, ...], \n",
    "            embeddings[None, ...]\n",
    "        )\n",
    "        \n",
    "        # Analyse des patterns\n",
    "        weights = attention_weights[0].numpy()\n",
    "        \n",
    "        # Trouvez les paires de mots avec la plus forte attention\n",
    "        max_attention_pairs = []\n",
    "        for j in range(len(tokens)):\n",
    "            max_idx = np.argmax(weights[j])\n",
    "            if max_idx != j:  # √âviter l'auto-attention\n",
    "                max_attention_pairs.append(\n",
    "                    (tokens[j], tokens[max_idx], weights[j, max_idx])\n",
    "                )\n",
    "        \n",
    "        # Affichage des top 3 relations\n",
    "        max_attention_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        print(\"   Relations d'attention les plus fortes:\")\n",
    "        for word1, word2, strength in max_attention_pairs[:3]:\n",
    "            print(f\"   ‚Ä¢ '{word1}' ‚Üí '{word2}' ({strength:.3f})\")\n",
    "        \n",
    "        # Visualisation rapide (premi√®re phrase seulement)\n",
    "        if i == 0:\n",
    "            visualize_attention_weights(attention_weights, tokens)\n",
    "\n",
    "interactive_attention_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üîÆ Perspectives et Applications Avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_attention_applications():\n",
    "    \"\"\"\n",
    "    Pr√©sente les applications avanc√©es de l'attention\n",
    "    \"\"\"\n",
    "    print(\"üîÆ Applications Avanc√©es de l'Attention\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    applications = {\n",
    "        \"üåç Traduction Automatique\": {\n",
    "            \"Description\": \"Cross-attention entre langues source et cible\",\n",
    "            \"Avantage\": \"Alignement automatique des mots\",\n",
    "            \"Exemple\": \"'Je mange' ‚Üí 'I eat' avec attention sur correspondances\"\n",
    "        },\n",
    "        \"üìù R√©sum√© de Texte\": {\n",
    "            \"Description\": \"Attention sur phrases et concepts importants\",\n",
    "            \"Avantage\": \"Identification automatique des √©l√©ments cl√©s\",\n",
    "            \"Exemple\": \"Article de 1000 mots ‚Üí r√©sum√© de 100 mots\"\n",
    "        },\n",
    "        \"‚ùì Question-R√©ponse\": {\n",
    "            \"Description\": \"Attention entre question et passages du texte\",\n",
    "            \"Avantage\": \"Localisation pr√©cise des r√©ponses\",\n",
    "            \"Exemple\": \"'Qui est le pr√©sident?' ‚Üí attention sur noms propres\"\n",
    "        },\n",
    "        \"üé® G√©n√©ration d'Images\": {\n",
    "            \"Description\": \"Attention entre description et r√©gions de l'image\",\n",
    "            \"Avantage\": \"G√©n√©ration coh√©rente et contr√¥l√©e\",\n",
    "            \"Exemple\": \"'Un chat noir sur un tapis rouge' ‚Üí attention spatiale\"\n",
    "        },\n",
    "        \"üó£Ô∏è Reconnaissance Vocale\": {\n",
    "            \"Description\": \"Attention entre signal audio et transcription\",\n",
    "            \"Avantage\": \"Alignement temporel automatique\",\n",
    "            \"Exemple\": \"Spectrogramme ‚Üí texte avec correspondances temporelles\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for app_name, details in applications.items():\n",
    "        print(f\"\\n{app_name}:\")\n",
    "        print(f\"  üìã {details['Description']}\")\n",
    "        print(f\"  ‚ú® {details['Avantage']}\")\n",
    "        print(f\"  üí° {details['Exemple']}\")\n",
    "    \n",
    "    # Tendances futures\n",
    "    print(\"\\nüöÄ Tendances Futures:\")\n",
    "    future_trends = [\n",
    "        \"Sparse Attention: R√©duction de la complexit√© O(n¬≤)\",\n",
    "        \"Linear Attention: Attention en O(n)\",\n",
    "        \"Cross-Modal Attention: Vision + Texte + Audio\",\n",
    "        \"Hierarchical Attention: Attention multi-niveaux\",\n",
    "        \"Adaptive Attention: Longueur de contexte dynamique\"\n",
    "    ]\n",
    "    \n",
    "    for trend in future_trends:\n",
    "        print(f\"  ‚Ä¢ {trend}\")\n",
    "\n",
    "advanced_attention_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusion et R√©capitulatif\n",
    "\n",
    "### Ce que vous avez appris :\n",
    "\n",
    "‚úÖ **Concept d'Attention** : M√©canisme permettant de se concentrer sur les parties importantes  \n",
    "‚úÖ **Scaled Dot-Product Attention** : Formule math√©matique et impl√©mentation  \n",
    "‚úÖ **Multi-Head Attention** : Plusieurs perspectives d'attention en parall√®le  \n",
    "‚úÖ **Self vs Cross Attention** : Diff√©rences et cas d'usage  \n",
    "‚úÖ **Applications Pratiques** : Sentiment, traduction, g√©n√©ration  \n",
    "‚úÖ **Avantages vs RNN** : Parall√©lisation et m√©moire √† long terme\n",
    "\n",
    "### Prochaines √©tapes :\n",
    "- **Module 6.2** : Architecture compl√®te des Transformers\n",
    "- **Module 7** : BERT et GPT\n",
    "- **Projets pratiques** : Impl√©mentation de vos propres mod√®les\n",
    "\n",
    "### üõ†Ô∏è Exercices Pratiques Sugg√©r√©s :\n",
    "1. Modifiez les param√®tres d'attention (temp√©rature, nombre de t√™tes)\n",
    "2. Impl√©mentez diff√©rents types de masques\n",
    "3. Cr√©ez un mod√®le de traduction simple\n",
    "4. Analysez les patterns d'attention sur vos propres textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéä F√©licitations ! Vous ma√Ætrisez maintenant les m√©canismes d'attention !\n",
    "print(\"\"\"üéä F√âLICITATIONS ! üéä\n",
    "\n",
    "Vous venez de ma√Ætriser un des concepts les plus importants du NLP moderne !\n",
    "\n",
    "üß† L'attention est utilis√©e dans :\n",
    "  ‚Ä¢ GPT (ChatGPT, GPT-4)\n",
    "  ‚Ä¢ BERT (Google Search)\n",
    "  ‚Ä¢ T5 (Google Translate)\n",
    "  ‚Ä¢ DALL-E (g√©n√©ration d'images)\n",
    "  ‚Ä¢ Et bien d'autres...\n",
    "\n",
    "üöÄ Continuez vers l'architecture Transformer compl√®te !\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}