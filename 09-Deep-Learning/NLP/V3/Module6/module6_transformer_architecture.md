---
title: Module 6 - Architecture Transformer
description: Formation NLP - Module 6 - Architecture Transformer
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ðŸ—ï¸ Architecture Transformer

Comprendre la structure rÃ©volutionnaire qui a transformÃ© l'IA

### ðŸ—ºï¸ Navigation Module 6

Explorez les Transformers Ã©tape par Ã©tape

[ðŸ  Index Module 6](index.html) [ðŸ‘ï¸ MÃ©canismes d'Attention](module6_attention_mechanisms.html)

ðŸ—ï¸ Architecture (Actuel)

[ðŸš€ ImplÃ©mentation â†’](module6_implementation.html)

## ðŸ—ï¸ Vue d'Ensemble de l'Architecture

ðŸ” Vue d'ensemble ðŸ”„ Encodeur ðŸ“¤ DÃ©codeur ðŸŽ¯ Architecture ComplÃ¨te

### ðŸŽ¯ Structure GÃ©nÃ©rale du Transformer

**Architecture Encoder-Decoder**

ðŸ“¥ Input

"Bonjour le monde"

Tokenisation + Embeddings

â†’

ðŸ”„ Encodeur

6 couches

Self-Attention + FFN

â†’

ðŸ“¤ DÃ©codeur

6 couches

Masked Attention + Cross-Attention

â†’

ðŸ“‹ Output

"Hello world"

ProbabilitÃ©s des mots

**ðŸ”‘ Innovations ClÃ©s :**  
â€¢ ParallÃ©lisation complÃ¨te : Plus de traitement sÃ©quentiel  
â€¢ Self-Attention : Chaque mot "regarde" tous les autres  
â€¢ Multi-Head Attention : Plusieurs types d'attention en parallÃ¨le  
â€¢ Positional Encoding : Encodage de la position sans rÃ©currence  
â€¢ Residual Connections : Ã‰vite la disparition du gradient

### ðŸ”„ Bloc Encodeur DÃ©taillÃ©

#### Structure d'un Bloc Encodeur

**1\. Multi-Head Self-Attention**  
Chaque mot calcule son attention avec tous les autres mots

Input â†’ Q, K, V â†’ Attention(Q,K,V) â†’ Concat â†’ Linear

+

**2\. Add & Norm**  
Connexion rÃ©siduelle + normalisation de couche

LayerNorm(x + SelfAttention(x))

â†“

**3\. Feed Forward Network**  
RÃ©seau de neurones position-wise

Linear â†’ ReLU â†’ Linear (avec dimensions d\_model â†’ d\_ff â†’ d\_model)

+

**4\. Add & Norm**  
DeuxiÃ¨me connexion rÃ©siduelle + normalisation

LayerNorm(x + FFN(x))

**Formules MathÃ©matiques :**

Attention(Q,K,V) = softmax(QK^T/âˆšd\_k)V

MultiHead(Q,K,V) = Concat(headâ‚,...,head\_h)W^O

FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚

LayerNorm(x) = Î³((x-Î¼)/Ïƒ) + Î²

### ðŸ“¤ Bloc DÃ©codeur DÃ©taillÃ©

#### Structure d'un Bloc DÃ©codeur

**1\. Masked Multi-Head Self-Attention**  
Attention uniquement sur les positions prÃ©cÃ©dentes (prÃ©vention du futur)

Mask pour Ã©viter de voir les mots suivants

**2\. Add & Norm**  
PremiÃ¨re connexion rÃ©siduelle

**3\. Cross-Attention (Encoder-Decoder)**  
Q vient du dÃ©codeur, K et V viennent de l'encodeur

Permet au dÃ©codeur de "regarder" l'entrÃ©e originale

**4\. Add & Norm**  
DeuxiÃ¨me connexion rÃ©siduelle

**5\. Feed Forward Network**  
Identique Ã  l'encodeur

**6\. Add & Norm**  
TroisiÃ¨me connexion rÃ©siduelle

**ðŸ” DiffÃ©rences ClÃ©s Encodeur vs DÃ©codeur :**  
â€¢ Masked Attention : Le dÃ©codeur ne peut pas voir le futur  
â€¢ Cross-Attention : Connexion avec l'output de l'encodeur  
â€¢ GÃ©nÃ©ration Autoregressif : Production mot par mot  
â€¢ 3 couches d'attention : Masked Self + Cross + FFN

### ðŸŽ¯ Architecture ComplÃ¨te

#### Transformer Complet : Traduction "Hello" â†’ "Bonjour"

##### ðŸ”„ ENCODEUR

**Input Embeddings**  
"Hello" â†’ \[0.1, 0.8, 0.3, ...\]

**\+ Positional Encoding**  
Position 0: \[0.0, 1.0, 0.0, ...\]

**6 Ã— Encoder Layers**  
Self-Attention + FFN

**Output Representations**  
Contexte enrichi de "Hello"

â†’

##### ðŸ“¤ DÃ‰CODEUR

**Output Embeddings**  
"" â†’ gÃ©nÃ©ration progressive

**\+ Positional Encoding**  
Position de gÃ©nÃ©ration

**6 Ã— Decoder Layers**  
Masked Attention + Cross-Attention + FFN

**Linear + Softmax**  
ProbabilitÃ©s : "Bonjour" (0.95)

#### ðŸŽ¯ Flux Complet de Traduction

**1\. Tokenisation**  
"Hello" â†’ \[101, 7592, 102\]

**2\. Embeddings**  
Tokens â†’ Vecteurs denses

**3\. Encodage**  
ComprÃ©hension contextuelle

**4\. DÃ©codage**  
GÃ©nÃ©ration mot par mot

**5\. Output**  
"Bonjour" !

## âš–ï¸ Transformer vs RNN/LSTM

ðŸ”„ RNN/LSTM

**Traitement :** SÃ©quentiel  
**ParallÃ©lisation :** âŒ Impossible  
**MÃ©moire :** LimitÃ©e (~1000 mots)  
**Vitesse :** Lente  
**DÃ©pendances :** Difficiles long terme  
**ComplexitÃ© :** O(n) en temps

ðŸ¤– Transformer

**Traitement :** ParallÃ¨le  
**ParallÃ©lisation :** âœ… ComplÃ¨te  
**MÃ©moire :** IllimitÃ©e thÃ©oriquement  
**Vitesse :** TrÃ¨s rapide  
**DÃ©pendances :** Globales directes  
**ComplexitÃ© :** O(1) en temps (parallÃ¨le)

ðŸ“Š Performance

**BLEU Score Traduction :**  
â€¢ RNN/LSTM: ~28  
â€¢ Transformer: ~41  
  
**Vitesse d'entraÃ®nement :**  
â€¢ RNN: 1x (baseline)  
â€¢ Transformer: 10-100x

**ðŸš€ Pourquoi les Transformers dominent :**  
â€¢ ParallÃ©lisation : Utilisation optimale des GPU modernes  
â€¢ Attention globale : Chaque mot peut "voir" tous les autres directement  
â€¢ Pas de goulot d'Ã©tranglement : Plus de limite par la mÃ©moire sÃ©quentielle  
â€¢ ScalabilitÃ© : Performance amÃ©liore avec plus de donnÃ©es et de calcul  
â€¢ Transfert learning : PrÃ©-entraÃ®nement efficace sur de vastes corpus

## ðŸ”§ DÃ©tails Techniques AvancÃ©s

### ðŸ“ Dimensions et HyperparamÃ¨tres

**Configuration Standard (Transformer Base) :**  
  

**ModÃ¨le :**  
â€¢ d\_model = 512 (dimension des embeddings)  
â€¢ d\_ff = 2048 (dimension FFN)  
â€¢ h = 8 (nombre de tÃªtes d'attention)

**Architecture :**  
â€¢ N = 6 (couches encoder/decoder)  
â€¢ d\_k = d\_v = 64 (dimension par tÃªte)  
â€¢ Dropout = 0.1

**Training :**  
â€¢ ParamÃ¨tres totaux: ~65M  
â€¢ Adam optimizer  
â€¢ Warmup + decay learning rate

### ðŸ§® ComplexitÃ© Computationnelle

**Self-Attention**  

ComplexitÃ©: O(nÂ² Ã— d)  
MÃ©moire: O(nÂ²)  
ParallÃ©lisation: O(1)

n = longueur sÃ©quence, d = dimension

**RNN**  

ComplexitÃ©: O(n Ã— dÂ²)  
MÃ©moire: O(n Ã— d)  
ParallÃ©lisation: O(n)

SÃ©quentiel par nature

**âš ï¸ Trade-offs :**  
â€¢ SÃ©quences courtes : Transformer plus efficace  
â€¢ SÃ©quences trÃ¨s longues : Attention quadratique peut Ãªtre limitante  
â€¢ Solutions : Sparse Attention, Linear Attention, Longformer  
â€¢ GPU vs CPU : Transformers excellent sur GPU, RNN acceptable sur CPU

## âž¡ï¸ Prochaine Ã‰tape

Vous comprenez maintenant l'architecture rÃ©volutionnaire des Transformers ! Passons Ã  l'implÃ©mentation pratique.

**ðŸŽ¯ Ce que vous maÃ®trisez :**  
â€¢ Architecture Encoder-Decoder : Structure globale  
â€¢ Self-Attention : MÃ©canisme central  
â€¢ Multi-Head Attention : ParallÃ©lisation des perspectives  
â€¢ Avantages vs RNN : ParallÃ©lisation et performance

### ðŸš€ Continuez l'Exploration

ImplÃ©mentez votre propre Transformer et dÃ©couvrez ses applications

[ðŸš€ ImplÃ©mentation & Applications â†’](module6_implementation.html) [ðŸ¤– Module 7: BERT & GPT](../Module7/index.html)

// Animation de la barre de progression window.addEventListener('load', function() { setTimeout(() => { document.getElementById('progressBar').style.width = '75%'; }, 1000); }); // Gestion des onglets function showTab(tabName) { // Cacher tous les contenus d'onglets const contents = document.querySelectorAll('.tab-content'); contents.forEach(content => content.classList.remove('active')); // DÃ©sactiver tous les onglets const tabs = document.querySelectorAll('.tab'); tabs.forEach(tab => tab.classList.remove('active')); // Activer l'onglet et le contenu sÃ©lectionnÃ©s document.getElementById(tabName).classList.add('active'); event.target.classList.add('active'); } // Highlight des blocs transformer function highlightBlock(block, type) { // Reset all blocks document.querySelectorAll('.transformer-block').forEach(b => { b.style.transform = 'scale(1)'; b.style.boxShadow = ''; }); // Highlight clicked block block.style.transform = 'scale(1.1)'; block.style.boxShadow = '0 12px 30px rgba(255, 107, 107, 0.6)'; // Reset after 2 seconds setTimeout(() => { block.style.transform = 'scale(1)'; block.style.boxShadow = ''; }, 2000); }
