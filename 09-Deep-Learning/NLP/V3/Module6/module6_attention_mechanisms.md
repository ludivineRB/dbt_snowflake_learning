---
title: Module 6 - M√©canismes d'Attention
description: Formation NLP - Module 6 - M√©canismes d'Attention
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# üß† M√©canismes d'Attention

Le c≈ìur r√©volutionnaire des Transformers

**üéØ Question centrale :**  
"Comment permettre √† un mod√®le de se concentrer sur les parties importantes d'une s√©quence ?"

[‚Üê Introduction](module6_introduction.html)

**M√©canismes d'Attention**  
Le c≈ìur des Transformers

[Architecture Transformer ‚Üí](module6_transformer_architecture.html)

## 1\. üîç Qu'est-ce que l'Attention ?

#### üé≠ Analogie : Une Soir√©e Cocktail

Imaginez-vous dans une soir√©e bruyante. Votre ami vous parle, mais il y a de la musique, d'autres conversations, des bruits de verres...

**Question :** Comment votre cerveau fait-il pour se concentrer sur la voix de votre ami ?

**R√©ponse :** Il utilise un m√©canisme d'*attention s√©lective* qui amplifie les signaux importants et att√©nue le bruit.

**üí° C'est exactement ce que fait l'attention dans les Transformers !**

#### üß† L'Attention en NLP

Dans une phrase, tous les mots ne sont pas √©galement importants pour comprendre chaque mot individuel. L'attention permet au mod√®le de d√©cider quels mots regarder quand il traite un mot donn√©.

üéØ D√©monstration Interactive : Attention en Action

Cliquez sur un mot pour voir sur quoi il "porte son attention" :

Le

chat

noir

mange

la

souris

Cliquez sur un mot pour voir son pattern d'attention

### üî¨ Les Types d'Attention

#### üîÑ Attention Crois√©e

Un mot porte attention √† d'autres mots de la s√©quence. Utile pour la traduction.

**Exemple :** "chat" en fran√ßais porte attention √† "cat" en anglais.

#### ü™û Self-Attention

Chaque mot porte attention √† tous les mots de la m√™me s√©quence, y compris lui-m√™me.

**R√©volution :** C'est la cl√© des Transformers !

## 2\. üîë Les Concepts Query, Key, Value

#### üèõÔ∏è Analogie : Une Biblioth√®que

Imaginez que vous cherchez des informations dans une biblioth√®que immense :

*   **Query (Requ√™te) :** "Je cherche des livres sur l'IA"
*   **Key (Cl√©) :** L'√©tiquette sur chaque √©tag√®re : "Informatique", "Histoire", "Sciences"...
*   **Value (Valeur) :** Le contenu r√©el des livres sur l'√©tag√®re

**Processus :** Votre requ√™te compare avec chaque √©tiquette, trouve les plus pertinentes, et r√©cup√®re le contenu correspondant.

üîç

#### Query (Q)

**"Qu'est-ce que je cherche ?"**

La repr√©sentation de ce que le mot actuel veut savoir sur les autres mots.

**Exemple :**  
Pour "mange", la query pourrait √™tre : "Qui fait l'action ?" et "Quoi est mang√© ?"

üóùÔ∏è

#### Key (K)

**"Qu'est-ce que je peux offrir ?"**

La repr√©sentation de l'information que chaque mot peut fournir aux autres.

**Exemple :**  
"chat" pourrait avoir une key : "Je suis un sujet qui peut faire des actions"

üíé

#### Value (V)

**"Quelle information je fournis ?"**

Le contenu r√©el de l'information √† transmettre.

**Exemple :**  
La value de "chat" contient toute l'information s√©mantique sur ce qu'est un chat

#### üîÑ Le Processus d'Attention

1\. **Comparaison :** La Query de chaque mot compare avec toutes les Keys  
2\. **Score :** Plus la Query et la Key sont similaires, plus le score est √©lev√©  
3\. **Pond√©ration :** Les scores deviennent des poids d'attention (softmax)  
4\. **Agr√©gation :** Les Values sont combin√©es selon ces poids

## 3\. üìê Les Math√©matiques de l'Attention

#### üí° Ne Paniquez Pas !

Les maths peuvent sembler intimidantes, mais le concept est simple : **calculer des similarit√©s et faire des moyennes pond√©r√©es**.

**Formule de base de l'Attention :**  
  
Attention(Q,K,V) = softmax(QKT/‚àödk)V

### üî¢ D√©composition √âtape par √âtape

#### √âtape 1 : Calcul des Scores

**Formule :** Scores = QKT

On multiplie chaque Query par toutes les Keys pour obtenir des scores de compatibilit√©.

Si Q = \[q‚ÇÅ, q‚ÇÇ, ...\] et K = \[k‚ÇÅ, k‚ÇÇ, ...\]  
Alors Score(i,j) = q·µ¢ ¬∑ k‚±º

#### √âtape 2 : Normalisation

**Formule :** Scores = Scores / ‚àödk

On divise par la racine de la dimension pour stabiliser les gradients.

**Pourquoi ?** Sans cela, les scores deviennent trop grands et le softmax sature.

#### √âtape 3 : Softmax

**Formule :** Weights = softmax(Scores)

Conversion des scores en probabilit√©s qui somment √† 1.

softmax(x·µ¢) = ex·µ¢ / Œ£‚±º ex‚±º

#### √âtape 4 : Agr√©gation Pond√©r√©e

**Formule :** Output = Weights √ó V

On combine les Values selon les poids d'attention calcul√©s.

R√©sultat : une repr√©sentation enrichie qui capture les informations pertinentes.

#### üéÆ D√©monstration Interactive

Matrice d'attention pour : "Le chat mange"

Le

chat

mange

Le

0.1

0.3

0.6

chat

0.2

0.5

0.3

mange

0.1

0.7

0.2

Passez la souris sur les cellules pour voir l'interpr√©tation

Cliquez sur une cellule pour voir ce que signifie ce score d'attention

## 4\. üß© Multi-Head Attention

#### üë• Analogie : Un Conseil d'Experts

Imaginez que vous demandez conseil pour acheter une voiture. Vous consultez :

*   **Expert 1 :** Se concentre sur la s√©curit√©
*   **Expert 2 :** Se concentre sur l'√©conomie
*   **Expert 3 :** Se concentre sur le design
*   **Expert 4 :** Se concentre sur la performance

**R√©sultat :** Vous obtenez une vision compl√®te en combinant tous ces points de vue sp√©cialis√©s.

#### üéØ Pourquoi Plusieurs "T√™tes" ?

Une seule t√™te d'attention ne peut capturer qu'un type de relation. Le Multi-Head Attention permet de capturer diff√©rents types de relations simultan√©ment :

*   Relations syntaxiques (sujet-verbe)
*   Relations s√©mantiques (synonymes)
*   Relations de position (proche/loin)
*   Relations contextuelles (anaphore)

#### üë• 8 T√™tes d'Attention Sp√©cialis√©es

Phrase : "Le chat noir mange la souris grise"

T√™te 1: Syntaxe

Se concentre sur sujet-verbe  
"chat" ‚Üî "mange"

T√™te 2: Objets

Se concentre sur verbe-objet  
"mange" ‚Üî "souris"

T√™te 3: Adjectifs

Se concentre sur nom-adjectif  
"chat" ‚Üî "noir"

T√™te 4: D√©terminants

Se concentre sur articles  
"Le" ‚Üî "chat"

T√™te 5: Distance

Mots adjacents  
Relations de proximit√©

T√™te 6: S√©mantique

Concepts li√©s  
"chat" ‚Üî "souris" (pr√©dateur-proie)

T√™te 7: Position

D√©but/fin de phrase  
Structure globale

T√™te 8: Contexte

Informations globales  
Th√®me g√©n√©ral

**üí° R√©sultat :** Chaque t√™te capture un aspect diff√©rent, puis toutes les informations sont combin√©es pour une compr√©hension compl√®te et nuanc√©e.

**Formule Multi-Head Attention :**  
  
MultiHead(Q,K,V) = Concat(head‚ÇÅ, head‚ÇÇ, ..., head‚Çà)WO  
  
o√π head‚Çç·µ¢‚Çé = Attention(QW‚Çç·µ¢‚ÇéQ, KW‚Çç·µ¢‚ÇéK, VW‚Çç·µ¢‚ÇéV)

### üîß Avantages du Multi-Head

#### üéØ Sp√©cialisation

Chaque t√™te peut se sp√©cialiser dans un type de relation diff√©rent.

#### üîÑ Parall√©lisation

Toutes les t√™tes sont calcul√©es en parall√®le, pas de ralentissement.

#### üß† Richesse

Repr√©sentation plus riche et nuanc√©e du contexte.

## 5\. ü™û Self-Attention : La R√©volution

#### üîÑ Self-Attention Expliqu√©e

Dans la Self-Attention, chaque mot de la s√©quence porte attention √† tous les mots de la m√™me s√©quence, y compris lui-m√™me. C'est comme si chaque mot "discutait" avec tous les autres pour enrichir sa propre compr√©hension.

üîç Self-Attention en Action

Regardez comment "mange" enrichit sa repr√©sentation :

**"Le chat noir mange la souris grise"**

**üéØ "mange" demande :**

*   "Qui fait l'action ?" ‚Üí **Attention forte sur "chat"**
*   "Quoi est mang√© ?" ‚Üí **Attention forte sur "souris"**
*   "Quelles propri√©t√©s ?" ‚Üí **Attention moyenne sur "noir", "grise"**
*   "Contexte grammatical ?" ‚Üí **Attention faible sur "le", "la"**

**üìä R√©sultat :** La repr√©sentation de "mange" est enrichie avec toutes ces informations contextuelles.

### üöÄ Pourquoi Self-Attention R√©volutionne Tout

#### ‚úÖ Connexions Directes

Chaque mot peut directement acc√©der √† tout autre mot, quelle que soit la distance.

**Impact :** D√©pendances √† long terme parfaitement captur√©es.

#### ‚ö° Parall√©lisation

Tous les calculs d'attention peuvent √™tre faits simultan√©ment.

**Impact :** Vitesse d'entra√Ænement drastiquement am√©lior√©e.

#### üéØ Flexibilit√©

Le mod√®le apprend automatiquement quelles connexions sont importantes.

**Impact :** Adaptation automatique √† diff√©rents types de t√¢ches.

[‚Üê Introduction](module6_introduction.html)

**Pr√™t pour l'architecture compl√®te ?**  
D√©couvrez le Transformer au complet

[Architecture Transformer ‚Üí](module6_transformer_architecture.html)

// D√©monstration interactive d'attention const attentionPatterns = { '0': { weights: \[0.8, 0.1, 0.05, 0.02, 0.02, 0.01\], explanation: "Le d√©terminant 'Le' porte surtout attention √† lui-m√™me et un peu au nom qu'il d√©termine." }, '1': { weights: \[0.2, 0.4, 0.15, 0.2, 0.03, 0.02\], explanation: "Le nom 'chat' porte attention au d√©terminant, √† son adjectif 'noir', et au verbe 'mange'." }, '2': { weights: \[0.05, 0.6, 0.3, 0.03, 0.01, 0.01\], explanation: "L'adjectif 'noir' porte principalement attention au nom qu'il qualifie : 'chat'." }, '3': { weights: \[0.02, 0.5, 0.05, 0.3, 0.08, 0.05\], explanation: "Le verbe 'mange' porte attention au sujet 'chat' et √† l'objet 'souris'." }, '4': { weights: \[0.1, 0.02, 0.01, 0.05, 0.7, 0.12\], explanation: "Le d√©terminant 'la' porte surtout attention √† lui-m√™me et au nom qu'il d√©termine." }, '5': { weights: \[0.01, 0.02, 0.01, 0.3, 0.15, 0.51\], explanation: "Le nom 'souris' porte attention au verbe qui l'affecte et √† son d√©terminant." } }; document.getElementById('attentionDemo').addEventListener('click', function(e) { if (e.target.classList.contains('word-token')) { // Reset all tokens document.querySelectorAll('.word-token').forEach(token => { token.classList.remove('active'); const existing = token.querySelector('.attention-weight'); if (existing) existing.remove(); }); // Activate clicked token e.target.classList.add('active'); // Get attention pattern const wordIndex = e.target.dataset.word; const pattern = attentionPatterns\[wordIndex\]; // Add attention weights document.querySelectorAll('.word-token').forEach((token, i) => { const weight = pattern.weights\[i\]; if (weight > 0.1) { const weightElement = document.createElement('div'); weightElement.className = 'attention-weight'; weightElement.textContent = weight.toFixed(1); token.appendChild(weightElement); } }); // Update explanation document.getElementById('attentionExplanation').textContent = pattern.explanation; } }); // Matrice d'attention interactive const matrixExplanations = { '0.1': "Attention faible - relation grammaticale basique", '0.2': "Attention faible-moyenne - lien contextuel", '0.3': "Attention moyenne - relation syntaxique", '0.5': "Attention forte - auto-attention (le mot se regarde lui-m√™me)", '0.6': "Attention forte - relation grammaticale importante", '0.7': "Attention tr√®s forte - d√©pendance syntaxique directe" }; document.getElementById('attentionMatrix').addEventListener('click', function(e) { if (e.target.classList.contains('matrix-value')) { const score = e.target.dataset.score; const explanation = matrixExplanations\[score\] || "Score d'attention"; document.getElementById('matrixExplanation').innerHTML = \`<strong>Score ${score} :</strong> ${explanation}\`; } }); // Animation au scroll function animateOnScroll() { const elements = document.querySelectorAll('.content-section'); elements.forEach(element => { const elementTop = element.getBoundingClientRect().top; const elementVisible = 150; if (elementTop < window.innerHeight - elementVisible) { element.style.opacity = '1'; element.style.transform = 'translateY(0)'; } }); } window.addEventListener('scroll', animateOnScroll); document.addEventListener('DOMContentLoaded', animateOnScroll);
