{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Cr√©ation et Gestion de Stopwords Personnalis√©s\n",
        "\n",
        "**Module 2 - Preprocessing et Tokenisation**\n\n",
        "Ce notebook vous apprend √† cr√©er, g√©rer et optimiser vos propres listes de stopwords selon vos besoins sp√©cifiques. Parce qu'une liste universelle ne convient jamais √† tous les cas !\n",
        "\n",
        "## üìã Plan du Notebook\n",
        "\n",
        "1. **Pourquoi des Stopwords Personnalis√©s ?**\n",
        "2. **M√©thodes d'Identification Automatique**\n",
        "3. **Stopwords par Domaine d'Application**\n",
        "4. **Gestionnaire Avanc√© de Stopwords**\n",
        "5. **Validation et Tests**\n",
        "6. **Optimisation par Fr√©quence**\n",
        "7. **Cas d'Usage Pratiques**\n",
        "8. **Export et R√©utilisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation et Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des packages n√©cessaires\n",
        "# !pip install nltk spacy matplotlib seaborn pandas wordcloud scikit-learn\n",
        "# !python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import json\n",
        "from typing import List, Set, Dict, Tuple\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration matplotlib\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"‚úÖ Tous les imports r√©alis√©s avec succ√®s !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement des ressources de base\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Chargement spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"fr_core_news_sm\")\n",
        "    print(\"‚úÖ Mod√®le spaCy fran√ßais charg√©\")\n",
        "except OSError:\n",
        "    print(\"‚ö†Ô∏è Mod√®le spaCy fran√ßais non trouv√©, utilisation des fonctions de base\")\n",
        "    nlp = None\n",
        "\n",
        "# Stopwords de base\n",
        "stopwords_base_fr = set(stopwords.words('french'))\n",
        "print(f\"üìö {len(stopwords_base_fr)} stopwords fran√ßais de base charg√©s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ü§î Pourquoi des Stopwords Personnalis√©s ?\n",
        "\n",
        "### Les limites des listes standard :\n",
        "- **Trop g√©n√©rales** : ne tiennent pas compte de votre domaine\n",
        "- **Parfois incompl√®tes** : manquent des mots fr√©quents dans votre contexte\n",
        "- **Parfois trop strictes** : suppriment des mots importants pour votre analyse\n",
        "\n",
        "### Exemples concrets :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemples o√π les stopwords standard posent probl√®me\n",
        "exemples_problematiques = {\n",
        "    \"E-commerce - Avis produit\": {\n",
        "        \"texte\": \"Ce produit est vraiment bien. Service client excellent. Livraison rapide. Je recommande ce site.\",\n",
        "        \"probleme\": \"'produit', 'service', 'client' sont fr√©quents mais informatifs\",\n",
        "        \"mots_a_ajouter\": [\"vraiment\", \"bien\", \"excellent\", \"rapide\"]\n",
        "    },\n",
        "    \n",
        "    \"M√©dical - Comptes-rendus\": {\n",
        "        \"texte\": \"Le patient pr√©sente des sympt√¥mes. Examen clinique normal. Traitement prescrit.\",\n",
        "        \"probleme\": \"'patient', 'examen', 'traitement' sont techniques mais r√©p√©titifs\",\n",
        "        \"mots_a_ajouter\": [\"patient\", \"examen\", \"clinique\", \"traitement\"]\n",
        "    },\n",
        "    \n",
        "    \"R√©seaux sociaux - Tweets\": {\n",
        "        \"texte\": \"Salut les amis ! MDR ce truc est ouf ! Genre trop cool lol\",\n",
        "        \"probleme\": \"Argot et abr√©viations non couverts par les listes standard\",\n",
        "        \"mots_a_ajouter\": [\"salut\", \"amis\", \"mdr\", \"lol\", \"genre\", \"truc\", \"ouf\"]\n",
        "    },\n",
        "    \n",
        "    \"Technique - Documentation\": {\n",
        "        \"texte\": \"Cette fonction permet de configurer le syst√®me. Voir documentation pour plus d'informations.\",\n",
        "        \"probleme\": \"'fonction', 'syst√®me', 'documentation' sont r√©p√©titifs en tech\",\n",
        "        \"mots_a_ajouter\": [\"fonction\", \"permet\", \"configurer\", \"syst√®me\", \"voir\", \"documentation\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üéØ EXEMPLES DE CAS PROBL√âMATIQUES\\n\")\n",
        "\n",
        "for domaine, info in exemples_problematiques.items():\n",
        "    print(f\"üìÇ {domaine}\")\n",
        "    print(f\"   Texte : {info['texte']}\")\n",
        "    print(f\"   ‚ö†Ô∏è  Probl√®me : {info['probleme']}\")\n",
        "    print(f\"   üí° Stopwords √† ajouter : {', '.join(info['mots_a_ajouter'])}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. üîç M√©thodes d'Identification Automatique\n",
        "\n",
        "Comment identifier automatiquement les candidats stopwords dans vos donn√©es ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DetecteurStopwords:\n",
        "    \"\"\"Classe pour d√©tecter automatiquement les stopwords candidats.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.frequences = Counter()\n",
        "        self.documents_count = 0\n",
        "        self.document_frequencies = defaultdict(int)\n",
        "    \n",
        "    def analyser_corpus(self, textes: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Analyse un corpus pour calculer les fr√©quences.\n",
        "        \n",
        "        Args:\n",
        "            textes: Liste de textes √† analyser\n",
        "        \"\"\"\n",
        "        self.documents_count = len(textes)\n",
        "        \n",
        "        for texte in textes:\n",
        "            # Tokenisation simple\n",
        "            mots = re.findall(r'\\w+', texte.lower())\n",
        "            \n",
        "            # Comptage global\n",
        "            self.frequences.update(mots)\n",
        "            \n",
        "            # Comptage par document (pour IDF)\n",
        "            mots_uniques = set(mots)\n",
        "            for mot in mots_uniques:\n",
        "                self.document_frequencies[mot] += 1\n",
        "    \n",
        "    def candidats_par_frequence(self, seuil_frequence: int = 10, \n",
        "                               pourcentage_top: float = 0.05) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Identifie les candidats stopwords par fr√©quence √©lev√©e.\n",
        "        \n",
        "        Args:\n",
        "            seuil_frequence: Fr√©quence minimale\n",
        "            pourcentage_top: Pourcentage des mots les plus fr√©quents\n",
        "        \n",
        "        Returns:\n",
        "            Liste des candidats avec leur fr√©quence\n",
        "        \"\"\"\n",
        "        # Mots tr√®s fr√©quents\n",
        "        mots_frequents = [(mot, freq) for mot, freq in self.frequences.most_common() \n",
        "                         if freq >= seuil_frequence]\n",
        "        \n",
        "        # Top X% des mots\n",
        "        nb_top = max(1, int(len(self.frequences) * pourcentage_top))\n",
        "        top_mots = self.frequences.most_common(nb_top)\n",
        "        \n",
        "        # Combinaison des crit√®res\n",
        "        candidats = list(set(mots_frequents + top_mots))\n",
        "        candidats.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        return candidats\n",
        "    \n",
        "    def candidats_par_distribution(self, seuil_presence: float = 0.7) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Identifie les candidats par distribution dans les documents.\n",
        "        \n",
        "        Args:\n",
        "            seuil_presence: Pourcentage minimum de documents contenant le mot\n",
        "        \n",
        "        Returns:\n",
        "            Liste des candidats avec leur taux de pr√©sence\n",
        "        \"\"\"\n",
        "        candidats = []\n",
        "        \n",
        "        for mot, doc_freq in self.document_frequencies.items():\n",
        "            taux_presence = doc_freq / self.documents_count\n",
        "            if taux_presence >= seuil_presence:\n",
        "                candidats.append((mot, taux_presence))\n",
        "        \n",
        "        candidats.sort(key=lambda x: x[1], reverse=True)\n",
        "        return candidats\n",
        "    \n",
        "    def candidats_par_longueur(self, longueur_max: int = 3) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Identifie les mots courts tr√®s fr√©quents (souvent des stopwords).\n",
        "        \n",
        "        Args:\n",
        "            longueur_max: Longueur maximale des mots\n",
        "        \n",
        "        Returns:\n",
        "            Liste des candidats courts avec leur fr√©quence\n",
        "        \"\"\"\n",
        "        candidats = [(mot, freq) for mot, freq in self.frequences.most_common() \n",
        "                    if len(mot) <= longueur_max and freq >= 5]\n",
        "        \n",
        "        return candidats[:20]  # Top 20\n",
        "    \n",
        "    def rapport_analyse(self) -> Dict:\n",
        "        \"\"\"\n",
        "        G√©n√®re un rapport d'analyse complet.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les statistiques du corpus\n",
        "        \"\"\"\n",
        "        total_mots = sum(self.frequences.values())\n",
        "        vocabulaire_unique = len(self.frequences)\n",
        "        \n",
        "        return {\n",
        "            'documents_count': self.documents_count,\n",
        "            'total_mots': total_mots,\n",
        "            'vocabulaire_unique': vocabulaire_unique,\n",
        "            'mots_par_document': total_mots / self.documents_count if self.documents_count > 0 else 0,\n",
        "            'diversite_lexicale': vocabulaire_unique / total_mots if total_mots > 0 else 0,\n",
        "            'top_10_mots': self.frequences.most_common(10)\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Classe DetecteurStopwords cr√©√©e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test du d√©tecteur sur un corpus d'exemple\n",
        "corpus_ecommerce = [\n",
        "    \"Ce produit est vraiment excellent, je le recommande √† tous mes amis et ma famille.\",\n",
        "    \"Service client tr√®s professionnel, livraison rapide, emballage soign√©, parfait.\",\n",
        "    \"Produit conforme √† la description, qualit√© au rendez-vous, tr√®s satisfait de mon achat.\",\n",
        "    \"Prix un peu √©lev√© mais la qualit√© justifie largement cet investissement, je recommande.\",\n",
        "    \"Commande re√ßue rapidement, produit de bonne qualit√©, service client r√©actif.\",\n",
        "    \"Tr√®s bon rapport qualit√© prix, produit fiable, livraison dans les temps.\",\n",
        "    \"Service apr√®s vente excellent, probl√®me r√©solu rapidement, √©quipe tr√®s professionnelle.\",\n",
        "    \"Produit exactement comme d√©crit, emballage parfait, livraison ultra rapide.\",\n",
        "    \"Qualit√© exceptionnelle, service client √† l'√©coute, je recommande vivement ce site.\",\n",
        "    \"Achat sans probl√®me, produit conforme, d√©lai de livraison respect√©, parfait.\"\n",
        "]\n",
        "\n",
        "# Analyse du corpus\n",
        "detecteur = DetecteurStopwords()\n",
        "detecteur.analyser_corpus(corpus_ecommerce)\n",
        "\n",
        "# Rapport d'analyse\n",
        "rapport = detecteur.rapport_analyse()\n",
        "print(\"üìä RAPPORT D'ANALYSE DU CORPUS E-COMMERCE\\n\")\n",
        "print(f\"Documents analys√©s : {rapport['documents_count']}\")\n",
        "print(f\"Total de mots : {rapport['total_mots']}\")\n",
        "print(f\"Vocabulaire unique : {rapport['vocabulaire_unique']}\")\n",
        "print(f\"Mots par document : {rapport['mots_par_document']:.1f}\")\n",
        "print(f\"Diversit√© lexicale : {rapport['diversite_lexicale']:.3f}\")\n",
        "\n",
        "print(f\"\\nüîù Top 10 des mots les plus fr√©quents :\")\n",
        "for mot, freq in rapport['top_10_mots']:\n",
        "    print(f\"  '{mot}' : {freq} fois\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identification des candidats stopwords\n",
        "print(\"üîç IDENTIFICATION DES CANDIDATS STOPWORDS\\n\")\n",
        "\n",
        "# Par fr√©quence\n",
        "candidats_freq = detecteur.candidats_par_frequence(seuil_frequence=3, pourcentage_top=0.1)\n",
        "print(f\"üìà Candidats par fr√©quence √©lev√©e ({len(candidats_freq)}) :\")\n",
        "for mot, freq in candidats_freq[:15]:\n",
        "    print(f\"  '{mot}' : {freq} fois\")\n",
        "\n",
        "# Par distribution\n",
        "candidats_distrib = detecteur.candidats_par_distribution(seuil_presence=0.6)\n",
        "print(f\"\\nüìä Candidats par distribution ({len(candidats_distrib)}) :\")\n",
        "for mot, taux in candidats_distrib[:15]:\n",
        "    print(f\"  '{mot}' : {taux:.1%} des documents\")\n",
        "\n",
        "# Par longueur\n",
        "candidats_courts = detecteur.candidats_par_longueur(longueur_max=3)\n",
        "print(f\"\\nüî§ Mots courts fr√©quents ({len(candidats_courts)}) :\")\n",
        "for mot, freq in candidats_courts[:15]:\n",
        "    print(f\"  '{mot}' ({len(mot)} lettres) : {freq} fois\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. üè¢ Stopwords par Domaine d'Application\n",
        "\n",
        "Cr√©ation de listes sp√©cialis√©es pour diff√©rents domaines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionnaire complet de stopwords par domaine\n",
        "STOPWORDS_DOMAINES = {\n",
        "    'e_commerce': {\n",
        "        'description': 'Commerce √©lectronique, avis clients, produits',\n",
        "        'mots': {\n",
        "            # Mots li√©s au commerce\n",
        "            'produit', 'article', 'commande', 'achat', 'vente', 'prix', 'euros', 'euro',\n",
        "            'livraison', 'service', 'client', 'clients', 'boutique', 'magasin', 'site',\n",
        "            'qualit√©', 'rapport', 'satisfait', 'satisfaction', 'recommande', 'recommander',\n",
        "            \n",
        "            # Adverbes d'intensit√© fr√©quents\n",
        "            'vraiment', 'tr√®s', 'super', 'excellent', 'parfait', 'g√©nial', 'top',\n",
        "            'bien', 'bon', 'bonne', 'mieux', 'meilleur', 'meilleure',\n",
        "            \n",
        "            # Mots d'√©valuation\n",
        "            'avis', 'commentaire', 'note', '√©toiles', '√©toile', 'points',\n",
        "            'exp√©rience', 'test', 'essai', 'utilisation'\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    'medical': {\n",
        "        'description': 'Domaine m√©dical, comptes-rendus, diagnostics',\n",
        "        'mots': {\n",
        "            # Personnel m√©dical\n",
        "            'patient', 'patients', 'm√©decin', 'docteur', 'infirmier', 'infirmi√®re',\n",
        "            'praticien', 'sp√©cialiste', 'chirurgien',\n",
        "            \n",
        "            # Lieux m√©dicaux\n",
        "            'h√¥pital', 'clinique', 'cabinet', 'consultation', 'urgence', 'urgences',\n",
        "            'service', 'd√©partement', 'unit√©',\n",
        "            \n",
        "            # Proc√©dures\n",
        "            'examen', 'diagnostic', 'traitement', 'th√©rapie', 'intervention',\n",
        "            'op√©ration', 'chirurgie', 'analyse', 'bilan', 'contr√¥le',\n",
        "            \n",
        "            # M√©dicaments\n",
        "            'm√©dicament', 'ordonnance', 'prescription', 'posologie', 'dose',\n",
        "            'traitement', 'th√©rapeutique'\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    'reseaux_sociaux': {\n",
        "        'description': 'R√©seaux sociaux, langage informel, argot',\n",
        "        'mots': {\n",
        "            # Salutations informelles\n",
        "            'salut', 'coucou', 'hello', 'hey', 'yo', 'bjr', 'bsr', 'slt',\n",
        "            \n",
        "            # Expressions d'humour/√©motion\n",
        "            'mdr', 'lol', 'ptdr', 'xdr', 'haha', 'hihi', 'lmao', 'rofl',\n",
        "            \n",
        "            # Mots de liaison informels\n",
        "            'genre', 'style', 'comme', 'enfin', 'bref', 'donc', 'alors',\n",
        "            'bon', 'ben', 'bah', 'euh', 'heu',\n",
        "            \n",
        "            # Mots vagues\n",
        "            'truc', 'machin', 'chose', 'bidule', 'truc', 'trucs',\n",
        "            \n",
        "            # Intensificateurs informels\n",
        "            'trop', 'hyper', 'm√©ga', 'ultra', 'super', 'grave', 'carr√©ment',\n",
        "            \n",
        "            # Abr√©viations courantes\n",
        "            'jsp', 'jpp', 'jtm', 'jte', 'cv', '√ßa va', 'qqun', 'qqch'\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    'technique': {\n",
        "        'description': 'Documentation technique, informatique',\n",
        "        'mots': {\n",
        "            # Termes techniques g√©n√©riques\n",
        "            'syst√®me', 'fonction', 'm√©thode', 'classe', 'objet', 'variable',\n",
        "            'param√®tre', 'configuration', 'option', 'valeur', 'donn√©es',\n",
        "            \n",
        "            # Actions techniques\n",
        "            'permet', 'utilise', 'configure', 'd√©finit', 'sp√©cifie', 'indique',\n",
        "            'ex√©cute', 'lance', 'd√©marre', 'arr√™te', 'charge', 'sauvegarde',\n",
        "            \n",
        "            # Documentation\n",
        "            'voir', 'consulter', 'r√©f√©rence', 'documentation', 'manuel', 'guide',\n",
        "            'exemple', 'note', 'remarque', 'attention', 'important',\n",
        "            \n",
        "            # Versions et formats\n",
        "            'version', 'format', 'type', 'mode', 'niveau', 'statut', '√©tat'\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    'juridique': {\n",
        "        'description': 'Domaine juridique, contrats, lois',\n",
        "        'mots': {\n",
        "            # Acteurs juridiques\n",
        "            'tribunal', 'juge', 'avocat', 'procureur', 'partie', 'parties',\n",
        "            'demandeur', 'd√©fendeur', 't√©moin', 'expert',\n",
        "            \n",
        "            # Proc√©dures\n",
        "            'proc√©dure', 'instance', 'jugement', 'arr√™t', 'd√©cision', 'ordonnance',\n",
        "            'sentence', 'verdict', 'd√©lib√©r√©',\n",
        "            \n",
        "            # Documents\n",
        "            'contrat', 'accord', 'convention', 'clause', 'article', 'alin√©a',\n",
        "            'paragraphe', 'annexe', 'pi√®ce',\n",
        "            \n",
        "            # Expressions juridiques\n",
        "            'consid√©rant', 'attendu', 'vu', 'statuant', 'ordonne', 'd√©clare',\n",
        "            'condamne', 'd√©bout√©'\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    'actualites': {\n",
        "        'description': 'Presse, actualit√©s, journalisme',\n",
        "        'mots': {\n",
        "            # Connecteurs journalistiques\n",
        "            'selon', 'd\\'apr√®s', 'conform√©ment', 'suite', 'concernant', 'regarding',\n",
        "            '√©galement', 'notamment', 'particuli√®rement', 'sp√©cialement',\n",
        "            \n",
        "            # Nuances et transitions\n",
        "            'toutefois', 'cependant', 'n√©anmoins', 'malgr√©', 'en d√©pit',\n",
        "            'par ailleurs', 'en outre', 'de plus', 'en effet', 'ainsi',\n",
        "            \n",
        "            # Temporalit√©\n",
        "            'hier', 'aujourd\\'hui', 'demain', 'r√©cemment', 'actuellement',\n",
        "            'd√©sormais', 'dor√©navant', 'prochainement',\n",
        "            \n",
        "            # Sources et r√©f√©rences\n",
        "            'source', 'd√©claration', 'communiqu√©', 'rapport', '√©tude', 'enqu√™te',\n",
        "            't√©moignage', 'interview', 'entretien'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìö BIBLIOTH√àQUE DE STOPWORDS PAR DOMAINE\\n\")\n",
        "for domaine, info in STOPWORDS_DOMAINES.items():\n",
        "    print(f\"üè¢ {domaine.upper().replace('_', ' ')} ({len(info['mots'])} mots)\")\n",
        "    print(f\"   üìù {info['description']}\")\n",
        "    print(f\"   üî§ √âchantillon : {', '.join(list(info['mots'])[:8])}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. üõ†Ô∏è Gestionnaire Avanc√© de Stopwords\n",
        "\n",
        "Classe compl√®te pour g√©rer, combiner et optimiser vos stopwords personnalis√©s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GestionnaireStopwords:\n",
        "    \"\"\"Gestionnaire avanc√© pour la cr√©ation et gestion de stopwords personnalis√©s.\"\"\"\n",
        "    \n",
        "    def __init__(self, base: str = 'nltk'):\n",
        "        \"\"\"\n",
        "        Initialise le gestionnaire.\n",
        "        \n",
        "        Args:\n",
        "            base: Base de stopwords ('nltk', 'spacy', 'vide')\n",
        "        \"\"\"\n",
        "        self.stopwords = set()\n",
        "        self.historique = []\n",
        "        self.domaines_actifs = set()\n",
        "        \n",
        "        # Chargement de la base\n",
        "        if base == 'nltk':\n",
        "            self.stopwords = set(stopwords.words('french'))\n",
        "            self.historique.append(f\"Base NLTK charg√©e ({len(self.stopwords)} mots)\")\n",
        "        elif base == 'spacy' and nlp:\n",
        "            self.stopwords = nlp.Defaults.stop_words.copy()\n",
        "            self.historique.append(f\"Base spaCy charg√©e ({len(self.stopwords)} mots)\")\n",
        "        else:\n",
        "            self.historique.append(\"Gestionnaire initialis√© vide\")\n",
        "    \n",
        "    def ajouter_domaine(self, domaine: str, force: bool = False) -> bool:\n",
        "        \"\"\"\n",
        "        Ajoute les stopwords d'un domaine.\n",
        "        \n",
        "        Args:\n",
        "            domaine: Nom du domaine\n",
        "            force: Forcer l'ajout m√™me si d√©j√† pr√©sent\n",
        "        \n",
        "        Returns:\n",
        "            True si ajout√© avec succ√®s\n",
        "        \"\"\"\n",
        "        if domaine not in STOPWORDS_DOMAINES:\n",
        "            print(f\"‚ùå Domaine '{domaine}' non reconnu\")\n",
        "            print(f\"   Domaines disponibles : {list(STOPWORDS_DOMAINES.keys())}\")\n",
        "            return False\n",
        "        \n",
        "        if domaine in self.domaines_actifs and not force:\n",
        "            print(f\"‚ö†Ô∏è Domaine '{domaine}' d√©j√† ajout√© (utilisez force=True pour forcer)\")\n",
        "            return False\n",
        "        \n",
        "        mots_domaine = STOPWORDS_DOMAINES[domaine]['mots']\n",
        "        ancienne_taille = len(self.stopwords)\n",
        "        self.stopwords.update(mots_domaine)\n",
        "        nouveaux_mots = len(self.stopwords) - ancienne_taille\n",
        "        \n",
        "        self.domaines_actifs.add(domaine)\n",
        "        self.historique.append(f\"Domaine '{domaine}' ajout√© (+{nouveaux_mots} nouveaux mots)\")\n",
        "        print(f\"‚úÖ Domaine '{domaine}' ajout√© : {nouveaux_mots} nouveaux mots\")\n",
        "        return True\n",
        "    \n",
        "    def retirer_domaine(self, domaine: str) -> bool:\n",
        "        \"\"\"\n",
        "        Retire les stopwords d'un domaine.\n",
        "        \n",
        "        Args:\n",
        "            domaine: Nom du domaine √† retirer\n",
        "        \n",
        "        Returns:\n",
        "            True si retir√© avec succ√®s\n",
        "        \"\"\"\n",
        "        if domaine not in self.domaines_actifs:\n",
        "            print(f\"‚ö†Ô∏è Domaine '{domaine}' pas actuellement actif\")\n",
        "            return False\n",
        "        \n",
        "        mots_domaine = STOPWORDS_DOMAINES[domaine]['mots']\n",
        "        ancienne_taille = len(self.stopwords)\n",
        "        self.stopwords -= mots_domaine\n",
        "        mots_retires = ancienne_taille - len(self.stopwords)\n",
        "        \n",
        "        self.domaines_actifs.remove(domaine)\n",
        "        self.historique.append(f\"Domaine '{domaine}' retir√© (-{mots_retires} mots)\")\n",
        "        print(f\"‚úÖ Domaine '{domaine}' retir√© : {mots_retires} mots supprim√©s\")\n",
        "        return True\n",
        "    \n",
        "    def ajouter_mots(self, mots: List[str], categorie: str = 'personnalise') -> None:\n",
        "        \"\"\"\n",
        "        Ajoute une liste de mots personnalis√©s.\n",
        "        \n",
        "        Args:\n",
        "            mots: Liste des mots √† ajouter\n",
        "            categorie: Cat√©gorie pour l'historique\n",
        "        \"\"\"\n",
        "        mots_normalises = [mot.lower().strip() for mot in mots if mot.strip()]\n",
        "        nouveaux_mots = [mot for mot in mots_normalises if mot not in self.stopwords]\n",
        "        \n",
        "        self.stopwords.update(mots_normalises)\n",
        "        self.historique.append(f\"Ajout {categorie} : {len(nouveaux_mots)} nouveaux mots\")\n",
        "        print(f\"‚úÖ {len(nouveaux_mots)} nouveaux mots ajout√©s ({categorie})\")\n",
        "        \n",
        "        if len(nouveaux_mots) < len(mots_normalises):\n",
        "            doublons = len(mots_normalises) - len(nouveaux_mots)\n",
        "            print(f\"   ‚ÑπÔ∏è {doublons} mots √©taient d√©j√† pr√©sents\")\n",
        "    \n",
        "    def retirer_mots(self, mots: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Retire une liste de mots.\n",
        "        \n",
        "        Args:\n",
        "            mots: Liste des mots √† retirer\n",
        "        \"\"\"\n",
        "        mots_normalises = {mot.lower().strip() for mot in mots if mot.strip()}\n",
        "        mots_present = mots_normalises.intersection(self.stopwords)\n",
        "        \n",
        "        self.stopwords -= mots_normalises\n",
        "        self.historique.append(f\"Suppression : {len(mots_present)} mots retir√©s\")\n",
        "        print(f\"‚úÖ {len(mots_present)} mots supprim√©s\")\n",
        "        \n",
        "        if len(mots_present) < len(mots_normalises):\n",
        "            absents = len(mots_normalises) - len(mots_present)\n",
        "            print(f\"   ‚ÑπÔ∏è {absents} mots n'√©taient pas pr√©sents\")\n",
        "    \n",
        "    def filtrer_texte(self, texte: str, retourner_stopwords: bool = False) -> List[str] or Tuple[List[str], List[str]]:\n",
        "        \"\"\"\n",
        "        Filtre un texte en supprimant les stopwords.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† filtrer\n",
        "            retourner_stopwords: Si True, retourne aussi les stopwords trouv√©s\n",
        "        \n",
        "        Returns:\n",
        "            Liste des mots filtr√©s, optionnellement avec les stopwords trouv√©s\n",
        "        \"\"\"\n",
        "        mots = re.findall(r'\\w+', texte.lower())\n",
        "        mots_filtres = [mot for mot in mots if mot not in self.stopwords]\n",
        "        \n",
        "        if retourner_stopwords:\n",
        "            stopwords_trouves = [mot for mot in mots if mot in self.stopwords]\n",
        "            return mots_filtres, stopwords_trouves\n",
        "        \n",
        "        return mots_filtres\n",
        "    \n",
        "    def analyser_texte(self, texte: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyse compl√®te d'un texte avec les stopwords actuels.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† analyser\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les statistiques\n",
        "        \"\"\"\n",
        "        mots_originaux = re.findall(r'\\w+', texte.lower())\n",
        "        mots_filtres, stopwords_trouves = self.filtrer_texte(texte, retourner_stopwords=True)\n",
        "        \n",
        "        return {\n",
        "            'mots_originaux': len(mots_originaux),\n",
        "            'mots_filtres': len(mots_filtres),\n",
        "            'stopwords_supprimes': len(stopwords_trouves),\n",
        "            'reduction_pourcentage': round((1 - len(mots_filtres)/len(mots_originaux))*100, 1) if mots_originaux else 0,\n",
        "            'stopwords_uniques': len(set(stopwords_trouves)),\n",
        "            'vocabulaire_unique_filtre': len(set(mots_filtres)),\n",
        "            'echantillon_filtres': mots_filtres[:10],\n",
        "            'echantillon_stopwords': list(set(stopwords_trouves))[:10]\n",
        "        }\n",
        "    \n",
        "    def statistiques(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Retourne les statistiques du gestionnaire.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les statistiques\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'total_stopwords': len(self.stopwords),\n",
        "            'domaines_actifs': list(self.domaines_actifs),\n",
        "            'nb_domaines': len(self.domaines_actifs),\n",
        "            'historique_operations': len(self.historique),\n",
        "            'longueur_moyenne': round(sum(len(mot) for mot in self.stopwords) / len(self.stopwords), 2) if self.stopwords else 0,\n",
        "            'mot_plus_long': max(self.stopwords, key=len) if self.stopwords else None,\n",
        "            'mot_plus_court': min(self.stopwords, key=len) if self.stopwords else None\n",
        "        }\n",
        "    \n",
        "    def afficher_historique(self) -> None:\n",
        "        \"\"\"\n",
        "        Affiche l'historique des op√©rations.\n",
        "        \"\"\"\n",
        "        print(\"üìú HISTORIQUE DES OP√âRATIONS\\n\")\n",
        "        for i, operation in enumerate(self.historique, 1):\n",
        "            print(f\"  {i:2d}. {operation}\")\n",
        "    \n",
        "    def sauvegarder(self, fichier: str) -> None:\n",
        "        \"\"\"\n",
        "        Sauvegarde la configuration dans un fichier JSON.\n",
        "        \n",
        "        Args:\n",
        "            fichier: Chemin du fichier de sauvegarde\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'stopwords': sorted(list(self.stopwords)),\n",
        "            'domaines_actifs': list(self.domaines_actifs),\n",
        "            'historique': self.historique,\n",
        "            'statistiques': self.statistiques()\n",
        "        }\n",
        "        \n",
        "        with open(fichier, 'w', encoding='utf-8') as f:\n",
        "            json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        print(f\"‚úÖ Configuration sauvegard√©e dans '{fichier}'\")\n",
        "    \n",
        "    def charger(self, fichier: str) -> None:\n",
        "        \"\"\"\n",
        "        Charge une configuration depuis un fichier JSON.\n",
        "        \n",
        "        Args:\n",
        "            fichier: Chemin du fichier √† charger\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(fichier, 'r', encoding='utf-8') as f:\n",
        "                config = json.load(f)\n",
        "            \n",
        "            self.stopwords = set(config['stopwords'])\n",
        "            self.domaines_actifs = set(config['domaines_actifs'])\n",
        "            self.historique = config['historique']\n",
        "            self.historique.append(f\"Configuration charg√©e depuis '{fichier}'\")\n",
        "            \n",
        "            print(f\"‚úÖ Configuration charg√©e depuis '{fichier}'\")\n",
        "            print(f\"   {len(self.stopwords)} stopwords, {len(self.domaines_actifs)} domaines actifs\")\n",
        "            \n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Fichier '{fichier}' non trouv√©\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"‚ùå Erreur de format JSON dans '{fichier}'\")\n",
        "\n",
        "print(\"‚úÖ Classe GestionnaireStopwords cr√©√©e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# D√©monstration du gestionnaire avanc√©\n",
        "print(\"üõ†Ô∏è D√âMONSTRATION DU GESTIONNAIRE AVANC√â\\n\")\n",
        "\n",
        "# Cr√©ation d'un gestionnaire pour e-commerce\n",
        "gestionnaire = GestionnaireStopwords(base='nltk')\n",
        "print(f\"Base NLTK : {gestionnaire.statistiques()['total_stopwords']} stopwords\")\n",
        "\n",
        "# Ajout du domaine e-commerce\n",
        "gestionnaire.ajouter_domaine('e_commerce')\n",
        "\n",
        "# Ajout de mots personnalis√©s\n",
        "mots_perso = ['franchement', 'sinc√®rement', 'honn√™tement', 'globalement', 'finalement']\n",
        "gestionnaire.ajouter_mots(mots_perso, 'adverbes_opinion')\n",
        "\n",
        "# Statistiques finales\n",
        "stats = gestionnaire.statistiques()\n",
        "print(f\"\\nüìä Configuration finale :\")\n",
        "print(f\"  Total stopwords : {stats['total_stopwords']}\")\n",
        "print(f\"  Domaines actifs : {', '.join(stats['domaines_actifs'])}\")\n",
        "print(f\"  Longueur moyenne : {stats['longueur_moyenne']} caract√®res\")\n",
        "print(f\"  Mot le plus long : '{stats['mot_plus_long']}'\")\n",
        "print(f\"  Mot le plus court : '{stats['mot_plus_court']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ‚úÖ Validation et Tests\n",
        "\n",
        "Comment valider que vos stopwords personnalis√©s sont efficaces ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test comparatif sur diff√©rents textes\n",
        "textes_test = {\n",
        "    \"Avis positif e-commerce\": \"\"\"\n",
        "    Ce produit est vraiment excellent ! Service client tr√®s professionnel, \n",
        "    livraison ultra rapide. Je recommande vivement cet article √† tous. \n",
        "    Parfait rapport qualit√©-prix, franchement je suis tr√®s satisfait.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"Avis n√©gatif e-commerce\": \"\"\"\n",
        "    Produit d√©cevant, qualit√© m√©diocre. Service client pas du tout √† l'√©coute.\n",
        "    Livraison tardive et emballage ab√Æm√©. Honn√™tement, je ne recommande pas \n",
        "    du tout ce site. Tr√®s mauvaise exp√©rience d'achat.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"Description technique\": \"\"\"\n",
        "    Cette fonction permet de configurer les param√®tres du syst√®me. \n",
        "    Elle utilise une m√©thode avanc√©e pour optimiser les performances.\n",
        "    Voir la documentation pour plus d'informations sur l'utilisation.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Comparaison avec diff√©rentes configurations\n",
        "configurations = {\n",
        "    'Standard NLTK': GestionnaireStopwords(base='nltk'),\n",
        "    'E-commerce': GestionnaireStopwords(base='nltk'),\n",
        "    'E-commerce + Perso': GestionnaireStopwords(base='nltk')\n",
        "}\n",
        "\n",
        "# Configuration des gestionnaires\n",
        "configurations['E-commerce'].ajouter_domaine('e_commerce')\n",
        "configurations['E-commerce + Perso'].ajouter_domaine('e_commerce')\n",
        "configurations['E-commerce + Perso'].ajouter_mots(\n",
        "    ['franchement', 'sinc√®rement', 'honn√™tement', 'globalement', 'vraiment', 'tr√®s']\n",
        ")\n",
        "\n",
        "print(\"üß™ TESTS COMPARATIFS DE VALIDATION\\n\")\n",
        "\n",
        "resultats_test = []\n",
        "\n",
        "for titre_texte, texte in textes_test.items():\n",
        "    print(f\"üìÑ {titre_texte.upper()}\")\n",
        "    print(f\"Texte : {texte.strip()[:80]}...\\n\")\n",
        "    \n",
        "    for nom_config, gestionnaire in configurations.items():\n",
        "        analyse = gestionnaire.analyser_texte(texte)\n",
        "        \n",
        "        resultats_test.append({\n",
        "            'Texte': titre_texte,\n",
        "            'Configuration': nom_config,\n",
        "            'Mots_originaux': analyse['mots_originaux'],\n",
        "            'Mots_filtres': analyse['mots_filtres'],\n",
        "            'Reduction_%': analyse['reduction_pourcentage'],\n",
        "            'Stopwords_uniques': analyse['stopwords_uniques']\n",
        "        })\n",
        "        \n",
        "        print(f\"  {nom_config:20s} : {analyse['mots_filtres']:2d} mots ({analyse['reduction_pourcentage']:5.1f}% r√©duction)\")\n",
        "        print(f\"  {'':22s} ‚Üí {' '.join(analyse['echantillon_filtres'][:6])}...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Analyse des r√©sultats\n",
        "df_resultats = pd.DataFrame(resultats_test)\n",
        "print(\"üìä TABLEAU R√âCAPITULATIF :\")\n",
        "print(df_resultats.pivot_table(\n",
        "    index='Texte', \n",
        "    columns='Configuration', \n",
        "    values='Reduction_%', \n",
        "    aggfunc='first'\n",
        ").round(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des r√©sultats de validation\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Graphique 1: R√©duction par configuration\n",
        "reduction_par_config = df_resultats.groupby('Configuration')['Reduction_%'].mean()\n",
        "bars1 = axes[0,0].bar(reduction_par_config.index, reduction_par_config.values, \n",
        "                      color=['skyblue', 'lightgreen', 'orange'], alpha=0.7)\n",
        "axes[0,0].set_title('üìâ R√©duction Moyenne par Configuration', fontweight='bold')\n",
        "axes[0,0].set_ylabel('R√©duction (%)')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Ajout des valeurs sur les barres\n",
        "for bar, value in zip(bars1, reduction_par_config.values):\n",
        "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Graphique 2: Mots conserv√©s par type de texte\n",
        "textes_order = ['Avis positif e-commerce', 'Avis n√©gatif e-commerce', 'Description technique']\n",
        "configs = df_resultats['Configuration'].unique()\n",
        "x = np.arange(len(textes_order))\n",
        "width = 0.25\n",
        "\n",
        "for i, config in enumerate(configs):\n",
        "    data = df_resultats[df_resultats['Configuration'] == config]\n",
        "    values = [data[data['Texte'] == texte]['Mots_filtres'].iloc[0] for texte in textes_order]\n",
        "    axes[0,1].bar(x + i*width, values, width, label=config, alpha=0.7)\n",
        "\n",
        "axes[0,1].set_title('üìä Mots Conserv√©s par Type de Texte', fontweight='bold')\n",
        "axes[0,1].set_xlabel('Type de texte')\n",
        "axes[0,1].set_ylabel('Nombre de mots conserv√©s')\n",
        "axes[0,1].set_xticks(x + width)\n",
        "axes[0,1].set_xticklabels([t.replace(' e-commerce', '') for t in textes_order], rotation=45)\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Graphique 3: Distribution des r√©ductions\n",
        "for config in configs:\n",
        "    data = df_resultats[df_resultats['Configuration'] == config]['Reduction_%']\n",
        "    axes[1,0].hist(data, alpha=0.6, label=config, bins=5)\n",
        "\n",
        "axes[1,0].set_title('üìà Distribution des Taux de R√©duction', fontweight='bold')\n",
        "axes[1,0].set_xlabel('R√©duction (%)')\n",
        "axes[1,0].set_ylabel('Fr√©quence')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Graphique 4: Efficacit√© par domaine\n",
        "efficacite_data = {\n",
        "    'E-commerce\\n(avis)': df_resultats[\n",
        "        (df_resultats['Configuration'] == 'E-commerce + Perso') & \n",
        "        (df_resultats['Texte'].str.contains('Avis'))\n",
        "    ]['Reduction_%'].mean(),\n",
        "    'Technique\\n(description)': df_resultats[\n",
        "        (df_resultats['Configuration'] == 'E-commerce + Perso') & \n",
        "        (df_resultats['Texte'].str.contains('technique'))\n",
        "    ]['Reduction_%'].mean(),\n",
        "    'Standard\\n(moyenne)': df_resultats[\n",
        "        df_resultats['Configuration'] == 'Standard NLTK'\n",
        "    ]['Reduction_%'].mean()\n",
        "}\n",
        "\n",
        "bars4 = axes[1,1].bar(efficacite_data.keys(), efficacite_data.values(), \n",
        "                      color=['lightcoral', 'lightblue', 'lightgray'], alpha=0.7)\n",
        "axes[1,1].set_title('üéØ Efficacit√© par Domaine d\\'Application', fontweight='bold')\n",
        "axes[1,1].set_ylabel('R√©duction (%)')\n",
        "\n",
        "# Ajout des valeurs\n",
        "for bar, value in zip(bars4, efficacite_data.values()):\n",
        "    if not np.isnan(value):\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                       f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° CONCLUSIONS DE LA VALIDATION :\")\n",
        "print(f\"  ‚Ä¢ La personnalisation am√©liore l'efficacit√© de {reduction_par_config['E-commerce + Perso'] - reduction_par_config['Standard NLTK']:.1f} points\")\n",
        "print(f\"  ‚Ä¢ R√©duction optimale pour le domaine cibl√© : {efficacite_data['E-commerce\\n(avis)']:.1f}%\")\n",
        "print(f\"  ‚Ä¢ Les stopwords personnalis√©s sont plus s√©lectifs et pr√©cis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üìä Optimisation par Fr√©quence\n",
        "\n",
        "Techniques pour optimiser automatiquement vos listes de stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimisateurStopwords:\n",
        "    \"\"\"Optimise automatiquement les listes de stopwords selon des crit√®res.\"\"\"\n",
        "    \n",
        "    def __init__(self, corpus: List[str]):\n",
        "        \"\"\"\n",
        "        Initialise avec un corpus de r√©f√©rence.\n",
        "        \n",
        "        Args:\n",
        "            corpus: Liste de textes pour l'analyse\n",
        "        \"\"\"\n",
        "        self.corpus = corpus\n",
        "        self.stats = self._analyser_corpus()\n",
        "    \n",
        "    def _analyser_corpus(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyse statistique compl√®te du corpus.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les statistiques\n",
        "        \"\"\"\n",
        "        tous_mots = []\n",
        "        frequences_doc = defaultdict(int)\n",
        "        \n",
        "        for texte in self.corpus:\n",
        "            mots = re.findall(r'\\w+', texte.lower())\n",
        "            tous_mots.extend(mots)\n",
        "            \n",
        "            # Fr√©quence par document\n",
        "            for mot in set(mots):\n",
        "                frequences_doc[mot] += 1\n",
        "        \n",
        "        frequences_globales = Counter(tous_mots)\n",
        "        \n",
        "        return {\n",
        "            'frequences_globales': frequences_globales,\n",
        "            'frequences_document': frequences_doc,\n",
        "            'nb_documents': len(self.corpus),\n",
        "            'nb_mots_total': len(tous_mots),\n",
        "            'vocabulaire_unique': len(frequences_globales)\n",
        "        }\n",
        "    \n",
        "    def candidats_frequence_elevee(self, percentile: float = 90) -> List[Tuple[str, int, float]]:\n",
        "        \"\"\"\n",
        "        Identifie les mots avec une fr√©quence tr√®s √©lev√©e.\n",
        "        \n",
        "        Args:\n",
        "            percentile: Percentile de fr√©quence (ex: 90 = top 10%)\n",
        "        \n",
        "        Returns:\n",
        "            Liste des candidats avec fr√©quence et percentile\n",
        "        \"\"\"\n",
        "        frequences = list(self.stats['frequences_globales'].values())\n",
        "        seuil = np.percentile(frequences, percentile)\n",
        "        \n",
        "        candidats = []\n",
        "        for mot, freq in self.stats['frequences_globales'].items():\n",
        "            if freq >= seuil:\n",
        "                pct = (sum(1 for f in frequences if f < freq) / len(frequences)) * 100\n",
        "                candidats.append((mot, freq, pct))\n",
        "        \n",
        "        candidats.sort(key=lambda x: x[1], reverse=True)\n",
        "        return candidats\n",
        "    \n",
        "    def candidats_ubiquite(self, seuil_presence: float = 0.8) -> List[Tuple[str, float, int]]:\n",
        "        \"\"\"\n",
        "        Identifie les mots pr√©sents dans beaucoup de documents.\n",
        "        \n",
        "        Args:\n",
        "            seuil_presence: Pourcentage minimum de documents\n",
        "        \n",
        "        Returns:\n",
        "            Liste des candidats avec taux de pr√©sence\n",
        "        \"\"\"\n",
        "        candidats = []\n",
        "        nb_docs = self.stats['nb_documents']\n",
        "        \n",
        "        for mot, doc_freq in self.stats['frequences_document'].items():\n",
        "            taux_presence = doc_freq / nb_docs\n",
        "            if taux_presence >= seuil_presence:\n",
        "                freq_globale = self.stats['frequences_globales'][mot]\n",
        "                candidats.append((mot, taux_presence, freq_globale))\n",
        "        \n",
        "        candidats.sort(key=lambda x: x[1], reverse=True)\n",
        "        return candidats\n",
        "    \n",
        "    def score_stopword(self, mot: str) -> float:\n",
        "        \"\"\"\n",
        "        Calcule un score de \"stopword-ness\" pour un mot.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot √† √©valuer\n",
        "        \n",
        "        Returns:\n",
        "            Score entre 0 et 1 (1 = tr√®s probable stopword)\n",
        "        \"\"\"\n",
        "        if mot not in self.stats['frequences_globales']:\n",
        "            return 0.0\n",
        "        \n",
        "        freq_globale = self.stats['frequences_globales'][mot]\n",
        "        freq_doc = self.stats['frequences_document'][mot]\n",
        "        \n",
        "        # Normalisation\n",
        "        freq_norm = freq_globale / self.stats['nb_mots_total']\n",
        "        ubiquite_norm = freq_doc / self.stats['nb_documents']\n",
        "        longueur_norm = max(0, (5 - len(mot)) / 5)  # Mots courts = score plus √©lev√©\n",
        "        \n",
        "        # Score compos√© (pond√©ration personnalisable)\n",
        "        score = (freq_norm * 0.4) + (ubiquite_norm * 0.4) + (longueur_norm * 0.2)\n",
        "        \n",
        "        return min(1.0, score)\n",
        "    \n",
        "    def optimiser_liste(self, stopwords_actuels: Set[str], \n",
        "                       seuil_ajout: float = 0.7, \n",
        "                       seuil_suppression: float = 0.3) -> Tuple[Set[str], Set[str]]:\n",
        "        \"\"\"\n",
        "        Optimise une liste de stopwords existante.\n",
        "        \n",
        "        Args:\n",
        "            stopwords_actuels: Liste actuelle des stopwords\n",
        "            seuil_ajout: Score minimum pour ajouter un mot\n",
        "            seuil_suppression: Score maximum pour garder un mot\n",
        "        \n",
        "        Returns:\n",
        "            Tuple (mots_a_ajouter, mots_a_supprimer)\n",
        "        \"\"\"\n",
        "        mots_a_ajouter = set()\n",
        "        mots_a_supprimer = set()\n",
        "        \n",
        "        # Candidats √† ajouter\n",
        "        for mot in self.stats['frequences_globales']:\n",
        "            if mot not in stopwords_actuels:\n",
        "                score = self.score_stopword(mot)\n",
        "                if score >= seuil_ajout:\n",
        "                    mots_a_ajouter.add(mot)\n",
        "        \n",
        "        # Candidats √† supprimer\n",
        "        for mot in stopwords_actuels:\n",
        "            if mot in self.stats['frequences_globales']:\n",
        "                score = self.score_stopword(mot)\n",
        "                if score <= seuil_suppression:\n",
        "                    mots_a_supprimer.add(mot)\n",
        "        \n",
        "        return mots_a_ajouter, mots_a_supprimer\n",
        "    \n",
        "    def rapport_optimisation(self, stopwords_actuels: Set[str]) -> None:\n",
        "        \"\"\"\n",
        "        G√©n√®re un rapport d'optimisation d√©taill√©.\n",
        "        \n",
        "        Args:\n",
        "            stopwords_actuels: Liste actuelle des stopwords\n",
        "        \"\"\"\n",
        "        mots_a_ajouter, mots_a_supprimer = self.optimiser_liste(stopwords_actuels)\n",
        "        \n",
        "        print(\"üîß RAPPORT D'OPTIMISATION\\n\")\n",
        "        print(f\"üìä Statistiques du corpus :\")\n",
        "        print(f\"  ‚Ä¢ Documents : {self.stats['nb_documents']}\")\n",
        "        print(f\"  ‚Ä¢ Mots total : {self.stats['nb_mots_total']:,}\")\n",
        "        print(f\"  ‚Ä¢ Vocabulaire unique : {self.stats['vocabulaire_unique']:,}\")\n",
        "        \n",
        "        print(f\"\\nüéØ Recommandations :\")\n",
        "        print(f\"  ‚Ä¢ Stopwords actuels : {len(stopwords_actuels)}\")\n",
        "        print(f\"  ‚Ä¢ √Ä ajouter : {len(mots_a_ajouter)}\")\n",
        "        print(f\"  ‚Ä¢ √Ä supprimer : {len(mots_a_supprimer)}\")\n",
        "        print(f\"  ‚Ä¢ Total optimis√© : {len(stopwords_actuels) + len(mots_a_ajouter) - len(mots_a_supprimer)}\")\n",
        "        \n",
        "        if mots_a_ajouter:\n",
        "            print(f\"\\n‚ûï Mots √† ajouter (score √©lev√©) :\")\n",
        "            for mot in sorted(mots_a_ajouter, key=lambda x: self.score_stopword(x), reverse=True)[:10]:\n",
        "                score = self.score_stopword(mot)\n",
        "                freq = self.stats['frequences_globales'][mot]\n",
        "                print(f\"  '{mot}' (score: {score:.3f}, fr√©q: {freq})\")\n",
        "        \n",
        "        if mots_a_supprimer:\n",
        "            print(f\"\\n‚ûñ Mots √† supprimer (score faible) :\")\n",
        "            for mot in sorted(mots_a_supprimer, key=lambda x: self.score_stopword(x))[:10]:\n",
        "                score = self.score_stopword(mot)\n",
        "                freq = self.stats['frequences_globales'].get(mot, 0)\n",
        "                print(f\"  '{mot}' (score: {score:.3f}, fr√©q: {freq})\")\n",
        "\n",
        "print(\"‚úÖ Classe OptimisateurStopwords cr√©√©e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test de l'optimisateur sur notre corpus e-commerce\n",
        "corpus_ecommerce_etendu = [\n",
        "    \"Ce produit est vraiment excellent, je le recommande √† tous mes amis.\",\n",
        "    \"Service client tr√®s professionnel, livraison rapide, emballage soign√©.\",\n",
        "    \"Produit conforme √† la description, qualit√© au rendez-vous, tr√®s satisfait.\",\n",
        "    \"Prix un peu √©lev√© mais la qualit√© justifie largement cet investissement.\",\n",
        "    \"Commande re√ßue rapidement, produit de bonne qualit√©, service r√©actif.\",\n",
        "    \"Tr√®s bon rapport qualit√© prix, produit fiable, livraison dans les temps.\",\n",
        "    \"Service apr√®s vente excellent, probl√®me r√©solu rapidement, √©quipe professionnelle.\",\n",
        "    \"Produit exactement comme d√©crit, emballage parfait, livraison ultra rapide.\",\n",
        "    \"Qualit√© exceptionnelle, service client √† l'√©coute, je recommande vivement.\",\n",
        "    \"Achat sans probl√®me, produit conforme, d√©lai de livraison respect√©.\",\n",
        "    \"D√©√ßu par ce produit, qualit√© d√©cevante, service client pas √† l'√©coute.\",\n",
        "    \"Livraison tardive, emballage ab√Æm√©, produit endommag√©, tr√®s d√©√ßu.\",\n",
        "    \"Service client incomp√©tent, probl√®me non r√©solu, je d√©conseille.\",\n",
        "    \"Produit ne correspond pas √† la description, qualit√© m√©diocre.\",\n",
        "    \"Commande annul√©e sans pr√©avis, service client injoignable, scandaleux.\"\n",
        "]\n",
        "\n",
        "# Cr√©ation de l'optimisateur\n",
        "optimisateur = OptimisateurStopwords(corpus_ecommerce_etendu)\n",
        "\n",
        "# Test avec les stopwords NLTK de base\n",
        "stopwords_base = set(stopwords.words('french'))\n",
        "optimisateur.rapport_optimisation(stopwords_base)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Application des optimisations\n",
        "mots_a_ajouter, mots_a_supprimer = optimisateur.optimiser_liste(stopwords_base)\n",
        "stopwords_optimises = (stopwords_base | mots_a_ajouter) - mots_a_supprimer\n",
        "\n",
        "print(f\"\\n‚ú® R√âSULTATS DE L'OPTIMISATION :\")\n",
        "print(f\"  ‚Ä¢ Stopwords optimis√©s : {len(stopwords_optimises)} (+{len(mots_a_ajouter)-len(mots_a_supprimer)})\")\n",
        "print(f\"  ‚Ä¢ Efficacit√© th√©orique am√©lior√©e pour ce corpus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. üé™ Cas d'Usage Pratiques\n",
        "\n",
        "Exemples concrets d'application dans diff√©rents contextes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cas d'usage 1: Analyse de sentiment e-commerce\n",
        "print(\"üé™ CAS D'USAGE 1 : ANALYSE DE SENTIMENT E-COMMERCE\\n\")\n",
        "\n",
        "# Donn√©es d'exemple\n",
        "avis_clients = [\n",
        "    (\"Produit fantastique, livraison rapide, service client excellent!\", \"positif\"),\n",
        "    (\"Tr√®s d√©√ßu, qualit√© m√©diocre, je ne recommande absolument pas.\", \"n√©gatif\"),\n",
        "    (\"Correct sans plus, √ßa fait le travail mais rien d'exceptionnel.\", \"neutre\"),\n",
        "    (\"Parfait! Exactement ce que je cherchais, tr√®s satisfait de mon achat.\", \"positif\"),\n",
        "    (\"Service apr√®s-vente inexistant, produit d√©faillant, tr√®s mauvaise exp√©rience.\", \"n√©gatif\")\n",
        "]\n",
        "\n",
        "# Configuration sp√©cialis√©e pour l'analyse de sentiment\n",
        "gestionnaire_sentiment = GestionnaireStopwords(base='nltk')\n",
        "gestionnaire_sentiment.ajouter_domaine('e_commerce')\n",
        "\n",
        "# Ajout de mots sp√©cifiques au sentiment (mais non informatifs)\n",
        "mots_sentiment_generiques = [\n",
        "    'produit', 'service', 'client', 'livraison', 'achat', 'commande',\n",
        "    'tr√®s', 'vraiment', 'super', 'assez', 'plut√¥t', 'franchement'\n",
        "]\n",
        "gestionnaire_sentiment.ajouter_mots(mots_sentiment_generiques, 'sentiment_generiques')\n",
        "\n",
        "print(\"Configuration pour analyse de sentiment :\")\n",
        "print(f\"  ‚Ä¢ {gestionnaire_sentiment.statistiques()['total_stopwords']} stopwords\")\n",
        "print(f\"  ‚Ä¢ Domaines : {', '.join(gestionnaire_sentiment.statistiques()['domaines_actifs'])}\")\n",
        "\n",
        "print(\"\\nüìä R√©sultats du filtrage :\")\n",
        "for avis, sentiment in avis_clients:\n",
        "    mots_filtres = gestionnaire_sentiment.filtrer_texte(avis)\n",
        "    print(f\"  {sentiment:8s} : {' '.join(mots_filtres[:6])}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cas d'usage 2: Classification automatique de tickets support\n",
        "print(\"üé™ CAS D'USAGE 2 : CLASSIFICATION DE TICKETS SUPPORT\\n\")\n",
        "\n",
        "tickets_support = [\n",
        "    (\"Mon compte est bloqu√©, je n'arrive plus √† me connecter depuis hier.\", \"compte\"),\n",
        "    (\"Le paiement n'a pas fonctionn√©, erreur lors de la validation de ma carte.\", \"paiement\"),\n",
        "    (\"Ma commande n'est toujours pas arriv√©e, pouvez-vous v√©rifier le statut?\", \"livraison\"),\n",
        "    (\"Le produit re√ßu est d√©fectueux, je souhaite un remboursement.\", \"produit\"),\n",
        "    (\"Comment modifier mes informations personnelles dans mon profil?\", \"compte\")\n",
        "]\n",
        "\n",
        "# Configuration pour classification de tickets\n",
        "gestionnaire_tickets = GestionnaireStopwords(base='nltk')\n",
        "\n",
        "# Stopwords sp√©cifiques au support client\n",
        "mots_support = [\n",
        "    'pouvez', 'vous', 'comment', 'pourquoi', 'depuis', 'toujours',\n",
        "    'probl√®me', 'souhaite', 'v√©rifier', 'modifier', 'information'\n",
        "]\n",
        "gestionnaire_tickets.ajouter_mots(mots_support, 'support_client')\n",
        "\n",
        "print(\"Configuration pour tickets support :\")\n",
        "print(f\"  ‚Ä¢ {gestionnaire_tickets.statistiques()['total_stopwords']} stopwords\")\n",
        "\n",
        "print(\"\\nüéØ Mots-cl√©s extraits pour classification :\")\n",
        "for ticket, categorie in tickets_support:\n",
        "    mots_cles = gestionnaire_tickets.filtrer_texte(ticket)\n",
        "    print(f\"  {categorie:10s} : {' '.join(mots_cles[:5])}...\")\n",
        "    print(f\"  {'':12s} ‚Üí Ticket: {ticket[:50]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cas d'usage 3: Analyse de r√©seaux sociaux\n",
        "print(\"üé™ CAS D'USAGE 3 : ANALYSE DE R√âSEAUX SOCIAUX\\n\")\n",
        "\n",
        "posts_sociaux = [\n",
        "    \"Salut les amis! Alors, qu'est-ce que vous pensez du nouveau film? Moi je l'ai trouv√© g√©nial!\",\n",
        "    \"MDR cette vid√©o! Genre trop dr√¥le, je suis mort de rire üòÇ\",\n",
        "    \"Bon alors, qui vient √† la soir√©e ce soir? On va bien s'amuser!\",\n",
        "    \"Franchement, ce truc est ouf! Vous devez absolument essayer.\",\n",
        "    \"Bof... pas terrible cette s√©rie. J'ai regard√© 2 √©pisodes, √ßa m'a saoul√©.\"\n",
        "]\n",
        "\n",
        "# Configuration pour r√©seaux sociaux\n",
        "gestionnaire_social = GestionnaireStopwords(base='nltk')\n",
        "gestionnaire_social.ajouter_domaine('reseaux_sociaux')\n",
        "\n",
        "# Ajout d'expressions typiques des r√©seaux sociaux\n",
        "expressions_sociales = [\n",
        "    'vous', 'pensez', 'trouv√©', 'regard√©', 'vient', 'devez', 'alors',\n",
        "    'bien', 'pas', 'cette', 'film', 'vid√©o', 'soir√©e', 's√©rie'\n",
        "]\n",
        "gestionnaire_social.ajouter_mots(expressions_sociales, 'expressions_sociales')\n",
        "\n",
        "print(\"Configuration pour r√©seaux sociaux :\")\n",
        "print(f\"  ‚Ä¢ {gestionnaire_social.statistiques()['total_stopwords']} stopwords\")\n",
        "print(f\"  ‚Ä¢ Sp√©cialis√© pour le langage informel\")\n",
        "\n",
        "print(\"\\nüí¨ Contenu filtr√© (focus sur l'opinion/√©motion) :\")\n",
        "for i, post in enumerate(posts_sociaux, 1):\n",
        "    mots_pertinents = gestionnaire_social.filtrer_texte(post)\n",
        "    print(f\"  Post {i}: {' '.join(mots_pertinents[:8])}\")\n",
        "    print(f\"  Original: {post[:60]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. üíæ Export et R√©utilisation\n",
        "\n",
        "Comment sauvegarder, partager et r√©utiliser vos configurations de stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©ation de configurations pr√™tes √† l'emploi\n",
        "configurations_preetes = {\n",
        "    'e_commerce_avis': {\n",
        "        'description': 'Optimis√© pour l\\'analyse d\\'avis clients e-commerce',\n",
        "        'base': 'nltk',\n",
        "        'domaines': ['e_commerce'],\n",
        "        'mots_supplementaires': ['vraiment', 'tr√®s', 'super', 'excellent', 'parfait', 'g√©nial']\n",
        "    },\n",
        "    \n",
        "    'support_client': {\n",
        "        'description': 'Sp√©cialis√© pour la classification de tickets support',\n",
        "        'base': 'nltk',\n",
        "        'domaines': ['technique'],\n",
        "        'mots_supplementaires': ['probl√®me', 'aide', 'question', 'comment', 'pourquoi']\n",
        "    },\n",
        "    \n",
        "    'reseaux_sociaux_fr': {\n",
        "        'description': 'Adapt√© aux r√©seaux sociaux fran√ßais (argot, abr√©viations)',\n",
        "        'base': 'nltk',\n",
        "        'domaines': ['reseaux_sociaux'],\n",
        "        'mots_supplementaires': ['alors', 'bon', 'bref', 'enfin']\n",
        "    },\n",
        "    \n",
        "    'medical_minimal': {\n",
        "        'description': 'Version l√©g√®re pour textes m√©dicaux (garde termes techniques)',\n",
        "        'base': 'nltk',\n",
        "        'domaines': [],\n",
        "        'mots_supplementaires': ['tr√®s', 'assez', 'plut√¥t', 'bien', 'mal']\n",
        "    }\n",
        "}\n",
        "\n",
        "def creer_configuration(nom_config: str) -> GestionnaireStopwords:\n",
        "    \"\"\"\n",
        "    Cr√©e un gestionnaire selon une configuration pr√©d√©finie.\n",
        "    \n",
        "    Args:\n",
        "        nom_config: Nom de la configuration\n",
        "    \n",
        "    Returns:\n",
        "        Gestionnaire configur√©\n",
        "    \"\"\"\n",
        "    if nom_config not in configurations_preetes:\n",
        "        raise ValueError(f\"Configuration '{nom_config}' inconnue\")\n",
        "    \n",
        "    config = configurations_preetes[nom_config]\n",
        "    gestionnaire = GestionnaireStopwords(base=config['base'])\n",
        "    \n",
        "    # Ajout des domaines\n",
        "    for domaine in config['domaines']:\n",
        "        gestionnaire.ajouter_domaine(domaine)\n",
        "    \n",
        "    # Ajout des mots suppl√©mentaires\n",
        "    if config['mots_supplementaires']:\n",
        "        gestionnaire.ajouter_mots(config['mots_supplementaires'], 'configuration_preete')\n",
        "    \n",
        "    return gestionnaire\n",
        "\n",
        "# Test des configurations pr√™tes √† l'emploi\n",
        "print(\"üíæ CONFIGURATIONS PR√äTES √Ä L'EMPLOI\\n\")\n",
        "\n",
        "for nom, config in configurations_preetes.items():\n",
        "    gestionnaire = creer_configuration(nom)\n",
        "    stats = gestionnaire.statistiques()\n",
        "    \n",
        "    print(f\"üîß {nom.upper().replace('_', ' ')}\")\n",
        "    print(f\"   üìù {config['description']}\")\n",
        "    print(f\"   üìä {stats['total_stopwords']} stopwords\")\n",
        "    if stats['domaines_actifs']:\n",
        "        print(f\"   üè¢ Domaines : {', '.join(stats['domaines_actifs'])}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarde et chargement des configurations\n",
        "print(\"üíæ SAUVEGARDE ET CHARGEMENT\\n\")\n",
        "\n",
        "# Cr√©ation d'une configuration personnalis√©e\n",
        "mon_gestionnaire = GestionnaireStopwords(base='nltk')\n",
        "mon_gestionnaire.ajouter_domaine('e_commerce')\n",
        "mon_gestionnaire.ajouter_mots(['absolument', 'certainement', 'effectivement'], 'adverbes_certitude')\n",
        "\n",
        "# Sauvegarde\n",
        "fichier_config = 'ma_config_stopwords.json'\n",
        "mon_gestionnaire.sauvegarder(fichier_config)\n",
        "\n",
        "# Test de chargement\n",
        "nouveau_gestionnaire = GestionnaireStopwords(base='vide')\n",
        "nouveau_gestionnaire.charger(fichier_config)\n",
        "\n",
        "print(f\"\\nüìã V√©rification du chargement :\")\n",
        "print(f\"  ‚Ä¢ Stopwords charg√©s : {nouveau_gestionnaire.statistiques()['total_stopwords']}\")\n",
        "print(f\"  ‚Ä¢ Domaines actifs : {', '.join(nouveau_gestionnaire.statistiques()['domaines_actifs'])}\")\n",
        "\n",
        "# Affichage de l'historique\n",
        "print(\"\\nüìú Historique des op√©rations :\")\n",
        "for i, operation in enumerate(nouveau_gestionnaire.historique[-5:], 1):\n",
        "    print(f\"  {i}. {operation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction utilitaire pour export vers diff√©rents formats\n",
        "def exporter_stopwords(gestionnaire: GestionnaireStopwords, format_export: str = 'txt') -> str:\n",
        "    \"\"\"\n",
        "    Exporte les stopwords vers diff√©rents formats.\n",
        "    \n",
        "    Args:\n",
        "        gestionnaire: Gestionnaire de stopwords\n",
        "        format_export: Format ('txt', 'csv', 'python', 'json')\n",
        "    \n",
        "    Returns:\n",
        "        Contenu format√© pour l'export\n",
        "    \"\"\"\n",
        "    stopwords_list = sorted(list(gestionnaire.stopwords))\n",
        "    stats = gestionnaire.statistiques()\n",
        "    \n",
        "    if format_export == 'txt':\n",
        "        header = f\"# Stopwords personnalis√©s - {stats['total_stopwords']} mots\\n\"\n",
        "        header += f\"# Domaines: {', '.join(stats['domaines_actifs'])}\\n\\n\"\n",
        "        return header + '\\n'.join(stopwords_list)\n",
        "    \n",
        "    elif format_export == 'csv':\n",
        "        return 'stopword\\n' + '\\n'.join(stopwords_list)\n",
        "    \n",
        "    elif format_export == 'python':\n",
        "        return f\"\"\"# Configuration stopwords g√©n√©r√©e automatiquement\n",
        "# Total: {stats['total_stopwords']} mots\n",
        "# Domaines: {', '.join(stats['domaines_actifs'])}\n",
        "\n",
        "STOPWORDS_PERSONNALISES = {{\n",
        "    {', '.join([f\"'{mot}'\" for mot in stopwords_list[:10]])},\n",
        "    # ... {stats['total_stopwords'] - 10} autres mots\n",
        "}}\n",
        "\"\"\"\n",
        "    \n",
        "    elif format_export == 'json':\n",
        "        return json.dumps({\n",
        "            'stopwords': stopwords_list,\n",
        "            'metadata': {\n",
        "                'total': stats['total_stopwords'],\n",
        "                'domaines': stats['domaines_actifs'],\n",
        "                'export_date': str(pd.Timestamp.now())\n",
        "            }\n",
        "        }, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Test des exports\n",
        "print(\"üì§ EXEMPLES D'EXPORT\\n\")\n",
        "\n",
        "# Export Python\n",
        "export_python = exporter_stopwords(mon_gestionnaire, 'python')\n",
        "print(\"üêç Export Python :\")\n",
        "print(export_python[:300] + \"...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Export JSON\n",
        "export_json = exporter_stopwords(mon_gestionnaire, 'json')\n",
        "print(\"\\nüìã Export JSON (extrait) :\")\n",
        "print(export_json[:300] + \"...\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les formats d'export sont disponibles !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Conclusion et Bonnes Pratiques\n",
        "\n",
        "### ‚úÖ Ce que vous avez appris :\n",
        "\n",
        "1. **Identifier automatiquement** les candidats stopwords dans vos donn√©es\n",
        "2. **Cr√©er des listes sp√©cialis√©es** par domaine d'application\n",
        "3. **G√©rer efficacement** vos configurations avec le GestionnaireStopwords\n",
        "4. **Optimiser automatiquement** vos listes selon des crit√®res statistiques\n",
        "5. **Valider l'efficacit√©** de vos stopwords personnalis√©s\n",
        "6. **Appliquer concr√®tement** dans diff√©rents cas d'usage\n",
        "7. **Sauvegarder et r√©utiliser** vos configurations\n",
        "\n",
        "### üí° Bonnes Pratiques Finales :\n",
        "\n",
        "#### ‚úÖ √Ä FAIRE :\n",
        "- **Analyser votre corpus** avant de cr√©er des stopwords\n",
        "- **Tester diff√©rentes configurations** et mesurer l'impact\n",
        "- **Documenter vos choix** et garder un historique\n",
        "- **Adapter selon le contexte** (domaine, objectif, type de texte)\n",
        "- **Valider sur des √©chantillons** manuellement\n",
        "- **It√©rer et am√©liorer** selon les r√©sultats obtenus\n",
        "\n",
        "#### ‚ùå √Ä √âVITER :\n",
        "- Utiliser la m√™me liste pour tous les projets\n",
        "- Supprimer des mots importants pour votre analyse\n",
        "- Ignorer les sp√©cificit√©s de votre domaine\n",
        "- Ne pas mesurer l'impact sur les performances finales\n",
        "- Oublier de sauvegarder vos configurations optimis√©es\n",
        "\n",
        "### üöÄ Prochaines √âtapes :\n",
        "\n",
        "Maintenant que vous ma√Ætrisez les stopwords personnalis√©s, vous √™tes pr√™t pour :\n",
        "- **Lemmatisation avanc√©e** avec des dictionnaires sp√©cialis√©s\n",
        "- **Pipeline de preprocessing complet** int√©grant toutes les techniques\n",
        "- **√âvaluation quantitative** de la qualit√© du preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "**üéØ Message final :** Les stopwords ne sont pas une science exacte ! C'est un √©quilibre entre suppression du bruit et conservation de l'information. Exp√©rimentez, mesurez, adaptez !"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}