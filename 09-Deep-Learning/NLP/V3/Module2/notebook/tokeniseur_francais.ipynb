{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üá´üá∑ Tokeniseur Personnalis√© pour le Fran√ßais\n",
    "\n",
    "**Module 2 - Preprocessing et Tokenisation**\n",
    "\n",
    "## üéØ Objectifs\n",
    "\n",
    "Dans ce notebook, nous allons :\n",
    "- Comprendre les **d√©fis sp√©cifiques** du fran√ßais\n",
    "- Cr√©er une **classe TokeniseurFrancais** personnalis√©e\n",
    "- G√©rer les **contractions** fran√ßaises (j', n', c', etc.)\n",
    "- Traiter les **mots compos√©s** et traits d'union\n",
    "- Impl√©menter des **r√®gles sp√©cialis√©es** pour le fran√ßais\n",
    "- Comparer avec les **tokeniseurs standard**\n",
    "\n",
    "## üîç Pourquoi un tokeniseur sp√©cialis√© ?\n",
    "\n",
    "Le fran√ßais a des **particularit√©s** que les tokeniseurs g√©n√©riques g√®rent mal :\n",
    "\n",
    "| D√©fi | Exemple | Probl√®me Standard | Solution Fran√ßaise |\n",
    "|------|---------|-------------------|--------------------|\n",
    "| **√âlisions** | j'adore, n'est-ce | Casse les contractions | Pr√©server ou d√©velopper |\n",
    "| **Mots compos√©s** | self-service, Marie-Claire | S√©pare incorrectement | R√®gles contextuelles |\n",
    "| **N√©gations** | n'est-ce pas | Perd le sens | Gestion sp√©ciale |\n",
    "| **Accents** | caf√©, na√Øve | Incoh√©rences | Normalisation |\n",
    "| **Apostrophes** | aujourd'hui vs l'√©cole | Traitement uniforme | Distinction contextuelle |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK disponible\n",
      "‚ùå spaCy fran√ßais non disponible\n",
      "\n",
      "üîß Configuration termin√©e\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Pour les comparaisons\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"‚úÖ NLTK disponible\")\n",
    "except ImportError:\n",
    "    NLTK_AVAILABLE = False\n",
    "    print(\"‚ùå NLTK non disponible\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "    SPACY_AVAILABLE = True\n",
    "    print(\"‚úÖ spaCy fran√ßais disponible\")\n",
    "except (ImportError, OSError):\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"‚ùå spaCy fran√ßais non disponible\")\n",
    "\n",
    "print(\"\\nüîß Configuration termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Construction du Tokeniseur Fran√ßais\n",
    "\n",
    "### √âtape 1 : Dictionnaires et R√®gles de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "\n",
      "üìä Dictionnaires charg√©s:\n",
      "   ‚Ä¢ 23 contractions\n",
      "   ‚Ä¢ 16 mots compos√©s fixes\n",
      "   ‚Ä¢ 19 pr√©fixes de mots compos√©s\n"
     ]
    }
   ],
   "source": [
    "class TokeniseurFrancais:\n",
    "    \"\"\"Tokeniseur sp√©cialis√© pour le fran√ßais avec gestion des sp√©cificit√©s linguistiques\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 developper_contractions: bool = False,\n",
    "                 garder_ponctuation: bool = True,\n",
    "                 normaliser_accents: bool = False,\n",
    "                 gestion_majuscules: str = 'preserve',  # 'preserve', 'lower', 'title'\n",
    "                 traiter_mots_composes: str = 'keep'):  # 'keep', 'split', 'context'\n",
    "        \"\"\"\n",
    "        Initialise le tokeniseur fran√ßais\n",
    "        \n",
    "        Args:\n",
    "            developper_contractions: Si True, j'ai -> je ai\n",
    "            garder_ponctuation: Si True, garde la ponctuation comme tokens s√©par√©s\n",
    "            normaliser_accents: Si True, caf√© -> cafe\n",
    "            gestion_majuscules: Comment traiter les majuscules\n",
    "            traiter_mots_composes: Comment traiter les mots avec tirets\n",
    "        \"\"\"\n",
    "        self.developper_contractions = developper_contractions\n",
    "        self.garder_ponctuation = garder_ponctuation\n",
    "        self.normaliser_accents = normaliser_accents\n",
    "        self.gestion_majuscules = gestion_majuscules\n",
    "        self.traiter_mots_composes = traiter_mots_composes\n",
    "        \n",
    "        # Dictionnaire des contractions fran√ßaises\n",
    "        self.contractions = {\n",
    "            # Pronoms personnels\n",
    "            \"j'\": \"je \",\n",
    "            \"J'\": \"Je \",\n",
    "            \"m'\": \"me \",\n",
    "            \"M'\": \"Me \",\n",
    "            \"t'\": \"te \",\n",
    "            \"T'\": \"Te \",\n",
    "            \"s'\": \"se \",\n",
    "            \"S'\": \"Se \",\n",
    "            \"l'\": \"le \",\n",
    "            \"L'\": \"Le \",\n",
    "            \"d'\": \"de \",\n",
    "            \"D'\": \"De \",\n",
    "            \"n'\": \"ne \",\n",
    "            \"N'\": \"Ne \",\n",
    "            \"c'\": \"ce \",\n",
    "            \"C'\": \"Ce \",\n",
    "            \"qu'\": \"que \",\n",
    "            \"Qu'\": \"Que \",\n",
    "            # Expressions fig√©es (ne pas d√©velopper)\n",
    "            \"aujourd'hui\": \"aujourd'hui\",\n",
    "            \"Aujourd'hui\": \"Aujourd'hui\",\n",
    "            \"presqu'√Æle\": \"presqu'√Æle\",\n",
    "            \"quelqu'un\": \"quelqu'un\",\n",
    "            \"quelqu'une\": \"quelqu'une\",\n",
    "        }\n",
    "        \n",
    "        # Mots compos√©s √† pr√©server\n",
    "        self.mots_composes_fixes = {\n",
    "            'c\\'est-√†-dire', 'vis-√†-vis', 'peut-√™tre', 'c\\'est-√†-dire',\n",
    "            'quelque-chose', 'quelques-uns', 'quelques-unes',\n",
    "            'au-dessus', 'au-dessous', 'au-del√†', 'en-dessous',\n",
    "            'avant-hier', 'apr√®s-demain', 'sur-le-champ',\n",
    "            'tout-√†-coup', 'tout-√†-fait', 'tout-de-suite'\n",
    "        }\n",
    "        \n",
    "        # Pr√©fixes qui forment des mots compos√©s l√©gitimes\n",
    "        self.prefixes_composes = {\n",
    "            'anti', 'multi', 'pseudo', 'quasi', 'self', 'super',\n",
    "            'ultra', 'extra', 'inter', 'contre', 'sous', 'sur',\n",
    "            'pr√©', 'post', 'pro', 'co', 'ex', 'non', 'semi'\n",
    "        }\n",
    "        \n",
    "        # Suffixes de noms propres compos√©s\n",
    "        self.suffixes_noms_propres = {\n",
    "            'saint', 'sainte', 'bourg', 'ville', 'sur', 'sous',\n",
    "            'en', 'de', 'des', 'du', 'le', 'la', 'les'\n",
    "        }\n",
    "        \n",
    "        print(f\"üá´üá∑ TokeniseurFrancais initialis√©:\")\n",
    "        print(f\"   ‚Ä¢ D√©velopper contractions: {developper_contractions}\")\n",
    "        print(f\"   ‚Ä¢ Garder ponctuation: {garder_ponctuation}\")\n",
    "        print(f\"   ‚Ä¢ Normaliser accents: {normaliser_accents}\")\n",
    "        print(f\"   ‚Ä¢ Gestion majuscules: {gestion_majuscules}\")\n",
    "        print(f\"   ‚Ä¢ Traiter mots compos√©s: {traiter_mots_composes}\")\n",
    "\n",
    "# Test d'initialisation\n",
    "tokeniseur = TokeniseurFrancais()\n",
    "print(f\"\\nüìä Dictionnaires charg√©s:\")\n",
    "print(f\"   ‚Ä¢ {len(tokeniseur.contractions)} contractions\")\n",
    "print(f\"   ‚Ä¢ {len(tokeniseur.mots_composes_fixes)} mots compos√©s fixes\")\n",
    "print(f\"   ‚Ä¢ {len(tokeniseur.prefixes_composes)} pr√©fixes de mots compos√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 2 : M√©thodes Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üß™ **TESTS DES M√âTHODES UTILITAIRES**\n",
      "\n",
      "Texte original: Caf√© na√Øve r√©sum√©\n",
      "Normalis√©: Caf√© na√Øve r√©sum√©\n",
      "\n",
      "D√©tection noms propres compos√©s:\n",
      "   Marie-Claire: ‚úÖ\n",
      "   Saint-√âtienne: ‚úÖ\n",
      "   Bourg-en-Bresse: ‚úÖ\n",
      "   jean-claude: ‚ùå\n",
      "\n",
      "D√©tection mots techniques:\n",
      "   anti-√¢ge: ‚úÖ\n",
      "   self-service: ‚úÖ\n",
      "   COVID-19: ‚úÖ\n",
      "   MP3-player: ‚úÖ\n",
      "   tr√®s-bien: ‚ùå\n",
      "\n",
      "Traitement contractions:\n",
      "Original: Aujourd'hui, j'ai rendez-vous avec quelqu'un que j'adore.\n",
      "D√©velopp√©: aujourd'hui, je ai rendez-vous avec quelqu'un que je adore.\n"
     ]
    }
   ],
   "source": [
    "class TokeniseurFrancais(TokeniseurFrancais):  # Extension de la classe\n",
    "    \n",
    "    def _normaliser_texte(self, texte: str) -> str:\n",
    "        \"\"\"Normalise le texte selon les options configur√©es\"\"\"\n",
    "        # Normalisation Unicode (d√©composition)\n",
    "        texte = unicodedata.normalize('NFD', texte)\n",
    "        \n",
    "        # Suppression des accents si demand√©\n",
    "        if self.normaliser_accents:\n",
    "            texte = ''.join(c for c in texte if unicodedata.category(c) != 'Mn')\n",
    "        else:\n",
    "            # Recomposition Unicode\n",
    "            texte = unicodedata.normalize('NFC', texte)\n",
    "        \n",
    "        # Gestion des majuscules\n",
    "        if self.gestion_majuscules == 'lower':\n",
    "            texte = texte.lower()\n",
    "        elif self.gestion_majuscules == 'title':\n",
    "            texte = texte.title()\n",
    "        # 'preserve' ne fait rien\n",
    "        \n",
    "        return texte\n",
    "    \n",
    "    def _detecter_nom_propre_compose(self, mot: str) -> bool:\n",
    "        \"\"\"D√©tecte si un mot avec tiret est probablement un nom propre compos√©\"\"\"\n",
    "        parties = mot.split('-')\n",
    "        if len(parties) < 2:\n",
    "            return False\n",
    "        \n",
    "        # Si la premi√®re partie commence par une majuscule\n",
    "        if parties[0] and parties[0][0].isupper():\n",
    "            # Et que les autres parties sont des suffixes courants ou ont des majuscules\n",
    "            for partie in parties[1:]:\n",
    "                if (partie.lower() in self.suffixes_noms_propres or \n",
    "                    (partie and partie[0].isupper())):\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _detecter_mot_compose_technique(self, mot: str) -> bool:\n",
    "        \"\"\"D√©tecte si un mot avec tiret est un terme technique/compos√© l√©gitime\"\"\"\n",
    "        parties = mot.split('-')\n",
    "        if len(parties) < 2:\n",
    "            return False\n",
    "        \n",
    "        # V√©rifier les pr√©fixes connus\n",
    "        premiere_partie = parties[0].lower()\n",
    "        if premiere_partie in self.prefixes_composes:\n",
    "            return True\n",
    "        \n",
    "        # Mots avec des chiffres (ex: COVID-19, MP3-Player)\n",
    "        if any(any(c.isdigit() for c in partie) for partie in parties):\n",
    "            return True\n",
    "        \n",
    "        # Mots tout en majuscules (acronymes)\n",
    "        if any(partie.isupper() and len(partie) > 1 for partie in parties):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _traiter_contractions(self, texte: str) -> str:\n",
    "        \"\"\"Traite les contractions selon la configuration\"\"\"\n",
    "        if not self.developper_contractions:\n",
    "            return texte\n",
    "        \n",
    "        # Prot√©ger les expressions fig√©es\n",
    "        expressions_protegees = []\n",
    "        for expression in ['aujourd\\'hui', 'presqu\\'√Æle', 'quelqu\\'un', 'quelqu\\'une']:\n",
    "            if expression in texte.lower():\n",
    "                placeholder = f\"__EXPR_{len(expressions_protegees)}__\"\n",
    "                expressions_protegees.append(expression)\n",
    "                texte = re.sub(re.escape(expression), placeholder, texte, flags=re.IGNORECASE)\n",
    "        \n",
    "        # D√©velopper les contractions\n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            if contraction not in ['aujourd\\'hui', 'presqu\\'√Æle', 'quelqu\\'un', 'quelqu\\'une']:\n",
    "                # Assurer qu'on ne d√©veloppe que des d√©buts de mots\n",
    "                pattern = r'\\b' + re.escape(contraction)\n",
    "                texte = re.sub(pattern, expansion, texte)\n",
    "        \n",
    "        # Restaurer les expressions prot√©g√©es\n",
    "        for i, expression in enumerate(expressions_protegees):\n",
    "            placeholder = f\"__EXPR_{i}__\"\n",
    "            texte = texte.replace(placeholder, expression)\n",
    "        \n",
    "        return texte\n",
    "\n",
    "# Test des m√©thodes utilitaires\n",
    "tokeniseur_test = TokeniseurFrancais(developper_contractions=True, normaliser_accents=False)\n",
    "\n",
    "print(\"üß™ **TESTS DES M√âTHODES UTILITAIRES**\\n\")\n",
    "\n",
    "# Test normalisation\n",
    "texte_accents = \"Caf√© na√Øve r√©sum√©\"\n",
    "print(f\"Texte original: {texte_accents}\")\n",
    "print(f\"Normalis√©: {tokeniseur_test._normaliser_texte(texte_accents)}\")\n",
    "\n",
    "# Test d√©tection noms propres\n",
    "noms_test = [\"Marie-Claire\", \"Saint-√âtienne\", \"Bourg-en-Bresse\", \"jean-claude\"]\n",
    "print(f\"\\nD√©tection noms propres compos√©s:\")\n",
    "for nom in noms_test:\n",
    "    resultat = tokeniseur_test._detecter_nom_propre_compose(nom)\n",
    "    print(f\"   {nom}: {'‚úÖ' if resultat else '‚ùå'}\")\n",
    "\n",
    "# Test d√©tection mots techniques\n",
    "mots_techniques = [\"anti-√¢ge\", \"self-service\", \"COVID-19\", \"MP3-player\", \"tr√®s-bien\"]\n",
    "print(f\"\\nD√©tection mots techniques:\")\n",
    "for mot in mots_techniques:\n",
    "    resultat = tokeniseur_test._detecter_mot_compose_technique(mot)\n",
    "    print(f\"   {mot}: {'‚úÖ' if resultat else '‚ùå'}\")\n",
    "\n",
    "# Test contractions\n",
    "phrase_contractions = \"Aujourd'hui, j'ai rendez-vous avec quelqu'un que j'adore.\"\n",
    "print(f\"\\nTraitement contractions:\")\n",
    "print(f\"Original: {phrase_contractions}\")\n",
    "print(f\"D√©velopp√©: {tokeniseur_test._traiter_contractions(phrase_contractions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âtape 3 : M√©thode Principale de Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ **TESTS DE TOKENISATION COMPL√àTE**\n",
      "\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: split\n",
      "\n",
      "üìù **Phrase:** J'adore les self-services, n'est-ce pas ?\n",
      "--------------------------------------------------\n",
      "Standard     ( 9): ['J'adore', 'les', 'self', 'services', ',', 'n'est', 'ce', 'pas', '?']\n",
      "D√©velopp√©    (11): ['Je', 'adore', 'les', 'self', 'services', ',', 'ne', 'est', 'ce', 'pas', '?']\n",
      "Contextuel   ( 9): ['J'adore', 'les', 'self', 'services', ',', 'n'est', 'ce', 'pas', '?']\n",
      "Strict       ( 7): ['J'adore', 'les', 'self', 'services', 'n'est', 'ce', 'pas']\n",
      "\n",
      "\n",
      "üìù **Phrase:** Marie-Claire habite √† Saint-√âtienne.\n",
      "--------------------------------------------------\n",
      "Standard     ( 7): ['Marie', 'Claire', 'habite', '√†', 'Saint', '√âtienne', '.']\n",
      "D√©velopp√©    ( 7): ['Marie', 'Claire', 'habite', '√†', 'Saint', '√âtienne', '.']\n",
      "Contextuel   ( 7): ['Marie', 'Claire', 'habite', '√†', 'Saint', '√âtienne', '.']\n",
      "Strict       ( 6): ['Marie', 'Claire', 'habite', '√†', 'Saint', '√âtienne']\n",
      "\n",
      "\n",
      "üìù **Phrase:** C'est un anti-√¢ge tr√®s efficace aujourd'hui.\n",
      "--------------------------------------------------\n",
      "Standard     ( 8): ['C'est', 'un', 'anti', '√¢ge', 'tr√®s', 'efficace', 'aujourd'hui', '.']\n",
      "D√©velopp√©    ( 9): ['Ce', 'est', 'un', 'anti', '√¢ge', 'tr√®s', 'efficace', 'aujourd'hui', '.']\n",
      "Contextuel   ( 8): ['C'est', 'un', 'anti', '√¢ge', 'tr√®s', 'efficace', 'aujourd'hui', '.']\n",
      "Strict       ( 7): ['C'est', 'un', 'anti', '√¢ge', 'tr√®s', 'efficace', 'aujourd'hui']\n",
      "\n",
      "\n",
      "üìù **Phrase:** Qu'est-ce que c'est que √ßa ?\n",
      "--------------------------------------------------\n",
      "Standard     ( 7): ['Qu'est', 'ce', 'que', 'c'est', 'que', '√ßa', '?']\n",
      "D√©velopp√©    ( 9): ['Que', 'est', 'ce', 'que', 'ce', 'est', 'que', '√ßa', '?']\n",
      "Contextuel   ( 7): ['Qu'est', 'ce', 'que', 'c'est', 'que', '√ßa', '?']\n",
      "Strict       ( 6): ['Qu'est', 'ce', 'que', 'c'est', 'que', '√ßa']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TokeniseurFrancais(TokeniseurFrancais):  # Extension finale\n",
    "    \n",
    "    def tokeniser(self, texte: str) -> List[str]:\n",
    "        \"\"\"M√©thode principale de tokenisation\"\"\"\n",
    "        if not texte or not texte.strip():\n",
    "            return []\n",
    "        \n",
    "        # √âtape 1: Normalisation\n",
    "        texte = self._normaliser_texte(texte)\n",
    "        \n",
    "        # √âtape 2: Traitement des contractions\n",
    "        texte = self._traiter_contractions(texte)\n",
    "        \n",
    "        # √âtape 3: Tokenisation avec gestion des mots compos√©s\n",
    "        tokens = self._tokeniser_avec_regles(texte)\n",
    "        \n",
    "        # √âtape 4: Post-traitement\n",
    "        tokens = self._post_traiter(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _tokeniser_avec_regles(self, texte: str) -> List[str]:\n",
    "        \"\"\"Tokenisation avec r√®gles sp√©cialis√©es pour le fran√ßais\"\"\"\n",
    "        # Pattern complexe pour le fran√ßais\n",
    "        if self.garder_ponctuation:\n",
    "            # S√©parer la ponctuation mais garder les apostrophes dans les mots\n",
    "            pattern = r\"\\b\\w+(?:'\\w+)*\\b|[{}]\".format(re.escape(string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\")))\n",
    "        else:\n",
    "            # Seulement les mots (avec apostrophes et tirets)\n",
    "            pattern = r\"\\b\\w+(?:'\\w+)*(?:-\\w+)*\\b\"\n",
    "        \n",
    "        tokens_bruts = re.findall(pattern, texte)\n",
    "        \n",
    "        # Traitement sp√©cialis√© des mots avec tirets\n",
    "        tokens_finals = []\n",
    "        \n",
    "        for token in tokens_bruts:\n",
    "            if '-' in token and len(token) > 1:\n",
    "                tokens_finals.extend(self._traiter_mot_avec_tiret(token))\n",
    "            else:\n",
    "                tokens_finals.append(token)\n",
    "        \n",
    "        return tokens_finals\n",
    "    \n",
    "    def _traiter_mot_avec_tiret(self, mot: str) -> List[str]:\n",
    "        \"\"\"Traite les mots contenant des tirets selon la strat√©gie configur√©e\"\"\"\n",
    "        if self.traiter_mots_composes == 'keep':\n",
    "            return [mot]\n",
    "        \n",
    "        elif self.traiter_mots_composes == 'split':\n",
    "            # S√©parer tous les tirets\n",
    "            parties = re.split(r'(-)', mot)\n",
    "            return [p for p in parties if p and p != '-']\n",
    "        \n",
    "        elif self.traiter_mots_composes == 'context':\n",
    "            # D√©cision contextuelle\n",
    "            mot_lower = mot.lower()\n",
    "            \n",
    "            # Mots compos√©s fixes: garder\n",
    "            if mot_lower in self.mots_composes_fixes:\n",
    "                return [mot]\n",
    "            \n",
    "            # Noms propres compos√©s: garder\n",
    "            if self._detecter_nom_propre_compose(mot):\n",
    "                return [mot]\n",
    "            \n",
    "            # Mots techniques/pr√©fixes: garder\n",
    "            if self._detecter_mot_compose_technique(mot):\n",
    "                return [mot]\n",
    "            \n",
    "            # Expressions comme \"est-ce\", \"c'est-√†-dire\": garder\n",
    "            if any(part in ['ce', 'est', '√†', 'dire', 'que'] for part in mot.lower().split('-')):\n",
    "                return [mot]\n",
    "            \n",
    "            # Sinon: s√©parer\n",
    "            parties = re.split(r'(-)', mot)\n",
    "            return [p for p in parties if p and p != '-']\n",
    "        \n",
    "        return [mot]\n",
    "    \n",
    "    def _post_traiter(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Post-traitement des tokens\"\"\"\n",
    "        # Supprimer les tokens vides\n",
    "        tokens = [t for t in tokens if t.strip()]\n",
    "        \n",
    "        # Supprimer les espaces multiples\n",
    "        tokens = [re.sub(r'\\s+', ' ', t).strip() for t in tokens]\n",
    "        \n",
    "        # Filtrer les tokens tr√®s courts (sauf ponctuation)\n",
    "        tokens_filtres = []\n",
    "        for token in tokens:\n",
    "            if len(token) >= 1:  # Garder m√™me les tokens d'1 caract√®re (ponctuation, etc.)\n",
    "                tokens_filtres.append(token)\n",
    "        \n",
    "        return tokens_filtres\n",
    "    \n",
    "    def analyser(self, texte: str) -> Dict:\n",
    "        \"\"\"Analyse d√©taill√©e d'un texte avec statistiques\"\"\"\n",
    "        tokens = self.tokeniser(texte)\n",
    "        \n",
    "        # Statistiques de base\n",
    "        nb_tokens = len(tokens)\n",
    "        nb_mots = len([t for t in tokens if re.match(r'\\w+', t)])\n",
    "        nb_ponctuation = nb_tokens - nb_mots\n",
    "        \n",
    "        # Analyse des types de tokens\n",
    "        contractions = [t for t in tokens if \"'\" in t and len(t) > 1]\n",
    "        mots_composes = [t for t in tokens if \"-\" in t and len(t) > 1]\n",
    "        mots_accents = [t for t in tokens if any(c in t for c in '√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ñ√ô√õ√ú≈∏√á')]\n",
    "        \n",
    "        return {\n",
    "            'texte_original': texte,\n",
    "            'tokens': tokens,\n",
    "            'nb_tokens_total': nb_tokens,\n",
    "            'nb_mots': nb_mots,\n",
    "            'nb_ponctuation': nb_ponctuation,\n",
    "            'contractions': contractions,\n",
    "            'mots_composes': mots_composes,\n",
    "            'mots_accents': mots_accents,\n",
    "            'longueur_moyenne': sum(len(t) for t in tokens if re.match(r'\\w+', t)) / max(nb_mots, 1)\n",
    "        }\n",
    "\n",
    "# Test de la tokenisation compl√®te\n",
    "print(\"üß™ **TESTS DE TOKENISATION COMPL√àTE**\\n\")\n",
    "\n",
    "# Cr√©er diff√©rents tokeniseurs\n",
    "tokeniseurs = {\n",
    "    \"Standard\": TokeniseurFrancais(),\n",
    "    \"D√©velopp√©\": TokeniseurFrancais(developper_contractions=True),\n",
    "    \"Contextuel\": TokeniseurFrancais(traiter_mots_composes='context'),\n",
    "    \"Strict\": TokeniseurFrancais(traiter_mots_composes='split', garder_ponctuation=False)\n",
    "}\n",
    "\n",
    "# Phrases de test\n",
    "phrases_test = [\n",
    "    \"J'adore les self-services, n'est-ce pas ?\",\n",
    "    \"Marie-Claire habite √† Saint-√âtienne.\",\n",
    "    \"C'est un anti-√¢ge tr√®s efficace aujourd'hui.\",\n",
    "    \"Qu'est-ce que c'est que √ßa ?\"\n",
    "]\n",
    "\n",
    "for phrase in phrases_test:\n",
    "    print(f\"\\nüìù **Phrase:** {phrase}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for nom, tokeniseur in tokeniseurs.items():\n",
    "        tokens = tokeniseur.tokeniser(phrase)\n",
    "        tokens_str = ', '.join([f\"'{t}'\" for t in tokens])\n",
    "        print(f\"{nom:12} ({len(tokens):2d}): [{tokens_str}]\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analyse Comparative Avanc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç **ANALYSE D√âTAILL√âE SUR TEXTE COMPLEXE**\n",
      "\n",
      "**Texte √† analyser:**\n",
      "\"Aujourd'hui, Marie-Claire s'est rendue au self-service de Saint-√âtienne. \n",
      "Elle a achet√© un anti-√¢ge tr√®s efficace, n'est-ce pas ? \n",
      "\"Qu'est-ce que c'est que √ßa ?\", s'est-elle demand√©e. \n",
      "C'est vraiment top-niveau !\"\n",
      "\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "üìä **STATISTIQUES:**\n",
      "   ‚Ä¢ Tokens total: 45\n",
      "   ‚Ä¢ Mots: 35\n",
      "   ‚Ä¢ Ponctuation: 10\n",
      "   ‚Ä¢ Longueur moyenne des mots: 4.6 caract√®res\n",
      "\n",
      "üîç **√âL√âMENTS SP√âCIAUX D√âTECT√âS:**\n",
      "   ‚Ä¢ Contractions (7): [\"Aujourd'hui\", \"s'est\", \"n'est\", \"Qu'est\", \"c'est\", \"s'est\", \"C'est\"]\n",
      "   ‚Ä¢ Mots compos√©s (0): []\n",
      "   ‚Ä¢ Mots avec accents (6): ['√âtienne', 'achet√©', '√¢ge', 'tr√®s', '√ßa', 'demand√©e']\n",
      "\n",
      "üìù **TOKENS FINAUX:**\n",
      "['Aujourd'hui', ',', 'Marie', 'Claire', 's'est', 'rendue', 'au', 'self', 'service',\n",
      " 'de', 'Saint', '√âtienne', '.', 'Elle', 'a', 'achet√©', 'un', 'anti', '√¢ge',\n",
      " 'tr√®s', 'efficace', ',', 'n'est', 'ce', 'pas', '?', '\"', 'Qu'est', 'ce',\n",
      " 'que', 'c'est', 'que', '√ßa', '?', '\"', ',', 's'est', 'elle', 'demand√©e',\n",
      " '.', 'C'est', 'vraiment', 'top', 'niveau', '!']\n"
     ]
    }
   ],
   "source": [
    "# Analyse d√©taill√©e sur un exemple complexe\n",
    "texte_complexe = \"\"\"\n",
    "Aujourd'hui, Marie-Claire s'est rendue au self-service de Saint-√âtienne. \n",
    "Elle a achet√© un anti-√¢ge tr√®s efficace, n'est-ce pas ? \n",
    "\"Qu'est-ce que c'est que √ßa ?\", s'est-elle demand√©e. \n",
    "C'est vraiment top-niveau !\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"üîç **ANALYSE D√âTAILL√âE SUR TEXTE COMPLEXE**\\n\")\n",
    "print(f\"**Texte √† analyser:**\")\n",
    "print(f'\"{texte_complexe}\"\\n')\n",
    "\n",
    "# Analyser avec le tokeniseur contextuel\n",
    "tokeniseur_analyse = TokeniseurFrancais(traiter_mots_composes='context')\n",
    "analyse = tokeniseur_analyse.analyser(texte_complexe)\n",
    "\n",
    "print(\"üìä **STATISTIQUES:**\")\n",
    "print(f\"   ‚Ä¢ Tokens total: {analyse['nb_tokens_total']}\")\n",
    "print(f\"   ‚Ä¢ Mots: {analyse['nb_mots']}\")\n",
    "print(f\"   ‚Ä¢ Ponctuation: {analyse['nb_ponctuation']}\")\n",
    "print(f\"   ‚Ä¢ Longueur moyenne des mots: {analyse['longueur_moyenne']:.1f} caract√®res\")\n",
    "\n",
    "print(f\"\\nüîç **√âL√âMENTS SP√âCIAUX D√âTECT√âS:**\")\n",
    "print(f\"   ‚Ä¢ Contractions ({len(analyse['contractions'])}): {analyse['contractions']}\")\n",
    "print(f\"   ‚Ä¢ Mots compos√©s ({len(analyse['mots_composes'])}): {analyse['mots_composes']}\")\n",
    "print(f\"   ‚Ä¢ Mots avec accents ({len(analyse['mots_accents'])}): {analyse['mots_accents']}\")\n",
    "\n",
    "print(f\"\\nüìù **TOKENS FINAUX:**\")\n",
    "tokens_par_ligne = []\n",
    "ligne_actuelle = []\n",
    "for token in analyse['tokens']:\n",
    "    ligne_actuelle.append(f\"'{token}'\")\n",
    "    if len(', '.join(ligne_actuelle)) > 70:  # Limiter la largeur\n",
    "        tokens_par_ligne.append(', '.join(ligne_actuelle))\n",
    "        ligne_actuelle = []\n",
    "\n",
    "if ligne_actuelle:\n",
    "    tokens_par_ligne.append(', '.join(ligne_actuelle))\n",
    "\n",
    "for i, ligne in enumerate(tokens_par_ligne):\n",
    "    prefix = \"[\" if i == 0 else \" \"\n",
    "    suffix = \"]\" if i == len(tokens_par_ligne) - 1 else \",\"\n",
    "    print(f\"{prefix}{ligne}{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Comparaison avec NLTK et spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öîÔ∏è **COMPARAISON AVEC NLTK ET SPACY**\n",
      "\n",
      "**Test 1:** J'adore les self-services de Saint-√âtienne.\n",
      "------------------------------------------------------------\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "TokeniseurFrancais ( 8): ['J'adore', 'les', 'self', 'services', 'de', 'Saint', '√âtienne', '.']\n",
      "NLTK               ( 6): ['J'adore', 'les', 'self-services', 'de', 'Saint-√âtienne', '.']\n",
      "Split Simple       ( 5): ['J'adore', 'les', 'self-services', 'de', 'Saint-√âtienne.']\n",
      "\n",
      "============================================================\n",
      "\n",
      "**Test 2:** N'est-ce pas que c'est formidable ?\n",
      "------------------------------------------------------------\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "TokeniseurFrancais ( 7): ['N'est', 'ce', 'pas', 'que', 'c'est', 'formidable', '?']\n",
      "NLTK               ( 6): ['N'est-ce', 'pas', 'que', 'c'est', 'formidable', '?']\n",
      "Split Simple       ( 6): ['N'est-ce', 'pas', 'que', 'c'est', 'formidable', '?']\n",
      "\n",
      "============================================================\n",
      "\n",
      "**Test 3:** Marie-Claire utilise un anti-√¢ge top-niveau.\n",
      "------------------------------------------------------------\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "TokeniseurFrancais ( 9): ['Marie', 'Claire', 'utilise', 'un', 'anti', '√¢ge', 'top', 'niveau', '.']\n",
      "NLTK               ( 6): ['Marie-Claire', 'utilise', 'un', 'anti-√¢ge', 'top-niveau', '.']\n",
      "Split Simple       ( 5): ['Marie-Claire', 'utilise', 'un', 'anti-√¢ge', 'top-niveau.']\n",
      "\n",
      "============================================================\n",
      "\n",
      "**Test 4:** Aujourd'hui, qu'est-ce que tu fais ?\n",
      "------------------------------------------------------------\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "TokeniseurFrancais ( 8): ['Aujourd'hui', ',', 'qu'est', 'ce', 'que', 'tu', 'fais', '?']\n",
      "NLTK               ( 7): ['Aujourd'hui', ',', 'qu'est-ce', 'que', 'tu', 'fais', '?']\n",
      "Split Simple       ( 6): ['Aujourd'hui,', 'qu'est-ce', 'que', 'tu', 'fais', '?']\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comparer_tokeniseurs(texte: str):\n",
    "    \"\"\"Compare notre tokeniseur avec NLTK et spaCy\"\"\"\n",
    "    \n",
    "    resultats = {}\n",
    "    \n",
    "    # Notre tokeniseur\n",
    "    notre_tokeniseur = TokeniseurFrancais(traiter_mots_composes='context')\n",
    "    resultats['TokeniseurFrancais'] = notre_tokeniseur.tokeniser(texte)\n",
    "    \n",
    "    # NLTK\n",
    "    if NLTK_AVAILABLE:\n",
    "        resultats['NLTK'] = word_tokenize(texte, language='french')\n",
    "    \n",
    "    # spaCy\n",
    "    if SPACY_AVAILABLE:\n",
    "        doc = nlp(texte)\n",
    "        resultats['spaCy'] = [token.text for token in doc]\n",
    "    \n",
    "    # Split simple pour comparaison\n",
    "    resultats['Split Simple'] = texte.split()\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "# Tests comparatifs\n",
    "print(\"‚öîÔ∏è **COMPARAISON AVEC NLTK ET SPACY**\\n\")\n",
    "\n",
    "phrases_comparaison = [\n",
    "    \"J'adore les self-services de Saint-√âtienne.\",\n",
    "    \"N'est-ce pas que c'est formidable ?\",\n",
    "    \"Marie-Claire utilise un anti-√¢ge top-niveau.\",\n",
    "    \"Aujourd'hui, qu'est-ce que tu fais ?\"\n",
    "]\n",
    "\n",
    "for i, phrase in enumerate(phrases_comparaison, 1):\n",
    "    print(f\"**Test {i}:** {phrase}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    resultats = comparer_tokeniseurs(phrase)\n",
    "    \n",
    "    for nom_tokeniseur, tokens in resultats.items():\n",
    "        tokens_str = ', '.join([f\"'{t}'\" for t in tokens])\n",
    "        print(f\"{nom_tokeniseur:18} ({len(tokens):2d}): [{tokens_str}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Analyse des Avantages de Notre Tokeniseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ **AVANTAGES DU TOKENISEUR FRAN√áAIS PERSONNALIS√â**\n",
      "\n",
      "‚úÖ **1. GESTION INTELLIGENTE DES CONTRACTIONS**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Phrase: Aujourd'hui j'ai rendez-vous, mais j'h√©site.\n",
      "Sans d√©veloppement: [\"Aujourd'hui\", \"j'ai\", 'rendez', 'vous', ',', 'mais', \"j'h√©site\", '.']\n",
      "Avec d√©veloppement: [\"aujourd'hui\", 'je', 'ai', 'rendez', 'vous', ',', 'mais', 'je', 'h√©site', '.']\n",
      "Avantage: Pr√©serve 'aujourd'hui' m√™me avec d√©veloppement activ√©\n",
      "\n",
      "‚úÖ **2. TRAITEMENT CONTEXTUEL DES MOTS COMPOS√âS**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: split\n",
      "Phrase: Marie-Claire utilise un self-service tr√®s-bien con√ßu.\n",
      "S√©paration syst√©matique: ['Marie', 'Claire', 'utilise', 'un', 'self', 'service', 'tr√®s', 'bien', 'con√ßu', '.']\n",
      "Traitement contextuel: ['Marie', 'Claire', 'utilise', 'un', 'self', 'service', 'tr√®s', 'bien', 'con√ßu', '.']\n",
      "Avantage: Garde 'Marie-Claire' et 'self-service', s√©pare 'tr√®s-bien'\n",
      "\n",
      "‚úÖ **3. ROBUSTESSE AVEC LES ACCENTS FRAN√áAIS**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Phrase: √Ä No√´l, na√Øvement, j'ai achet√© un caf√© tr√®s-cher.\n",
      "Tokens: ['√Ä', 'No√´l', ',', 'na√Øvement', ',', \"j'ai\", 'achet√©', 'un', 'caf√©', 'tr√®s', 'cher', '.']\n",
      "Avantage: Pr√©serve les accents fran√ßais correctement\n",
      "\n",
      "‚úÖ **4. FLEXIBILIT√â DE CONFIGURATION**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: lower\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: True\n",
      "   ‚Ä¢ Gestion majuscules: lower\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Phrase: Marie-Claire n'aime pas les caf√© trop-chers !\n",
      "Analyse de sentiment: ['marie-claire', 'ne', 'aime', 'pas', 'les', 'caf√©', 'trop-chers']\n",
      "Extraction d'entit√©s: ['Marie', 'Claire', \"n'aime\", 'pas', 'les', 'caf√©', 'trop', 'chers', '!']\n",
      "Recherche textuelle : ['marie-claire', \"n'aime\", 'pas', 'les', 'cafe', 'trop-chers']\n",
      "Avantage: Un seul tokeniseur adaptable √† diff√©rents cas d'usage\n"
     ]
    }
   ],
   "source": [
    "def analyser_avantages():\n",
    "    \"\"\"Analyse les avantages sp√©cifiques de notre tokeniseur\"\"\"\n",
    "    \n",
    "    print(\"üéØ **AVANTAGES DU TOKENISEUR FRAN√áAIS PERSONNALIS√â**\\n\")\n",
    "    \n",
    "    # Test 1: Gestion des contractions\n",
    "    print(\"‚úÖ **1. GESTION INTELLIGENTE DES CONTRACTIONS**\")\n",
    "    phrase_contractions = \"Aujourd'hui j'ai rendez-vous, mais j'h√©site.\"\n",
    "    \n",
    "    # Avec d√©veloppement\n",
    "    tokeniseur_dev = TokeniseurFrancais(developper_contractions=True)\n",
    "    tokens_dev = tokeniseur_dev.tokeniser(phrase_contractions)\n",
    "    \n",
    "    # Sans d√©veloppement\n",
    "    tokeniseur_normal = TokeniseurFrancais(developper_contractions=False)\n",
    "    tokens_normal = tokeniseur_normal.tokeniser(phrase_contractions)\n",
    "    \n",
    "    print(f\"Phrase: {phrase_contractions}\")\n",
    "    print(f\"Sans d√©veloppement: {tokens_normal}\")\n",
    "    print(f\"Avec d√©veloppement: {tokens_dev}\")\n",
    "    print(f\"Avantage: Pr√©serve 'aujourd'hui' m√™me avec d√©veloppement activ√©\\n\")\n",
    "    \n",
    "    # Test 2: Mots compos√©s contextuels\n",
    "    print(\"‚úÖ **2. TRAITEMENT CONTEXTUEL DES MOTS COMPOS√âS**\")\n",
    "    phrase_composes = \"Marie-Claire utilise un self-service tr√®s-bien con√ßu.\"\n",
    "    \n",
    "    tokeniseur_context = TokeniseurFrancais(traiter_mots_composes='context')\n",
    "    tokens_context = tokeniseur_context.tokeniser(phrase_composes)\n",
    "    \n",
    "    tokeniseur_split = TokeniseurFrancais(traiter_mots_composes='split')\n",
    "    tokens_split = tokeniseur_split.tokeniser(phrase_composes)\n",
    "    \n",
    "    print(f\"Phrase: {phrase_composes}\")\n",
    "    print(f\"S√©paration syst√©matique: {tokens_split}\")\n",
    "    print(f\"Traitement contextuel: {tokens_context}\")\n",
    "    print(f\"Avantage: Garde 'Marie-Claire' et 'self-service', s√©pare 'tr√®s-bien'\\n\")\n",
    "    \n",
    "    # Test 3: Robustesse avec accents\n",
    "    print(\"‚úÖ **3. ROBUSTESSE AVEC LES ACCENTS FRAN√áAIS**\")\n",
    "    phrase_accents = \"√Ä No√´l, na√Øvement, j'ai achet√© un caf√© tr√®s-cher.\"\n",
    "    \n",
    "    tokeniseur_accents = TokeniseurFrancais(normaliser_accents=False)\n",
    "    tokens_accents = tokeniseur_accents.tokeniser(phrase_accents)\n",
    "    \n",
    "    print(f\"Phrase: {phrase_accents}\")\n",
    "    print(f\"Tokens: {tokens_accents}\")\n",
    "    print(f\"Avantage: Pr√©serve les accents fran√ßais correctement\\n\")\n",
    "    \n",
    "    # Test 4: Flexibilit√© de configuration\n",
    "    print(\"‚úÖ **4. FLEXIBILIT√â DE CONFIGURATION**\")\n",
    "    configurations = {\n",
    "        \"Analyse de sentiment\": TokeniseurFrancais(\n",
    "            developper_contractions=True, \n",
    "            garder_ponctuation=False,\n",
    "            gestion_majuscules='lower'\n",
    "        ),\n",
    "        \"Extraction d'entit√©s\": TokeniseurFrancais(\n",
    "            developper_contractions=False,\n",
    "            traiter_mots_composes='context',\n",
    "            gestion_majuscules='preserve'\n",
    "        ),\n",
    "        \"Recherche textuelle\": TokeniseurFrancais(\n",
    "            normaliser_accents=True,\n",
    "            gestion_majuscules='lower',\n",
    "            garder_ponctuation=False\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    phrase_demo = \"Marie-Claire n'aime pas les caf√© trop-chers !\"\n",
    "    print(f\"Phrase: {phrase_demo}\")\n",
    "    \n",
    "    for nom_config, tokeniseur_config in configurations.items():\n",
    "        tokens_config = tokeniseur_config.tokeniser(phrase_demo)\n",
    "        print(f\"{nom_config:20}: {tokens_config}\")\n",
    "    \n",
    "    print(f\"Avantage: Un seul tokeniseur adaptable √† diff√©rents cas d'usage\")\n",
    "\n",
    "analyser_avantages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Benchmark de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° **BENCHMARK DE PERFORMANCE**\n",
      "\n",
      "Dataset: 500 textes\n",
      "Iterations: 10 par m√©thode\n",
      "\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "üìä **R√âSULTATS (temps en secondes):**\n",
      "1. Split simple: 0.0008s\n",
      "2. TokeniseurFrancais (contextuel): 0.0475s\n",
      "3. TokeniseurFrancais (simple): 0.0523s\n",
      "4. NLTK: 0.1043s\n",
      "5. TokeniseurFrancais (d√©velopp√©): 0.1623s\n",
      "\n",
      "üìà **RATIOS (par rapport au plus rapide):**\n",
      "Split simple: 1.0x\n",
      "TokeniseurFrancais (contextuel): 56.6x\n",
      "TokeniseurFrancais (simple): 62.4x\n",
      "NLTK: 124.4x\n",
      "TokeniseurFrancais (d√©velopp√©): 193.6x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_performance():\n",
    "    \"\"\"Benchmark de performance des diff√©rents tokeniseurs\"\"\"\n",
    "    \n",
    "    # Cr√©er un dataset de test\n",
    "    textes_test = [\n",
    "        \"J'adore les self-services de Marie-Claire.\",\n",
    "        \"N'est-ce pas que c'est formidable aujourd'hui ?\",\n",
    "        \"Les anti-√¢ges sont tr√®s-efficaces, qu'est-ce que tu en penses ?\",\n",
    "        \"√Ä Saint-√âtienne, on trouve de tout, m√™me des caf√© haut-de-gamme.\",\n",
    "        \"C'est vraiment top-niveau, n'est-ce pas ?\"\n",
    "    ] * 100  # R√©p√©ter pour avoir assez de donn√©es\n",
    "    \n",
    "    print(f\"‚ö° **BENCHMARK DE PERFORMANCE**\\n\")\n",
    "    print(f\"Dataset: {len(textes_test)} textes\")\n",
    "    print(f\"Iterations: 10 par m√©thode\\n\")\n",
    "    \n",
    "    resultats = {}\n",
    "    \n",
    "    # Notre tokeniseur (diff√©rentes configurations)\n",
    "    tokeniseurs_test = {\n",
    "        'TokeniseurFrancais (simple)': TokeniseurFrancais(),\n",
    "        'TokeniseurFrancais (contextuel)': TokeniseurFrancais(traiter_mots_composes='context'),\n",
    "        'TokeniseurFrancais (d√©velopp√©)': TokeniseurFrancais(developper_contractions=True)\n",
    "    }\n",
    "    \n",
    "    for nom, tokeniseur in tokeniseurs_test.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):\n",
    "            for texte in textes_test:\n",
    "                tokeniseur.tokeniser(texte)\n",
    "        \n",
    "        temps_total = time.time() - start_time\n",
    "        resultats[nom] = temps_total\n",
    "    \n",
    "    # NLTK\n",
    "    if NLTK_AVAILABLE:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):\n",
    "            for texte in textes_test:\n",
    "                word_tokenize(texte, language='french')\n",
    "        \n",
    "        temps_total = time.time() - start_time\n",
    "        resultats['NLTK'] = temps_total\n",
    "    \n",
    "    # spaCy\n",
    "    if SPACY_AVAILABLE:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(10):\n",
    "            for texte in textes_test:\n",
    "                doc = nlp(texte)\n",
    "                [token.text for token in doc]\n",
    "        \n",
    "        temps_total = time.time() - start_time\n",
    "        resultats['spaCy'] = temps_total\n",
    "    \n",
    "    # Split simple\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(10):\n",
    "        for texte in textes_test:\n",
    "            texte.split()\n",
    "    \n",
    "    temps_total = time.time() - start_time\n",
    "    resultats['Split simple'] = temps_total\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(\"üìä **R√âSULTATS (temps en secondes):**\")\n",
    "    sorted_results = sorted(resultats.items(), key=lambda x: x[1])\n",
    "    \n",
    "    for i, (method, time_taken) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {method}: {time_taken:.4f}s\")\n",
    "    \n",
    "    # Calculer les ratios\n",
    "    fastest_time = min(resultats.values())\n",
    "    print(f\"\\nüìà **RATIOS (par rapport au plus rapide):**\")\n",
    "    for method, time_taken in sorted_results:\n",
    "        ratio = time_taken / fastest_time\n",
    "        print(f\"{method}: {ratio:.1f}x\")\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "# Lancer le benchmark\n",
    "resultats_benchmark = benchmark_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Exemples d'Utilisation Pratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è **EXEMPLES D'UTILISATION PRATIQUE**\n",
      "\n",
      "üìä **CAS 1: ANALYSE DE SENTIMENT**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: True\n",
      "   ‚Ä¢ Gestion majuscules: lower\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Avis client: J'ADORE ce produit ! Il n'y a rien √† redire, c'est parfait !\n",
      "Tokens pour ML: ['je', 'adore', 'ce', 'produit', 'il', 'ne', 'y', 'a', 'rien', 'a', 'redire', 'ce', 'est', 'parfait']\n",
      "Avantages: Tout en minuscules, contractions d√©velopp√©es, pas de ponctuation\n",
      "\n",
      "üè∑Ô∏è **CAS 2: EXTRACTION D'ENTIT√âS NOMM√âES**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "Texte: Marie-Claire Dubois habite √† Saint-√âtienne depuis 2020.\n",
      "Tokens: ['Marie', 'Claire', 'Dubois', 'habite', '√†', 'Saint', '√âtienne', 'depuis', '2020', '.']\n",
      "Avantages: Pr√©serve les noms compos√©s et les majuscules\n",
      "\n",
      "üîç **CAS 3: RECHERCHE ET INDEXATION**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: True\n",
      "   ‚Ä¢ Gestion majuscules: lower\n",
      "   ‚Ä¢ Traiter mots compos√©s: split\n",
      "Document: Les self-services fran√ßais offrent des caf√© de qualit√©.\n",
      "Tokens index√©s: ['les', 'self', 'services', 'francais', 'offrent', 'des', 'cafe', 'de', 'qualite']\n",
      "Avantages: Normalis√© pour la recherche, mots compos√©s s√©par√©s\n",
      "\n",
      "üí¨ **CAS 4: INTERFACE UTILISATEUR (CHATBOT)**\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "Message utilisateur: Bonjour ! Qu'est-ce que vous avez comme self-service ?\n",
      "Tokens: ['Bonjour', '!', \"Qu'est\", 'ce', 'que', 'vous', 'avez', 'comme', 'self', 'service', '?']\n",
      "Avantages: Pr√©serve le style naturel du message\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def exemples_cas_usage():\n",
    "    \"\"\"Exemples d'utilisation pour diff√©rents cas d'usage\"\"\"\n",
    "    \n",
    "    print(\"üõ†Ô∏è **EXEMPLES D'UTILISATION PRATIQUE**\\n\")\n",
    "    \n",
    "    # Cas 1: Analyse de sentiment\n",
    "    print(\"üìä **CAS 1: ANALYSE DE SENTIMENT**\")\n",
    "    tokeniseur_sentiment = TokeniseurFrancais(\n",
    "        developper_contractions=True,\n",
    "        garder_ponctuation=False,\n",
    "        gestion_majuscules='lower',\n",
    "        normaliser_accents=True\n",
    "    )\n",
    "    \n",
    "    avis_client = \"J'ADORE ce produit ! Il n'y a rien √† redire, c'est parfait !\"\n",
    "    tokens_sentiment = tokeniseur_sentiment.tokeniser(avis_client)\n",
    "    \n",
    "    print(f\"Avis client: {avis_client}\")\n",
    "    print(f\"Tokens pour ML: {tokens_sentiment}\")\n",
    "    print(\"Avantages: Tout en minuscules, contractions d√©velopp√©es, pas de ponctuation\\n\")\n",
    "    \n",
    "    # Cas 2: Extraction d'entit√©s nomm√©es\n",
    "    print(\"üè∑Ô∏è **CAS 2: EXTRACTION D'ENTIT√âS NOMM√âES**\")\n",
    "    tokeniseur_entites = TokeniseurFrancais(\n",
    "        developper_contractions=False,\n",
    "        traiter_mots_composes='context',\n",
    "        gestion_majuscules='preserve',\n",
    "        garder_ponctuation=True\n",
    "    )\n",
    "    \n",
    "    texte_entites = \"Marie-Claire Dubois habite √† Saint-√âtienne depuis 2020.\"\n",
    "    tokens_entites = tokeniseur_entites.tokeniser(texte_entites)\n",
    "    \n",
    "    print(f\"Texte: {texte_entites}\")\n",
    "    print(f\"Tokens: {tokens_entites}\")\n",
    "    print(\"Avantages: Pr√©serve les noms compos√©s et les majuscules\\n\")\n",
    "    \n",
    "    # Cas 3: Recherche et indexation\n",
    "    print(\"üîç **CAS 3: RECHERCHE ET INDEXATION**\")\n",
    "    tokeniseur_recherche = TokeniseurFrancais(\n",
    "        developper_contractions=True,\n",
    "        garder_ponctuation=False,\n",
    "        gestion_majuscules='lower',\n",
    "        normaliser_accents=True,\n",
    "        traiter_mots_composes='split'\n",
    "    )\n",
    "    \n",
    "    document = \"Les self-services fran√ßais offrent des caf√© de qualit√©.\"\n",
    "    tokens_recherche = tokeniseur_recherche.tokeniser(document)\n",
    "    \n",
    "    print(f\"Document: {document}\")\n",
    "    print(f\"Tokens index√©s: {tokens_recherche}\")\n",
    "    print(\"Avantages: Normalis√© pour la recherche, mots compos√©s s√©par√©s\\n\")\n",
    "    \n",
    "    # Cas 4: Interface utilisateur\n",
    "    print(\"üí¨ **CAS 4: INTERFACE UTILISATEUR (CHATBOT)**\")\n",
    "    tokeniseur_chatbot = TokeniseurFrancais(\n",
    "        developper_contractions=False,\n",
    "        garder_ponctuation=True,\n",
    "        gestion_majuscules='preserve',\n",
    "        traiter_mots_composes='context'\n",
    "    )\n",
    "    \n",
    "    message_user = \"Bonjour ! Qu'est-ce que vous avez comme self-service ?\"\n",
    "    tokens_chatbot = tokeniseur_chatbot.tokeniser(message_user)\n",
    "    \n",
    "    print(f\"Message utilisateur: {message_user}\")\n",
    "    print(f\"Tokens: {tokens_chatbot}\")\n",
    "    print(\"Avantages: Pr√©serve le style naturel du message\\n\")\n",
    "\n",
    "exemples_cas_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Zone d'Exp√©rimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zone libre pour tester le tokeniseur\n",
    "def tester_tokeniseur_personnalise(texte: str, **kwargs):\n",
    "    \"\"\"Fonction pour tester le tokeniseur avec des param√®tres personnalis√©s\"\"\"\n",
    "    \n",
    "    tokeniseur_test = TokeniseurFrancais(**kwargs)\n",
    "    analyse = tokeniseur_test.analyser(texte)\n",
    "    \n",
    "    print(f\"üß™ **TEST PERSONNALIS√â**\\n\")\n",
    "    print(f\"Configuration: {kwargs}\")\n",
    "    print(f\"Texte: {texte}\\n\")\n",
    "    \n",
    "    print(f\"üìä Statistiques:\")\n",
    "    print(f\"   ‚Ä¢ {analyse['nb_tokens_total']} tokens total\")\n",
    "    print(f\"   ‚Ä¢ {analyse['nb_mots']} mots\")\n",
    "    print(f\"   ‚Ä¢ {len(analyse['contractions'])} contractions\")\n",
    "    print(f\"   ‚Ä¢ {len(analyse['mots_composes'])} mots compos√©s\\n\")\n",
    "    \n",
    "    print(f\"üîç Tokens: {analyse['tokens']}\")\n",
    "    \n",
    "    return analyse\n",
    "\n",
    "# Exemples √† tester (modifiez √† votre guise)\n",
    "mon_texte_test = \"Marie-Claire n'aime pas les self-services trop-chers de Saint-√âtienne !\"\n",
    "\n",
    "# D√©commentez et modifiez pour tester :\n",
    "# tester_tokeniseur_personnalise(\n",
    "#     mon_texte_test,\n",
    "#     developper_contractions=True,\n",
    "#     traiter_mots_composes='context',\n",
    "#     gestion_majuscules='lower'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercices Pratiques\n",
    "\n",
    "### Exercice 1: Configuration Optimale\n",
    "Pour chaque cas d'usage, trouvez la configuration optimale :\n",
    "\n",
    "**Texte:** `\"Jean-Claude n'aime pas les e-commerces mal-con√ßus de Saint-Denis !\"`\n",
    "\n",
    "- **Classification de texte** : Quelle configuration pour maximiser la g√©n√©ralisation ?\n",
    "- **Extraction d'informations** : Comment pr√©server les entit√©s nomm√©es ?\n",
    "- **Analyse syntaxique** : Quels param√®tres pour garder la structure ?\n",
    "\n",
    "### Exercice 2: Extension du Dictionnaire\n",
    "Ajoutez des r√®gles pour g√©rer :\n",
    "- Les contractions qu√©b√©coises : `\"icitte\"`, `\"to√©\"`, etc.\n",
    "- Les mots compos√©s techniques : `\"machine-learning\"`, `\"deep-learning\"`\n",
    "- Les expressions famili√®res : `\"y'a\"`, `\"qu'est-c'que\"`\n",
    "\n",
    "### Exercice 3: Optimisation\n",
    "Optimisez le tokeniseur pour traiter :\n",
    "- 1 million de tweets fran√ßais\n",
    "- Des documents juridiques avec beaucoup de r√©f√©rences\n",
    "- Du code source m√©lang√© fran√ßais/anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù **EXERCICES PRATIQUES**\n",
      "\n",
      "Texte d'exercice: Jean-Claude n'aime pas les e-commerces mal-con√ßus de Saint-Denis !\n",
      "\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: True\n",
      "   ‚Ä¢ Garder ponctuation: False\n",
      "   ‚Ä¢ Normaliser accents: True\n",
      "   ‚Ä¢ Gestion majuscules: lower\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Classification      : ['jean-claude', 'ne', 'aime', 'pas', 'les', 'e-commerces', 'mal-concus', 'de', 'saint-denis']\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: context\n",
      "Extraction entit√©s  : ['Jean', 'Claude', \"n'aime\", 'pas', 'les', 'e', 'commerces', 'mal', 'con√ßus', 'de', 'Saint', 'Denis', '!']\n",
      "üá´üá∑ TokeniseurFrancais initialis√©:\n",
      "   ‚Ä¢ D√©velopper contractions: False\n",
      "   ‚Ä¢ Garder ponctuation: True\n",
      "   ‚Ä¢ Normaliser accents: False\n",
      "   ‚Ä¢ Gestion majuscules: preserve\n",
      "   ‚Ä¢ Traiter mots compos√©s: keep\n",
      "Analyse syntaxique  : ['Jean', 'Claude', \"n'aime\", 'pas', 'les', 'e', 'commerces', 'mal', 'con√ßus', 'de', 'Saint', 'Denis', '!']\n",
      "\n",
      "üí° Analysez les diff√©rences et choisissez la meilleure configuration pour chaque cas !\n"
     ]
    }
   ],
   "source": [
    "# Zone pour les exercices\n",
    "print(\"üìù **EXERCICES PRATIQUES**\\n\")\n",
    "\n",
    "# Exercice 1: Configurations optimales\n",
    "texte_exercice = \"Jean-Claude n'aime pas les e-commerces mal-con√ßus de Saint-Denis !\"\n",
    "\n",
    "configs_test = {\n",
    "    \"Classification\": {\n",
    "        'developper_contractions': True,\n",
    "        'garder_ponctuation': False,\n",
    "        'gestion_majuscules': 'lower',\n",
    "        'normaliser_accents': True\n",
    "    },\n",
    "    \"Extraction entit√©s\": {\n",
    "        'developper_contractions': False,\n",
    "        'traiter_mots_composes': 'context',\n",
    "        'gestion_majuscules': 'preserve'\n",
    "    },\n",
    "    \"Analyse syntaxique\": {\n",
    "        'developper_contractions': False,\n",
    "        'garder_ponctuation': True,\n",
    "        'gestion_majuscules': 'preserve',\n",
    "        'traiter_mots_composes': 'keep'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Texte d'exercice: {texte_exercice}\\n\")\n",
    "\n",
    "for nom_config, config in configs_test.items():\n",
    "    tokeniseur_ex = TokeniseurFrancais(**config)\n",
    "    tokens_ex = tokeniseur_ex.tokeniser(texte_exercice)\n",
    "    print(f\"{nom_config:20}: {tokens_ex}\")\n",
    "\n",
    "print(\"\\nüí° Analysez les diff√©rences et choisissez la meilleure configuration pour chaque cas !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã R√©sum√© et Bonnes Pratiques\n",
    "\n",
    "### üéØ Ce que vous avez appris :\n",
    "\n",
    "1. **Sp√©cificit√©s du fran√ßais** : contractions, mots compos√©s, accents\n",
    "2. **Architecture modulaire** : classe configurable selon le besoin\n",
    "3. **Gestion contextuelle** : d√©cisions intelligentes selon le contenu\n",
    "4. **Performance √©quilibr√©e** : entre pr√©cision et vitesse\n",
    "\n",
    "### üí° Bonnes pratiques :\n",
    "\n",
    "#### ‚úÖ √Ä faire :\n",
    "- **Adapter la configuration** au cas d'usage\n",
    "- **Tester sur vos donn√©es** r√©elles\n",
    "- **Mesurer l'impact** sur les performances finales\n",
    "- **Documenter vos choix** de configuration\n",
    "- **Valider manuellement** sur des √©chantillons\n",
    "\n",
    "#### ‚ùå √Ä √©viter :\n",
    "- **Configuration unique** pour tous les cas\n",
    "- **Sur-optimisation** sur un petit dataset\n",
    "- **Ignorer les sp√©cificit√©s** de votre domaine\n",
    "- **Performance sans qualit√©** (ou l'inverse)\n",
    "- **Modifications sans tests** de r√©gression\n",
    "\n",
    "### üöÄ Pour aller plus loin :\n",
    "\n",
    "1. **Int√©gration avec d'autres outils** : spaCy pipelines, scikit-learn\n",
    "2. **Tokenisation sous-mots** : BPE, SentencePiece pour les mod√®les modernes\n",
    "3. **Adaptation au domaine** : m√©dical, juridique, technique\n",
    "4. **Multilinguisme** : extension √† d'autres langues\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations !** Vous avez cr√©√© un tokeniseur personnalis√© adapt√© au fran√ßais !\n",
    "\n",
    "**‚û°Ô∏è Suite du cours :** [Module 2.4 - Techniques Avanc√©es](../module2_avance.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
