{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Pipeline de Preprocessing Complet\n",
        "\n",
        "**Module 2 : Preprocessing et Tokenisation - Synth√®se Finale**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objectifs de ce Notebook\n",
        "\n",
        "Ce notebook constitue la **synth√®se finale** du Module 2. Vous allez :\n",
        "\n",
        "- üîß **Construire un pipeline complet** int√©grant toutes les techniques\n",
        "- ‚öôÔ∏è **Cr√©er une classe configurable** et r√©utilisable\n",
        "- üìä **Mesurer l'impact** de chaque √©tape\n",
        "- üéØ **Adapter le pipeline** selon le cas d'usage\n",
        "- üöÄ **Optimiser les performances** pour la production\n",
        "- ‚úÖ **Valider la qualit√©** du preprocessing\n",
        "\n",
        "## üó∫Ô∏è Architecture du Pipeline\n",
        "\n",
        "```\n",
        "üìù Texte Brut\n",
        "     ‚Üì\n",
        "üßπ 1. Nettoyage (casse, ponctuation, URLs, emojis)\n",
        "     ‚Üì  \n",
        "üîß 2. Normalisation (accents, espaces, entit√©s)\n",
        "     ‚Üì\n",
        "‚úÇÔ∏è 3. Tokenisation (d√©coupage en mots)\n",
        "     ‚Üì\n",
        "üõë 4. Filtrage (stopwords, longueur minimale)\n",
        "     ‚Üì\n",
        "üå± 5. Lemmatisation/Stemming (forme canonique)\n",
        "     ‚Üì\n",
        "‚úÖ Texte Pr√™t pour ML/NLP\n",
        "```\n",
        "\n",
        "## üí° Pourquoi un Pipeline ?\n",
        "\n",
        "**Avantages :**\n",
        "- üîÑ **Reproductible** : M√™me traitement √† chaque fois\n",
        "- ‚öôÔ∏è **Configurable** : Adaptation selon le contexte\n",
        "- üìà **Mesurable** : M√©triques √† chaque √©tape\n",
        "- üöÄ **Optimis√©** : Performance pour la production\n",
        "- üß™ **Testable** : Validation et debugging facilit√©s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation et imports\n",
        "# !pip install nltk spacy regex pandas matplotlib seaborn\n",
        "# !python -m spacy download fr_core_news_sm\n",
        "\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from dataclasses import dataclass, field\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# Data science\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP Libraries\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import SnowballStemmer\n",
        "    # Download required NLTK data\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    NLTK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è NLTK non disponible, certaines fonctionnalit√©s seront limit√©es\")\n",
        "    NLTK_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    nlp_fr = spacy.load('fr_core_news_sm')\n",
        "    SPACY_AVAILABLE = True\n",
        "except (ImportError, OSError):\n",
        "    print(\"‚ö†Ô∏è spaCy fran√ßais non disponible, utilisation du mode fallback\")\n",
        "    SPACY_AVAILABLE = False\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Configuration termin√©e !\")\n",
        "print(f\"üìö NLTK disponible : {'‚úÖ' if NLTK_AVAILABLE else '‚ùå'}\")\n",
        "print(f\"‚ö° spaCy fran√ßais disponible : {'‚úÖ' if SPACY_AVAILABLE else '‚ùå'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìä 1. Configuration du Pipeline\n",
        "\n",
        "Cr√©ons d'abord une classe de configuration pour param√©trer notre pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PipelineConfig:\n",
        "    \"\"\"Configuration du pipeline de preprocessing\"\"\"\n",
        "    \n",
        "    # === NETTOYAGE ===\n",
        "    lowercase: bool = True\n",
        "    remove_punctuation: bool = True\n",
        "    remove_numbers: bool = False\n",
        "    remove_urls: bool = True\n",
        "    remove_emails: bool = True\n",
        "    remove_mentions: bool = True  # @username\n",
        "    remove_hashtags: bool = True  # #hashtag\n",
        "    \n",
        "    # === EMOJIS ===\n",
        "    emoji_strategy: str = \"remove\"  # \"remove\", \"convert\", \"keep\"\n",
        "    \n",
        "    # === NORMALISATION ===\n",
        "    normalize_whitespace: bool = True\n",
        "    normalize_accents: bool = False  # Garder les accents fran√ßais par d√©faut\n",
        "    normalize_entities: bool = True  # Dates, montants, etc.\n",
        "    \n",
        "    # === TOKENISATION ===\n",
        "    tokenizer: str = \"spacy\"  # \"spacy\", \"nltk\", \"regex\", \"split\"\n",
        "    handle_contractions: bool = True  # j' ‚Üí j'\n",
        "    \n",
        "    # === FILTRAGE ===\n",
        "    remove_stopwords: bool = True\n",
        "    custom_stopwords: List[str] = field(default_factory=list)\n",
        "    min_token_length: int = 2\n",
        "    max_token_length: int = 50\n",
        "    \n",
        "    # === LEMMATISATION/STEMMING ===\n",
        "    lemmatize: bool = True\n",
        "    stem: bool = False  # Alternative au lemmatize\n",
        "    \n",
        "    # === PERFORMANCE ===\n",
        "    batch_size: int = 1000\n",
        "    n_jobs: int = 1  # Pour traitement parall√®le futur\n",
        "    \n",
        "    # === DEBUG ===\n",
        "    verbose: bool = False\n",
        "    save_intermediate: bool = False\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validation de la configuration\"\"\"\n",
        "        if self.lemmatize and self.stem:\n",
        "            print(\"‚ö†Ô∏è Lemmatize et stem activ√©s simultan√©ment. Priorit√© √† lemmatize.\")\n",
        "            self.stem = False\n",
        "        \n",
        "        if self.tokenizer == \"spacy\" and not SPACY_AVAILABLE:\n",
        "            print(\"‚ö†Ô∏è spaCy non disponible, basculement vers NLTK\")\n",
        "            self.tokenizer = \"nltk\" if NLTK_AVAILABLE else \"regex\"\n",
        "    \n",
        "    @classmethod\n",
        "    def for_domain(cls, domain: str) -> 'PipelineConfig':\n",
        "        \"\"\"Configurations pr√©d√©finies par domaine\"\"\"\n",
        "        configs = {\n",
        "            \"general\": cls(),\n",
        "            \n",
        "            \"social_media\": cls(\n",
        "                remove_mentions=False,  # @user peut √™tre important\n",
        "                remove_hashtags=False,  # #topic peut √™tre important\n",
        "                emoji_strategy=\"convert\",  # Convertir emojis en sentiment\n",
        "                normalize_entities=True\n",
        "            ),\n",
        "            \n",
        "            \"ecommerce\": cls(\n",
        "                remove_numbers=False,  # Prix et quantit√©s importants\n",
        "                normalize_entities=False,  # Garder les montants\n",
        "                remove_urls=True,  # Nettoyer les liens\n",
        "                lemmatize=True\n",
        "            ),\n",
        "            \n",
        "            \"news\": cls(\n",
        "                normalize_entities=True,  # Anonymiser dates/lieux\n",
        "                remove_urls=True,\n",
        "                lemmatize=True,\n",
        "                min_token_length=3\n",
        "            ),\n",
        "            \n",
        "            \"academic\": cls(\n",
        "                remove_punctuation=False,  # Ponctuation peut √™tre importante\n",
        "                normalize_accents=False,  # Garder pr√©cision\n",
        "                lemmatize=True,\n",
        "                min_token_length=3\n",
        "            ),\n",
        "            \n",
        "            \"fast\": cls(\n",
        "                tokenizer=\"split\",\n",
        "                lemmatize=False,\n",
        "                stem=True,  # Plus rapide que lemmatize\n",
        "                normalize_entities=False\n",
        "            )\n",
        "        }\n",
        "        \n",
        "        return configs.get(domain, configs[\"general\"])\n",
        "    \n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Export configuration en dictionnaire\"\"\"\n",
        "        return {\n",
        "            field.name: getattr(self, field.name) \n",
        "            for field in self.__dataclass_fields__.values()\n",
        "        }\n",
        "    \n",
        "    def save(self, filepath: str):\n",
        "        \"\"\"Sauvegarde la configuration\"\"\"\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Test des configurations\n",
        "print(\"üß™ Configurations disponibles :\")\n",
        "for domain in [\"general\", \"social_media\", \"ecommerce\", \"news\", \"fast\"]:\n",
        "    config = PipelineConfig.for_domain(domain)\n",
        "    print(f\"  ‚Ä¢ {domain:<12} : tokenizer={config.tokenizer}, lemmatize={config.lemmatize}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîß 2. Classes de Preprocessing\n",
        "\n",
        "Cr√©ons les classes pour chaque √©tape du pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextCleaner:\n",
        "    \"\"\"Nettoyage et normalisation du texte\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PipelineConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # Patterns regex pour nettoyage\n",
        "        self.patterns = {\n",
        "            'url': r'https?://[^\\s<>\"\\[\\]{}|\\\\^`]+|www\\.[^\\s<>\"\\[\\]{}|\\\\^`]+',\n",
        "            'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "            'mention': r'@\\w+',\n",
        "            'hashtag': r'#\\w+',\n",
        "            'phone': r'\\+?\\d{1,3}[-.]?\\(?\\d{1,3}\\)?[-.]?\\d{1,4}[-.]?\\d{1,4}[-.]?\\d{1,9}',\n",
        "            'date': r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b|\\b\\d{1,2}\\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\\s+\\d{4}\\b',\n",
        "            'money': r'\\d+[.,]?\\d*\\s*(?:‚Ç¨|euros?|\\$|dollars?)\\b|\\b(?:‚Ç¨|\\$)\\d+[.,]?\\d*',\n",
        "            'emoji': r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002600-\\U000026FF\\U00002700-\\U000027BF]'\n",
        "        }\n",
        "        \n",
        "        # Dictionnaire emoji ‚Üí sentiment (simplifi√©)\n",
        "        self.emoji_sentiment = {\n",
        "            'üòÄ': ' positif ', 'üòÅ': ' positif ', 'üòÇ': ' tr√®s positif ', 'üòÉ': ' positif ',\n",
        "            'üòÑ': ' positif ', 'üòÖ': ' positif ', 'üòä': ' positif ', 'üòå': ' positif ',\n",
        "            'üòç': ' tr√®s positif ', 'üòò': ' positif ', 'üòã': ' positif ', 'üòé': ' positif ',\n",
        "            'üò¢': ' n√©gatif ', 'üò≠': ' tr√®s n√©gatif ', 'üò°': ' tr√®s n√©gatif ', 'üò†': ' n√©gatif ',\n",
        "            'üòí': ' n√©gatif ', 'üòû': ' n√©gatif ', 'üòî': ' n√©gatif ', 'üòü': ' n√©gatif ',\n",
        "            'üëç': ' positif ', 'üëé': ' n√©gatif ', '‚ù§Ô∏è': ' tr√®s positif ', 'üíî': ' n√©gatif ',\n",
        "            'üî•': ' positif ', 'üíØ': ' tr√®s positif ', '‚ú®': ' positif ', '‚≠ê': ' positif '\n",
        "        }\n",
        "    \n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Applique toutes les √©tapes de nettoyage\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        \n",
        "        result = text\n",
        "        \n",
        "        # 1. Gestion des emojis AVANT suppression ponctuation\n",
        "        if self.config.emoji_strategy == \"convert\":\n",
        "            for emoji, sentiment in self.emoji_sentiment.items():\n",
        "                result = result.replace(emoji, sentiment)\n",
        "            # Supprimer les emojis restants\n",
        "            result = re.sub(self.patterns['emoji'], '', result)\n",
        "        elif self.config.emoji_strategy == \"remove\":\n",
        "            result = re.sub(self.patterns['emoji'], '', result)\n",
        "        # Si \"keep\", on ne fait rien\n",
        "        \n",
        "        # 2. Suppression √©l√©ments web et entit√©s\n",
        "        if self.config.remove_urls:\n",
        "            result = re.sub(self.patterns['url'], ' URL ', result)\n",
        "        \n",
        "        if self.config.remove_emails:\n",
        "            result = re.sub(self.patterns['email'], ' EMAIL ', result)\n",
        "        \n",
        "        if self.config.remove_mentions:\n",
        "            result = re.sub(self.patterns['mention'], ' MENTION ', result)\n",
        "        \n",
        "        if self.config.remove_hashtags:\n",
        "            result = re.sub(self.patterns['hashtag'], ' HASHTAG ', result)\n",
        "        \n",
        "        # 3. Normalisation entit√©s (si activ√©e)\n",
        "        if self.config.normalize_entities:\n",
        "            result = re.sub(self.patterns['phone'], ' TELEPHONE ', result)\n",
        "            result = re.sub(self.patterns['date'], ' DATE ', result, flags=re.IGNORECASE)\n",
        "            result = re.sub(self.patterns['money'], ' MONTANT ', result, flags=re.IGNORECASE)\n",
        "        \n",
        "        # 4. Minuscules\n",
        "        if self.config.lowercase:\n",
        "            result = result.lower()\n",
        "        \n",
        "        # 5. Suppression ponctuation\n",
        "        if self.config.remove_punctuation:\n",
        "            # Garder les apostrophes pour les contractions fran√ßaises\n",
        "            if self.config.handle_contractions:\n",
        "                result = re.sub(r\"[^\\w\\s'√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ø√ß-]\", ' ', result)\n",
        "            else:\n",
        "                result = re.sub(r'[^\\w\\s]', ' ', result)\n",
        "        \n",
        "        # 6. Suppression nombres (si demand√©)\n",
        "        if self.config.remove_numbers:\n",
        "            result = re.sub(r'\\d+', '', result)\n",
        "        \n",
        "        # 7. Normalisation accents (si demand√©)\n",
        "        if self.config.normalize_accents:\n",
        "            import unicodedata\n",
        "            result = unicodedata.normalize('NFD', result)\n",
        "            result = ''.join(c for c in result if unicodedata.category(c) != 'Mn')\n",
        "        \n",
        "        # 8. Normalisation espaces\n",
        "        if self.config.normalize_whitespace:\n",
        "            result = re.sub(r'\\s+', ' ', result).strip()\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Test du nettoyeur\n",
        "config_test = PipelineConfig()\n",
        "cleaner = TextCleaner(config_test)\n",
        "\n",
        "print(\"üß™ Test du nettoyeur de texte :\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "texte_test = \"RT @user: Super!!! üòç https://bit.ly/xyz J'adore ce produit, co√ªte 50‚Ç¨ le 15/03/2024\"\n",
        "print(f\"üìù Original : {texte_test}\")\n",
        "print(f\"üßπ Nettoy√© : {cleaner.clean_text(texte_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextTokenizer:\n",
        "    \"\"\"Tokenisation du texte\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PipelineConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialiser le tokenizer selon la config\n",
        "        if config.tokenizer == \"spacy\" and SPACY_AVAILABLE:\n",
        "            self.nlp = nlp_fr\n",
        "        elif config.tokenizer == \"nltk\" and NLTK_AVAILABLE:\n",
        "            self.nltk_available = True\n",
        "        else:\n",
        "            # Fallback vers regex\n",
        "            if config.handle_contractions:\n",
        "                # Pattern qui g√®re les contractions fran√ßaises\n",
        "                self.token_pattern = r\"\\b\\w+(?:'\\w+)?\\b|\\b\\w+\\b\"\n",
        "            else:\n",
        "                self.token_pattern = r'\\w+'\n",
        "    \n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenise le texte selon la m√©thode configur√©e\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return []\n",
        "        \n",
        "        tokens = []\n",
        "        \n",
        "        if self.config.tokenizer == \"spacy\" and SPACY_AVAILABLE:\n",
        "            doc = self.nlp(text)\n",
        "            tokens = [token.text for token in doc if not token.is_space]\n",
        "        \n",
        "        elif self.config.tokenizer == \"nltk\" and NLTK_AVAILABLE:\n",
        "            tokens = word_tokenize(text, language='french')\n",
        "        \n",
        "        elif self.config.tokenizer == \"split\":\n",
        "            tokens = text.split()\n",
        "        \n",
        "        else:  # regex fallback\n",
        "            tokens = re.findall(self.token_pattern, text)\n",
        "        \n",
        "        return [token for token in tokens if token and token.strip()]\n",
        "\n",
        "# Test du tokenizer\n",
        "tokenizer = TextTokenizer(config_test)\n",
        "\n",
        "print(\"üß™ Test du tokenizer :\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "texte_clean = \"j adore ce produit coute montant le date\"\n",
        "tokens = tokenizer.tokenize(texte_clean)\n",
        "print(f\"üìù Texte : {texte_clean}\")\n",
        "print(f\"‚úÇÔ∏è Tokens : {tokens}\")\n",
        "print(f\"üìä Nombre de tokens : {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextFilter:\n",
        "    \"\"\"Filtrage des tokens (stopwords, longueur, etc.)\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PipelineConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # Charger les stopwords fran√ßais\n",
        "        self.stopwords = set()\n",
        "        \n",
        "        if config.remove_stopwords:\n",
        "            # Stopwords de base fran√ßais\n",
        "            base_stopwords = {\n",
        "                'le', 'de', 'et', '√†', 'un', 'il', '√™tre', 'avoir', 'que', 'pour',\n",
        "                'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout',\n",
        "                'plus', 'par', 'grand', 'en', 'son', 'que', 'ce', 'lui', 'au', 'du',\n",
        "                'des', 'la', 'les', 'je', 'tu', 'nous', 'vous', 'ils', 'elles',\n",
        "                'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'sa', 'ses', 'notre', 'nos',\n",
        "                'votre', 'vos', 'leur', 'leurs', 'est', 'sont', '√©tait', '√©taient',\n",
        "                'ai', 'as', 'a', 'avons', 'avez', 'ont', '√©t√©', 'faire', 'fait',\n",
        "                'faire', 'dis', 'dit', 'cette', 'ces', 'ou', 'o√π', 'mais', 'donc',\n",
        "                'car', 'si', 'comme', 'quand', 'bien', 'tr√®s', 'aussi', 'alors',\n",
        "                'ici', 'l√†', 'maintenant', 'aujourd', 'hui', 'demain', 'hier'\n",
        "            }\n",
        "            \n",
        "            # Ajouter NLTK stopwords si disponible\n",
        "            if NLTK_AVAILABLE:\n",
        "                try:\n",
        "                    nltk_stopwords = set(stopwords.words('french'))\n",
        "                    base_stopwords.update(nltk_stopwords)\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            self.stopwords = base_stopwords\n",
        "            \n",
        "            # Ajouter stopwords personnalis√©s\n",
        "            if config.custom_stopwords:\n",
        "                self.stopwords.update(config.custom_stopwords)\n",
        "    \n",
        "    def filter_tokens(self, tokens: List[str]) -> List[str]:\n",
        "        \"\"\"Filtre les tokens selon les crit√®res configur√©s\"\"\"\n",
        "        if not tokens:\n",
        "            return []\n",
        "        \n",
        "        filtered = []\n",
        "        \n",
        "        for token in tokens:\n",
        "            token_clean = token.lower().strip()\n",
        "            \n",
        "            # V√©rifier longueur\n",
        "            if len(token_clean) < self.config.min_token_length:\n",
        "                continue\n",
        "            \n",
        "            if len(token_clean) > self.config.max_token_length:\n",
        "                continue\n",
        "            \n",
        "            # V√©rifier stopwords\n",
        "            if self.config.remove_stopwords and token_clean in self.stopwords:\n",
        "                continue\n",
        "            \n",
        "            # √âviter les tokens vides ou seulement ponctuation\n",
        "            if not re.search(r'\\w', token_clean):\n",
        "                continue\n",
        "            \n",
        "            filtered.append(token_clean)\n",
        "        \n",
        "        return filtered\n",
        "\n",
        "# Test du filtre\n",
        "text_filter = TextFilter(config_test)\n",
        "\n",
        "print(\"üß™ Test du filtre :\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üìä Nombre de stopwords fran√ßais : {len(text_filter.stopwords)}\")\n",
        "print(f\"üî§ Exemples stopwords : {list(text_filter.stopwords)[:10]}\")\n",
        "\n",
        "tokens_test = ['j', 'adore', 'ce', 'produit', 'coute', 'montant', 'le', 'date', 'et', 'url']\n",
        "tokens_filtered = text_filter.filter_tokens(tokens_test)\n",
        "print(f\"\\nüìù Tokens avant : {tokens_test}\")\n",
        "print(f\"üõë Tokens apr√®s filtrage : {tokens_filtered}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextLemmatizer:\n",
        "    \"\"\"Lemmatisation et stemming\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PipelineConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialiser lemmatizer/stemmer\n",
        "        if config.lemmatize and SPACY_AVAILABLE:\n",
        "            self.nlp = nlp_fr\n",
        "            self.method = \"spacy_lemma\"\n",
        "        elif config.stem and NLTK_AVAILABLE:\n",
        "            self.stemmer = SnowballStemmer('french')\n",
        "            self.method = \"nltk_stem\"\n",
        "        else:\n",
        "            # Lemmatisation basique avec dictionnaire\n",
        "            self.basic_lemmas = {\n",
        "                # Verbes fr√©quents\n",
        "                'suis': '√™tre', 'es': '√™tre', 'est': '√™tre', 'sommes': '√™tre', '√™tes': '√™tre', 'sont': '√™tre',\n",
        "                'ai': 'avoir', 'as': 'avoir', 'a': 'avoir', 'avons': 'avoir', 'avez': 'avoir', 'ont': 'avoir',\n",
        "                'fais': 'faire', 'fait': 'faire', 'faisons': 'faire', 'faites': 'faire', 'font': 'faire',\n",
        "                'vais': 'aller', 'vas': 'aller', 'va': 'aller', 'allons': 'aller', 'allez': 'aller', 'vont': 'aller',\n",
        "                # Pluriels simples\n",
        "                'chats': 'chat', 'chiens': 'chien', 'voitures': 'voiture', 'maisons': 'maison',\n",
        "                'produits': 'produit', 'services': 'service', 'clients': 'client',\n",
        "                # Adjectifs\n",
        "                'beaux': 'beau', 'belles': 'beau', 'grands': 'grand', 'grandes': 'grand',\n",
        "                'bons': 'bon', 'bonnes': 'bon', 'nouveaux': 'nouveau', 'nouvelles': 'nouveau'\n",
        "            }\n",
        "            self.method = \"basic_lemma\"\n",
        "    \n",
        "    def lemmatize_tokens(self, tokens: List[str]) -> List[str]:\n",
        "        \"\"\"Applique lemmatisation ou stemming aux tokens\"\"\"\n",
        "        if not tokens:\n",
        "            return []\n",
        "        \n",
        "        if not (self.config.lemmatize or self.config.stem):\n",
        "            return tokens\n",
        "        \n",
        "        result = []\n",
        "        \n",
        "        if self.method == \"spacy_lemma\":\n",
        "            # Joindre les tokens pour traitement spaCy\n",
        "            text = \" \".join(tokens)\n",
        "            doc = self.nlp(text)\n",
        "            result = [token.lemma_ for token in doc if not token.is_space]\n",
        "        \n",
        "        elif self.method == \"nltk_stem\":\n",
        "            result = [self.stemmer.stem(token) for token in tokens]\n",
        "        \n",
        "        else:  # basic_lemma\n",
        "            for token in tokens:\n",
        "                lemma = self.basic_lemmas.get(token.lower(), token)\n",
        "                result.append(lemma)\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Test du lemmatizer\n",
        "lemmatizer = TextLemmatizer(config_test)\n",
        "\n",
        "print(\"üß™ Test du lemmatizer :\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"üîß M√©thode utilis√©e : {lemmatizer.method}\")\n",
        "\n",
        "tokens_test = ['adore', 'produit', 'produits', 'voitures', 'suis', 'fait']\n",
        "tokens_lemmatized = lemmatizer.lemmatize_tokens(tokens_test)\n",
        "print(f\"\\nüìù Tokens avant : {tokens_test}\")\n",
        "print(f\"üå± Tokens lemmatis√©s : {tokens_lemmatized}\")\n",
        "\n",
        "# Comparaison mot par mot\n",
        "print(\"\\nüîç Comparaison d√©taill√©e :\")\n",
        "for original, lemma in zip(tokens_test, tokens_lemmatized):\n",
        "    if original != lemma:\n",
        "        print(f\"  ‚Ä¢ {original} ‚Üí {lemma}\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ {original} (inchang√©)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üèóÔ∏è 3. Pipeline Principal\n",
        "\n",
        "Assemblons maintenant toutes les √©tapes dans une classe pipeline compl√®te :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ProcessingStats:\n",
        "    \"\"\"Statistiques du preprocessing\"\"\"\n",
        "    original_text: str = \"\"\n",
        "    final_tokens: List[str] = field(default_factory=list)\n",
        "    processing_time: float = 0.0\n",
        "    \n",
        "    # Compteurs par √©tape\n",
        "    chars_before: int = 0\n",
        "    chars_after_cleaning: int = 0\n",
        "    tokens_after_tokenization: int = 0\n",
        "    tokens_after_filtering: int = 0\n",
        "    tokens_after_lemmatization: int = 0\n",
        "    \n",
        "    # M√©triques calcul√©es\n",
        "    @property\n",
        "    def char_reduction_percent(self) -> float:\n",
        "        if self.chars_before == 0:\n",
        "            return 0.0\n",
        "        return round((self.chars_before - self.chars_after_cleaning) / self.chars_before * 100, 1)\n",
        "    \n",
        "    @property\n",
        "    def token_reduction_percent(self) -> float:\n",
        "        if self.tokens_after_tokenization == 0:\n",
        "            return 0.0\n",
        "        return round((self.tokens_after_tokenization - len(self.final_tokens)) / self.tokens_after_tokenization * 100, 1)\n",
        "    \n",
        "    def summary(self) -> str:\n",
        "        return f\"\"\"üìä Statistiques de preprocessing :\n",
        "‚Ä¢ Temps de traitement : {self.processing_time:.3f}s\n",
        "‚Ä¢ R√©duction caract√®res : {self.char_reduction_percent}% ({self.chars_before} ‚Üí {self.chars_after_cleaning})\n",
        "‚Ä¢ R√©duction tokens : {self.token_reduction_percent}% ({self.tokens_after_tokenization} ‚Üí {len(self.final_tokens)})\n",
        "‚Ä¢ Pipeline : {self.tokens_after_tokenization} ‚Üí {self.tokens_after_filtering} ‚Üí {self.tokens_after_lemmatization}\n",
        "‚Ä¢ Tokens finaux : {len(self.final_tokens)}\"\"\"\n",
        "\n",
        "\n",
        "class PreprocessingPipeline:\n",
        "    \"\"\"Pipeline complet de preprocessing\"\"\"\n",
        "    \n",
        "    def __init__(self, config: PipelineConfig):\n",
        "        self.config = config\n",
        "        \n",
        "        # Initialiser les composants\n",
        "        self.cleaner = TextCleaner(config)\n",
        "        self.tokenizer = TextTokenizer(config)\n",
        "        self.filter = TextFilter(config)\n",
        "        self.lemmatizer = TextLemmatizer(config)\n",
        "        \n",
        "        # Historique des traitements\n",
        "        self.processing_history = []\n",
        "    \n",
        "    def process_text(self, text: str, return_stats: bool = False) -> Union[List[str], Tuple[List[str], ProcessingStats]]:\n",
        "        \"\"\"\n",
        "        Traite un texte √† travers tout le pipeline\n",
        "        \n",
        "        Args:\n",
        "            text: Texte √† traiter\n",
        "            return_stats: Si True, retourne aussi les statistiques\n",
        "        \n",
        "        Returns:\n",
        "            Liste des tokens finaux, optionnellement avec statistiques\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Initialiser les stats\n",
        "        stats = ProcessingStats(original_text=text)\n",
        "        stats.chars_before = len(text)\n",
        "        \n",
        "        # √âtape 1: Nettoyage\n",
        "        cleaned_text = self.cleaner.clean_text(text)\n",
        "        stats.chars_after_cleaning = len(cleaned_text)\n",
        "        \n",
        "        if self.config.verbose:\n",
        "            print(f\"üßπ Apr√®s nettoyage : {cleaned_text}\")\n",
        "        \n",
        "        # √âtape 2: Tokenisation\n",
        "        tokens = self.tokenizer.tokenize(cleaned_text)\n",
        "        stats.tokens_after_tokenization = len(tokens)\n",
        "        \n",
        "        if self.config.verbose:\n",
        "            print(f\"‚úÇÔ∏è Apr√®s tokenisation : {tokens}\")\n",
        "        \n",
        "        # √âtape 3: Filtrage\n",
        "        filtered_tokens = self.filter.filter_tokens(tokens)\n",
        "        stats.tokens_after_filtering = len(filtered_tokens)\n",
        "        \n",
        "        if self.config.verbose:\n",
        "            print(f\"üõë Apr√®s filtrage : {filtered_tokens}\")\n",
        "        \n",
        "        # √âtape 4: Lemmatisation\n",
        "        final_tokens = self.lemmatizer.lemmatize_tokens(filtered_tokens)\n",
        "        stats.tokens_after_lemmatization = len(final_tokens)\n",
        "        stats.final_tokens = final_tokens\n",
        "        \n",
        "        if self.config.verbose:\n",
        "            print(f\"üå± Apr√®s lemmatisation : {final_tokens}\")\n",
        "        \n",
        "        # Finaliser les stats\n",
        "        stats.processing_time = time.time() - start_time\n",
        "        \n",
        "        # Sauvegarder l'historique si demand√©\n",
        "        if self.config.save_intermediate:\n",
        "            self.processing_history.append({\n",
        "                'original': text,\n",
        "                'cleaned': cleaned_text,\n",
        "                'tokens': tokens,\n",
        "                'filtered': filtered_tokens,\n",
        "                'final': final_tokens,\n",
        "                'stats': stats\n",
        "            })\n",
        "        \n",
        "        if return_stats:\n",
        "            return final_tokens, stats\n",
        "        return final_tokens\n",
        "    \n",
        "    def process_batch(self, texts: List[str], show_progress: bool = True) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Traite une liste de textes\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for i, text in enumerate(texts):\n",
        "            if show_progress and i % 100 == 0:\n",
        "                print(f\"üìà Progression : {i}/{len(texts)} textes trait√©s\")\n",
        "            \n",
        "            tokens = self.process_text(text)\n",
        "            results.append(tokens)\n",
        "        \n",
        "        if show_progress:\n",
        "            print(f\"‚úÖ Traitement termin√© : {len(texts)} textes\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def analyze_vocabulary(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyse le vocabulaire avant/apr√®s preprocessing\n",
        "        \"\"\"\n",
        "        # Vocabulaire avant\n",
        "        vocab_before = set()\n",
        "        for text in texts:\n",
        "            words = text.lower().split()\n",
        "            vocab_before.update(words)\n",
        "        \n",
        "        # Vocabulaire apr√®s\n",
        "        vocab_after = set()\n",
        "        processed = self.process_batch(texts, show_progress=False)\n",
        "        for tokens in processed:\n",
        "            vocab_after.update(tokens)\n",
        "        \n",
        "        return {\n",
        "            'vocab_size_before': len(vocab_before),\n",
        "            'vocab_size_after': len(vocab_after),\n",
        "            'reduction_percent': round((len(vocab_before) - len(vocab_after)) / len(vocab_before) * 100, 1),\n",
        "            'vocab_before': vocab_before,\n",
        "            'vocab_after': vocab_after\n",
        "        }\n",
        "    \n",
        "    def save_config(self, filepath: str):\n",
        "        \"\"\"Sauvegarde la configuration\"\"\"\n",
        "        self.config.save(filepath)\n",
        "    \n",
        "    def get_pipeline_info(self) -> str:\n",
        "        \"\"\"Retourne info sur la configuration du pipeline\"\"\"\n",
        "        info = f\"\"\"üîß Configuration du Pipeline :\n",
        "‚Ä¢ Tokenizer : {self.config.tokenizer}\n",
        "‚Ä¢ Lemmatisation : {self.config.lemmatize}\n",
        "‚Ä¢ Stemming : {self.config.stem}\n",
        "‚Ä¢ Suppression stopwords : {self.config.remove_stopwords}\n",
        "‚Ä¢ Suppression ponctuation : {self.config.remove_punctuation}\n",
        "‚Ä¢ Gestion emojis : {self.config.emoji_strategy}\n",
        "‚Ä¢ Normalisation entit√©s : {self.config.normalize_entities}\n",
        "‚Ä¢ Longueur min tokens : {self.config.min_token_length}\n",
        "‚Ä¢ Stopwords personnalis√©s : {len(self.config.custom_stopwords)}\"\"\"\n",
        "        return info\n",
        "\n",
        "print(\"‚úÖ Pipeline complet cr√©√© !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üß™ 4. Tests et D√©monstrations\n",
        "\n",
        "Testons notre pipeline sur diff√©rents types de textes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr√©er pipeline avec config par d√©faut\n",
        "config_demo = PipelineConfig(verbose=True)\n",
        "pipeline = PreprocessingPipeline(config_demo)\n",
        "\n",
        "print(\"üöÄ D√âMONSTRATION DU PIPELINE COMPLET\")\n",
        "print(\"=\" * 60)\n",
        "print(pipeline.get_pipeline_info())\n",
        "print()\n",
        "\n",
        "# Test sur un texte complexe\n",
        "texte_complexe = \"\"\"\n",
        "Salut @marie ! üòç J'ai achet√© ce super produit sur https://boutique.com pour 199,99‚Ç¨ le 15/03/2024.\n",
        "Contact : support@boutique.fr ou 01.23.45.67.89. \n",
        "C'est vraiment G√âNIAL !!! Je recommande √† 100% üëç #shopping #bonplan\n",
        "\"\"\".strip()\n",
        "\n",
        "print(\"üìù TEXTE DE TEST :\")\n",
        "print(f\"\\n{texte_complexe}\\n\")\n",
        "\n",
        "print(\"üîß TRAITEMENT √âTAPE PAR √âTAPE :\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "tokens_finaux, stats = pipeline.process_text(texte_complexe, return_stats=True)\n",
        "\n",
        "print(f\"\\n‚úÖ R√âSULTAT FINAL :\")\n",
        "print(f\"Tokens : {tokens_finaux}\")\n",
        "print(f\"Phrase reconstruite : '{' '.join(tokens_finaux)}'\")\n",
        "print(f\"\\n{stats.summary()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test comparatif entre diff√©rentes configurations\n",
        "textes_test = [\n",
        "    \"RT @user: LOL!!! C'est G√âNIAL üòçüòçüòç https://bit.ly/xyz #amazing\",\n",
        "    \"J'adore ce produit! Co√ªte 50‚Ç¨, livraison gratuite. Contact: info@shop.fr\",\n",
        "    \"Rendez-vous le 15 mars 2024 √† 14h30 pour discuter du budget.\",\n",
        "    \"Super service client!!! Ils sont tr√®s professionnels et rapides üëç\",\n",
        "    \"N'h√©sitez pas √† me contacter au 01.23.45.67.89 pour plus d'infos.\"\n",
        "]\n",
        "\n",
        "# Tester diff√©rentes configs\n",
        "configs_test = {\n",
        "    \"Social Media\": PipelineConfig.for_domain(\"social_media\"),\n",
        "    \"E-commerce\": PipelineConfig.for_domain(\"ecommerce\"),\n",
        "    \"News\": PipelineConfig.for_domain(\"news\"),\n",
        "    \"Fast\": PipelineConfig.for_domain(\"fast\")\n",
        "}\n",
        "\n",
        "print(\"‚öîÔ∏è COMPARAISON DES CONFIGURATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analyser chaque config\n",
        "resultats_comparaison = []\n",
        "\n",
        "for nom_config, config in configs_test.items():\n",
        "    pipeline_test = PreprocessingPipeline(config)\n",
        "    \n",
        "    # Analyser le vocabulaire\n",
        "    vocab_analysis = pipeline_test.analyze_vocabulary(textes_test)\n",
        "    \n",
        "    # Mesurer le temps moyen\n",
        "    start_time = time.time()\n",
        "    processed = pipeline_test.process_batch(textes_test, show_progress=False)\n",
        "    avg_time = (time.time() - start_time) / len(textes_test)\n",
        "    \n",
        "    # Calculer tokens moyens\n",
        "    avg_tokens = np.mean([len(tokens) for tokens in processed])\n",
        "    \n",
        "    resultats_comparaison.append({\n",
        "        'Configuration': nom_config,\n",
        "        'Vocab avant': vocab_analysis['vocab_size_before'],\n",
        "        'Vocab apr√®s': vocab_analysis['vocab_size_after'],\n",
        "        'R√©duction (%)': vocab_analysis['reduction_percent'],\n",
        "        'Tokens moyens': round(avg_tokens, 1),\n",
        "        'Temps (ms)': round(avg_time * 1000, 1)\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nüîß **{nom_config}** :\")\n",
        "    print(f\"  ‚Ä¢ R√©duction vocabulaire : {vocab_analysis['reduction_percent']}%\")\n",
        "    print(f\"  ‚Ä¢ Tokens moyens par texte : {avg_tokens:.1f}\")\n",
        "    print(f\"  ‚Ä¢ Temps moyen : {avg_time*1000:.1f}ms\")\n",
        "\n",
        "# Cr√©er DataFrame pour analyse\n",
        "df_comparaison = pd.DataFrame(resultats_comparaison)\n",
        "print(f\"\\nüìä **TABLEAU R√âCAPITULATIF :**\")\n",
        "print(df_comparaison.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üìà 5. Visualisations et Analyses\n",
        "\n",
        "Cr√©ons des visualisations pour analyser l'impact du preprocessing :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des comparaisons\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Graphique 1: R√©duction du vocabulaire\n",
        "configs = df_comparaison['Configuration']\n",
        "reductions = df_comparaison['R√©duction (%)']\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(configs)))\n",
        "\n",
        "bars1 = ax1.bar(configs, reductions, color=colors, alpha=0.8)\n",
        "ax1.set_title('üìâ R√©duction du Vocabulaire par Configuration', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('R√©duction (%)')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "for bar, val in zip(bars1, reductions):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{val}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Graphique 2: Taille vocabulaire avant/apr√®s\n",
        "x = np.arange(len(configs))\n",
        "width = 0.35\n",
        "\n",
        "ax2.bar(x - width/2, df_comparaison['Vocab avant'], width, \n",
        "        label='Avant', color='lightcoral', alpha=0.8)\n",
        "ax2.bar(x + width/2, df_comparaison['Vocab apr√®s'], width,\n",
        "        label='Apr√®s', color='lightblue', alpha=0.8)\n",
        "\n",
        "ax2.set_title('üìä Taille du Vocabulaire Avant/Apr√®s', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Nombre de mots uniques')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(configs, rotation=45)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Graphique 3: Nombre moyen de tokens\n",
        "bars3 = ax3.bar(configs, df_comparaison['Tokens moyens'], \n",
        "                color=plt.cm.plasma(np.linspace(0, 1, len(configs))), alpha=0.8)\n",
        "ax3.set_title('üìù Nombre Moyen de Tokens par Texte', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Tokens moyens')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "for bar, val in zip(bars3, df_comparaison['Tokens moyens']):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{val}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Graphique 4: Performance (temps de traitement)\n",
        "bars4 = ax4.bar(configs, df_comparaison['Temps (ms)'],\n",
        "                color=plt.cm.cool(np.linspace(0, 1, len(configs))), alpha=0.8)\n",
        "ax4.set_title('‚ö° Performance (Temps de Traitement)', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Temps moyen (ms)')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "for bar, val in zip(bars4, df_comparaison['Temps (ms)']):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{val}ms', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyse des r√©sultats\n",
        "print(\"\\nüí° **ANALYSE DES R√âSULTATS :**\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Configuration la plus efficace\n",
        "idx_max_reduction = df_comparaison['R√©duction (%)'].idxmax()\n",
        "config_max_reduction = df_comparaison.loc[idx_max_reduction, 'Configuration']\n",
        "max_reduction = df_comparaison.loc[idx_max_reduction, 'R√©duction (%)']\n",
        "\n",
        "print(f\"üèÜ **Meilleure r√©duction vocabulaire** : {config_max_reduction} ({max_reduction}%)\")\n",
        "\n",
        "# Configuration la plus rapide\n",
        "idx_min_time = df_comparaison['Temps (ms)'].idxmin()\n",
        "config_min_time = df_comparaison.loc[idx_min_time, 'Configuration']\n",
        "min_time = df_comparaison.loc[idx_min_time, 'Temps (ms)']\n",
        "\n",
        "print(f\"‚ö° **Configuration la plus rapide** : {config_min_time} ({min_time}ms)\")\n",
        "\n",
        "# Recommandations\n",
        "print(f\"\\nüéØ **RECOMMANDATIONS :**\")\n",
        "print(f\"‚Ä¢ Pour **r√©duction maximale** du vocabulaire ‚Üí {config_max_reduction}\")\n",
        "print(f\"‚Ä¢ Pour **performance optimale** ‚Üí {config_min_time}\")\n",
        "print(f\"‚Ä¢ Pour **√©quilibre** qualit√©/vitesse ‚Üí News ou E-commerce\")\n",
        "print(f\"‚Ä¢ Pour **m√©dias sociaux** ‚Üí Social Media (g√®re emojis et mentions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ 6. Exercices Pratiques\n",
        "\n",
        "√Ä votre tour ! Exp√©rimentez avec le pipeline :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERCICE 1: Cr√©ez votre configuration personnalis√©e\n",
        "\n",
        "print(\"üéØ EXERCICE 1 : Configuration Personnalis√©e\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Modifiez ces param√®tres selon vos besoins\n",
        "ma_config = PipelineConfig(\n",
        "    # √Ä personnaliser :\n",
        "    remove_punctuation=True,\n",
        "    emoji_strategy=\"convert\",  # \"remove\", \"convert\", \"keep\"\n",
        "    remove_stopwords=True,\n",
        "    lemmatize=True,\n",
        "    min_token_length=3,\n",
        "    tokenizer=\"spacy\",  # \"spacy\", \"nltk\", \"regex\", \"split\"\n",
        "    normalize_entities=True,\n",
        "    \n",
        "    # Stopwords personnalis√©s pour votre domaine\n",
        "    custom_stopwords=[\"super\", \"vraiment\", \"tr√®s\", \"assez\"],\n",
        "    \n",
        "    verbose=True  # Pour voir les √©tapes\n",
        ")\n",
        "\n",
        "mon_pipeline = PreprocessingPipeline(ma_config)\n",
        "\n",
        "print(mon_pipeline.get_pipeline_info())\n",
        "print()\n",
        "\n",
        "# Testez sur vos propres textes\n",
        "mes_textes = [\n",
        "    \"Remplacez ce texte par vos propres exemples !\",\n",
        "    \"Exemple : J'adore vraiment ce super produit üòç co√ªte 50‚Ç¨\",\n",
        "    \"Autre exemple : Contact@entreprise.fr pour plus d'infos\"\n",
        "    # Ajoutez vos textes ici...\n",
        "]\n",
        "\n",
        "print(\"üß™ **Test de votre configuration :**\")\n",
        "for i, texte in enumerate(mes_textes, 1):\n",
        "    print(f\"\\nüìù **Texte {i}** : {texte}\")\n",
        "    tokens, stats = mon_pipeline.process_text(texte, return_stats=True)\n",
        "    print(f\"‚úÖ **R√©sultat** : {tokens}\")\n",
        "    print(f\"üìä **Stats** : {stats.token_reduction_percent}% r√©duction, {stats.processing_time:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERCICE 2: Analyse de corpus\n",
        "\n",
        "print(\"\\nüéØ EXERCICE 2 : Analyse d'un Corpus\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Dataset d'exemple (remplacez par vos donn√©es)\n",
        "corpus_exemple = [\n",
        "    \"Excellent produit ! Je le recommande vivement üòç Prix : 99‚Ç¨\",\n",
        "    \"Service client d√©cevant... Pas de r√©ponse depuis 3 jours üòû\",\n",
        "    \"Livraison rapide, emballage soign√©. Contact : support@shop.fr\",\n",
        "    \"Qualit√©/prix imbattable ! Disponible sur https://boutique.com\",\n",
        "    \"Attention arnaque !!! N'achetez pas ici üò° #attention\",\n",
        "    \"Super exp√©rience d'achat, je recommande √† 100% üëç\",\n",
        "    \"Produit conforme √† la description. Livr√© le 15/03/2024\",\n",
        "    \"SAV tr√®s r√©actif au 01.23.45.67.89. Probl√®me r√©solu rapidement.\",\n",
        "    \"Promo int√©ressante : -30% jusqu'au 31/12/2024 #bonplan\",\n",
        "    \"Article de qualit√©, mais un peu cher √† 150‚Ç¨... Livraison OK\"\n",
        "]\n",
        "\n",
        "# Analyser avec diff√©rentes configurations\n",
        "configs_analyse = {\n",
        "    \"Basique\": PipelineConfig(lemmatize=False, normalize_entities=False),\n",
        "    \"Complet\": PipelineConfig(),\n",
        "    \"E-commerce\": PipelineConfig.for_domain(\"ecommerce\")\n",
        "}\n",
        "\n",
        "resultats_analyse = {}\n",
        "\n",
        "for nom, config in configs_analyse.items():\n",
        "    pipeline_analyse = PreprocessingPipeline(config)\n",
        "    \n",
        "    # Traitement du corpus\n",
        "    start_time = time.time()\n",
        "    tokens_corpus = pipeline_analyse.process_batch(corpus_exemple, show_progress=False)\n",
        "    temps_total = time.time() - start_time\n",
        "    \n",
        "    # Analyse vocabulaire\n",
        "    vocab_analysis = pipeline_analyse.analyze_vocabulary(corpus_exemple)\n",
        "    \n",
        "    # Statistiques\n",
        "    tous_tokens = [token for tokens in tokens_corpus for token in tokens]\n",
        "    freq_tokens = Counter(tous_tokens)\n",
        "    \n",
        "    resultats_analyse[nom] = {\n",
        "        'tokens_corpus': tokens_corpus,\n",
        "        'vocab_analysis': vocab_analysis,\n",
        "        'temps_total': temps_total,\n",
        "        'freq_tokens': freq_tokens,\n",
        "        'tokens_total': len(tous_tokens),\n",
        "        'tokens_uniques': len(set(tous_tokens))\n",
        "    }\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(\"üìä **R√âSULTATS DE L'ANALYSE :**\\n\")\n",
        "\n",
        "for nom, resultats in resultats_analyse.items():\n",
        "    print(f\"üîß **Configuration {nom}** :\")\n",
        "    print(f\"  ‚Ä¢ Vocabulaire : {resultats['vocab_analysis']['vocab_size_before']} ‚Üí {resultats['vocab_analysis']['vocab_size_after']} mots ({resultats['vocab_analysis']['reduction_percent']}%)\")\n",
        "    print(f\"  ‚Ä¢ Tokens total : {resultats['tokens_total']}\")\n",
        "    print(f\"  ‚Ä¢ Tokens uniques : {resultats['tokens_uniques']}\")\n",
        "    print(f\"  ‚Ä¢ Temps traitement : {resultats['temps_total']:.3f}s\")\n",
        "    print(f\"  ‚Ä¢ Top 5 mots : {list(resultats['freq_tokens'].most_common(5))}\")\n",
        "    print()\n",
        "\n",
        "# Comparaison visuelle simple\n",
        "noms = list(resultats_analyse.keys())\n",
        "reductions = [resultats_analyse[nom]['vocab_analysis']['reduction_percent'] for nom in noms]\n",
        "temps = [resultats_analyse[nom]['temps_total'] * 1000 for nom in noms]  # en ms\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# R√©duction vocabulaire\n",
        "ax1.bar(noms, reductions, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "ax1.set_title('üìâ R√©duction Vocabulaire (%)')\n",
        "ax1.set_ylabel('R√©duction (%)')\n",
        "for i, v in enumerate(reductions):\n",
        "    ax1.text(i, v + 1, f'{v}%', ha='center', fontweight='bold')\n",
        "\n",
        "# Temps de traitement\n",
        "ax2.bar(noms, temps, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
        "ax2.set_title('‚ö° Temps de Traitement (ms)')\n",
        "ax2.set_ylabel('Temps (ms)')\n",
        "for i, v in enumerate(temps):\n",
        "    ax2.text(i, v + 0.1, f'{v:.1f}ms', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üí° **OBSERVATIONS :**\")\n",
        "print(f\"‚Ä¢ Configuration la plus efficace : {noms[reductions.index(max(reductions))]} ({max(reductions)}% r√©duction)\")\n",
        "print(f\"‚Ä¢ Configuration la plus rapide : {noms[temps.index(min(temps))]} ({min(temps):.1f}ms)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ EXERCICE 3: Optimisation de performance\n",
        "\n",
        "print(\"\\nüéØ EXERCICE 3 : Benchmark de Performance\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# G√©n√©rer un corpus de test plus grand\n",
        "def generer_corpus_test(taille: int) -> List[str]:\n",
        "    \"\"\"G√©n√®re un corpus de test de taille donn√©e\"\"\"\n",
        "    templates = [\n",
        "        \"Super produit ! Je recommande üòç Prix : {}‚Ç¨\",\n",
        "        \"Service d√©cevant... Contact : {}@email.fr\",\n",
        "        \"Livr√© le {} rapidement. Site : https://{}.com\",\n",
        "        \"Attention ! N'achetez pas ici üò° #arnaque\",\n",
        "        \"Exp√©rience parfaite üëç T√©l : 01.{}.{}.{}.{}\",\n",
        "        \"Qualit√©/prix correct. Disponible jusqu'au {}/{}/2024\"\n",
        "    ]\n",
        "    \n",
        "    corpus = []\n",
        "    for i in range(taille):\n",
        "        template = templates[i % len(templates)]\n",
        "        if '{}' in template:\n",
        "            # Remplacer les placeholders\n",
        "            if 'Prix' in template:\n",
        "                text = template.format(50 + (i % 100))\n",
        "            elif '@email' in template:\n",
        "                text = template.format(f\"user{i%100}\")\n",
        "            elif 'https' in template:\n",
        "                text = template.format(f\"site{i%10}\")\n",
        "            elif 'T√©l' in template:\n",
        "                text = template.format(\n",
        "                    20 + (i%8), 30 + (i%7), 40 + (i%6), 50 + (i%9)\n",
        "                )\n",
        "            elif '2024' in template:\n",
        "                text = template.format((i%28)+1, (i%12)+1)\n",
        "            else:\n",
        "                text = template\n",
        "        else:\n",
        "            text = template\n",
        "        corpus.append(text)\n",
        "    \n",
        "    return corpus\n",
        "\n",
        "# G√©n√©rer diff√©rentes tailles de corpus\n",
        "tailles = [100, 500, 1000]\n",
        "configs_bench = {\n",
        "    \"Fast\": PipelineConfig.for_domain(\"fast\"),\n",
        "    \"Standard\": PipelineConfig(),\n",
        "    \"Complet\": PipelineConfig(normalize_entities=True, emoji_strategy=\"convert\")\n",
        "}\n",
        "\n",
        "resultats_bench = []\n",
        "\n",
        "print(\"‚è±Ô∏è **BENCHMARK EN COURS...**\\n\")\n",
        "\n",
        "for taille in tailles:\n",
        "    corpus_test = generer_corpus_test(taille)\n",
        "    print(f\"üìä Corpus de {taille} textes :\")\n",
        "    \n",
        "    for nom_config, config in configs_bench.items():\n",
        "        pipeline_bench = PreprocessingPipeline(config)\n",
        "        \n",
        "        # Mesurer le temps\n",
        "        start_time = time.time()\n",
        "        resultats = pipeline_bench.process_batch(corpus_test, show_progress=False)\n",
        "        temps_total = time.time() - start_time\n",
        "        \n",
        "        # Calculer m√©triques\n",
        "        temps_par_texte = temps_total / len(corpus_test) * 1000  # ms\n",
        "        tokens_moyens = np.mean([len(tokens) for tokens in resultats])\n",
        "        \n",
        "        resultats_bench.append({\n",
        "            'Taille Corpus': taille,\n",
        "            'Configuration': nom_config,\n",
        "            'Temps Total (s)': round(temps_total, 3),\n",
        "            'Temps/Texte (ms)': round(temps_par_texte, 2),\n",
        "            'Tokens Moyens': round(tokens_moyens, 1),\n",
        "            'Vitesse (textes/s)': round(len(corpus_test) / temps_total, 1)\n",
        "        })\n",
        "        \n",
        "        print(f\"  ‚Ä¢ {nom_config:<10} : {temps_total:.3f}s ({temps_par_texte:.2f}ms/texte)\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Cr√©er DataFrame et visualiser\n",
        "df_bench = pd.DataFrame(resultats_bench)\n",
        "\n",
        "print(\"üìä **R√âSULTATS COMPLETS :**\")\n",
        "print(df_bench.to_string(index=False))\n",
        "\n",
        "# Graphique de performance\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Subplot 1: Temps par texte selon la taille\n",
        "plt.subplot(2, 2, 1)\n",
        "for config in configs_bench.keys():\n",
        "    data = df_bench[df_bench['Configuration'] == config]\n",
        "    plt.plot(data['Taille Corpus'], data['Temps/Texte (ms)'], \n",
        "             marker='o', label=config, linewidth=2)\n",
        "plt.xlabel('Taille du Corpus')\n",
        "plt.ylabel('Temps par Texte (ms)')\n",
        "plt.title('‚ö° Performance vs Taille Corpus')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Vitesse de traitement\n",
        "plt.subplot(2, 2, 2)\n",
        "for config in configs_bench.keys():\n",
        "    data = df_bench[df_bench['Configuration'] == config]\n",
        "    plt.plot(data['Taille Corpus'], data['Vitesse (textes/s)'], \n",
        "             marker='s', label=config, linewidth=2)\n",
        "plt.xlabel('Taille du Corpus')\n",
        "plt.ylabel('Vitesse (textes/seconde)')\n",
        "plt.title('üöÄ D√©bit de Traitement')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Comparaison configs pour 1000 textes\n",
        "plt.subplot(2, 2, 3)\n",
        "data_1000 = df_bench[df_bench['Taille Corpus'] == 1000]\n",
        "plt.bar(data_1000['Configuration'], data_1000['Temps/Texte (ms)'],\n",
        "        color=['red', 'blue', 'green'], alpha=0.7)\n",
        "plt.ylabel('Temps par Texte (ms)')\n",
        "plt.title('üìä Performance sur 1000 Textes')\n",
        "plt.xticks(rotation=45)\n",
        "for i, v in enumerate(data_1000['Temps/Texte (ms)']):\n",
        "    plt.text(i, v + 0.1, f'{v}ms', ha='center', fontweight='bold')\n",
        "\n",
        "# Subplot 4: Tokens moyens par config\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.bar(data_1000['Configuration'], data_1000['Tokens Moyens'],\n",
        "        color=['orange', 'purple', 'brown'], alpha=0.7)\n",
        "plt.ylabel('Tokens Moyens')\n",
        "plt.title('üìù Nombre Moyen de Tokens')\n",
        "plt.xticks(rotation=45)\n",
        "for i, v in enumerate(data_1000['Tokens Moyens']):\n",
        "    plt.text(i, v + 0.1, f'{v}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Recommandations finales\n",
        "print(\"\\nüéØ **RECOMMANDATIONS FINALES :**\")\n",
        "config_plus_rapide = data_1000.loc[data_1000['Temps/Texte (ms)'].idxmin(), 'Configuration']\n",
        "vitesse_max = data_1000['Vitesse (textes/s)'].max()\n",
        "\n",
        "print(f\"‚Ä¢ **Configuration la plus rapide** : {config_plus_rapide}\")\n",
        "print(f\"‚Ä¢ **D√©bit maximum** : {vitesse_max} textes/seconde\")\n",
        "print(f\"‚Ä¢ **Pour production** : Privil√©gier 'Fast' si volume important\")\n",
        "print(f\"‚Ä¢ **Pour qualit√©** : Privil√©gier 'Complet' si pr√©cision importante\")\n",
        "print(f\"‚Ä¢ **Compromis** : 'Standard' pour √©quilibre qualit√©/vitesse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üîß 7. Sauvegarde et Utilisation en Production\n",
        "\n",
        "Apprenons √† sauvegarder et r√©utiliser nos pipelines :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarde de configuration\n",
        "print(\"üíæ SAUVEGARDE ET CHARGEMENT\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Cr√©er une config optimis√©e\n",
        "config_production = PipelineConfig(\n",
        "    # Optimis√© pour la production\n",
        "    tokenizer=\"spacy\",\n",
        "    lemmatize=True,\n",
        "    remove_stopwords=True,\n",
        "    normalize_entities=True,\n",
        "    emoji_strategy=\"convert\",\n",
        "    min_token_length=3,\n",
        "    \n",
        "    # Performance\n",
        "    batch_size=1000,\n",
        "    verbose=False,\n",
        "    \n",
        "    # Stopwords m√©tier\n",
        "    custom_stopwords=[\"produit\", \"service\", \"client\", \"entreprise\"]\n",
        ")\n",
        "\n",
        "# Sauvegarder\n",
        "try:\n",
        "    config_production.save(\"config_production.json\")\n",
        "    print(\"‚úÖ Configuration sauvegard√©e dans 'config_production.json'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur sauvegarde : {e}\")\n",
        "\n",
        "# Cr√©er le pipeline de production\n",
        "pipeline_production = PreprocessingPipeline(config_production)\n",
        "\n",
        "print(\"\\nüè≠ **PIPELINE DE PRODUCTION CR√â√â :**\")\n",
        "print(pipeline_production.get_pipeline_info())\n",
        "\n",
        "# Fonction utilitaire pour usage simple\n",
        "def preprocess_text_simple(text: str, domain: str = \"general\") -> List[str]:\n",
        "    \"\"\"\n",
        "    Fonction simple pour preprocessing rapide\n",
        "    \n",
        "    Args:\n",
        "        text: Texte √† traiter\n",
        "        domain: Domaine d'application (\"general\", \"social_media\", \"ecommerce\", etc.)\n",
        "    \n",
        "    Returns:\n",
        "        Liste de tokens preprocess√©s\n",
        "    \"\"\"\n",
        "    config = PipelineConfig.for_domain(domain)\n",
        "    pipeline = PreprocessingPipeline(config)\n",
        "    return pipeline.process_text(text)\n",
        "\n",
        "# Test de la fonction simple\n",
        "print(\"\\nüß™ **TEST FONCTION SIMPLE :**\")\n",
        "texte_test = \"J'adore ce produit! üòç Co√ªte 50‚Ç¨, contact@shop.fr\"\n",
        "\n",
        "for domain in [\"general\", \"ecommerce\", \"social_media\"]:\n",
        "    tokens = preprocess_text_simple(texte_test, domain)\n",
        "    print(f\"‚Ä¢ {domain:<12} : {tokens}\")\n",
        "\n",
        "print(\"\\nüìã **CLASSE WRAPPER POUR PRODUCTION :**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    \"\"\"\n",
        "    Classe wrapper simplifi√©e pour utilisation en production\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, domain: str = \"general\", config_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialise le preprocessor\n",
        "        \n",
        "        Args:\n",
        "            domain: Domaine pr√©d√©fini ou \"custom\" si config_path fourni\n",
        "            config_path: Chemin vers fichier config JSON (optionnel)\n",
        "        \"\"\"\n",
        "        if config_path:\n",
        "            # Charger config depuis fichier\n",
        "            try:\n",
        "                with open(config_path, 'r', encoding='utf-8') as f:\n",
        "                    config_dict = json.load(f)\n",
        "                # Reconstruire la config (simplifi√©)\n",
        "                self.config = PipelineConfig(**config_dict)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Erreur chargement config : {e}\")\n",
        "                self.config = PipelineConfig.for_domain(\"general\")\n",
        "        else:\n",
        "            self.config = PipelineConfig.for_domain(domain)\n",
        "        \n",
        "        self.pipeline = PreprocessingPipeline(self.config)\n",
        "        self.domain = domain\n",
        "    \n",
        "    def preprocess(self, text: Union[str, List[str]]) -> Union[List[str], List[List[str]]]:\n",
        "        \"\"\"\n",
        "        Preprocess un texte ou une liste de textes\n",
        "        \n",
        "        Args:\n",
        "            text: Texte(s) √† traiter\n",
        "        \n",
        "        Returns:\n",
        "            Tokens ou liste de tokens\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            return self.pipeline.process_text(text)\n",
        "        elif isinstance(text, list):\n",
        "            return self.pipeline.process_batch(text, show_progress=True)\n",
        "        else:\n",
        "            raise ValueError(\"Input doit √™tre str ou List[str]\")\n",
        "    \n",
        "    def analyze(self, texts: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyse un corpus de textes\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec statistiques\n",
        "        \"\"\"\n",
        "        return self.pipeline.analyze_vocabulary(texts)\n",
        "    \n",
        "    def get_stats(self, text: str) -> ProcessingStats:\n",
        "        \"\"\"\n",
        "        Obtient statistiques d√©taill√©es pour un texte\n",
        "        \"\"\"\n",
        "        _, stats = self.pipeline.process_text(text, return_stats=True)\n",
        "        return stats\n",
        "    \n",
        "    def save_config(self, filepath: str):\n",
        "        \"\"\"Sauvegarde la configuration actuelle\"\"\"\n",
        "        self.config.save(filepath)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"TextPreprocessor(domain='{self.domain}', tokenizer='{self.config.tokenizer}')\"\n",
        "\n",
        "\n",
        "# D√©monstration de la classe wrapper\n",
        "print(\"üöÄ **D√âMONSTRATION CLASSE WRAPPER :**\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Cr√©er preprocessors pour diff√©rents domaines\n",
        "preprocessors = {\n",
        "    \"E-commerce\": TextPreprocessor(\"ecommerce\"),\n",
        "    \"Social Media\": TextPreprocessor(\"social_media\"),\n",
        "    \"Rapide\": TextPreprocessor(\"fast\")\n",
        "}\n",
        "\n",
        "# Texte de test\n",
        "texte_demo = \"Super exp√©rience ! üòç J'ai achet√© pour 99‚Ç¨ sur https://shop.fr. Contact : info@shop.fr #g√©nial\"\n",
        "\n",
        "print(f\"üìù **Texte de test :**\\n{texte_demo}\\n\")\n",
        "\n",
        "for nom, preprocessor in preprocessors.items():\n",
        "    print(f\"üîß **{nom}** :\")\n",
        "    \n",
        "    # Preprocessing simple\n",
        "    tokens = preprocessor.preprocess(texte_demo)\n",
        "    print(f\"  ‚Ä¢ Tokens : {tokens}\")\n",
        "    \n",
        "    # Statistiques\n",
        "    stats = preprocessor.get_stats(texte_demo)\n",
        "    print(f\"  ‚Ä¢ R√©duction : {stats.token_reduction_percent}%\")\n",
        "    print(f\"  ‚Ä¢ Temps : {stats.processing_time:.3f}s\")\n",
        "    print()\n",
        "\n",
        "# Test sur plusieurs textes\n",
        "print(\"üìä **TEST SUR CORPUS :**\")\n",
        "corpus_demo = [\n",
        "    \"Produit excellent ! Je recommande üòç\",\n",
        "    \"Service client d√©cevant... üòû Contact : help@site.com\",\n",
        "    \"Livraison rapide, prix correct : 45‚Ç¨\"\n",
        "]\n",
        "\n",
        "preprocessor_demo = TextPreprocessor(\"ecommerce\")\n",
        "resultats_corpus = preprocessor_demo.preprocess(corpus_demo)\n",
        "\n",
        "print(f\"üìà Corpus trait√© : {len(resultats_corpus)} textes\")\n",
        "for i, (original, tokens) in enumerate(zip(corpus_demo, resultats_corpus)):\n",
        "    print(f\"  {i+1}. {original[:40]}... ‚Üí {len(tokens)} tokens\")\n",
        "\n",
        "# Analyse du corpus\n",
        "analyse_corpus = preprocessor_demo.analyze(corpus_demo)\n",
        "print(f\"\\nüìä **Analyse vocabulaire :**\")\n",
        "print(f\"  ‚Ä¢ Vocabulaire avant : {analyse_corpus['vocab_size_before']} mots\")\n",
        "print(f\"  ‚Ä¢ Vocabulaire apr√®s : {analyse_corpus['vocab_size_after']} mots\")\n",
        "print(f\"  ‚Ä¢ R√©duction : {analyse_corpus['reduction_percent']}%\")\n",
        "\n",
        "print(\"\\n‚úÖ **Classe wrapper pr√™te pour la production !**\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéâ Conclusion et Synth√®se\n",
        "\n",
        "## üèÜ Ce que Vous Avez Accompli\n",
        "\n",
        "F√©licitations ! Vous avez cr√©√© un **pipeline de preprocessing NLP complet et professionnel**. Voici ce que vous ma√Ætrisez maintenant :\n",
        "\n",
        "### ‚úÖ **Comp√©tences Techniques Acquises**\n",
        "\n",
        "1. **üßπ Nettoyage de texte** : Gestion de la casse, ponctuation, URLs, emojis\n",
        "2. **üîß Normalisation** : Accents, espaces, entit√©s (dates, montants, etc.)\n",
        "3. **‚úÇÔ∏è Tokenisation** : Diff√©rentes strat√©gies (spaCy, NLTK, regex)\n",
        "4. **üõë Filtrage** : Stopwords, longueur, tokens sp√©ciaux\n",
        "5. **üå± Lemmatisation/Stemming** : R√©duction √† la forme canonique\n",
        "6. **üìä M√©triques et analyse** : Mesure de l'impact du preprocessing\n",
        "7. **‚öôÔ∏è Configuration modulaire** : Adaptation par domaine\n",
        "8. **üöÄ Optimisation** : Performance et scalabilit√©\n",
        "\n",
        "### üéØ **Configurations Ma√Ætris√©es**\n",
        "\n",
        "| **Domaine** | **Cas d'usage** | **Sp√©cificit√©s** |\n",
        "|---|---|---|\n",
        "| **General** | Usage polyvalent | √âquilibre qualit√©/vitesse |\n",
        "| **Social Media** | Tweets, posts | G√®re emojis, mentions, hashtags |\n",
        "| **E-commerce** | Avis clients | Pr√©serve montants, supprime URLs |\n",
        "| **News** | Articles presse | Anonymise entit√©s, qualit√© √©lev√©e |\n",
        "| **Fast** | Gros volumes | Performance maximale |\n",
        "\n",
        "### üìà **M√©triques de Performance**\n",
        "\n",
        "- **R√©duction vocabulaire** : 20-40% selon configuration\n",
        "- **Vitesse de traitement** : 100-1000+ textes/seconde\n",
        "- **Qualit√©** : Pr√©servation du sens, tokens coh√©rents\n",
        "- **Flexibilit√©** : Adaptation facile aux besoins m√©tier\n",
        "\n",
        "## üöÄ **Utilisation en Production**\n",
        "\n",
        "Votre pipeline est maintenant pr√™t pour :\n",
        "\n",
        "```python\n",
        "# Usage simple\n",
        "from text_preprocessor import TextPreprocessor\n",
        "\n",
        "# Initialisation\n",
        "preprocessor = TextPreprocessor(\"ecommerce\")\n",
        "\n",
        "# Traitement\n",
        "tokens = preprocessor.preprocess(\"Texte √† traiter\")\n",
        "corpus_tokens = preprocessor.preprocess([\"Texte 1\", \"Texte 2\"])\n",
        "\n",
        "# Analyse\n",
        "stats = preprocessor.analyze(corpus)\n",
        "```\n",
        "\n",
        "## üéì **Prochaines √âtapes**\n",
        "\n",
        "1. **üìä Module 3** : Repr√©sentation vectorielle (TF-IDF, embeddings)\n",
        "2. **ü§ñ Module 4** : Mod√®les de classification (Naive Bayes, SVM)\n",
        "3. **üß† Module 5** : Deep Learning (r√©seaux de neurones, LSTM)\n",
        "4. **ü§ó Module 6** : Transformers et mod√®les pr√©-entra√Æn√©s\n",
        "\n",
        "## üí° **Conseils pour la Suite**\n",
        "\n",
        "- **Testez** votre pipeline sur vos propres donn√©es\n",
        "- **Mesurez** l'impact sur vos m√©triques business\n",
        "- **Adaptez** les configurations selon vos besoins\n",
        "- **Documentez** vos choix de preprocessing\n",
        "- **It√©rez** en fonction des r√©sultats\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Bravo ! Vous √™tes maintenant capable de construire des pipelines de preprocessing NLP robustes et performants !**\n",
        "\n",
        "**üìö Notebook suivant :** `../module3/index.html` - Repr√©sentation Vectorielle du Texte"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}