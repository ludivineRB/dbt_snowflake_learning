{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üå± Lemmatisation Fran√ßaise : Guide Complet\n",
        "\n",
        "**Module 2 - Preprocessing et Tokenisation**\n\n",
        "Ce notebook vous apprend tout sur la lemmatisation en fran√ßais : concepts, outils, d√©fis sp√©cifiques et comparaison avec le stemming. Ma√Ætrisez cette technique essentielle pour un preprocessing de qualit√© !\n",
        "\n",
        "## üìã Plan du Notebook\n",
        "\n",
        "1. **Lemmatisation vs Stemming : Les Concepts**\n",
        "2. **Sp√©cificit√©s du Fran√ßais**\n",
        "3. **Lemmatisation avec spaCy**\n",
        "4. **Stemming en Fran√ßais**\n",
        "5. **Comparaison D√©taill√©e**\n",
        "6. **Cas Difficiles et Solutions**\n",
        "7. **Lemmatiseur Personnalis√©**\n",
        "8. **Tests et √âvaluation**\n",
        "9. **Recommandations Pratiques**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation et Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des packages n√©cessaires\n",
        "# !pip install spacy nltk matplotlib seaborn pandas wordcloud\n",
        "# !python -m spacy download fr_core_news_sm\n",
        "# !python -m spacy download fr_core_news_md  # Mod√®le plus pr√©cis (optionnel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration des graphiques\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set1\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"‚úÖ Imports r√©alis√©s avec succ√®s !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement des mod√®les et ressources\n",
        "print(\"üîÑ Chargement des mod√®les de traitement...\")\n",
        "\n",
        "# spaCy fran√ßais\n",
        "try:\n",
        "    nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "    print(f\"‚úÖ spaCy fran√ßais charg√© : {nlp_fr.meta['name']} v{nlp_fr.meta['version']}\")\nexcept OSError:\n",
        "    print(\"‚ùå Mod√®le spaCy fran√ßais non trouv√©\")\n",
        "    print(\"   Ex√©cutez : python -m spacy download fr_core_news_sm\")\n",
        "    nlp_fr = None\n",
        "\n",
        "# NLTK pour le stemming\n",
        "nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    from nltk.stem import SnowballStemmer\n",
        "    stemmer_fr = SnowballStemmer('french')\n",
        "    print(\"‚úÖ Stemmer fran√ßais NLTK charg√©\")\nexcept:\n",
        "    stemmer_fr = None\n",
        "    print(\"‚ùå Probl√®me avec le stemmer NLTK\")\n",
        "\n",
        "print(\"\\nüéØ Pr√™t pour la lemmatisation fran√ßaise !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ü§î Lemmatisation vs Stemming : Les Concepts\n",
        "\n",
        "### D√©finitions\n",
        "\n",
        "**üå± Lemmatisation** : R√©duction d'un mot √† sa **forme canonique** (lemme) en utilisant un dictionnaire et des r√®gles grammaticales.\n",
        "\n",
        "**‚úÇÔ∏è Stemming** : Suppression **m√©canique** des suffixes pour obtenir la racine du mot.\n",
        "\n",
        "### Exemple illustratif :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# D√©monstration simple des concepts\n",
        "exemples_demonstration = [\n",
        "    \"mangeons\", \"mangeait\", \"mangeur\", \"mangeable\",\n",
        "    \"courions\", \"courait\", \"coureur\", \"course\",\n",
        "    \"finissons\", \"finissait\", \"finisseur\", \"finition\",\n",
        "    \"parlons\", \"parlait\", \"parleur\", \"parole\"\n",
        "]\n",
        "\n",
        "def demo_lemma_vs_stem(mots: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    D√©monstration comparative lemmatisation vs stemming.\n",
        "    \"\"\"\n",
        "    print(\"üî¨ D√âMONSTRATION : LEMMATISATION vs STEMMING\\n\")\n",
        "    print(f\"{'Mot Original':15s} {'Lemme (spaCy)':15s} {'Stem (NLTK)':15s} {'Diff√©rence':15s}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for mot in mots:\n",
        "        # Lemmatisation avec spaCy\n",
        "        if nlp_fr:\n",
        "            doc = nlp_fr(mot)\n",
        "            lemme = doc[0].lemma_ if len(doc) > 0 else mot\n",
        "        else:\n",
        "            lemme = \"N/A\"\n",
        "        \n",
        "        # Stemming avec NLTK\n",
        "        if stemmer_fr:\n",
        "            stem = stemmer_fr.stem(mot)\n",
        "        else:\n",
        "            stem = \"N/A\"\n",
        "        \n",
        "        # Analyse de la diff√©rence\n",
        "        if lemme != \"N/A\" and stem != \"N/A\":\n",
        "            if lemme == stem:\n",
        "                diff = \"Identique\"\n",
        "            elif len(lemme) > len(stem):\n",
        "                diff = \"Lemme > Stem\"\n",
        "            else:\n",
        "                diff = \"Lemme < Stem\"\n",
        "        else:\n",
        "            diff = \"N/A\"\n",
        "        \n",
        "        print(f\"{mot:15s} {lemme:15s} {stem:15s} {diff:15s}\")\n",
        "\n",
        "# Ex√©cution de la d√©monstration\n",
        "demo_lemma_vs_stem(exemples_demonstration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Analyse de la d√©monstration :\n",
        "\n",
        "- **Lemmatisation** : Produit des mots **valides** du fran√ßais (\"manger\", \"courir\", \"finir\")\n",
        "- **Stemming** : Produit des **racines** parfois non-mots (\"mang\", \"cour\", \"fin\")\n",
        "- **Pr√©cision** : La lemmatisation est plus pr√©cise mais plus co√ªteuse\n",
        "- **Vitesse** : Le stemming est plus rapide mais moins pr√©cis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. üá´üá∑ Sp√©cificit√©s du Fran√ßais\n",
        "\n",
        "Le fran√ßais pr√©sente des d√©fis particuliers pour la lemmatisation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# D√©fis sp√©cifiques du fran√ßais\n",
        "defis_francais = {\n",
        "    \"Verbes irr√©guliers\": {\n",
        "        \"exemples\": [\"vais\", \"va\", \"irai\", \"ira\", \"suis\", \"est\", \"√©tait\", \"fus\"],\n",
        "        \"lemme_attendu\": [\"aller\", \"aller\", \"aller\", \"aller\", \"√™tre\", \"√™tre\", \"√™tre\", \"√™tre\"],\n",
        "        \"difficulte\": \"Formes tr√®s diff√©rentes du lemme\"\n",
        "    },\n",
        "    \n",
        "    \"Homonymie\": {\n",
        "        \"exemples\": [\"fils\", \"fils\", \"suis\", \"suis\"],\n",
        "        \"lemme_attendu\": [\"fil\", \"fils\", \"suivre\", \"√™tre\"],\n",
        "        \"difficulte\": \"M√™me forme, lemmes diff√©rents selon le contexte\"\n",
        "    },\n",
        "    \n",
        "    \"Accords complexes\": {\n",
        "        \"exemples\": [\"mang√©es\", \"finies\", \"prises\", \"venues\"],\n",
        "        \"lemme_attendu\": [\"manger\", \"finir\", \"prendre\", \"venir\"],\n",
        "        \"difficulte\": \"Accords du participe pass√©\"\n",
        "    },\n",
        "    \n",
        "    \"Contractions\": {\n",
        "        \"exemples\": [\"aujourd'hui\", \"quelqu'un\", \"lorsqu'il\", \"puisqu'elle\"],\n",
        "        \"lemme_attendu\": [\"aujourd'hui\", \"quelqu'un\", \"lorsque\", \"puisque\"],\n",
        "        \"difficulte\": \"√âlisions et contractions fig√©es\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def tester_defis_francais() -> None:\n",
        "    \"\"\"\n",
        "    Teste la lemmatisation sur les d√©fis sp√©cifiques du fran√ßais.\n",
        "    \"\"\"\n",
        "    print(\"üá´üá∑ D√âFIS SP√âCIFIQUES DU FRAN√áAIS\\n\")\n",
        "    \n",
        "    for categorie, info in defis_francais.items():\n",
        "        print(f\"üìÇ {categorie.upper()}\")\n",
        "        print(f\"   üí° Difficult√© : {info['difficulte']}\")\n",
        "        print(f\"   üîç Tests :\")\n",
        "        \n",
        "        for i, mot in enumerate(info['exemples']):\n",
        "            attendu = info['lemme_attendu'][i]\n",
        "            \n",
        "            if nlp_fr:\n",
        "                doc = nlp_fr(mot)\n",
        "                obtenu = doc[0].lemma_ if len(doc) > 0 else \"Erreur\"\n",
        "                \n",
        "                # V√©rification\n",
        "                if obtenu.lower() == attendu.lower():\n",
        "                    status = \"‚úÖ\"\n",
        "                else:\n",
        "                    status = \"‚ùå\"\n",
        "                \n",
        "                print(f\"      {mot:12s} ‚Üí {obtenu:12s} (attendu: {attendu:10s}) {status}\")\n",
        "            else:\n",
        "                print(f\"      {mot:12s} ‚Üí N/A (spaCy non disponible)\")\n",
        "        \n",
        "        print()\n",
        "\n",
        "# Test des d√©fis fran√ßais\n",
        "tester_defis_francais()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. üöÄ Lemmatisation avec spaCy\n",
        "\n",
        "spaCy est l'outil de r√©f√©rence pour la lemmatisation fran√ßaise de qualit√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LemmatiseurSpacy:\n",
        "    \"\"\"Classe pour la lemmatisation fran√ßaise avec spaCy.\"\"\"\n",
        "    \n",
        "    def __init__(self, modele: str = \"fr_core_news_sm\"):\n",
        "        \"\"\"\n",
        "        Initialise le lemmatiseur.\n",
        "        \n",
        "        Args:\n",
        "            modele: Nom du mod√®le spaCy √† utiliser\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(modele)\n",
        "            self.modele_charge = True\n",
        "            print(f\"‚úÖ Lemmatiseur spaCy initialis√© avec {modele}\")\n",
        "        except OSError:\n",
        "            self.nlp = None\n",
        "            self.modele_charge = False\n",
        "            print(f\"‚ùå Mod√®le {modele} non trouv√©\")\n",
        "    \n",
        "    def lemmatiser_mot(self, mot: str) -> str:\n",
        "        \"\"\"\n",
        "        Lemmatise un mot unique.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot √† lemmatiser\n",
        "        \n",
        "        Returns:\n",
        "            Lemme du mot\n",
        "        \"\"\"\n",
        "        if not self.modele_charge:\n",
        "            return mot\n",
        "        \n",
        "        doc = self.nlp(mot)\n",
        "        return doc[0].lemma_ if len(doc) > 0 else mot\n",
        "    \n",
        "    def lemmatiser_texte(self, texte: str, garder_pos: List[str] = None) -> List[str]:\n",
        "        \"\"\"\n",
        "        Lemmatise un texte complet.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† lemmatiser\n",
        "            garder_pos: Types de mots √† conserver (NOUN, VERB, ADJ, etc.)\n",
        "        \n",
        "        Returns:\n",
        "            Liste des lemmes\n",
        "        \"\"\"\n",
        "        if not self.modele_charge:\n",
        "            return texte.split()\n",
        "        \n",
        "        doc = self.nlp(texte)\n",
        "        lemmes = []\n",
        "        \n",
        "        for token in doc:\n",
        "            # Filtrage par type de mot si sp√©cifi√©\n",
        "            if garder_pos and token.pos_ not in garder_pos:\n",
        "                continue\n",
        "            \n",
        "            # Ignorer la ponctuation et les espaces\n",
        "            if not token.is_punct and not token.is_space:\n",
        "                lemmes.append(token.lemma_.lower())\n",
        "        \n",
        "        return lemmes\n",
        "    \n",
        "    def analyse_detaillee(self, texte: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Analyse d√©taill√©e avec informations grammaticales.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† analyser\n",
        "        \n",
        "        Returns:\n",
        "            Liste de dictionnaires avec les analyses\n",
        "        \"\"\"\n",
        "        if not self.modele_charge:\n",
        "            return []\n",
        "        \n",
        "        doc = self.nlp(texte)\n",
        "        analyses = []\n",
        "        \n",
        "        for token in doc:\n",
        "            if not token.is_punct and not token.is_space:\n",
        "                analyses.append({\n",
        "                    'mot': token.text,\n",
        "                    'lemme': token.lemma_,\n",
        "                    'pos': token.pos_,\n",
        "                    'tag': token.tag_,\n",
        "                    'description': spacy.explain(token.tag_) or \"Non d√©fini\",\n",
        "                    'est_alpha': token.is_alpha,\n",
        "                    'est_stopword': token.is_stop\n",
        "                })\n",
        "        \n",
        "        return analyses\n",
        "    \n",
        "    def statistiques_lemmatisation(self, texte: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Calcule des statistiques sur la lemmatisation.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† analyser\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les statistiques\n",
        "        \"\"\"\n",
        "        if not self.modele_charge:\n",
        "            return {}\n",
        "        \n",
        "        analyse = self.analyse_detaillee(texte)\n",
        "        \n",
        "        # Mots uniques avant/apr√®s lemmatisation\n",
        "        mots_originaux = {item['mot'].lower() for item in analyse}\n",
        "        lemmes_uniques = {item['lemme'].lower() for item in analyse}\n",
        "        \n",
        "        # R√©duction du vocabulaire\n",
        "        reduction = (len(mots_originaux) - len(lemmes_uniques)) / len(mots_originaux) * 100 if mots_originaux else 0\n",
        "        \n",
        "        # Distribution par type de mot\n",
        "        pos_counts = Counter(item['pos'] for item in analyse)\n",
        "        \n",
        "        return {\n",
        "            'mots_total': len(analyse),\n",
        "            'mots_uniques_avant': len(mots_originaux),\n",
        "            'lemmes_uniques_apres': len(lemmes_uniques),\n",
        "            'reduction_vocabulaire_pct': round(reduction, 1),\n",
        "            'distribution_pos': dict(pos_counts),\n",
        "            'mots_changes': sum(1 for item in analyse if item['mot'].lower() != item['lemme'].lower()),\n",
        "            'pourcentage_changes': round(sum(1 for item in analyse if item['mot'].lower() != item['lemme'].lower()) / len(analyse) * 100, 1) if analyse else 0\n",
        "        }\n",
        "\n",
        "# Test du lemmatiseur spaCy\n",
        "lemmatiseur = LemmatiseurSpacy()\n",
        "\n",
        "if lemmatiseur.modele_charge:\n",
        "    # Test sur une phrase d'exemple\n",
        "    phrase_test = \"Les enfants couraient rapidement vers leurs parents en criant joyeusement.\"\n",
        "    \n",
        "    print(\"\\nüß™ TEST DU LEMMATISEUR SPACY\\n\")\n",
        "    print(f\"üìù Phrase originale : {phrase_test}\")\n",
        "    \n",
        "    # Lemmatisation simple\n",
        "    lemmes = lemmatiseur.lemmatiser_texte(phrase_test)\n",
        "    print(f\"üå± Lemmes : {' '.join(lemmes)}\")\n",
        "    \n",
        "    # Statistiques\n",
        "    stats = lemmatiseur.statistiques_lemmatisation(phrase_test)\n",
        "    print(f\"\\nüìä Statistiques :\")\n",
        "    print(f\"  ‚Ä¢ Mots trait√©s : {stats['mots_total']}\")\n",
        "    print(f\"  ‚Ä¢ Mots chang√©s : {stats['mots_changes']} ({stats['pourcentage_changes']}%)\")\n",
        "    print(f\"  ‚Ä¢ R√©duction vocabulaire : {stats['reduction_vocabulaire_pct']}%\")\nelse:\n",
        "    print(\"‚ö†Ô∏è Lemmatiseur spaCy non disponible pour les tests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyse d√©taill√©e d'un texte\n",
        "if lemmatiseur.modele_charge:\n",
        "    texte_analyse = \"Les musiciens jouaient magnifiquement, les spectateurs applaudissaient enthousiastement.\"\n",
        "    \n",
        "    analyse_complete = lemmatiseur.analyse_detaillee(texte_analyse)\n",
        "    \n",
        "    print(\"üî¨ ANALYSE GRAMMATICALE D√âTAILL√âE\\n\")\n",
        "    print(f\"üìù Texte : {texte_analyse}\\n\")\n",
        "    \n",
        "    # Cr√©ation du tableau d'analyse\n",
        "    df_analyse = pd.DataFrame(analyse_complete)\n",
        "    \n",
        "    # S√©lection des colonnes importantes\n",
        "    colonnes_affichage = ['mot', 'lemme', 'pos', 'description']\n",
        "    df_affichage = df_analyse[colonnes_affichage].copy()\n",
        "    \n",
        "    # Ajout d'une colonne indiquant si le mot a chang√©\n",
        "    df_affichage['chang√©'] = df_analyse['mot'].str.lower() != df_analyse['lemme'].str.lower()\n",
        "    \n",
        "    print(\"üìã Analyse token par token :\")\n",
        "    print(df_affichage.to_string(index=False))\n",
        "    \n",
        "    # Statistiques par type de mot\n",
        "    print(\"\\nüìä Distribution par type de mot :\")\n",
        "    pos_stats = df_analyse['pos'].value_counts()\n",
        "    for pos, count in pos_stats.items():\n",
        "        print(f\"  {pos:8s} : {count:2d} ({count/len(df_analyse)*100:.1f}%)\")\nelse:\n",
        "    print(\"‚ö†Ô∏è Analyse d√©taill√©e non disponible (spaCy requis)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ‚úÇÔ∏è Stemming en Fran√ßais\n",
        "\n",
        "Le stemming comme alternative rapide √† la lemmatisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StemmerFrancais:\n",
        "    \"\"\"Classe pour le stemming fran√ßais avec diff√©rents algorithmes.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialise les stemmers disponibles.\n",
        "        \"\"\"\n",
        "        # Stemmer NLTK (Snowball)\n",
        "        try:\n",
        "            self.stemmer_snowball = SnowballStemmer('french')\n",
        "            self.snowball_disponible = True\n",
        "        except:\n",
        "            self.stemmer_snowball = None\n",
        "            self.snowball_disponible = False\n",
        "        \n",
        "        # Stemmer simple basique (r√®gles manuelles)\n",
        "        self.suffixes_fr = [\n",
        "            'ement', 'ation', 'ition', 'tion', 'sion',\n",
        "            'ment', 'able', 'ible', 'ique', 'isme',\n",
        "            'iste', 'eur', 'euse', 'ant', 'ent',\n",
        "            'ons', 'ent', 'ait', 'ais', 'ez',\n",
        "            'er', 'ir', 'es', 'e', 's'\n",
        "        ]\n",
        "        \n",
        "        print(f\"‚úÖ StemmerFrancais initialis√©\")\n",
        "        print(f\"   Snowball : {'‚úÖ' if self.snowball_disponible else '‚ùå'}\")\n",
        "        print(f\"   Stemmer simple : ‚úÖ\")\n",
        "    \n",
        "    def stem_snowball(self, mot: str) -> str:\n",
        "        \"\"\"\n",
        "        Stemming avec l'algorithme Snowball.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot √† stemmer\n",
        "        \n",
        "        Returns:\n",
        "            Radical du mot\n",
        "        \"\"\"\n",
        "        if self.snowball_disponible:\n",
        "            return self.stemmer_snowball.stem(mot)\n",
        "        return mot\n",
        "    \n",
        "    def stem_simple(self, mot: str, longueur_min: int = 3) -> str:\n",
        "        \"\"\"\n",
        "        Stemming simple par suppression de suffixes.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot √† stemmer\n",
        "            longueur_min: Longueur minimale du radical\n",
        "        \n",
        "        Returns:\n",
        "            Radical du mot\n",
        "        \"\"\"\n",
        "        mot_lower = mot.lower()\n",
        "        \n",
        "        for suffixe in self.suffixes_fr:\n",
        "            if mot_lower.endswith(suffixe) and len(mot_lower) - len(suffixe) >= longueur_min:\n",
        "                return mot_lower[:-len(suffixe)]\n",
        "        \n",
        "        return mot_lower\n",
        "    \n",
        "    def stem_texte(self, texte: str, methode: str = 'snowball') -> List[str]:\n",
        "        \"\"\"\n",
        "        Stemme un texte complet.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† stemmer\n",
        "            methode: M√©thode ('snowball' ou 'simple')\n",
        "        \n",
        "        Returns:\n",
        "            Liste des radicaux\n",
        "        \"\"\"\n",
        "        # Tokenisation simple\n",
        "        mots = re.findall(r'\\w+', texte.lower())\n",
        "        \n",
        "        if methode == 'snowball' and self.snowball_disponible:\n",
        "            return [self.stem_snowball(mot) for mot in mots]\n",
        "        else:\n",
        "            return [self.stem_simple(mot) for mot in mots]\n",
        "    \n",
        "    def comparer_methodes(self, mots: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compare les diff√©rentes m√©thodes de stemming.\n",
        "        \n",
        "        Args:\n",
        "            mots: Liste de mots √† tester\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame avec les comparaisons\n",
        "        \"\"\"\n",
        "        resultats = []\n",
        "        \n",
        "        for mot in mots:\n",
        "            resultat = {\n",
        "                'mot_original': mot,\n",
        "                'snowball': self.stem_snowball(mot) if self.snowball_disponible else 'N/A',\n",
        "                'simple': self.stem_simple(mot)\n",
        "            }\n",
        "            resultats.append(resultat)\n",
        "        \n",
        "        return pd.DataFrame(resultats)\n",
        "\n",
        "# Test du stemmer fran√ßais\n",
        "stemmer = StemmerFrancais()\n",
        "\n",
        "# Mots de test\n",
        "mots_test_stem = [\n",
        "    \"mangeons\", \"mangeait\", \"mangeable\", \"finissement\",\n",
        "    \"courions\", \"courait\", \"rapidement\", \"lentement\",\n",
        "    \"magnifique\", \"magnifiquement\", \"musiciens\", \"spectateurs\"\n",
        "]\n",
        "\n",
        "print(\"\\n‚úÇÔ∏è TEST DU STEMMING FRAN√áAIS\\n\")\n",
        "\n",
        "# Comparaison des m√©thodes\n",
        "if stemmer.snowball_disponible or True:  # Le stemmer simple est toujours disponible\n",
        "    comparaison = stemmer.comparer_methodes(mots_test_stem)\n",
        "    print(\"üìä Comparaison des m√©thodes de stemming :\")\n",
        "    print(comparaison.to_string(index=False))\n",
        "    \n",
        "    # Analyse des diff√©rences\n",
        "    if stemmer.snowball_disponible:\n",
        "        differences = sum(1 for _, row in comparaison.iterrows() \n",
        "                         if row['snowball'] != row['simple'])\n",
        "        print(f\"\\nüîç Diff√©rences entre m√©thodes : {differences}/{len(mots_test_stem)} mots\")\nelse:\n",
        "    print(\"‚ö†Ô∏è Aucun stemmer disponible pour les tests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ‚öîÔ∏è Comparaison D√©taill√©e : Lemmatisation vs Stemming\n",
        "\n",
        "Comparons les performances, la pr√©cision et l'usage des deux approches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComparateurLemmaStem:\n",
        "    \"\"\"Classe pour comparer lemmatisation et stemming.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.lemmatiseur = LemmatiseurSpacy()\n",
        "        self.stemmer = StemmerFrancais()\n",
        "    \n",
        "    def test_performance(self, texte: str, nb_iterations: int = 10) -> Dict:\n",
        "        \"\"\"\n",
        "        Teste les performances temporelles.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte de test\n",
        "            nb_iterations: Nombre d'it√©rations pour la mesure\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les temps de traitement\n",
        "        \"\"\"\n",
        "        # Test lemmatisation\n",
        "        if self.lemmatiseur.modele_charge:\n",
        "            start_time = time.time()\n",
        "            for _ in range(nb_iterations):\n",
        "                self.lemmatiseur.lemmatiser_texte(texte)\n",
        "            temps_lemma = (time.time() - start_time) / nb_iterations\n",
        "        else:\n",
        "            temps_lemma = 0\n",
        "        \n",
        "        # Test stemming\n",
        "        start_time = time.time()\n",
        "        for _ in range(nb_iterations):\n",
        "            self.stemmer.stem_texte(texte)\n",
        "        temps_stem = (time.time() - start_time) / nb_iterations\n",
        "        \n",
        "        return {\n",
        "            'temps_lemmatisation': temps_lemma,\n",
        "            'temps_stemming': temps_stem,\n",
        "            'ratio_vitesse': temps_lemma / temps_stem if temps_stem > 0 else 0\n",
        "        }\n",
        "    \n",
        "    def test_qualite(self, texte: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Teste la qualit√© des r√©sultats.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte de test\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les m√©triques de qualit√©\n",
        "        \"\"\"\n",
        "        # Extraction des mots\n",
        "        mots_originaux = re.findall(r'\\w+', texte.lower())\n",
        "        \n",
        "        # Lemmatisation\n",
        "        if self.lemmatiseur.modele_charge:\n",
        "            lemmes = self.lemmatiseur.lemmatiser_texte(texte)\n",
        "            vocabulaire_lemma = len(set(lemmes))\n",
        "            reduction_lemma = (len(set(mots_originaux)) - vocabulaire_lemma) / len(set(mots_originaux)) * 100\n",
        "        else:\n",
        "            lemmes = []\n",
        "            vocabulaire_lemma = 0\n",
        "            reduction_lemma = 0\n",
        "        \n",
        "        # Stemming\n",
        "        stems = self.stemmer.stem_texte(texte)\n",
        "        vocabulaire_stem = len(set(stems))\n",
        "        reduction_stem = (len(set(mots_originaux)) - vocabulaire_stem) / len(set(mots_originaux)) * 100\n",
        "        \n",
        "        return {\n",
        "            'mots_originaux': len(set(mots_originaux)),\n",
        "            'vocabulaire_lemmatisation': vocabulaire_lemma,\n",
        "            'vocabulaire_stemming': vocabulaire_stem,\n",
        "            'reduction_lemmatisation': round(reduction_lemma, 1),\n",
        "            'reduction_stemming': round(reduction_stem, 1),\n",
        "            'echantillon_lemmes': lemmes[:10],\n",
        "            'echantillon_stems': stems[:10]\n",
        "        }\n",
        "    \n",
        "    def test_lisibilite(self, mots: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Teste la lisibilit√© des r√©sultats (mots valides du fran√ßais).\n",
        "        \n",
        "        Args:\n",
        "            mots: Liste de mots √† tester\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les scores de lisibilit√©\n",
        "        \"\"\"\n",
        "        resultats = {'mot': [], 'lemme': [], 'stem': [], 'lemme_valide': [], 'stem_valide': []}\n",
        "        \n",
        "        for mot in mots:\n",
        "            # Lemmatisation\n",
        "            if self.lemmatiseur.modele_charge:\n",
        "                lemme = self.lemmatiseur.lemmatiser_mot(mot)\n",
        "                lemme_valide = self._mot_francais_valide(lemme)\n",
        "            else:\n",
        "                lemme = mot\n",
        "                lemme_valide = True\n",
        "            \n",
        "            # Stemming\n",
        "            stem = self.stemmer.stem_snowball(mot)\n",
        "            stem_valide = self._mot_francais_valide(stem)\n",
        "            \n",
        "            resultats['mot'].append(mot)\n",
        "            resultats['lemme'].append(lemme)\n",
        "            resultats['stem'].append(stem)\n",
        "            resultats['lemme_valide'].append(lemme_valide)\n",
        "            resultats['stem_valide'].append(stem_valide)\n",
        "        \n",
        "        # Calcul des scores\n",
        "        score_lemme = sum(resultats['lemme_valide']) / len(resultats['lemme_valide']) * 100\n",
        "        score_stem = sum(resultats['stem_valide']) / len(resultats['stem_valide']) * 100\n",
        "        \n",
        "        return {\n",
        "            'details': pd.DataFrame(resultats),\n",
        "            'score_lisibilite_lemme': round(score_lemme, 1),\n",
        "            'score_lisibilite_stem': round(score_stem, 1)\n",
        "        }\n",
        "    \n",
        "    def _mot_francais_valide(self, mot: str) -> bool:\n",
        "        \"\"\"\n",
        "        Heuristique simple pour v√©rifier si un mot ressemble √† du fran√ßais valide.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot √† v√©rifier\n",
        "        \n",
        "        Returns:\n",
        "            True si le mot semble valide\n",
        "        \"\"\"\n",
        "        # Crit√®res basiques de validit√©\n",
        "        if len(mot) < 2:\n",
        "            return False\n",
        "        \n",
        "        # Pas trop de consonnes cons√©cutives\n",
        "        consonnes_consecutives = 0\n",
        "        voyelles = 'aeiou√©√®√™√´√†√¢√§√Æ√Ø√¥√∂√π√ª√º√øy'\n",
        "        \n",
        "        for char in mot.lower():\n",
        "            if char in voyelles:\n",
        "                consonnes_consecutives = 0\n",
        "            else:\n",
        "                consonnes_consecutives += 1\n",
        "                if consonnes_consecutives > 3:\n",
        "                    return False\n",
        "        \n",
        "        # Doit contenir au moins une voyelle\n",
        "        if not any(char in voyelles for char in mot.lower()):\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def rapport_complet(self, texte: str) -> None:\n",
        "        \"\"\"\n",
        "        G√©n√®re un rapport de comparaison complet.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte pour les tests\n",
        "        \"\"\"\n",
        "        print(\"‚öîÔ∏è RAPPORT COMPARATIF COMPLET\\n\")\n",
        "        print(f\"üìù Texte test√© : {texte[:100]}...\\n\")\n",
        "        \n",
        "        # Test de performance\n",
        "        perf = self.test_performance(texte, nb_iterations=5)\n",
        "        print(\"üöÄ PERFORMANCE :\")\n",
        "        print(f\"  ‚Ä¢ Lemmatisation : {perf['temps_lemmatisation']*1000:.1f} ms\")\n",
        "        print(f\"  ‚Ä¢ Stemming : {perf['temps_stemming']*1000:.1f} ms\")\n",
        "        if perf['ratio_vitesse'] > 0:\n",
        "            print(f\"  ‚Ä¢ Stemming {perf['ratio_vitesse']:.1f}x plus rapide\")\n",
        "        \n",
        "        # Test de qualit√©\n",
        "        qualite = self.test_qualite(texte)\n",
        "        print(f\"\\nüìä QUALIT√â :\")\n",
        "        print(f\"  ‚Ä¢ Vocabulaire original : {qualite['mots_originaux']} mots uniques\")\n",
        "        print(f\"  ‚Ä¢ Apr√®s lemmatisation : {qualite['vocabulaire_lemmatisation']} ({qualite['reduction_lemmatisation']}% r√©duction)\")\n",
        "        print(f\"  ‚Ä¢ Apr√®s stemming : {qualite['vocabulaire_stemming']} ({qualite['reduction_stemming']}% r√©duction)\")\n",
        "        \n",
        "        print(f\"\\nüîç √âCHANTILLONS :\")\n",
        "        print(f\"  ‚Ä¢ Lemmes : {' '.join(qualite['echantillon_lemmes'])}\")\n",
        "        print(f\"  ‚Ä¢ Stems : {' '.join(qualite['echantillon_stems'])}\")\n",
        "\n",
        "# Test comparatif complet\n",
        "comparateur = ComparateurLemmaStem()\n",
        "\n",
        "texte_test_complet = \"\"\"\n",
        "Les musiciens jouaient magnifiquement dans la salle de concert. \n",
        "Les spectateurs applaudissaient enthusiastement apr√®s chaque morceau. \n",
        "Les instrumentistes d√©montrer une ma√Ætrise technique exceptionnelle. \n",
        "Cette soir√©e musicale restera m√©morable pour tous les participants.\n",
        "\"\"\"\n",
        "\n",
        "comparateur.rapport_complet(texte_test_complet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test de lisibilit√© d√©taill√©\n",
        "mots_lisibilite = [\n",
        "    \"magnifiquement\", \"rapidement\", \"courions\", \"finissaient\",\n",
        "    \"musiciens\", \"spectateurs\", \"applaudissaient\", \"d√©montrer\",\n",
        "    \"exceptionnelle\", \"m√©morable\", \"participants\", \"instrumentistes\"\n",
        "]\n",
        "\n",
        "if comparateur.lemmatiseur.modele_charge and comparateur.stemmer.snowball_disponible:\n",
        "    test_lisibilite = comparateur.test_lisibilite(mots_lisibilite)\n",
        "    \n",
        "    print(\"\\nüëÅÔ∏è TEST DE LISIBILIT√â\\n\")\n",
        "    print(\"üìã R√©sultats d√©taill√©s :\")\n",
        "    \n",
        "    # Affichage du tableau\n",
        "    df_lisibilite = test_lisibilite['details']\n",
        "    df_affichage = df_lisibilite[['mot', 'lemme', 'stem', 'lemme_valide', 'stem_valide']].copy()\n",
        "    \n",
        "    # Formatage pour l'affichage\n",
        "    df_affichage['lemme_valide'] = df_affichage['lemme_valide'].map({True: '‚úÖ', False: '‚ùå'})\n",
        "    df_affichage['stem_valide'] = df_affichage['stem_valide'].map({True: '‚úÖ', False: '‚ùå'})\n",
        "    \n",
        "    print(df_affichage.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nüìä Scores de lisibilit√© :\")\n",
        "    print(f\"  ‚Ä¢ Lemmatisation : {test_lisibilite['score_lisibilite_lemme']}% de mots valides\")\n",
        "    print(f\"  ‚Ä¢ Stemming : {test_lisibilite['score_lisibilite_stem']}% de mots valides\")\n",
        "    \n",
        "    # Recommandation\n",
        "    if test_lisibilite['score_lisibilite_lemme'] > test_lisibilite['score_lisibilite_stem']:\n",
        "        print(f\"\\nüí° Recommandation : La lemmatisation produit des r√©sultats plus lisibles\")\n",
        "    else:\n",
        "        print(f\"\\nüí° Recommandation : Le stemming donne des r√©sultats acceptables\")\nelse:\n",
        "    print(\"‚ö†Ô∏è Test de lisibilit√© non disponible (spaCy ou NLTK requis)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. üéØ Cas Difficiles et Solutions\n",
        "\n",
        "Comment g√©rer les cas complexes de la lemmatisation fran√ßaise ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cas difficiles sp√©cifiques au fran√ßais\n",
        "cas_difficiles = {\n",
        "    \"Verbes irr√©guliers\": {\n",
        "        \"mots\": [\"va\", \"vont\", \"irai\", \"iras\", \"suis\", \"es\", \"est\", \"sommes\", \"√™tes\", \"sont\"],\n",
        "        \"lemmes_attendus\": [\"aller\", \"aller\", \"aller\", \"aller\", \"√™tre\", \"√™tre\", \"√™tre\", \"√™tre\", \"√™tre\", \"√™tre\"],\n",
        "        \"difficulte\": \"Formes radicalement diff√©rentes du lemme\"\n",
        "    },\n",
        "    \n",
        "    \"Participes pass√©s irr√©guliers\": {\n",
        "        \"mots\": [\"pris\", \"prise\", \"prises\", \"mis\", \"mise\", \"mises\", \"fait\", \"faite\", \"faites\"],\n",
        "        \"lemmes_attendus\": [\"prendre\", \"prendre\", \"prendre\", \"mettre\", \"mettre\", \"mettre\", \"faire\", \"faire\", \"faire\"],\n",
        "        \"difficulte\": \"Participes tr√®s diff√©rents de l'infinitif\"\n",
        "    },\n",
        "    \n",
        "    \"Homonymie contextuelle\": {\n",
        "        \"mots\": [\"fils\", \"fils\", \"livre\", \"livre\", \"rose\", \"rose\"],\n",
        "        \"contextes\": [\"le fils de m√©tal\", \"mon fils\", \"un livre rouge\", \"il livre demain\", \"une rose rouge\", \"elle rose\"],\n",
        "        \"lemmes_attendus\": [\"fil\", \"fils\", \"livre\", \"livrer\", \"rose\", \"roser\"],\n",
        "        \"difficulte\": \"M√™me forme, sens diff√©rents selon contexte\"\n",
        "    },\n",
        "    \n",
        "    \"Mots compos√©s\": {\n",
        "        \"mots\": [\"aujourd'hui\", \"c'est-√†-dire\", \"peut-√™tre\", \"quelqu'un\", \"rendez-vous\"],\n",
        "        \"lemmes_attendus\": [\"aujourd'hui\", \"c'est-√†-dire\", \"peut-√™tre\", \"quelqu'un\", \"rendez-vous\"],\n",
        "        \"difficulte\": \"Gestion des apostrophes et traits d'union\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def tester_cas_difficiles() -> None:\n",
        "    \"\"\"\n",
        "    Teste la lemmatisation sur les cas difficiles du fran√ßais.\n",
        "    \"\"\"\n",
        "    print(\"üéØ CAS DIFFICILES DE LA LEMMATISATION FRAN√áAISE\\n\")\n",
        "    \n",
        "    if not lemmatiseur.modele_charge:\n",
        "        print(\"‚ö†Ô∏è Tests non disponibles (spaCy requis)\")\n",
        "        return\n",
        "    \n",
        "    for categorie, info in cas_difficiles.items():\n",
        "        print(f\"üìÇ {categorie.upper()}\")\n",
        "        print(f\"   üí° Difficult√© : {info['difficulte']}\")\n",
        "        print(f\"   üß™ Tests :\")\n",
        "        \n",
        "        # Cas avec contexte (homonymie)\n",
        "        if 'contextes' in info:\n",
        "            for i, contexte in enumerate(info['contextes']):\n",
        "                attendu = info['lemmes_attendus'][i]\n",
        "                \n",
        "                # Analyse du contexte complet\n",
        "                doc = nlp_fr(contexte)\n",
        "                mot_cible = info['mots'][i]\n",
        "                \n",
        "                # Trouver le lemme du mot cible dans le contexte\n",
        "                lemme_obtenu = \"Non trouv√©\"\n",
        "                for token in doc:\n",
        "                    if token.text.lower() == mot_cible.lower():\n",
        "                        lemme_obtenu = token.lemma_\n",
        "                        break\n",
        "                \n",
        "                # V√©rification\n",
        "                if lemme_obtenu.lower() == attendu.lower():\n",
        "                    status = \"‚úÖ\"\n",
        "                else:\n",
        "                    status = \"‚ùå\"\n",
        "                \n",
        "                print(f\"      '{mot_cible}' dans '{contexte}' ‚Üí {lemme_obtenu} (attendu: {attendu}) {status}\")\n",
        "        \n",
        "        # Cas normaux\n",
        "        else:\n",
        "            for i, mot in enumerate(info['mots']):\n",
        "                attendu = info['lemmes_attendus'][i]\n",
        "                lemme_obtenu = lemmatiseur.lemmatiser_mot(mot)\n",
        "                \n",
        "                # V√©rification\n",
        "                if lemme_obtenu.lower() == attendu.lower():\n",
        "                    status = \"‚úÖ\"\n",
        "                else:\n",
        "                    status = \"‚ùå\"\n",
        "                \n",
        "                print(f\"      {mot:12s} ‚Üí {lemme_obtenu:12s} (attendu: {attendu:10s}) {status}\")\n",
        "        \n",
        "        print()\n",
        "\n",
        "# Test des cas difficiles\n",
        "tester_cas_difficiles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solutions pour am√©liorer la lemmatisation\n",
        "class LemmatiseurAmeliore:\n",
        "    \"\"\"Lemmatiseur avec corrections pour les cas difficiles fran√ßais.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.lemmatiseur_base = LemmatiseurSpacy()\n",
        "        \n",
        "        # Dictionnaire de corrections manuelles\n",
        "        self.corrections_manuelles = {\n",
        "            # Verbes irr√©guliers probl√©matiques\n",
        "            'va': 'aller',\n",
        "            'vont': 'aller',\n",
        "            'irai': 'aller',\n",
        "            'iras': 'aller',\n",
        "            'suis': '√™tre',  # ambigu√´ avec \"suivre\"\n",
        "            'fus': '√™tre',\n",
        "            \n",
        "            # Participes pass√©s difficiles\n",
        "            'pris': 'prendre',\n",
        "            'prise': 'prendre',\n",
        "            'prises': 'prendre',\n",
        "            'mis': 'mettre',\n",
        "            'mise': 'mettre',\n",
        "            'mises': 'mettre',\n",
        "            \n",
        "            # Mots compos√©s\n",
        "            \"aujourd'hui\": \"aujourd'hui\",\n",
        "            \"c'est-√†-dire\": \"c'est-√†-dire\",\n",
        "            \"peut-√™tre\": \"peut-√™tre\"\n",
        "        }\n",
        "        \n",
        "        # R√®gles contextuelles pour l'homonymie\n",
        "        self.regles_contextuelles = {\n",
        "            'fils': {\n",
        "                'fil': ['m√©tal', 'coton', 'couture', 'tissu'],\n",
        "                'fils': ['enfant', 'famille', 'papa', 'maman']\n",
        "            },\n",
        "            'livre': {\n",
        "                'livre': ['page', 'roman', 'lecture', 'biblioth√®que'],\n",
        "                'livrer': ['commande', 'colis', 'transport', 'demain']\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def lemmatiser_avec_contexte(self, texte: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Lemmatise en tenant compte du contexte et des corrections.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte √† lemmatiser\n",
        "        \n",
        "        Returns:\n",
        "            Liste des lemmes corrig√©s\n",
        "        \"\"\"\n",
        "        if not self.lemmatiseur_base.modele_charge:\n",
        "            return texte.split()\n",
        "        \n",
        "        # Analyse avec spaCy\n",
        "        doc = nlp_fr(texte)\n",
        "        lemmes_corriges = []\n",
        "        \n",
        "        for i, token in enumerate(doc):\n",
        "            if token.is_punct or token.is_space:\n",
        "                continue\n",
        "            \n",
        "            mot = token.text.lower()\n",
        "            lemme_base = token.lemma_.lower()\n",
        "            \n",
        "            # 1. V√©rifier les corrections manuelles\n",
        "            if mot in self.corrections_manuelles:\n",
        "                lemme_final = self.corrections_manuelles[mot]\n",
        "            \n",
        "            # 2. Appliquer les r√®gles contextuelles\n",
        "            elif mot in self.regles_contextuelles:\n",
        "                lemme_final = self._resoudre_homonymie(mot, doc, i)\n",
        "            \n",
        "            # 3. Utiliser le lemme spaCy par d√©faut\n",
        "            else:\n",
        "                lemme_final = lemme_base\n",
        "            \n",
        "            lemmes_corriges.append(lemme_final)\n",
        "        \n",
        "        return lemmes_corriges\n",
        "    \n",
        "    def _resoudre_homonymie(self, mot: str, doc, position: int) -> str:\n",
        "        \"\"\"\n",
        "        R√©sout l'homonymie en analysant le contexte.\n",
        "        \n",
        "        Args:\n",
        "            mot: Mot ambigu\n",
        "            doc: Document spaCy\n",
        "            position: Position du mot dans le document\n",
        "        \n",
        "        Returns:\n",
        "            Lemme le plus probable\n",
        "        \"\"\"\n",
        "        if mot not in self.regles_contextuelles:\n",
        "            return doc[position].lemma_.lower()\n",
        "        \n",
        "        # Extraire le contexte (mots environnants)\n",
        "        contexte_mots = []\n",
        "        fenetre = 3  # Mots avant et apr√®s\n",
        "        \n",
        "        for i in range(max(0, position - fenetre), \n",
        "                      min(len(doc), position + fenetre + 1)):\n",
        "            if i != position and not doc[i].is_punct:\n",
        "                contexte_mots.append(doc[i].text.lower())\n",
        "        \n",
        "        # Calculer les scores pour chaque sens possible\n",
        "        scores = {}\n",
        "        for sens, mots_cles in self.regles_contextuelles[mot].items():\n",
        "            score = sum(1 for mot_cle in mots_cles \n",
        "                       if any(mot_cle in mot_contexte for mot_contexte in contexte_mots))\n",
        "            scores[sens] = score\n",
        "        \n",
        "        # Retourner le sens avec le meilleur score\n",
        "        if scores:\n",
        "            meilleur_sens = max(scores, key=scores.get)\n",
        "            return meilleur_sens\n",
        "        \n",
        "        # Par d√©faut, utiliser spaCy\n",
        "        return doc[position].lemma_.lower()\n",
        "    \n",
        "    def comparer_avec_base(self, texte: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Compare les r√©sultats avec et sans am√©liorations.\n",
        "        \n",
        "        Args:\n",
        "            texte: Texte de test\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec la comparaison\n",
        "        \"\"\"\n",
        "        lemmes_base = self.lemmatiseur_base.lemmatiser_texte(texte)\n",
        "        lemmes_ameliores = self.lemmatiser_avec_contexte(texte)\n",
        "        \n",
        "        # Identifier les diff√©rences\n",
        "        differences = []\n",
        "        for i, (base, ameliore) in enumerate(zip(lemmes_base, lemmes_ameliores)):\n",
        "            if base != ameliore:\n",
        "                differences.append({\n",
        "                    'position': i,\n",
        "                    'lemme_base': base,\n",
        "                    'lemme_ameliore': ameliore\n",
        "                })\n",
        "        \n",
        "        return {\n",
        "            'lemmes_base': lemmes_base,\n",
        "            'lemmes_ameliores': lemmes_ameliores,\n",
        "            'nb_differences': len(differences),\n",
        "            'differences': differences\n",
        "        }\n",
        "\n",
        "# Test du lemmatiseur am√©lior√©\n",
        "lemmatiseur_ameliore = LemmatiseurAmeliore()\n",
        "\n",
        "textes_test_amelioration = [\n",
        "    \"Je vais chez mon fils avec un livre.\",\n",
        "    \"Il livre le fils de m√©tal aujourd'hui.\",\n",
        "    \"Les roses sont prises dans le livre.\",\n",
        "    \"Mon fils a pris le livre et va le livrer.\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ TEST DU LEMMATISEUR AM√âLIOR√â\\n\")\n",
        "\n",
        "if lemmatiseur_ameliore.lemmatiseur_base.modele_charge:\n",
        "    for i, texte in enumerate(textes_test_amelioration, 1):\n",
        "        print(f\"üìù Test {i}: {texte}\")\n",
        "        \n",
        "        comparaison = lemmatiseur_ameliore.comparer_avec_base(texte)\n",
        "        \n",
        "        print(f\"   Base     : {' '.join(comparaison['lemmes_base'])}\")\n",
        "        print(f\"   Am√©lior√© : {' '.join(comparaison['lemmes_ameliores'])}\")\n",
        "        \n",
        "        if comparaison['nb_differences'] > 0:\n",
        "            print(f\"   üîß {comparaison['nb_differences']} am√©lioration(s) d√©tect√©e(s)\")\n",
        "            for diff in comparaison['differences']:\n",
        "                print(f\"      '{diff['lemme_base']}' ‚Üí '{diff['lemme_ameliore']}'\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Aucune am√©lioration n√©cessaire\")\n",
        "        print()\nelse:\n",
        "    print(\"‚ö†Ô∏è Test du lemmatiseur am√©lior√© non disponible (spaCy requis)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. üìä Tests et √âvaluation\n",
        "\n",
        "Comment √©valuer la qualit√© de votre lemmatisation ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Framework d'√©valuation pour la lemmatisation\n",
        "class EvaluateurLemmatisation:\n",
        "    \"\"\"√âvalue la qualit√© de la lemmatisation fran√ßaise.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Jeu de test avec v√©rit√© terrain\n",
        "        self.jeu_test = {\n",
        "            # Verbes r√©guliers\n",
        "            'mangeons': 'manger',\n",
        "            'mangeait': 'manger',\n",
        "            'finissons': 'finir',\n",
        "            'finissait': 'finir',\n",
        "            \n",
        "            # Verbes irr√©guliers\n",
        "            'va': 'aller',\n",
        "            'vont': 'aller',\n",
        "            'suis': '√™tre',  # contexte d√©pendant\n",
        "            'sommes': '√™tre',\n",
        "            \n",
        "            # Noms\n",
        "            'chats': 'chat',\n",
        "            'animaux': 'animal',\n",
        "            'chevaux': 'cheval',\n",
        "            \n",
        "            # Adjectifs\n",
        "            'beaux': 'beau',\n",
        "            'belles': 'beau',\n",
        "            'grandes': 'grand',\n",
        "            \n",
        "            # Participes pass√©s\n",
        "            'mang√©es': 'manger',\n",
        "            'finies': 'finir',\n",
        "            'prises': 'prendre'\n",
        "        }\n",
        "    \n",
        "    def evaluer_lemmatiseur(self, lemmatiseur, nom: str = \"Lemmatiseur\") -> Dict:\n",
        "        \"\"\"\n",
        "        √âvalue un lemmatiseur sur le jeu de test.\n",
        "        \n",
        "        Args:\n",
        "            lemmatiseur: Objet avec m√©thode lemmatiser_mot\n",
        "            nom: Nom du lemmatiseur pour l'affichage\n",
        "        \n",
        "        Returns:\n",
        "            Dictionnaire avec les m√©triques d'√©valuation\n",
        "        \"\"\"\n",
        "        resultats = []\n",
        "        correct = 0\n",
        "        \n",
        "        for mot, lemme_attendu in self.jeu_test.items():\n",
        "            if hasattr(lemmatiseur, 'lemmatiser_mot'):\n",
        "                lemme_obtenu = lemmatiseur.lemmatiser_mot(mot)\n",
        "            elif hasattr(lemmatiseur, 'stem'):\n",
        "                lemme_obtenu = lemmatiseur.stem(mot)\n",
        "            else:\n",
        "                lemme_obtenu = mot\n",
        "            \n",
        "            est_correct = lemme_obtenu.lower() == lemme_attendu.lower()\n",
        "            if est_correct:\n",
        "                correct += 1\n",
        "            \n",
        "            resultats.append({\n",
        "                'mot': mot,\n",
        "                'attendu': lemme_attendu,\n",
        "                'obtenu': lemme_obtenu,\n",
        "                'correct': est_correct\n",
        "            })\n",
        "        \n",
        "        precision = correct / len(self.jeu_test) * 100\n",
        "        \n",
        "        return {\n",
        "            'nom': nom,\n",
        "            'precision': round(precision, 1),\n",
        "            'correct': correct,\n",
        "            'total': len(self.jeu_test),\n",
        "            'details': resultats\n",
        "        }\n",
        "    \n",
        "    def comparer_lemmatiseurs(self, lemmatiseurs: Dict) -> None:\n",
        "        \"\"\"\n",
        "        Compare plusieurs lemmatiseurs.\n",
        "        \n",
        "        Args:\n",
        "            lemmatiseurs: Dictionnaire {nom: lemmatiseur}\n",
        "        \"\"\"\n",
        "        print(\"üìä √âVALUATION COMPARATIVE DES LEMMATISEURS\\n\")\n",
        "        \n",
        "        resultats_globaux = []\n",
        "        \n",
        "        for nom, lemmatiseur in lemmatiseurs.items():\n",
        "            evaluation = self.evaluer_lemmatiseur(lemmatiseur, nom)\n",
        "            resultats_globaux.append(evaluation)\n",
        "            \n",
        "            print(f\"üîß {nom.upper()}\")\n",
        "            print(f\"   Pr√©cision : {evaluation['precision']}% ({evaluation['correct']}/{evaluation['total']})\")\n",
        "            \n",
        "            # Afficher quelques erreurs\n",
        "            erreurs = [r for r in evaluation['details'] if not r['correct']]\n",
        "            if erreurs:\n",
        "                print(f\"   Erreurs : {', '.join([f\"{e['mot']}‚Üí{e['obtenu']}\" for e in erreurs[:3]])}...\")\n",
        "            print()\n",
        "        \n",
        "        # Classement\n",
        "        resultats_globaux.sort(key=lambda x: x['precision'], reverse=True)\n",
        "        print(\"üèÜ CLASSEMENT :\")\n",
        "        for i, result in enumerate(resultats_globaux, 1):\n",
        "            print(f\"   {i}. {result['nom']} : {result['precision']}%\")\n",
        "    \n",
        "    def analyser_erreurs(self, evaluation: Dict) -> None:\n",
        "        \"\"\"\n",
        "        Analyse d√©taill√©e des erreurs.\n",
        "        \n",
        "        Args:\n",
        "            evaluation: R√©sultat d'√©valuation\n",
        "        \"\"\"\n",
        "        erreurs = [r for r in evaluation['details'] if not r['correct']]\n",
        "        \n",
        "        if not erreurs:\n",
        "            print(f\"‚úÖ Aucune erreur d√©tect√©e pour {evaluation['nom']}\")\n",
        "            return\n",
        "        \n",
        "        print(f\"üîç ANALYSE DES ERREURS - {evaluation['nom'].upper()}\\n\")\n",
        "        \n",
        "        # Classification des erreurs\n",
        "        types_erreurs = {\n",
        "            'verbes_irreguliers': ['va', 'vont', 'suis', 'sommes'],\n",
        "            'pluriels_irreguliers': ['animaux', 'chevaux'],\n",
        "            'accords_participes': ['mang√©es', 'finies', 'prises'],\n",
        "            'adjectifs_irreguliers': ['beaux', 'belles']\n",
        "        }\n",
        "        \n",
        "        erreurs_par_type = defaultdict(list)\n",
        "        \n",
        "        for erreur in erreurs:\n",
        "            mot = erreur['mot']\n",
        "            type_trouve = 'autres'\n",
        "            \n",
        "            for type_erreur, mots_type in types_erreurs.items():\n",
        "                if mot in mots_type:\n",
        "                    type_trouve = type_erreur\n",
        "                    break\n",
        "            \n",
        "            erreurs_par_type[type_trouve].append(erreur)\n",
        "        \n",
        "        # Affichage par type\n",
        "        for type_erreur, erreurs_type in erreurs_par_type.items():\n",
        "            print(f\"üìÇ {type_erreur.replace('_', ' ').title()} ({len(erreurs_type)} erreurs) :\")\n",
        "            for erreur in erreurs_type:\n",
        "                print(f\"   '{erreur['mot']}' ‚Üí '{erreur['obtenu']}' (attendu: '{erreur['attendu']}')\")\n",
        "            print()\n",
        "\n",
        "# Test d'√©valuation\n",
        "evaluateur = EvaluateurLemmatisation()\n",
        "\n",
        "# Pr√©paration des lemmatiseurs √† tester\n",
        "lemmatiseurs_test = {}\n",
        "\n",
        "if lemmatiseur.modele_charge:\n",
        "    lemmatiseurs_test['spaCy'] = lemmatiseur\n",
        "\n",
        "if lemmatiseur_ameliore.lemmatiseur_base.modele_charge:\n",
        "    lemmatiseurs_test['spaCy Am√©lior√©'] = lemmatiseur_ameliore\n",
        "\n",
        "if stemmer.snowball_disponible:\n",
        "    lemmatiseurs_test['Stemming Snowball'] = stemmer\n",
        "\n",
        "if lemmatiseurs_test:\n",
        "    # Comparaison des lemmatiseurs\n",
        "    evaluateur.comparer_lemmatiseurs(lemmatiseurs_test)\n",
        "    \n",
        "    # Analyse d√©taill√©e des erreurs pour le meilleur lemmatiseur\n",
        "    if 'spaCy' in lemmatiseurs_test:\n",
        "        evaluation_spacy = evaluateur.evaluer_lemmatiseur(lemmatiseurs_test['spaCy'], 'spaCy')\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        evaluateur.analyser_erreurs(evaluation_spacy)\nelse:\n",
        "    print(\"‚ö†Ô∏è Aucun lemmatiseur disponible pour l'√©valuation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des r√©sultats d'√©valuation\n",
        "if lemmatiseurs_test:\n",
        "    # Collecte des donn√©es pour visualisation\n",
        "    donnees_viz = []\n",
        "    \n",
        "    for nom, lemmatiseur in lemmatiseurs_test.items():\n",
        "        evaluation = evaluateur.evaluer_lemmatiseur(lemmatiseur, nom)\n",
        "        donnees_viz.append({\n",
        "            'Lemmatiseur': nom,\n",
        "            'Pr√©cision (%)': evaluation['precision'],\n",
        "            'Correct': evaluation['correct'],\n",
        "            'Erreurs': evaluation['total'] - evaluation['correct']\n",
        "        })\n",
        "    \n",
        "    df_viz = pd.DataFrame(donnees_viz)\n",
        "    \n",
        "    # Cr√©ation des graphiques\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Graphique 1 : Pr√©cision par lemmatiseur\n",
        "    bars1 = axes[0].bar(df_viz['Lemmatiseur'], df_viz['Pr√©cision (%)'], \n",
        "                        color=['lightblue', 'lightgreen', 'lightcoral'][:len(df_viz)], \n",
        "                        alpha=0.7, edgecolor='darkblue')\n",
        "    axes[0].set_title('üìä Pr√©cision des Lemmatiseurs', fontweight='bold', fontsize=14)\n",
        "    axes[0].set_ylabel('Pr√©cision (%)')\n",
        "    axes[0].set_ylim(0, 100)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Ajout des valeurs sur les barres\n",
        "    for bar, value in zip(bars1, df_viz['Pr√©cision (%)']):\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                     f'{value}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Graphique 2 : R√©partition correct/erreurs\n",
        "    x = np.arange(len(df_viz))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars2 = axes[1].bar(x - width/2, df_viz['Correct'], width, \n",
        "                        label='Correct', color='lightgreen', alpha=0.7)\n",
        "    bars3 = axes[1].bar(x + width/2, df_viz['Erreurs'], width, \n",
        "                        label='Erreurs', color='lightcoral', alpha=0.7)\n",
        "    \n",
        "    axes[1].set_title('üìà R√©partition Correct/Erreurs', fontweight='bold', fontsize=14)\n",
        "    axes[1].set_ylabel('Nombre de mots')\n",
        "    axes[1].set_xticks(x)\n",
        "    axes[1].set_xticklabels(df_viz['Lemmatiseur'], rotation=45)\n",
        "    axes[1].legend()\n",
        "    \n",
        "    # Ajout des valeurs\n",
        "    for bars in [bars2, bars3]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                         f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Tableau r√©capitulatif\n",
        "    print(\"\\nüìã TABLEAU R√âCAPITULATIF :\")\n",
        "    print(df_viz.to_string(index=False))\nelse:\n",
        "    print(\"‚ö†Ô∏è Aucune donn√©e pour la visualisation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. üí° Recommandations Pratiques\n",
        "\n",
        "Quand utiliser quoi et comment optimiser vos choix ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guide de recommandations pratiques\n",
        "recommandations = {\n",
        "    \"Analyse de sentiment\": {\n",
        "        \"recommandation\": \"Lemmatisation spaCy\",\n",
        "        \"raison\": \"Pr√©cision importante pour capturer les nuances √©motionnelles\",\n",
        "        \"exemple\": \"'magnifiques' ‚Üí 'magnifique' vs 'magnif' (stemming)\",\n",
        "        \"performance\": \"√âlev√©e\",\n",
        "        \"cas_usage\": [\"Avis clients\", \"R√©seaux sociaux\", \"Satisfaction\"]\n",
        "    },\n",
        "    \n",
        "    \"Classification de documents\": {\n",
        "        \"recommandation\": \"Stemming Snowball\",\n",
        "        \"raison\": \"R√©duction maximale du vocabulaire, vitesse √©lev√©e\",\n",
        "        \"exemple\": \"Plus de compression pour machine learning\",\n",
        "        \"performance\": \"Tr√®s √©lev√©e\",\n",
        "        \"cas_usage\": [\"Cat√©gorisation automatique\", \"Filtrage spam\", \"Clustering\"]\n",
        "    },\n",
        "    \n",
        "    \"Recherche d'information\": {\n",
        "        \"recommandation\": \"Lemmatisation + Stemming hybride\",\n",
        "        \"raison\": \"√âquilibre entre pr√©cision et rappel\",\n",
        "        \"exemple\": \"Rechercher 'course' doit trouver 'courions', 'coureur'\",\n",
        "        \"performance\": \"Moyenne\",\n",
        "        \"cas_usage\": [\"Moteurs de recherche\", \"FAQ automatis√©es\", \"Documentation\"]\n",
        "    },\n",
        "    \n",
        "    \"Analyse syntaxique/s√©mantique\": {\n",
        "        \"recommandation\": \"Lemmatisation spaCy avanc√©e\",\n",
        "        \"raison\": \"Pr√©servation du sens et de la structure grammaticale\",\n",
        "        \"exemple\": \"Analyse des relations sujet-verbe-compl√©ment\",\n",
        "        \"performance\": \"Moyenne\",\n",
        "        \"cas_usage\": [\"Extraction d'entit√©s\", \"R√©sum√© automatique\", \"Traduction\"]\n",
        "    },\n",
        "    \n",
        "    \"Gros volumes (Big Data)\": {\n",
        "        \"recommandation\": \"Stemming simple optimis√©\",\n",
        "        \"raison\": \"Vitesse primordiale sur millions de documents\",\n",
        "        \"exemple\": \"Traitement temps r√©el de flux Twitter\",\n",
        "        \"performance\": \"Tr√®s √©lev√©e\",\n",
        "        \"cas_usage\": [\"Streaming\", \"Logs\", \"Monitoring\"]\n",
        "    },\n",
        "    \n",
        "    \"Interface utilisateur\": {\n",
        "        \"recommandation\": \"Lemmatisation spaCy\",\n",
        "        \"raison\": \"R√©sultats lisibles et compr√©hensibles\",\n",
        "        \"exemple\": \"Suggestions de recherche avec mots valides\",\n",
        "        \"performance\": \"Moyenne\",\n",
        "        \"cas_usage\": [\"Autocompl√©tion\", \"Suggestions\", \"Corrections\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "def afficher_recommandations() -> None:\n",
        "    \"\"\"\n",
        "    Affiche le guide des recommandations pratiques.\n",
        "    \"\"\"\n",
        "    print(\"üí° GUIDE DES RECOMMANDATIONS PRATIQUES\\n\")\n",
        "    \n",
        "    for contexte, info in recommandations.items():\n",
        "        print(f\"üìÇ {contexte.upper()}\")\n",
        "        print(f\"   üéØ Recommandation : {info['recommandation']}\")\n",
        "        print(f\"   üí≠ Raison : {info['raison']}\")\n",
        "        print(f\"   üìù Exemple : {info['exemple']}\")\n",
        "        print(f\"   ‚ö° Performance : {info['performance']}\")\n",
        "        print(f\"   üîß Cas d'usage : {', '.join(info['cas_usage'])}\")\n",
        "        print()\n",
        "\n",
        "afficher_recommandations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outil d'aide √† la d√©cision\n",
        "def aide_decision_lemmatisation(contexte: str = None, \n",
        "                               priorite_vitesse: bool = False,\n",
        "                               priorite_precision: bool = False,\n",
        "                               volume_donnees: str = \"moyen\") -> None:\n",
        "    \"\"\"\n",
        "    Aide √† choisir la meilleure approche de lemmatisation.\n",
        "    \n",
        "    Args:\n",
        "        contexte: Contexte d'application\n",
        "        priorite_vitesse: Si la vitesse est prioritaire\n",
        "        priorite_precision: Si la pr√©cision est prioritaire\n",
        "        volume_donnees: Volume de donn√©es (\"petit\", \"moyen\", \"gros\")\n",
        "    \"\"\"\n",
        "    print(\"üß≠ AIDE √Ä LA D√âCISION : LEMMATISATION\\n\")\n",
        "    \n",
        "    # Analyse des crit√®res\n",
        "    score_lemma = 0\n",
        "    score_stem = 0\n",
        "    \n",
        "    # Crit√®re : priorit√© vitesse\n",
        "    if priorite_vitesse:\n",
        "        score_stem += 3\n",
        "        print(\"‚ö° Priorit√© vitesse ‚Üí +3 points pour Stemming\")\n",
        "    \n",
        "    # Crit√®re : priorit√© pr√©cision\n",
        "    if priorite_precision:\n",
        "        score_lemma += 3\n",
        "        print(\"üéØ Priorit√© pr√©cision ‚Üí +3 points pour Lemmatisation\")\n",
        "    \n",
        "    # Crit√®re : volume de donn√©es\n",
        "    if volume_donnees == \"gros\":\n",
        "        score_stem += 2\n",
        "        print(\"üìä Gros volume ‚Üí +2 points pour Stemming\")\n",
        "    elif volume_donnees == \"petit\":\n",
        "        score_lemma += 1\n",
        "        print(\"üìä Petit volume ‚Üí +1 point pour Lemmatisation\")\n",
        "    \n",
        "    # Crit√®re : contexte sp√©cifique\n",
        "    contextes_lemma = [\"sentiment\", \"interface\", \"s√©mantique\"]\n",
        "    contextes_stem = [\"classification\", \"clustering\", \"big data\"]\n",
        "    \n",
        "    if contexte:\n",
        "        if any(ctx in contexte.lower() for ctx in contextes_lemma):\n",
        "            score_lemma += 2\n",
        "            print(f\"üé™ Contexte '{contexte}' ‚Üí +2 points pour Lemmatisation\")\n",
        "        elif any(ctx in contexte.lower() for ctx in contextes_stem):\n",
        "            score_stem += 2\n",
        "            print(f\"üé™ Contexte '{contexte}' ‚Üí +2 points pour Stemming\")\n",
        "    \n",
        "    # D√©cision finale\n",
        "    print(f\"\\nüìä SCORES FINAUX :\")\n",
        "    print(f\"   üå± Lemmatisation : {score_lemma} points\")\n",
        "    print(f\"   ‚úÇÔ∏è Stemming : {score_stem} points\")\n",
        "    \n",
        "    if score_lemma > score_stem:\n",
        "        print(f\"\\nüèÜ RECOMMANDATION : Lemmatisation (spaCy)\")\n",
        "        print(f\"   üí° Pourquoi : Meilleure pr√©cision pour votre contexte\")\n",
        "        if lemmatiseur.modele_charge:\n",
        "            print(f\"   üõ†Ô∏è Impl√©mentation : Utilisez LemmatiseurSpacy()\")\n",
        "    elif score_stem > score_lemma:\n",
        "        print(f\"\\nüèÜ RECOMMANDATION : Stemming (Snowball)\")\n",
        "        print(f\"   üí° Pourquoi : Vitesse et efficacit√© pour votre contexte\")\n",
        "        if stemmer.snowball_disponible:\n",
        "            print(f\"   üõ†Ô∏è Impl√©mentation : Utilisez StemmerFrancais()\")\n",
        "    else:\n",
        "        print(f\"\\nü§ù RECOMMANDATION : Approche hybride\")\n",
        "        print(f\"   üí° Pourquoi : Vos crit√®res sont √©quilibr√©s\")\n",
        "        print(f\"   üõ†Ô∏è Impl√©mentation : Testez les deux et choisissez selon les r√©sultats\")\n",
        "\n",
        "# Tests de l'aide √† la d√©cision\n",
        "print(\"üß™ TESTS DE L'AIDE √Ä LA D√âCISION\\n\")\n",
        "\n",
        "# Cas 1 : Analyse de sentiment\n",
        "print(\"üí¨ Cas 1 : Analyse de sentiment d'avis clients\")\n",
        "aide_decision_lemmatisation(\n",
        "    contexte=\"analyse sentiment\",\n",
        "    priorite_precision=True,\n",
        "    volume_donnees=\"moyen\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Cas 2 : Classification de documents\n",
        "print(\"üìã Cas 2 : Classification automatique de millions de documents\")\n",
        "aide_decision_lemmatisation(\n",
        "    contexte=\"classification big data\",\n",
        "    priorite_vitesse=True,\n",
        "    volume_donnees=\"gros\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Cas 3 : Interface utilisateur\n",
        "print(\"üñ•Ô∏è Cas 3 : Suggestions de recherche pour interface utilisateur\")\n",
        "aide_decision_lemmatisation(\n",
        "    contexte=\"interface recherche\",\n",
        "    priorite_precision=True,\n",
        "    volume_donnees=\"petit\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Conclusion et Synth√®se\n",
        "\n",
        "### ‚úÖ Ce que vous avez appris :\n",
        "\n",
        "1. **ü§î Concepts fondamentaux** : Diff√©rence claire entre lemmatisation et stemming\n",
        "2. **üá´üá∑ Sp√©cificit√©s fran√ßaises** : Verbes irr√©guliers, homonymie, accords complexes\n",
        "3. **üöÄ Outils pratiques** : Ma√Ætrise de spaCy et NLTK pour le fran√ßais\n",
        "4. **‚öîÔ∏è Comparaisons objectives** : Performance, qualit√©, cas d'usage\n",
        "5. **üéØ Gestion des cas difficiles** : Solutions pour am√©liorer la pr√©cision\n",
        "6. **üìä √âvaluation rigoureuse** : M√©triques et benchmarks pour valider vos choix\n",
        "7. **üí° Aide √† la d√©cision** : Recommandations selon votre contexte\n",
        "\n",
        "### üèÜ Recommandations finales :\n",
        "\n",
        "#### üå± Utilisez la LEMMATISATION quand :\n",
        "- L'**exactitude** est cruciale (analyse de sentiment, interface utilisateur)\n",
        "- Vous voulez des **mots lisibles** par les humains\n",
        "- Vous travaillez sur des **volumes mod√©r√©s**\n",
        "- Vous analysez le **sens** et la **s√©mantique**\n",
        "\n",
        "#### ‚úÇÔ∏è Utilisez le STEMMING quand :\n",
        "- La **vitesse** est prioritaire (gros volumes, temps r√©el)\n",
        "- Vous voulez une **r√©duction maximale** du vocabulaire\n",
        "- Vous faites de la **classification** ou du **clustering**\n",
        "- La pr√©cision exacte n'est **pas critique**\n",
        "\n",
        "### üîÆ Prochaines √©tapes :\n",
        "\n",
        "Maintenant que vous ma√Ætrisez la lemmatisation fran√ßaise, vous √™tes pr√™t pour :\n",
        "- **Pipeline complet** int√©grant nettoyage + tokenisation + lemmatisation\n",
        "- **Repr√©sentation vectorielle** des mots (TF-IDF, Word2Vec)\n",
        "- **Mod√®les avanc√©s** de NLP fran√ßais\n",
        "\n",
        "---\n",
        "\n",
        "**üéØ Message final :** La lemmatisation n'est pas qu'une √©tape technique, c'est un **choix strat√©gique** qui influence toute votre cha√Æne de traitement NLP. Choisissez consciemment selon votre contexte !"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}