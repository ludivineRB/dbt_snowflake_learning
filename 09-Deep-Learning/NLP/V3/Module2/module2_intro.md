---
title: 'Module 2 : Introduction au Preprocessing'
description: 'Formation NLP - Module 2 : Introduction au Preprocessing'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

[üìö Module 1](../module1/module1_intro.html) ‚Üí üßπ Module 2 : Preprocessing

# üßπ Module 2 : Preprocessing et Tokenisation

Transformer le texte brut en donn√©es exploitables pour le NLP

### üéØ Objectifs du Module

√Ä la fin de ce module, vous serez capable de :

*   Comprendre pourquoi le preprocessing est crucial en NLP
*   Ma√Ætriser les √©tapes essentielles de nettoyage de texte
*   Impl√©menter diff√©rentes strat√©gies de tokenisation
*   G√©rer les sp√©cificit√©s du fran√ßais vs anglais
*   Construire un pipeline de preprocessing robuste
*   √âvaluer la qualit√© d'un preprocessing

## ‚ùó Le Probl√®me : Texte Brut vs Texte Exploitable

Imaginez que vous voulez analyser le sentiment de ces vrais exemples :

#### üò± Texte Brut (Probl√©matique)

"RT @user: LOL!!! C'est G√âNIAL üòçüòçüòç https://bit.ly/xyz #amazing #best J'ADOOORE ce produit!!!" "Bof... pas terrible üòï Service client = üí© N'achetez PAS !!!" "Bon ben... c'est ok I guess ü§∑‚Äç‚ôÄÔ∏è"

#### ‚ú® Apr√®s Preprocessing

\["g√©nial", "adore", "produit"\] ‚Üí POSITIF \["bof", "terrible", "achetez", "pas"\] ‚Üí N√âGATIF \["bon", "ok"\] ‚Üí NEUTRE

15-30%

Am√©lioration performance  
avec bon preprocessing

70%

Du temps NLP consacr√©  
au preprocessing

5-10x

R√©duction taille  
vocabulaire

## üîç Pourquoi le Preprocessing est Crucial

#### ‚ö†Ô∏è Sans Preprocessing - Probl√®mes Typiques

*   **Vocabulaire explosif :** "SUPER", "super", "Super!" = 3 mots diff√©rents
*   **Bruit :** URLs, emails, hashtags polluent l'analyse
*   **Incoh√©rence :** "n'est pas" vs "n est pas" vs "nest pas"
*   **Mots vides :** "le", "de", "et" dominent sans apporter d'info
*   **Variabilit√© :** "üòç" vs "magnifique" = m√™me sentiment, encodage diff√©rent

#### ‚úÖ Avec Preprocessing - B√©n√©fices

*   **Coh√©rence :** Tous les mots en minuscules normalis√©es
*   **Focus :** Seuls les mots porteurs de sens sont conserv√©s
*   **Efficacit√© :** Vocabulaire r√©duit = calculs plus rapides
*   **Performance :** Meilleure g√©n√©ralisation des mod√®les
*   **Robustesse :** Gestion des cas particuliers (fautes, abr√©viations)

## üîß Le Pipeline de Preprocessing

### üìã √âtapes Typiques (Ordre Important !)

**1\. Nettoyage**  
Casse, ponctuation

‚Üí

**2\. Normalisation**  
Accents, espaces

‚Üí

**3\. Tokenisation**  
D√©coupage en mots

‚Üí

**4\. Filtrage**  
Stopwords, longueur

‚Üí

**5\. Lemmatisation**  
Forme canonique

#### üí° Principe Cl√©

Il n'existe pas de preprocessing "universel" ! Le bon preprocessing d√©pend de :

*   **Type de texte :** Tweet vs article acad√©mique vs SMS
*   **T√¢che finale :** Sentiment vs traduction vs r√©sum√©
*   **Langue :** Fran√ßais vs anglais vs multilingue
*   **Domaine :** M√©dical vs financier vs g√©n√©ral

## üß™ D√©mo Interactive : Avant/Apr√®s

### üî¨ Testez le Preprocessing en Temps R√©el

Tapez du texte "sale" et voyez la transformation :

‚ö° Pr√©processer üîÑ Reset

#### üîß √âtapes de Transformation :

**1\. Texte Original :**

**2\. Apr√®s Nettoyage (minuscules, URLs) :**

**3\. Apr√®s Suppression Ponctuation :**

**4\. Apr√®s Tokenisation :**

**5\. Apr√®s Filtrage (stopwords, longueur) :**

**üìä R√©sultat Final :**

## üìö Ce que Vous Allez Apprendre

### üßπ Nettoyage & Normalisation

*   Gestion de la casse
*   Suppression ponctuation
*   Normalisation Unicode
*   Suppression URLs, emails
*   Gestion des emojis

### ‚úÇÔ∏è Tokenisation

*   Tokenisation par mots
*   Gestion des contractions
*   Tokenisation sous-mots
*   Sp√©cificit√©s du fran√ßais
*   Comparaison NLTK vs spaCy

### ‚öôÔ∏è Techniques Avanc√©es

‚Ä¢ Stopwords intelligents

‚Ä¢ Lemmatisation fran√ßaise

‚Ä¢ Stemming vs Lemmatisation

‚Ä¢ Pipeline personnalis√©

[‚¨ÖÔ∏è Module 1 Termin√©](../module1/module1_resume.html) [üßπ Commencer le Nettoyage](module2_nettoyage.html)

### üöÄ Prochaine √âtape

Maintenant que vous comprenez l'importance du preprocessing, plongeons dans les techniques concr√®tes de nettoyage de texte !

[Ma√Ætriser le Nettoyage ‚ú®](module2_nettoyage.html)

// Stopwords fran√ßais basiques pour la d√©mo const stopwordsFrancais = new Set(\[ 'le', 'de', 'et', '√†', 'un', 'il', '√™tre', 'et', 'en', 'avoir', 'que', 'pour', 'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus', 'par', 'grand', 'en', 'le', 'son', 'que', 'ce', 'lui', 'au', 'du', 'des', 'la', 'les', 'je', 'tu', 'nous', 'vous', 'ils', 'elles', 'mon', 'ma', 'mes', 'ton', 'ta', 'tes', 'sa', 'ses', 'notre', 'nos', 'votre', 'vos', 'leur', 'leurs', 'est', 'sont', '√©tait', '√©taient', 'ai', 'as', 'a', 'avons', 'avez', 'ont' \]); function processText() { const rawText = document.getElementById('rawText').value.trim(); if (!rawText) { alert('Veuillez entrer du texte √† pr√©processer !'); return; } // Afficher la section des √©tapes document.getElementById('processingSteps').style.display = 'block'; // √âtape 0 : Texte original document.getElementById('step0').textContent = rawText; // √âtape 1 : Nettoyage initial let step1 = rawText.toLowerCase(); step1 = step1.replace(/https?:\\/\\/\[^\\s\]+/g, '\[URL\]'); // URLs step1 = step1.replace(/@\\w+/g, '\[MENTION\]'); // Mentions step1 = step1.replace(/#\\w+/g, '\[HASHTAG\]'); // Hashtags step1 = step1.replace(/rt\\s+/g, ''); // Retweets document.getElementById('step1').textContent = step1; // √âtape 2 : Suppression ponctuation et emojis let step2 = step1.replace(/\[^\\w\\s\]/g, ' '); // Garde seulement lettres, chiffres et espaces step2 = step2.replace(/\\s+/g, ' ').trim(); // Normalise les espaces document.getElementById('step2').textContent = step2; // √âtape 3 : Tokenisation let tokens = step2.split(/\\s+/).filter(token => token.length > 0); document.getElementById('step3').textContent = '\["' + tokens.join('", "') + '"\]'; // √âtape 4 : Filtrage let filteredTokens = tokens.filter(token => { return token.length > 2 && // Mots de plus de 2 caract√®res !stopwordsFrancais.has(token) && // Pas un stopword !/^\\d+$/.test(token) && // Pas un nombre pur token !== 'url' && token !== 'mention' && token !== 'hashtag'; // Pas nos marqueurs }); document.getElementById('step4').textContent = '\["' + filteredTokens.join('", "') + '"\]'; // R√©sultat final avec statistiques const originalWords = rawText.split(/\\s+/).length; const finalWords = filteredTokens.length; const reduction = Math.round((1 - finalWords / originalWords) \* 100); document.getElementById('finalResult').innerHTML = \` <strong>Tokens finaux :</strong> \["${filteredTokens.join('", "')}"\]<br> <strong>Statistiques :</strong><br> ‚Ä¢ Mots originaux : ${originalWords}<br> ‚Ä¢ Mots apr√®s preprocessing : ${finalWords}<br> ‚Ä¢ R√©duction : ${reduction}% \`; // Faire d√©filer vers les r√©sultats document.getElementById('processingSteps').scrollIntoView({ behavior: 'smooth', block: 'center' }); } function resetDemo() { document.getElementById('rawText').value = ''; document.getElementById('processingSteps').style.display = 'none'; } // Exemples pr√©remplis au clic document.getElementById('rawText').addEventListener('click', function() { if (!this.value) { const examples = \[ "RT @user: LOL!!! C'est G√âNIAL üòçüòçüòç https://bit.ly/xyz #amazing #best J'ADOOORE ce produit!!!", "Bof... pas terrible üòï Service client = üí© N'achetez PAS !!!", "Salut! Comment √ßa va??? J'esp√®re que tout va BIEN üôÇ @marie #bonnejourn√©e" \]; this.value = examples\[Math.floor(Math.random() \* examples.length)\]; } });
