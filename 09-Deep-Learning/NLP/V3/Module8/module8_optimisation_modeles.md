---
title: Module 8 - Optimisation des ModÃ¨les
description: Formation NLP - Module 8 - Optimisation des ModÃ¨les
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# âš¡ Optimisation des ModÃ¨les

RÃ©duire la latence et l'empreinte mÃ©moire pour la production

## ğŸ¯ DÃ©fis de Performance en Production

### âš¡ ProblÃ©matiques de Latence

Les modÃ¨les Transformer comme BERT et GPT, bien que trÃ¨s performants, posent des dÃ©fis majeurs en production :

BERT-Base Original

340M

ParamÃ¨tres

1.3GB RAM

Latence InfÃ©rence

200ms

Par prÃ©diction

Trop lent

CoÃ»t GPU

$500

Par mois

Instance V100

Objectif Production

<50ms

P95 Latence

ğŸ¯ Cible

**ğŸ’¡ Objectifs d'Optimisation :**  
â€¢ Latence : <50ms P95 pour classification  
â€¢ MÃ©moire : <500MB par instance  
â€¢ Throughput : >1000 req/sec par GPU  
â€¢ QualitÃ© : Maintenir >95% des performances originales

#### ğŸ§ª Comparateur d'Optimisations

BERT Original DistilBERT Quantization CombinÃ©

SÃ©lectionnez une technique pour voir l'impact...

## ğŸ“ Distillation de ModÃ¨les

### ğŸ‘¨â€ğŸ« Principe Teacher-Student

La distillation crÃ©e un modÃ¨le "Ã©tudiant" plus petit qui imite un modÃ¨le "professeur" plus large.

ğŸ§ 

DistilBERT

Version distillÃ©e de BERT avec 6 couches au lieu de 12. Conserve 97% des performances.

66M

paramÃ¨tres

2x

plus rapide

60%

moins de RAM

97%

performance

ğŸ“±

TinyBERT

Encore plus compact avec distillation en deux phases : prÃ©-entraÃ®nement + task-specific.

14M

paramÃ¨tres

9x

plus rapide

85%

moins de RAM

96%

performance

ğŸ”„

Distillation Custom

Distiller votre modÃ¨le fine-tunÃ© pour conserver les spÃ©cialisations mÃ©tier.

Variable

taille

3-5x

plus rapide

OptimisÃ©

pour votre tÃ¢che

98%

performance mÃ©tier

\# ImplÃ©mentation de distillation avec TensorFlow import tensorflow as tf from transformers import TFBertModel, TFDistilBertModel class DistillationTrainer: def \_\_init\_\_(self, teacher\_model, student\_model, temperature=3.0, alpha=0.7): self.teacher = teacher\_model self.student = student\_model self.temperature = temperature self.alpha = alpha # Balance entre loss distillation et task loss def distillation\_loss(self, y\_true, y\_pred\_student, y\_pred\_teacher): """Combine task loss et knowledge distillation loss""" # Task loss (classification standard) task\_loss = tf.keras.losses.sparse\_categorical\_crossentropy( y\_true, y\_pred\_student, from\_logits=True ) # Knowledge distillation loss (soft targets) teacher\_probs = tf.nn.softmax(y\_pred\_teacher / self.temperature) student\_log\_probs = tf.nn.log\_softmax(y\_pred\_student / self.temperature) kd\_loss = tf.keras.losses.KLDivergence()( teacher\_probs, student\_log\_probs ) # Loss combinÃ©e total\_loss = (self.alpha \* kd\_loss \* self.temperature \*\* 2 + (1 - self.alpha) \* task\_loss) return total\_loss def train\_step(self, batch\_data): """Step d'entraÃ®nement avec distillation""" inputs, labels = batch\_data with tf.GradientTape() as tape: # PrÃ©dictions teacher (frozen) teacher\_logits = self.teacher(inputs, training=False) # PrÃ©dictions student student\_logits = self.student(inputs, training=True) # Loss de distillation loss = self.distillation\_loss(labels, student\_logits, teacher\_logits) # Mise Ã  jour uniquement du student gradients = tape.gradient(loss, self.student.trainable\_variables) self.optimizer.apply\_gradients(zip(gradients, self.student.trainable\_variables)) return loss

**âš ï¸ ConsidÃ©rations pour la Distillation :**  
â€¢ DonnÃ©es d'entraÃ®nement : Besoin du mÃªme dataset que le teacher  
â€¢ Temps de calcul : Teacher doit faire l'infÃ©rence pendant l'entraÃ®nement  
â€¢ Task-specific : Redistiller aprÃ¨s fine-tuning pour maintenir performance  
â€¢ HyperparamÃ¨tres : TempÃ©rature et alpha critiques pour rÃ©ussir

## ğŸ—œï¸ Quantization et Compression

### ğŸ“Š RÃ©duction de PrÃ©cision

La quantization rÃ©duit la prÃ©cision des poids de FP32 vers FP16 ou INT8, diminuant drastiquement la taille et accÃ©lÃ©rant l'infÃ©rence.

Technique PrÃ©cision Taille ModÃ¨le Speedup Perte QualitÃ© Hardware **FP32 Original** 32-bit float 1.3GB 1x 0% CPU/GPU **FP16 Half-Precision** 16-bit float 650MB 1.5-2x <1% GPU moderne **INT8 Dynamic** 8-bit integer 325MB 2-4x 1-3% CPU optimisÃ© **INT8 Static** 8-bit integer 325MB 3-5x 2-5% CPU/Edge

ğŸ’¨

Post-Training Quantization

Quantization aprÃ¨s entraÃ®nement sans donnÃ©es supplÃ©mentaires. Rapide mais moins prÃ©cis.

ğŸ¯

Quantization-Aware Training

EntraÃ®nement avec simulation de quantization. Plus long mais meilleure qualitÃ©.

ğŸ”§

Dynamic Quantization

Quantization Ã  la volÃ©e des activations. Bon compromis performance/simplicitÃ©.

\# Quantization avec TensorFlow Lite import tensorflow as tf def quantize\_model(saved\_model\_path, calibration\_dataset=None): """ Quantize un modÃ¨le BERT avec TensorFlow Lite """ # Converter setup converter = tf.lite.TFLiteConverter.from\_saved\_model(saved\_model\_path) # Configuration de base converter.optimizations = \[tf.lite.Optimize.DEFAULT\] if calibration\_dataset: # INT8 Quantization avec dataset de calibration converter.target\_spec.supported\_ops = \[tf.lite.OpsSet.TFLITE\_BUILTINS\_INT8\] converter.target\_spec.supported\_types = \[tf.int8\] def representative\_dataset(): for batch in calibration\_dataset.take(100): # Prendre seulement les input\_ids pour calibration yield \[batch\['input\_ids'\].numpy().astype(np.float32)\] converter.representative\_dataset = representative\_dataset converter.inference\_input\_type = tf.int8 converter.inference\_output\_type = tf.int8 else: # FP16 Quantization (plus simple) converter.target\_spec.supported\_types = \[tf.float16\] # Conversion quantized\_model = converter.convert() return quantized\_model def benchmark\_quantized\_model(original\_model, quantized\_model\_path, test\_data): """ Compare les performances original vs quantized """ import time # Load quantized model interpreter = tf.lite.Interpreter(model\_path=quantized\_model\_path) interpreter.allocate\_tensors() input\_details = interpreter.get\_input\_details() output\_details = interpreter.get\_output\_details() # Benchmark original start\_time = time.time() original\_predictions = original\_model.predict(test\_data) original\_time = time.time() - start\_time # Benchmark quantized start\_time = time.time() quantized\_predictions = \[\] for sample in test\_data: interpreter.set\_tensor(input\_details\[0\]\['index'\], sample) interpreter.invoke() output = interpreter.get\_tensor(output\_details\[0\]\['index'\]) quantized\_predictions.append(output) quantized\_time = time.time() - start\_time # Calcul mÃ©triques speedup = original\_time / quantized\_time accuracy\_loss = calculate\_accuracy\_difference(original\_predictions, quantized\_predictions) return { 'speedup': speedup, 'accuracy\_loss': accuracy\_loss, 'original\_time': original\_time, 'quantized\_time': quantized\_time }

## ğŸš€ ONNX Runtime et Optimisations Hardware

### âš¡ Optimisations au niveau du Graph

ONNX Runtime applique des optimisations automatiques au niveau du graphe computationnel pour maximiser les performances.

ğŸ”—

Graph Optimization

Fusion d'opÃ©rations, Ã©limination de nÅ“uds inutiles, rÃ©organisation pour optimiser le cache.

15-30%

speedup CPU

Auto

optimisation

ğŸ®

TensorRT Integration

Utilise TensorRT NVIDIA pour optimisations GPU avancÃ©es avec prÃ©cision mixte.

2-5x

speedup GPU

FP16

prÃ©cision mixte

ğŸ“±

Edge Optimization

Optimisations spÃ©cifiques pour dÃ©ploiement mobile et edge computing.

<100MB

taille modÃ¨le

CPU

uniquement

\# Optimisation ONNX Runtime pour production import onnxruntime as ort import numpy as np class OptimizedBERTInference: def \_\_init\_\_(self, onnx\_model\_path, use\_gpu=True): # Configuration des providers providers = \[\] if use\_gpu and ort.get\_device() == 'GPU': providers.append(('TensorrtExecutionProvider', { 'trt\_fp16\_enable': True, 'trt\_max\_workspace\_size': 2147483648, # 2GB 'trt\_max\_partition\_iterations': 1000, })) providers.append('CUDAExecutionProvider') providers.append('CPUExecutionProvider') # Optimisations du graphe sess\_options = ort.SessionOptions() sess\_options.graph\_optimization\_level = ort.GraphOptimizationLevel.ORT\_ENABLE\_ALL sess\_options.optimized\_model\_filepath = "optimized\_model.onnx" # Parallel execution sess\_options.intra\_op\_num\_threads = 4 sess\_options.inter\_op\_num\_threads = 4 # CrÃ©ation de la session self.session = ort.InferenceSession( onnx\_model\_path, sess\_options=sess\_options, providers=providers ) self.input\_names = \[inp.name for inp in self.session.get\_inputs()\] self.output\_names = \[out.name for out in self.session.get\_outputs()\] def predict\_batch(self, input\_ids, attention\_mask, token\_type\_ids=None): """ PrÃ©diction optimisÃ©e avec support batch """ # PrÃ©paration inputs ort\_inputs = { 'input\_ids': input\_ids.astype(np.int64), 'attention\_mask': attention\_mask.astype(np.int64) } if token\_type\_ids is not None: ort\_inputs\['token\_type\_ids'\] = token\_type\_ids.astype(np.int64) # InfÃ©rence outputs = self.session.run(self.output\_names, ort\_inputs) return outputs\[0\] # logits def benchmark\_performance(self, test\_inputs, num\_runs=100): """ Benchmark des performances """ import time # Warmup for \_ in range(10): self.predict\_batch(\*\*test\_inputs) # Mesure start\_time = time.time() for \_ in range(num\_runs): self.predict\_batch(\*\*test\_inputs) avg\_time = (time.time() - start\_time) / num\_runs throughput = test\_inputs\['input\_ids'\].shape\[0\] / avg\_time return { 'avg\_latency\_ms': avg\_time \* 1000, 'throughput\_samples\_per\_sec': throughput, 'memory\_usage': self.get\_memory\_usage() }

**ğŸ“Š Gains de Performance Typiques :**  
â€¢ ONNX CPU : 20-40% plus rapide que TensorFlow  
â€¢ ONNX + TensorRT : 3-5x plus rapide sur GPU  
â€¢ Optimisations automatiques : Aucun code supplÃ©mentaire  
â€¢ CompatibilitÃ© : Support multi-plateforme

## ğŸª StratÃ©gies CombinÃ©es et Best Practices

### ğŸš€ Pipeline d'Optimisation ComplÃ¨te

#### ğŸ¯ Simulateur de Pipeline d'Optimisation

Conservative Ã‰quilibrÃ© Agressif

SÃ©lectionnez une approche pour voir le pipeline...

**ğŸ¯ Recommandations par Use Case :**  
â€¢ Latence critique (<10ms) : DistilBERT + INT8 + ONNX  
â€¢ QualitÃ© prioritaire : FP16 + Graph optimization  
â€¢ Edge deployment : TinyBERT + Quantization + Pruning  
â€¢ Cost-sensitive : CPU-only avec optimisations maximales

**âš ï¸ Validation et Tests :**  
â€¢ A/B Testing : Comparer avec modÃ¨le original en production  
â€¢ Regression Testing : Suite de tests automatisÃ©s  
â€¢ Performance Monitoring : MÃ©triques continues  
â€¢ Rollback Strategy : Plan de retour arriÃ¨re rapide

[â† Architecture](module8_architecture_production.html)

**Optimisation des ModÃ¨les**  
Distillation, Quantization, ONNX

[DÃ©ploiement â†’](module8_deploiement_production.html)

// Animation de la barre de progression window.addEventListener('load', function() { setTimeout(() => { document.getElementById('progressBar').style.width = '100%'; }, 1000); }); // Comparateur d'optimisations function compareOptimizations(technique) { let comparison = ''; switch(technique) { case 'original': comparison = \`ğŸ¤– BERT-Base Original ğŸ“Š CaractÃ©ristiques : â€¢ ParamÃ¨tres : 340M â€¢ Taille mÃ©moire : 1.3GB â€¢ Latence CPU : 200ms â€¢ Latence GPU : 50ms â€¢ QualitÃ© : 100% (rÃ©fÃ©rence) ğŸ’° CoÃ»t mensuel estimÃ© : â€¢ GPU V100 : $500/mois â€¢ Instances : 4x pour 1000 req/sec â€¢ Total infrastructure : $2000/mois âš ï¸ Limitations : â€¢ Trop lent pour temps rÃ©el â€¢ CoÃ»t Ã©levÃ© pour scaling â€¢ Empreinte mÃ©moire importante\`; break; case 'distillation': comparison = \`ğŸ“ DistilBERT (Distillation) ğŸ“Š CaractÃ©ristiques : â€¢ ParamÃ¨tres : 66M (-80%) â€¢ Taille mÃ©moire : 500MB (-62%) â€¢ Latence CPU : 100ms (-50%) â€¢ Latence GPU : 25ms (-50%) â€¢ QualitÃ© : 97% (-3%) ğŸ’° CoÃ»t mensuel estimÃ© : â€¢ GPU T4 : $200/mois â€¢ Instances : 2x pour 1000 req/sec â€¢ Total infrastructure : $400/mois âœ… Avantages : â€¢ Excellent rapport qualitÃ©/performance â€¢ Peu de perte de prÃ©cision â€¢ Compatible production temps rÃ©el\`; break; case 'quantization': comparison = \`ğŸ—œï¸ BERT + INT8 Quantization ğŸ“Š CaractÃ©ristiques : â€¢ ParamÃ¨tres : 340M (mÃªme) â€¢ Taille mÃ©moire : 325MB (-75%) â€¢ Latence CPU : 50ms (-75%) â€¢ Latence GPU : 20ms (-60%) â€¢ QualitÃ© : 96% (-4%) ğŸ’° CoÃ»t mensuel estimÃ© : â€¢ CPU instances : $100/mois â€¢ Instances : 2x pour 1000 req/sec â€¢ Total infrastructure : $200/mois âœ… Avantages : â€¢ DÃ©ploiement CPU viable â€¢ TrÃ¨s Ã©conomique â€¢ Bonne pour edge computing\`; break; case 'combined': comparison = \`ğŸš€ DistilBERT + INT8 + ONNX ğŸ“Š CaractÃ©ristiques : â€¢ ParamÃ¨tres : 66M (-80%) â€¢ Taille mÃ©moire : 165MB (-87%) â€¢ Latence CPU : 25ms (-87%) â€¢ Latence GPU : 10ms (-80%) â€¢ QualitÃ© : 95% (-5%) ğŸ’° CoÃ»t mensuel estimÃ© : â€¢ CPU instances : $50/mois â€¢ Instances : 1x pour 1000 req/sec â€¢ Total infrastructure : $50/mois ğŸ¯ Performance Production : â€¢ <10ms latence P95 â€¢ >10,000 req/sec single instance â€¢ DÃ©ploiement edge possible â€¢ ROI optimal\`; break; } document.getElementById('optimizationOutput').innerHTML = \`<div style="text-align: left; font-size: 0.9em; line-height: 1.4; white-space: pre-line;">${comparison}</div>\`; } // Simulateur de pipeline d'optimisation function simulateOptimizationPipeline(approach) { let pipeline = ''; switch(approach) { case 'conservative': pipeline = \`ğŸ›¡ï¸ Approche Conservative ğŸ¯ Objectif : Minimiser les risques ğŸ“Š Perte qualitÃ© acceptÃ©e : <2% ğŸ“‹ Pipeline : 1. ğŸ§ª Tests baseline exhaustifs 2. ğŸ’¨ FP16 quantization uniquement 3. ğŸ”§ ONNX graph optimization 4. ğŸ“Š Benchmarks approfondis 5. ğŸš€ DÃ©ploiement progressif â±ï¸ Timeline : 3-4 semaines ğŸ¯ Gains attendus : â€¢ Latence : -30% â€¢ MÃ©moire : -50% â€¢ QualitÃ© : -1% âœ… RecommandÃ© pour : â€¢ Applications critiques â€¢ Environnements rÃ©glementÃ©s â€¢ PremiÃ¨re optimisation\`; break; case 'balanced': pipeline = \`âš–ï¸ Approche Ã‰quilibrÃ©e ğŸ¯ Objectif : Bon compromis perf/qualitÃ© ğŸ“Š Perte qualitÃ© acceptÃ©e : <5% ğŸ“‹ Pipeline : 1. ğŸ“ Distillation vers DistilBERT 2. ğŸ—œï¸ INT8 quantization post-training 3. ğŸš€ ONNX Runtime optimization 4. ğŸ“Š A/B testing en production 5. ğŸ”„ Monitoring continu â±ï¸ Timeline : 6-8 semaines ğŸ¯ Gains attendus : â€¢ Latence : -70% â€¢ MÃ©moire : -75% â€¢ QualitÃ© : -3% âœ… RecommandÃ© pour : â€¢ Applications grand public â€¢ Besoins de scalabilitÃ© â€¢ Budget infrastructure limitÃ©\`; break; case 'aggressive': pipeline = \`ğŸš€ Approche Agressive ğŸ¯ Objectif : Performance maximale ğŸ“Š Perte qualitÃ© acceptÃ©e : <10% ğŸ“‹ Pipeline : 1. ğŸ“ TinyBERT distillation 2. ğŸ—œï¸ INT8 quantization + pruning 3. âœ‚ï¸ Architecture search optimization 4. ğŸš€ Custom ONNX operators 5. ğŸ“± Edge deployment ready â±ï¸ Timeline : 10-12 semaines ğŸ¯ Gains attendus : â€¢ Latence : -90% â€¢ MÃ©moire : -95% â€¢ QualitÃ© : -8% âœ… RecommandÃ© pour : â€¢ Applications mobile/edge â€¢ Contraintes hardware extrÃªmes â€¢ Use cases tolÃ©rants aux erreurs\`; break; } document.getElementById('pipelineOutput').innerHTML = \`<div style="text-align: left; font-size: 0.9em; line-height: 1.4; white-space: pre-line;">${pipeline}</div>\`; }
