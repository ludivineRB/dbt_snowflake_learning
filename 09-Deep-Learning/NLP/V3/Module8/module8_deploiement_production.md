---
title: Module 8 - DÃ©ploiement & Production
description: Formation NLP - Module 8 - DÃ©ploiement & Production
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ³ DÃ©ploiement & Production

Containerisation, orchestration et mise en production

## ğŸ¯ DÃ©fis du DÃ©ploiement NLP

### ğŸ­ De l'ExpÃ©rimentation Ã  la Production

DÃ©ployer des modÃ¨les NLP en production nÃ©cessite de rÃ©soudre des dÃ©fis spÃ©cifiques au machine learning et au traitement du langage naturel.

1

#### âš¡ Latence

ModÃ¨les BERT peuvent prendre 100ms+ par prÃ©diction. En production, il faut servir 1000+ utilisateurs simultanÃ©ment.

2

#### ğŸ’¾ MÃ©moire

BERT-large = 340M paramÃ¨tres = 1.3GB RAM. MultipliÃ© par le nombre de workers...

3

#### ğŸ”„ ScalabilitÃ©

Comment gÃ©rer les pics de charge ? Auto-scaling horizontal avec partage de modÃ¨les.

4

#### ğŸ“Š Monitoring

DÃ©rive des donnÃ©es, performance en temps rÃ©el, dÃ©tection d'anomalies linguistiques.

**ğŸ¯ Objectif du Module :** Transformer vos modÃ¨les BERT en services production robustes, capables de servir des milliers d'utilisateurs avec une latence < 100ms et une disponibilitÃ© > 99.9%.

## ğŸ³ Containerisation avec Docker

### ğŸ“¦ Images Docker OptimisÃ©es

La containerisation garantit la portabilitÃ© et la reproductibilitÃ© des dÃ©ploiements NLP.

ğŸ—ï¸

Multi-Stage Build

SÃ©paration des phases build et runtime pour rÃ©duire la taille finale de l'image.

âš¡

Images OptimisÃ©es

Utilisation d'images de base lÃ©gÃ¨res avec CUDA pour GPU et optimisations Python.

ğŸ”’

SÃ©curitÃ©

Utilisateur non-root, scan de vulnÃ©rabilitÃ©s, secrets via environment variables.

ğŸ“Š

Health Checks

Monitoring de santÃ© intÃ©grÃ© pour orchestration automatique et load balancing.

\# Dockerfile optimisÃ© pour production NLP FROM nvidia/cuda:11.8-devel-ubuntu20.04 as builder # Installation des dÃ©pendances de build RUN apt-get update && apt-get install -y \\ python3.9 python3.9-dev python3-pip \\ build-essential cmake git \\ && rm -rf /var/lib/apt/lists/\* # Installation des requirements COPY requirements.txt . RUN pip3 install --no-cache-dir -r requirements.txt # ============================================= # Stage final optimisÃ© FROM nvidia/cuda:11.8-runtime-ubuntu20.04 # Utilisateur non-root pour sÃ©curitÃ© RUN useradd --create-home --shell /bin/bash nlpuser # Runtime dependencies seulement RUN apt-get update && apt-get install -y \\ python3.9 python3-pip \\ && rm -rf /var/lib/apt/lists/\* # Copie des packages installÃ©s COPY --from=builder /usr/local/lib/python3.9/dist-packages /usr/local/lib/python3.9/dist-packages # Application WORKDIR /app COPY --chown=nlpuser:nlpuser . . # Pre-download des modÃ¨les pour Ã©viter download Ã  runtime RUN python3 -c "from transformers import BertTokenizer, BertModel; \\ BertTokenizer.from\_pretrained('bert-base-uncased'); \\ BertModel.from\_pretrained('bert-base-uncased')" # Health check HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\ CMD python3 -c "import requests; requests.get('http://localhost:8000/health')" USER nlpuser EXPOSE 8000 CMD \["python3", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"\]

**âš ï¸ Bonnes Pratiques Docker NLP :**  
â€¢ Layer caching : Ordre optimal des COPY pour cache hits  
â€¢ Model caching : Pre-download pour Ã©viter latence startup  
â€¢ Memory limits : DÃ©finir limits pour Ã©viter OOM kills  
â€¢ GPU support : Nvidia runtime et CUDA compatibility

## â˜¸ï¸ Orchestration Kubernetes

### ğŸš¢ DÃ©ploiement Production

#### âš™ï¸ Simulateur de DÃ©ploiement Kubernetes

DÃ©ploiement Basic Production Ready GPU Cluster

SÃ©lectionnez un type de dÃ©ploiement...

#### ğŸ—ï¸ Architecture Kubernetes NLP

**ğŸŒ Ingress Controller**  
Load balancing externe, SSL termination, rate limiting

**ğŸ”„ Services & Deployments**  
Services NLP avec auto-scaling et health checks

**ğŸ’¾ Persistent Volumes**  
Stockage modÃ¨les et donnÃ©es partagÃ©es

**ğŸ”§ ConfigMaps & Secrets**  
Configuration et credentials sÃ©curisÃ©s

\# Manifest Kubernetes pour service NLP apiVersion: apps/v1 kind: Deployment metadata: name: nlp-sentiment-service labels: app: nlp-sentiment spec: replicas: 3 selector: matchLabels: app: nlp-sentiment template: metadata: labels: app: nlp-sentiment spec: containers: - name: sentiment-api image: myregistry/nlp-sentiment:v2.1.0 ports: - containerPort: 8000 resources: requests: memory: "2Gi" cpu: "500m" nvidia.com/gpu: 1 limits: memory: "4Gi" cpu: "2" nvidia.com/gpu: 1 env: - name: MODEL\_PATH value: "/models/sentiment" - name: REDIS\_URL valueFrom: secretKeyRef: name: redis-secret key: url volumeMounts: - name: models-volume mountPath: /models livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8000 initialDelaySeconds: 5 periodSeconds: 5 volumes: - name: models-volume persistentVolumeClaim: claimName: nlp-models-pvc --- apiVersion: v1 kind: Service metadata: name: nlp-sentiment-service spec: selector: app: nlp-sentiment ports: - port: 80 targetPort: 8000 type: ClusterIP --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nlp-sentiment-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nlp-sentiment-service minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80

## ğŸ”„ CI/CD pour Machine Learning

### âš™ï¸ Pipeline MLOps

Les pipelines CI/CD pour ML nÃ©cessitent des Ã©tapes spÃ©cifiques : validation de modÃ¨le, tests de dÃ©rive, benchmarks de performance.

1

#### ğŸ§ª Tests ModÃ¨le

Validation accuracy, latence, memory usage sur dataset de test

2

#### ğŸ”¨ Build & Push

Container build, optimization, push vers registry

3

#### ğŸ¯ Staging Deploy

DÃ©ploiement environnement de staging pour tests intÃ©gration

4

#### âœ… Validation

Tests end-to-end, performance, smoke tests

5

#### ğŸš€ Production

DÃ©ploiement blue-green avec monitoring continu

**ğŸ¯ StratÃ©gies de DÃ©ploiement :**  
â€¢ Blue-Green : Basculement instantanÃ© avec rollback facile  
â€¢ Canary : DÃ©ploiement progressif avec monitoring  
â€¢ Rolling : Mise Ã  jour instance par instance  
â€¢ A/B Testing : Comparaison modÃ¨les en parallÃ¨le

[â† Optimisation](module8_optimisation_modeles.html)

**DÃ©ploiement & Production**  
Docker, Kubernetes, CI/CD

[Monitoring â†’](module8_monitoring_observabilite.html)

// Animation de la barre de progression window.addEventListener('load', function() { setTimeout(() => { document.getElementById('progressBar').style.width = '100%'; }, 1000); }); // Simulateur de dÃ©ploiement K8s function simulateK8sDeployment(type) { let deployment = ''; switch(type) { case 'basic': deployment = \`ğŸš€ DÃ©ploiement Kubernetes Basic ğŸ“¦ Configuration : â€¢ Replicas : 2 â€¢ Resources : 1 CPU, 2Gi RAM â€¢ GPU : Non â€¢ Auto-scaling : Basique (CPU 70%) ğŸ“‹ Ã‰tapes : 1. âœ… Apply ConfigMap 2. âœ… Deploy service (2 pods) 3. âœ… Setup Load Balancer 4. âœ… Health checks OK ğŸ¯ RÃ©sultat : â€¢ Pods : 2/2 Running â€¢ Latence : ~80ms P95 â€¢ Throughput : 500 req/sec â€¢ CoÃ»t : ~$100/mois ğŸ’¡ IdÃ©al pour : Development, tests, POCs\`; break; case 'production': deployment = \`ğŸ­ DÃ©ploiement Production Ready ğŸ“¦ Configuration : â€¢ Replicas : 5 (min 3, max 15) â€¢ Resources : 2 CPU, 4Gi RAM per pod â€¢ GPU : 1x T4 per pod â€¢ Auto-scaling : AvancÃ© (CPU + Memory + Custom) ğŸ“‹ Ã‰tapes : 1. âœ… Secrets & ConfigMaps 2. âœ… PVC pour modÃ¨les (100Gi) 3. âœ… Deploy avec rolling update 4. âœ… HPA configuration 5. âœ… Network policies 6. âœ… Monitoring stack 7. âœ… Ingress avec SSL ğŸ¯ RÃ©sultat : â€¢ Pods : 5/5 Running across 3 nodes â€¢ Latence : <50ms P95 â€¢ Throughput : 5,000 req/sec â€¢ Availability : 99.9% â€¢ CoÃ»t : ~$800/mois ğŸ‰ Ready for production traffic!\`; break; case 'gpu': deployment = \`ğŸ® DÃ©ploiement GPU Cluster ğŸ“¦ Configuration : â€¢ Node pool : 3x GPU nodes (V100) â€¢ Replicas : 8 (min 4, max 20) â€¢ Resources : 4 CPU, 8Gi RAM, 1 GPU per pod â€¢ CUDA : 11.8 compatible ğŸ“‹ Ã‰tapes : 1. âœ… GPU node pool ready 2. âœ… NVIDIA device plugin 3. âœ… Deploy GPU workloads 4. âœ… GPU monitoring setup 5. âœ… Model parallelism config ğŸ¯ RÃ©sultat : â€¢ Pods : 8/8 Running on GPU nodes â€¢ GPU utilization : 85% average â€¢ Latence : <20ms P95 â€¢ Throughput : 15,000 req/sec â€¢ VRAM usage : 6GB per pod ğŸ’° CoÃ»t : ~$2,500/mois ğŸš€ Ultra-high performance for demanding workloads\`; break; } document.getElementById('k8sOutput').innerHTML = \`<div style="text-align: left; font-size: 0.9em; line-height: 1.4; white-space: pre-line;">${deployment}</div>\`; } // Animation des layers document.querySelectorAll('.layer').forEach((layer, index) => { layer.addEventListener('click', function() { this.style.animation = 'none'; setTimeout(() => { this.style.animation = 'pulse 0.8s ease-in-out'; this.style.background = 'linear-gradient(135deg, #F87171, #EF4444)'; this.style.color = 'white'; setTimeout(() => { this.style.background = 'linear-gradient(135deg, #FEE2E2, #FECACA)'; this.style.color = 'inherit'; }, 800); }, 10); }); });
