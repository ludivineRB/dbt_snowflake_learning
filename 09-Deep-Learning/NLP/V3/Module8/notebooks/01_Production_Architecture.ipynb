{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Module 8.1 - Architecture de Production NLP\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "- Concevoir une architecture microservices pour NLP\n",
    "- Impl√©menter une API FastAPI robuste\n",
    "- Configurer Redis pour le cache et les queues\n",
    "- Int√©grer monitoring et health checks\n",
    "- D√©ployer avec Docker et docker-compose\n",
    "\n",
    "## Vue d'ensemble\n",
    "Ce notebook vous guide dans la cr√©ation d'une architecture de production compl√®te pour services NLP, capable de g√©rer des milliers de requ√™tes par seconde avec haute disponibilit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Architecture Cible\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Load Balancer‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API Gateway   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                ‚îÇ\n",
    "                                                ‚ñº\n",
    "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                    ‚îÇ  Service Discovery  ‚îÇ\n",
    "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                ‚îÇ\n",
    "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                   ‚ñº                            ‚ñº                            ‚ñº\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ Sentiment Service‚îÇ        ‚îÇ    NER Service  ‚îÇ        ‚îÇ   QA Service    ‚îÇ\n",
    "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                   ‚îÇ                            ‚îÇ                            ‚îÇ\n",
    "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                ‚ñº\n",
    "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                    ‚îÇ    Redis Cache      ‚îÇ\n",
    "                                    ‚îÇ  & Message Queue    ‚îÇ\n",
    "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                ‚îÇ\n",
    "                                                ‚ñº\n",
    "                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                                    ‚îÇ   PostgreSQL DB     ‚îÇ\n",
    "                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances n√©cessaires\n",
    "!pip install fastapi uvicorn redis prometheus-client psycopg2-binary\n",
    "!pip install transformers tensorflow\n",
    "!pip install celery flower\n",
    "!pip install python-multipart jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "import hashlib\n",
    "\n",
    "# FastAPI et middlewares\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Cache et monitoring\n",
    "import redis\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "\n",
    "# NLP\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration et Classes de Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration centralis√©e pour l'application\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    API_HOST = os.getenv(\"API_HOST\", \"0.0.0.0\")\n",
    "    API_PORT = int(os.getenv(\"API_PORT\", \"8000\"))\n",
    "    API_WORKERS = int(os.getenv(\"API_WORKERS\", \"4\"))\n",
    "    \n",
    "    # Redis Configuration\n",
    "    REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "    REDIS_PORT = int(os.getenv(\"REDIS_PORT\", \"6379\"))\n",
    "    REDIS_DB = int(os.getenv(\"REDIS_DB\", \"0\"))\n",
    "    REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n",
    "    \n",
    "    # Cache TTL (Time To Live) en secondes\n",
    "    CACHE_TTL_SHORT = 300    # 5 minutes\n",
    "    CACHE_TTL_MEDIUM = 3600  # 1 heure\n",
    "    CACHE_TTL_LONG = 86400   # 24 heures\n",
    "    \n",
    "    # Model Configuration\n",
    "    MODEL_CACHE_DIR = os.getenv(\"MODEL_CACHE_DIR\", \"./models\")\n",
    "    DEFAULT_MODEL = \"bert-base-uncased\"\n",
    "    \n",
    "    # Rate Limiting\n",
    "    RATE_LIMIT_PER_MINUTE = int(os.getenv(\"RATE_LIMIT_PER_MINUTE\", \"100\"))\n",
    "    \n",
    "    # Health Check\n",
    "    HEALTH_CHECK_INTERVAL = 30\n",
    "    \n",
    "    # Monitoring\n",
    "    ENABLE_METRICS = os.getenv(\"ENABLE_METRICS\", \"true\").lower() == \"true\"\n",
    "    METRICS_PORT = int(os.getenv(\"METRICS_PORT\", \"9090\"))\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration charg√©e\")\n",
    "print(f\"Redis: {config.REDIS_HOST}:{config.REDIS_PORT}\")\n",
    "print(f\"API: {config.API_HOST}:{config.API_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®les Pydantic pour validation des donn√©es\n",
    "class TextInput(BaseModel):\n",
    "    text: str = Field(..., min_length=1, max_length=5000, description=\"Texte √† analyser\")\n",
    "    model: Optional[str] = Field(default=\"bert-base\", description=\"Mod√®le √† utiliser\")\n",
    "    language: Optional[str] = Field(default=\"en\", description=\"Langue du texte\")\n",
    "    \n",
    "class SentimentResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    processing_time: float\n",
    "    model_used: str\n",
    "    cached: bool = False\n",
    "    \n",
    "class HealthStatus(BaseModel):\n",
    "    status: str\n",
    "    timestamp: datetime\n",
    "    version: str\n",
    "    uptime_seconds: float\n",
    "    services: Dict[str, str]\n",
    "    \n",
    "class BatchTextInput(BaseModel):\n",
    "    texts: List[str] = Field(..., max_items=100, description=\"Liste de textes √† analyser\")\n",
    "    model: Optional[str] = Field(default=\"bert-base\", description=\"Mod√®le √† utiliser\")\n",
    "    \n",
    "print(\"‚úÖ Mod√®les de donn√©es d√©finis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Service Manager pour NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModelManager:\n",
    "    \"\"\"Gestionnaire des mod√®les NLP avec cache et optimisations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.model_stats = {}\n",
    "        self.startup_time = time.time()\n",
    "        \n",
    "    def load_model(self, model_name: str, task: str = \"sentiment-analysis\"):\n",
    "        \"\"\"Charge un mod√®le en m√©moire avec gestion d'erreurs\"\"\"\n",
    "        try:\n",
    "            if model_name not in self.models:\n",
    "                logger.info(f\"Chargement du mod√®le {model_name} pour {task}\")\n",
    "                \n",
    "                # Configuration optimis√©e selon la t√¢che\n",
    "                if task == \"sentiment-analysis\":\n",
    "                    model = pipeline(\n",
    "                        task,\n",
    "                        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                        framework=\"tf\",\n",
    "                        device=0 if tf.config.list_physical_devices('GPU') else -1\n",
    "                    )\n",
    "                else:\n",
    "                    model = pipeline(task, model=model_name, framework=\"tf\")\n",
    "                \n",
    "                self.models[model_name] = model\n",
    "                self.model_stats[model_name] = {\n",
    "                    \"loaded_at\": datetime.now(),\n",
    "                    \"predictions\": 0,\n",
    "                    \"total_time\": 0.0\n",
    "                }\n",
    "                \n",
    "                logger.info(f\"‚úÖ Mod√®le {model_name} charg√© avec succ√®s\")\n",
    "                \n",
    "            return self.models[model_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors du chargement de {model_name}: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=f\"Erreur mod√®le: {e}\")\n",
    "    \n",
    "    def predict_sentiment(self, text: str, model_name: str = \"sentiment\") -> Dict:\n",
    "        \"\"\"Pr√©diction de sentiment avec m√©triques\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            model = self.load_model(model_name, \"sentiment-analysis\")\n",
    "            \n",
    "            # Pr√©diction\n",
    "            result = model(text)\n",
    "            \n",
    "            # Traitement du r√©sultat\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                pred = result[0]\n",
    "                sentiment = pred['label'].lower()\n",
    "                confidence = pred['score']\n",
    "            else:\n",
    "                sentiment = \"neutral\"\n",
    "                confidence = 0.5\n",
    "                \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Mise √† jour des statistiques\n",
    "            self.model_stats[model_name][\"predictions\"] += 1\n",
    "            self.model_stats[model_name][\"total_time\"] += processing_time\n",
    "            \n",
    "            return {\n",
    "                \"sentiment\": sentiment,\n",
    "                \"confidence\": confidence,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"model_used\": model_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur pr√©diction: {e}\")\n",
    "            raise HTTPException(status_code=500, detail=\"Erreur de pr√©diction\")\n",
    "    \n",
    "    def get_model_stats(self) -> Dict:\n",
    "        \"\"\"Retourne les statistiques des mod√®les\"\"\"\n",
    "        stats = {}\n",
    "        for model_name, model_stat in self.model_stats.items():\n",
    "            avg_time = (\n",
    "                model_stat[\"total_time\"] / model_stat[\"predictions\"]\n",
    "                if model_stat[\"predictions\"] > 0 else 0\n",
    "            )\n",
    "            \n",
    "            stats[model_name] = {\n",
    "                \"predictions\": model_stat[\"predictions\"],\n",
    "                \"average_time\": round(avg_time, 4),\n",
    "                \"total_time\": round(model_stat[\"total_time\"], 2),\n",
    "                \"loaded_since\": str(model_stat[\"loaded_at\"])\n",
    "            }\n",
    "            \n",
    "        return stats\n",
    "\n",
    "# Instance globale du gestionnaire de mod√®les\n",
    "nlp_manager = NLPModelManager()\n",
    "print(\"‚úÖ NLP Model Manager initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Service de Cache Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisCache:\n",
    "    \"\"\"Service de cache Redis avec gestion de connexion robuste\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.connected = False\n",
    "        self.connect()\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"√âtablit la connexion Redis avec retry automatique\"\"\"\n",
    "        try:\n",
    "            self.client = redis.Redis(\n",
    "                host=config.REDIS_HOST,\n",
    "                port=config.REDIS_PORT,\n",
    "                db=config.REDIS_DB,\n",
    "                password=config.REDIS_PASSWORD,\n",
    "                decode_responses=True,\n",
    "                socket_timeout=5,\n",
    "                socket_connect_timeout=5,\n",
    "                retry_on_timeout=True\n",
    "            )\n",
    "            \n",
    "            # Test de connexion\n",
    "            self.client.ping()\n",
    "            self.connected = True\n",
    "            logger.info(\"‚úÖ Redis connect√©\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Redis non disponible: {e}\")\n",
    "            self.connected = False\n",
    "            self.client = None\n",
    "    \n",
    "    def generate_key(self, text: str, model: str, additional_data: Dict = None) -> str:\n",
    "        \"\"\"G√©n√®re une cl√© de cache d√©terministe\"\"\"\n",
    "        # Normalisation du texte\n",
    "        normalized_text = text.strip().lower()\n",
    "        \n",
    "        # Donn√©es pour le hash\n",
    "        key_data = {\n",
    "            \"text\": normalized_text,\n",
    "            \"model\": model\n",
    "        }\n",
    "        \n",
    "        if additional_data:\n",
    "            key_data.update(additional_data)\n",
    "        \n",
    "        # Hash MD5 pour une cl√© courte et unique\n",
    "        key_string = json.dumps(key_data, sort_keys=True)\n",
    "        key_hash = hashlib.md5(key_string.encode()).hexdigest()\n",
    "        \n",
    "        return f\"nlp:sentiment:{key_hash}\"\n",
    "    \n",
    "    async def get(self, key: str) -> Optional[Dict]:\n",
    "        \"\"\"R√©cup√®re une valeur du cache\"\"\"\n",
    "        if not self.connected:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            value = self.client.get(key)\n",
    "            if value:\n",
    "                return json.loads(value)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lecture cache: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    async def set(self, key: str, value: Dict, ttl: int = None) -> bool:\n",
    "        \"\"\"Stocke une valeur dans le cache\"\"\"\n",
    "        if not self.connected:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            ttl = ttl or config.CACHE_TTL_MEDIUM\n",
    "            self.client.setex(key, ttl, json.dumps(value))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur √©criture cache: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def delete(self, key: str) -> bool:\n",
    "        \"\"\"Supprime une cl√© du cache\"\"\"\n",
    "        if not self.connected:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            return bool(self.client.delete(key))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur suppression cache: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retourne les statistiques Redis\"\"\"\n",
    "        if not self.connected:\n",
    "            return {\"status\": \"disconnected\"}\n",
    "            \n",
    "        try:\n",
    "            info = self.client.info()\n",
    "            return {\n",
    "                \"status\": \"connected\",\n",
    "                \"used_memory\": info.get(\"used_memory_human\", \"N/A\"),\n",
    "                \"connected_clients\": info.get(\"connected_clients\", 0),\n",
    "                \"total_commands_processed\": info.get(\"total_commands_processed\", 0),\n",
    "                \"keyspace_hits\": info.get(\"keyspace_hits\", 0),\n",
    "                \"keyspace_misses\": info.get(\"keyspace_misses\", 0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur stats Redis: {e}\")\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "# Instance globale du cache\n",
    "cache = RedisCache()\n",
    "print(f\"‚úÖ Cache Redis initialis√© (connect√©: {cache.connected})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Syst√®me de M√©triques Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©triques Prometheus pour monitoring\n",
    "if config.ENABLE_METRICS:\n",
    "    # Compteurs\n",
    "    REQUEST_COUNTER = Counter(\n",
    "        'nlp_requests_total',\n",
    "        'Nombre total de requ√™tes NLP',\n",
    "        ['method', 'endpoint', 'status', 'model']\n",
    "    )\n",
    "    \n",
    "    # Histogrammes pour la latence\n",
    "    REQUEST_LATENCY = Histogram(\n",
    "        'nlp_request_duration_seconds',\n",
    "        'Dur√©e des requ√™tes NLP en secondes',\n",
    "        ['method', 'endpoint', 'model'],\n",
    "        buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]\n",
    "    )\n",
    "    \n",
    "    # Histogramme pour la confiance des pr√©dictions\n",
    "    CONFIDENCE_HISTOGRAM = Histogram(\n",
    "        'nlp_prediction_confidence',\n",
    "        'Distribution des scores de confiance',\n",
    "        ['model', 'sentiment'],\n",
    "        buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    )\n",
    "    \n",
    "    # Gauges pour l'√©tat actuel\n",
    "    CACHE_HIT_RATE = Gauge(\n",
    "        'nlp_cache_hit_rate',\n",
    "        'Taux de succ√®s du cache Redis'\n",
    "    )\n",
    "    \n",
    "    ACTIVE_MODELS = Gauge(\n",
    "        'nlp_active_models',\n",
    "        'Nombre de mod√®les charg√©s en m√©moire'\n",
    "    )\n",
    "    \n",
    "    MEMORY_USAGE = Gauge(\n",
    "        'nlp_memory_usage_bytes',\n",
    "        'Utilisation m√©moire des mod√®les',\n",
    "        ['model']\n",
    "    )\n",
    "\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collecteur de m√©triques avec calculs avanc√©s\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.request_count = 0\n",
    "        self.cache_hits = 0\n",
    "        self.cache_total = 0\n",
    "        \n",
    "    def record_request(self, method: str, endpoint: str, status: int, \n",
    "                      model: str, duration: float, cached: bool = False):\n",
    "        \"\"\"Enregistre une requ√™te avec toutes ses m√©triques\"\"\"\n",
    "        if not config.ENABLE_METRICS:\n",
    "            return\n",
    "            \n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Compteur de requ√™tes\n",
    "        REQUEST_COUNTER.labels(\n",
    "            method=method,\n",
    "            endpoint=endpoint,\n",
    "            status=str(status),\n",
    "            model=model\n",
    "        ).inc()\n",
    "        \n",
    "        # Latence\n",
    "        REQUEST_LATENCY.labels(\n",
    "            method=method,\n",
    "            endpoint=endpoint,\n",
    "            model=model\n",
    "        ).observe(duration)\n",
    "        \n",
    "        # Cache hit rate\n",
    "        self.cache_total += 1\n",
    "        if cached:\n",
    "            self.cache_hits += 1\n",
    "            \n",
    "        if self.cache_total > 0:\n",
    "            hit_rate = self.cache_hits / self.cache_total\n",
    "            CACHE_HIT_RATE.set(hit_rate)\n",
    "    \n",
    "    def record_prediction(self, model: str, sentiment: str, confidence: float):\n",
    "        \"\"\"Enregistre les m√©triques d'une pr√©diction\"\"\"\n",
    "        if not config.ENABLE_METRICS:\n",
    "            return\n",
    "            \n",
    "        CONFIDENCE_HISTOGRAM.labels(\n",
    "            model=model,\n",
    "            sentiment=sentiment\n",
    "        ).observe(confidence)\n",
    "    \n",
    "    def update_model_metrics(self):\n",
    "        \"\"\"Met √† jour les m√©triques des mod√®les\"\"\"\n",
    "        if not config.ENABLE_METRICS:\n",
    "            return\n",
    "            \n",
    "        ACTIVE_MODELS.set(len(nlp_manager.models))\n",
    "        \n",
    "        # Simulation de l'utilisation m√©moire (en production, utiliser psutil)\n",
    "        for model_name in nlp_manager.models.keys():\n",
    "            # Estimation approximative pour BERT\n",
    "            estimated_memory = 500 * 1024 * 1024  # 500MB\n",
    "            MEMORY_USAGE.labels(model=model_name).set(estimated_memory)\n",
    "\n",
    "# Instance globale du collecteur\n",
    "metrics = MetricsCollector()\n",
    "print(\"‚úÖ Syst√®me de m√©triques Prometheus configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Application FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de l'application FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"NLP Production API\",\n",
    "    description=\"API de production pour services NLP avec cache, monitoring et haute disponibilit√©\",\n",
    "    version=\"2.1.0\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Configuration CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # En production, sp√©cifier les domaines autoris√©s\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Variables globales pour le monitoring\n",
    "app_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Middleware pour m√©triques automatiques\n",
    "@app.middleware(\"http\")\n",
    "async def metrics_middleware(request, call_next):\n",
    "    \"\"\"Middleware pour collecter automatiquement les m√©triques\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Traitement de la requ√™te\n",
    "    response = await call_next(request)\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    process_time = time.time() - start_time\n",
    "    \n",
    "    # Extraction des informations\n",
    "    method = request.method\n",
    "    path = request.url.path\n",
    "    status_code = response.status_code\n",
    "    \n",
    "    # Mod√®le utilis√© (depuis les headers de r√©ponse si disponible)\n",
    "    model_used = response.headers.get(\"X-Model-Used\", \"unknown\")\n",
    "    \n",
    "    # Enregistrement des m√©triques\n",
    "    metrics.record_request(\n",
    "        method=method,\n",
    "        endpoint=path,\n",
    "        status=status_code,\n",
    "        model=model_used,\n",
    "        duration=process_time,\n",
    "        cached=response.headers.get(\"X-Cache-Hit\", \"false\") == \"true\"\n",
    "    )\n",
    "    \n",
    "    # Ajout des headers de performance\n",
    "    response.headers[\"X-Process-Time\"] = str(round(process_time, 4))\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Middleware de m√©triques configur√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Endpoints de l'API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/health\", response_model=HealthStatus)\n",
    "async def health_check():\n",
    "    \"\"\"Health check complet avec statut des services\"\"\"\n",
    "    uptime = time.time() - app_start_time\n",
    "    \n",
    "    # V√©rification des services\n",
    "    services_status = {\n",
    "        \"redis\": \"healthy\" if cache.connected else \"unhealthy\",\n",
    "        \"nlp_models\": \"healthy\" if len(nlp_manager.models) > 0 else \"warming_up\",\n",
    "        \"api\": \"healthy\"\n",
    "    }\n",
    "    \n",
    "    # Statut global\n",
    "    overall_status = \"healthy\" if all(\n",
    "        status in [\"healthy\", \"warming_up\"] for status in services_status.values()\n",
    "    ) else \"degraded\"\n",
    "    \n",
    "    return HealthStatus(\n",
    "        status=overall_status,\n",
    "        timestamp=datetime.now(),\n",
    "        version=\"2.1.0\",\n",
    "        uptime_seconds=uptime,\n",
    "        services=services_status\n",
    "    )\n",
    "\n",
    "@app.get(\"/ready\")\n",
    "async def readiness_check():\n",
    "    \"\"\"Readiness probe pour Kubernetes\"\"\"\n",
    "    # V√©rification que les services critiques sont pr√™ts\n",
    "    if not cache.connected:\n",
    "        raise HTTPException(status_code=503, detail=\"Redis non disponible\")\n",
    "    \n",
    "    return {\"status\": \"ready\", \"timestamp\": datetime.now()}\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    \"\"\"Endpoint pour Prometheus scraping\"\"\"\n",
    "    if not config.ENABLE_METRICS:\n",
    "        raise HTTPException(status_code=404, detail=\"M√©triques d√©sactiv√©es\")\n",
    "    \n",
    "    # Mise √† jour des m√©triques avant export\n",
    "    metrics.update_model_metrics()\n",
    "    \n",
    "    return Response(\n",
    "        content=generate_latest(),\n",
    "        media_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Endpoints de sant√© configur√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/sentiment\", response_model=SentimentResponse)\n",
    "async def analyze_sentiment(input_data: TextInput):\n",
    "    \"\"\"Analyse de sentiment avec cache intelligent\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # G√©n√©ration de la cl√© de cache\n",
    "    cache_key = cache.generate_key(\n",
    "        input_data.text, \n",
    "        input_data.model,\n",
    "        {\"language\": input_data.language}\n",
    "    )\n",
    "    \n",
    "    # Tentative de r√©cup√©ration du cache\n",
    "    cached_result = await cache.get(cache_key)\n",
    "    if cached_result:\n",
    "        cached_result[\"cached\"] = True\n",
    "        \n",
    "        # Headers pour le monitoring\n",
    "        response = JSONResponse(content=cached_result)\n",
    "        response.headers[\"X-Cache-Hit\"] = \"true\"\n",
    "        response.headers[\"X-Model-Used\"] = cached_result[\"model_used\"]\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    # Pr√©diction avec le mod√®le\n",
    "    try:\n",
    "        result = nlp_manager.predict_sentiment(input_data.text, input_data.model)\n",
    "        \n",
    "        # Enrichissement du r√©sultat\n",
    "        result[\"text\"] = input_data.text\n",
    "        result[\"cached\"] = False\n",
    "        \n",
    "        # Mise en cache du r√©sultat\n",
    "        await cache.set(cache_key, result, config.CACHE_TTL_MEDIUM)\n",
    "        \n",
    "        # Enregistrement des m√©triques de pr√©diction\n",
    "        metrics.record_prediction(\n",
    "            model=result[\"model_used\"],\n",
    "            sentiment=result[\"sentiment\"],\n",
    "            confidence=result[\"confidence\"]\n",
    "        )\n",
    "        \n",
    "        # Headers pour le monitoring\n",
    "        response = JSONResponse(content=result)\n",
    "        response.headers[\"X-Cache-Hit\"] = \"false\"\n",
    "        response.headers[\"X-Model-Used\"] = result[\"model_used\"]\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur analyse sentiment: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Erreur lors de l'analyse\")\n",
    "\n",
    "@app.post(\"/sentiment/batch\")\n",
    "async def analyze_sentiment_batch(input_data: BatchTextInput):\n",
    "    \"\"\"Analyse de sentiment en lot pour traitement optimis√©\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text in input_data.texts:\n",
    "        try:\n",
    "            # R√©utilisation de la logique individuelle\n",
    "            text_input = TextInput(text=text, model=input_data.model)\n",
    "            result = await analyze_sentiment(text_input)\n",
    "            \n",
    "            # Extraction du contenu JSON si c'est une Response\n",
    "            if hasattr(result, 'body'):\n",
    "                import json\n",
    "                result = json.loads(result.body)\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur sur le texte '{text[:50]}...': {e}\")\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"error\": str(e),\n",
    "                \"sentiment\": \"error\",\n",
    "                \"confidence\": 0.0\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"total_processed\": len(input_data.texts),\n",
    "        \"successful\": len([r for r in results if \"error\" not in r]),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Endpoints d'analyse de sentiment configur√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/stats\")\n",
    "async def get_service_stats():\n",
    "    \"\"\"Statistiques d√©taill√©es des services\"\"\"\n",
    "    uptime = time.time() - app_start_time\n",
    "    \n",
    "    return {\n",
    "        \"service\": {\n",
    "            \"name\": \"NLP Production API\",\n",
    "            \"version\": \"2.1.0\",\n",
    "            \"uptime_seconds\": uptime,\n",
    "            \"uptime_human\": f\"{uptime//3600:.0f}h {(uptime%3600)//60:.0f}m {uptime%60:.0f}s\"\n",
    "        },\n",
    "        \"models\": nlp_manager.get_model_stats(),\n",
    "        \"cache\": cache.get_stats(),\n",
    "        \"requests\": {\n",
    "            \"total\": metrics.request_count,\n",
    "            \"cache_hit_rate\": round(metrics.cache_hits / max(metrics.cache_total, 1), 3)\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/models\")\n",
    "async def list_available_models():\n",
    "    \"\"\"Liste des mod√®les disponibles et leurs statuts\"\"\"\n",
    "    available_models = {\n",
    "        \"sentiment\": {\n",
    "            \"name\": \"Sentiment Analysis\",\n",
    "            \"description\": \"Analyse de sentiment (positif/n√©gatif/neutre)\",\n",
    "            \"languages\": [\"en\", \"fr\"],\n",
    "            \"loaded\": \"sentiment\" in nlp_manager.models,\n",
    "            \"model_id\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"available_models\": available_models,\n",
    "        \"loaded_models\": list(nlp_manager.models.keys()),\n",
    "        \"total_loaded\": len(nlp_manager.models)\n",
    "    }\n",
    "\n",
    "@app.delete(\"/cache\")\n",
    "async def clear_cache():\n",
    "    \"\"\"Vide le cache Redis (utile pour le debugging)\"\"\"\n",
    "    if not cache.connected:\n",
    "        raise HTTPException(status_code=503, detail=\"Redis non disponible\")\n",
    "    \n",
    "    try:\n",
    "        cache.client.flushdb()\n",
    "        return {\"message\": \"Cache vid√© avec succ√®s\", \"timestamp\": datetime.now()}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Erreur: {e}\")\n",
    "\n",
    "print(\"‚úÖ Endpoints d'administration configur√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Tests et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de base pour valider l'architecture\n",
    "import asyncio\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "# Client de test\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_health_endpoint():\n",
    "    \"\"\"Test de l'endpoint de sant√©\"\"\"\n",
    "    response = client.get(\"/health\")\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    data = response.json()\n",
    "    assert \"status\" in data\n",
    "    assert \"services\" in data\n",
    "    assert \"uptime_seconds\" in data\n",
    "    \n",
    "    print(\"‚úÖ Test health endpoint : OK\")\n",
    "    return data\n",
    "\n",
    "def test_sentiment_analysis():\n",
    "    \"\"\"Test de l'analyse de sentiment\"\"\"\n",
    "    test_data = {\n",
    "        \"text\": \"I love this product! It's amazing!\",\n",
    "        \"model\": \"sentiment\",\n",
    "        \"language\": \"en\"\n",
    "    }\n",
    "    \n",
    "    response = client.post(\"/sentiment\", json=test_data)\n",
    "    assert response.status_code == 200\n",
    "    \n",
    "    data = response.json()\n",
    "    assert \"sentiment\" in data\n",
    "    assert \"confidence\" in data\n",
    "    assert \"processing_time\" in data\n",
    "    \n",
    "    print(f\"‚úÖ Test sentiment analysis : {data['sentiment']} (confiance: {data['confidence']:.3f})\")\n",
    "    return data\n",
    "\n",
    "def test_cache_functionality():\n",
    "    \"\"\"Test du syst√®me de cache\"\"\"\n",
    "    test_text = \"This is a test for caching functionality.\"\n",
    "    \n",
    "    # Premi√®re requ√™te (cache miss)\n",
    "    response1 = client.post(\"/sentiment\", json={\"text\": test_text})\n",
    "    data1 = response1.json()\n",
    "    \n",
    "    # Deuxi√®me requ√™te (cache hit attendu)\n",
    "    response2 = client.post(\"/sentiment\", json={\"text\": test_text})\n",
    "    data2 = response2.json()\n",
    "    \n",
    "    # V√©rification que le cache fonctionne\n",
    "    cache_hit = response2.headers.get(\"X-Cache-Hit\") == \"true\"\n",
    "    \n",
    "    print(f\"‚úÖ Test cache : Cache hit = {cache_hit}\")\n",
    "    print(f\"   Premi√®re requ√™te: {data1['processing_time']:.4f}s\")\n",
    "    print(f\"   Deuxi√®me requ√™te: {data2['processing_time']:.4f}s\")\n",
    "    \n",
    "    return cache_hit\n",
    "\n",
    "# Ex√©cution des tests\n",
    "print(\"üß™ Lancement des tests...\\n\")\n",
    "\n",
    "health_data = test_health_endpoint()\n",
    "sentiment_data = test_sentiment_analysis()\n",
    "cache_working = test_cache_functionality()\n",
    "\n",
    "print(\"\\n‚úÖ Tous les tests sont pass√©s !\")\n",
    "print(f\"Service status: {health_data['status']}\")\n",
    "print(f\"Cache Redis: {'‚úÖ' if cache.connected else '‚ùå'}\")\n",
    "print(f\"Mod√®les charg√©s: {len(nlp_manager.models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Test de Charge et Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import statistics\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "\n",
    "def performance_test(num_requests=50, concurrent_threads=10):\n",
    "    \"\"\"Test de performance avec requ√™tes concurrentes\"\"\"\n",
    "    \n",
    "    test_texts = [\n",
    "        \"I love this product!\",\n",
    "        \"This is terrible quality.\",\n",
    "        \"The service was okay, nothing special.\",\n",
    "        \"Amazing experience, highly recommended!\",\n",
    "        \"Could be better, but acceptable.\",\n",
    "        \"Worst purchase ever, very disappointed.\",\n",
    "        \"Good value for money.\",\n",
    "        \"Excellent customer service and fast delivery.\",\n",
    "        \"Not what I expected, returning it.\",\n",
    "        \"Perfect! Exactly what I wanted.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    errors = []\n",
    "    \n",
    "    def make_request(text):\n",
    "        \"\"\"Fonction pour une requ√™te individuelle\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = client.post(\"/sentiment\", json={\"text\": text})\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"duration\": end_time - start_time,\n",
    "                    \"cached\": data.get(\"cached\", False),\n",
    "                    \"confidence\": data.get(\"confidence\", 0)\n",
    "                }\n",
    "            else:\n",
    "                return {\"success\": False, \"status_code\": response.status_code}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    print(f\"üöÄ Lancement test de charge: {num_requests} requ√™tes avec {concurrent_threads} threads\")\n",
    "    \n",
    "    # G√©n√©ration des requ√™tes\n",
    "    requests_to_make = []\n",
    "    for i in range(num_requests):\n",
    "        text = test_texts[i % len(test_texts)]\n",
    "        requests_to_make.append(text)\n",
    "    \n",
    "    # Ex√©cution concurrente\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_threads) as executor:\n",
    "        future_to_text = {executor.submit(make_request, text): text for text in requests_to_make}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_text):\n",
    "            result = future.result()\n",
    "            if result[\"success\"]:\n",
    "                results.append(result)\n",
    "            else:\n",
    "                errors.append(result)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyse des r√©sultats\n",
    "    if results:\n",
    "        durations = [r[\"duration\"] for r in results]\n",
    "        cached_requests = sum(1 for r in results if r[\"cached\"])\n",
    "        \n",
    "        stats = {\n",
    "            \"total_requests\": num_requests,\n",
    "            \"successful_requests\": len(results),\n",
    "            \"failed_requests\": len(errors),\n",
    "            \"total_time\": round(total_time, 2),\n",
    "            \"requests_per_second\": round(len(results) / total_time, 2),\n",
    "            \"avg_latency\": round(statistics.mean(durations) * 1000, 2),  # en ms\n",
    "            \"p50_latency\": round(statistics.median(durations) * 1000, 2),\n",
    "            \"p95_latency\": round(statistics.quantiles(durations, n=20)[18] * 1000, 2),\n",
    "            \"p99_latency\": round(statistics.quantiles(durations, n=100)[98] * 1000, 2),\n",
    "            \"min_latency\": round(min(durations) * 1000, 2),\n",
    "            \"max_latency\": round(max(durations) * 1000, 2),\n",
    "            \"cache_hit_rate\": round(cached_requests / len(results) * 100, 1),\n",
    "            \"success_rate\": round(len(results) / num_requests * 100, 1)\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüìä R√©sultats du test de performance:\")\n",
    "        print(f\"   Requ√™tes totales: {stats['total_requests']}\")\n",
    "        print(f\"   Succ√®s: {stats['successful_requests']} ({stats['success_rate']}%)\")\n",
    "        print(f\"   √âchecs: {stats['failed_requests']}\")\n",
    "        print(f\"   Dur√©e totale: {stats['total_time']}s\")\n",
    "        print(f\"   Req/sec: {stats['requests_per_second']}\")\n",
    "        print(f\"   Latence moyenne: {stats['avg_latency']}ms\")\n",
    "        print(f\"   Latence P95: {stats['p95_latency']}ms\")\n",
    "        print(f\"   Latence P99: {stats['p99_latency']}ms\")\n",
    "        print(f\"   Cache hit rate: {stats['cache_hit_rate']}%\")\n",
    "        \n",
    "        return stats\n",
    "    else:\n",
    "        print(\"‚ùå Aucune requ√™te r√©ussie\")\n",
    "        return None\n",
    "\n",
    "# Ex√©cution du test de performance\n",
    "perf_stats = performance_test(num_requests=100, concurrent_threads=20)\n",
    "\n",
    "if perf_stats:\n",
    "    print(\"\\nüéØ √âvaluation des performances:\")\n",
    "    if perf_stats['requests_per_second'] > 50:\n",
    "        print(\"   ‚úÖ Excellent throughput\")\n",
    "    elif perf_stats['requests_per_second'] > 20:\n",
    "        print(\"   ‚ö†Ô∏è Throughput acceptable\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Throughput faible\")\n",
    "        \n",
    "    if perf_stats['p95_latency'] < 100:\n",
    "        print(\"   ‚úÖ Excellente latence P95\")\n",
    "    elif perf_stats['p95_latency'] < 500:\n",
    "        print(\"   ‚ö†Ô∏è Latence P95 acceptable\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Latence P95 √©lev√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üê≥ Configuration Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des fichiers de configuration Docker\n",
    "\n",
    "dockerfile_content = '''# Dockerfile pour API NLP de production\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Variables d'environnement\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV PYTHONDONTWRITEBYTECODE=1\n",
    "\n",
    "# Cr√©ation utilisateur non-root\n",
    "RUN useradd --create-home --shell /bin/bash nlpuser\n",
    "\n",
    "# D√©pendances syst√®me\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# R√©pertoire de travail\n",
    "WORKDIR /app\n",
    "\n",
    "# Installation des d√©pendances Python\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copie de l'application\n",
    "COPY . .\n",
    "RUN chown -R nlpuser:nlpuser /app\n",
    "\n",
    "# Pr√©-t√©l√©chargement des mod√®les\n",
    "RUN python -c \"from transformers import pipeline; pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')\"\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Utilisateur non-root\n",
    "USER nlpuser\n",
    "\n",
    "# Port expos√©\n",
    "EXPOSE 8000\n",
    "\n",
    "# Commande de d√©marrage\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "'''\n",
    "\n",
    "print(\"üìù Dockerfile g√©n√©r√©\")\n",
    "print(dockerfile_content[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker-compose.yml pour environnement complet\n",
    "docker_compose_content = '''version: '3.8'\n",
    "\n",
    "services:\n",
    "  # API NLP\n",
    "  nlp-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - REDIS_PORT=6379\n",
    "      - ENABLE_METRICS=true\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - postgres\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "  # Cache Redis\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    command: redis-server --appendonly yes\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Base de donn√©es PostgreSQL\n",
    "  postgres:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      - POSTGRES_DB=nlp_production\n",
    "      - POSTGRES_USER=nlpuser\n",
    "      - POSTGRES_PASSWORD=nlppassword123\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Monitoring Prometheus\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus_data:/prometheus\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Dashboard Grafana\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin123\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Load Balancer Nginx\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf\n",
    "    depends_on:\n",
    "      - nlp-api\n",
    "    restart: unless-stopped\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "  postgres_data:\n",
    "  prometheus_data:\n",
    "  grafana_data:\n",
    "\n",
    "networks:\n",
    "  default:\n",
    "    name: nlp-production\n",
    "'''\n",
    "\n",
    "print(\"üìù docker-compose.yml g√©n√©r√©\")\n",
    "print(\"Services inclus: API NLP, Redis, PostgreSQL, Prometheus, Grafana, Nginx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Nginx pour load balancing\n",
    "nginx_config = '''events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream nlp_backend {\n",
    "        least_conn;\n",
    "        server nlp-api:8000 weight=1 max_fails=3 fail_timeout=30s;\n",
    "        # Ajouter plus d'instances ici pour scaling horizontal\n",
    "    }\n",
    "\n",
    "    # Cache configuration\n",
    "    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=nlp_cache:10m max_size=1g\n",
    "                     inactive=60m use_temp_path=off;\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "        \n",
    "        # Rate limiting\n",
    "        limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;\n",
    "        \n",
    "        location / {\n",
    "            proxy_pass http://nlp_backend;\n",
    "            \n",
    "            # Headers\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
    "            proxy_set_header X-Forwarded-Proto $scheme;\n",
    "            \n",
    "            # Timeouts\n",
    "            proxy_connect_timeout 30s;\n",
    "            proxy_send_timeout 30s;\n",
    "            proxy_read_timeout 30s;\n",
    "            \n",
    "            # Rate limiting\n",
    "            limit_req zone=api_limit burst=20 nodelay;\n",
    "        }\n",
    "        \n",
    "        # Cache pour les requ√™tes GET non-critiques\n",
    "        location ~ ^/(health|models|stats) {\n",
    "            proxy_pass http://nlp_backend;\n",
    "            proxy_cache nlp_cache;\n",
    "            proxy_cache_valid 200 5m;\n",
    "            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;\n",
    "            add_header X-Cache-Status $upstream_cache_status;\n",
    "        }\n",
    "        \n",
    "        # Health check endpoint pour load balancer\n",
    "        location /nginx-health {\n",
    "            access_log off;\n",
    "            return 200 \"healthy\\n\";\n",
    "            add_header Content-Type text/plain;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"üìù Configuration Nginx g√©n√©r√©e\")\n",
    "print(\"Features: Load balancing, rate limiting, caching, health checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dashboard de Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation simple des m√©triques\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_monitoring_dashboard():\n",
    "    \"\"\"Cr√©e un dashboard simple de monitoring\"\"\"\n",
    "    \n",
    "    # Donn√©es simul√©es pour la d√©monstration\n",
    "    hours = list(range(24))\n",
    "    \n",
    "    # Simulation de trafic avec pics aux heures de bureau\n",
    "    traffic = [50 + 30*np.sin((h-6)*np.pi/12) + np.random.normal(0, 10) for h in hours]\n",
    "    traffic = [max(0, t) for t in traffic]  # Pas de valeurs n√©gatives\n",
    "    \n",
    "    # Simulation de latence (plus √©lev√©e pendant les pics)\n",
    "    latency = [30 + 20*np.sin((h-6)*np.pi/12) + np.random.normal(0, 5) for h in hours]\n",
    "    latency = [max(10, l) for l in latency]  # Latence minimale de 10ms\n",
    "    \n",
    "    # Cache hit rate (plus √©lev√© pendant les heures de forte charge)\n",
    "    cache_hit_rate = [60 + 20*np.sin((h-6)*np.pi/12) + np.random.normal(0, 5) for h in hours]\n",
    "    cache_hit_rate = [max(20, min(95, c)) for c in cache_hit_rate]  # Entre 20% et 95%\n",
    "    \n",
    "    # Cr√©ation du dashboard\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üìä Dashboard NLP Production API - Derni√®res 24h', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Graphique 1: Trafic\n",
    "    ax1.plot(hours, traffic, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.set_title('üöÄ Trafic (Requ√™tes/heure)', fontweight='bold')\n",
    "    ax1.set_xlabel('Heure')\n",
    "    ax1.set_ylabel('Requ√™tes/heure')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.fill_between(hours, traffic, alpha=0.3, color='blue')\n",
    "    \n",
    "    # Graphique 2: Latence\n",
    "    ax2.plot(hours, latency, 'r-', linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_title('‚ö° Latence P95 (ms)', fontweight='bold')\n",
    "    ax2.set_xlabel('Heure')\n",
    "    ax2.set_ylabel('Latence (ms)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.axhline(y=100, color='orange', linestyle='--', alpha=0.7, label='Seuil alerte')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Graphique 3: Cache Hit Rate\n",
    "    ax3.plot(hours, cache_hit_rate, 'g-', linewidth=2, marker='^', markersize=4)\n",
    "    ax3.set_title('üíæ Cache Hit Rate (%)', fontweight='bold')\n",
    "    ax3.set_xlabel('Heure')\n",
    "    ax3.set_ylabel('Cache Hit Rate (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.fill_between(hours, cache_hit_rate, alpha=0.3, color='green')\n",
    "    ax3.set_ylim(0, 100)\n",
    "    \n",
    "    # Graphique 4: Distribution des sentiments\n",
    "    sentiments = ['Positif', 'Neutre', 'N√©gatif']\n",
    "    sentiment_counts = [45, 35, 20]  # Pourcentages simul√©s\n",
    "    colors = ['#4CAF50', '#FFC107', '#F44336']\n",
    "    \n",
    "    ax4.pie(sentiment_counts, labels=sentiments, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax4.set_title('üòä Distribution des Sentiments', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nlp_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # R√©sum√© des m√©triques\n",
    "    print(\"\\nüìà R√©sum√© des m√©triques (24h):\")\n",
    "    print(f\"   Trafic total: {sum(traffic):.0f} requ√™tes\")\n",
    "    print(f\"   Latence moyenne: {np.mean(latency):.1f}ms\")\n",
    "    print(f\"   Cache hit rate moyen: {np.mean(cache_hit_rate):.1f}%\")\n",
    "    print(f\"   Pic de trafic: {max(traffic):.0f} req/h √† {hours[traffic.index(max(traffic))]}h\")\n",
    "    \n",
    "    return {\n",
    "        'total_requests': sum(traffic),\n",
    "        'avg_latency': np.mean(latency),\n",
    "        'avg_cache_hit_rate': np.mean(cache_hit_rate),\n",
    "        'peak_hour': hours[traffic.index(max(traffic))],\n",
    "        'peak_traffic': max(traffic)\n",
    "    }\n",
    "\n",
    "# G√©n√©ration du dashboard\n",
    "dashboard_stats = create_monitoring_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusion et Recommandations\n",
    "\n",
    "### ‚úÖ Ce que nous avons impl√©ment√© :\n",
    "\n",
    "1. **Architecture Microservices**\n",
    "   - API FastAPI avec validation Pydantic\n",
    "   - Gestionnaire de mod√®les NLP optimis√©\n",
    "   - Cache Redis intelligent avec TTL adaptatif\n",
    "   - Syst√®me de m√©triques Prometheus\n",
    "\n",
    "2. **Fonctionnalit√©s de Production**\n",
    "   - Health checks et readiness probes\n",
    "   - Rate limiting et gestion d'erreurs\n",
    "   - Middleware de m√©triques automatique\n",
    "   - Support du traitement par lots\n",
    "\n",
    "3. **Monitoring et Observabilit√©**\n",
    "   - M√©triques RED (Rate, Errors, Duration)\n",
    "   - M√©triques business (confiance, distribution sentiment)\n",
    "   - Dashboard de performance\n",
    "   - Logs structur√©s\n",
    "\n",
    "4. **D√©ploiement**\n",
    "   - Configuration Docker optimis√©e\n",
    "   - docker-compose pour environnement complet\n",
    "   - Load balancing Nginx\n",
    "   - Scaling horizontal ready\n",
    "\n",
    "### üöÄ √âtapes suivantes pour la production :\n",
    "\n",
    "1. **S√©curit√©**\n",
    "   - Authentification JWT\n",
    "   - Rate limiting par utilisateur\n",
    "   - Validation et sanitisation des inputs\n",
    "   - HTTPS avec certificats SSL\n",
    "\n",
    "2. **Scaling**\n",
    "   - Kubernetes pour orchestration\n",
    "   - Auto-scaling bas√© sur m√©triques\n",
    "   - Load balancing multi-r√©gion\n",
    "   - CDN pour cache global\n",
    "\n",
    "3. **Monitoring Avanc√©**\n",
    "   - Alerting Prometheus/Grafana\n",
    "   - Distributed tracing (Jaeger)\n",
    "   - Log aggregation (ELK)\n",
    "   - ML monitoring (drift detection)\n",
    "\n",
    "4. **Optimisations**\n",
    "   - Model quantization\n",
    "   - GPU utilization\n",
    "   - Connection pooling\n",
    "   - Async processing\n",
    "\n",
    "### üìä M√©triques de Succ√®s :\n",
    "- **Latence P95** : < 100ms\n",
    "- **Throughput** : > 1000 req/sec\n",
    "- **Disponibilit√©** : > 99.9%\n",
    "- **Cache Hit Rate** : > 70%\n",
    "- **Error Rate** : < 0.1%\n",
    "\n",
    "Cette architecture est pr√™te pour supporter des charges de production r√©elles et peut √™tre facilement √©tendue selon les besoins sp√©cifiques de votre organisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}