---
title: Module 4 - GloVe
description: Formation NLP - Module 4 - GloVe
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ“Š GloVe

Global Vectors for Word Representation

[â† Word2Vec](module4_word2vec.html)

**GloVe - Statistiques Globales**  
Stanford NLP Group, 2014

[Notebook Pratique â†’](notebook/module4_glove_demo.ipynb)

## ğŸŒŸ Introduction Ã  GloVe

**GloVe (Global Vectors for Word Representation)** est une mÃ©thode dÃ©veloppÃ©e par Jeffrey Pennington, Richard Socher et Christopher Manning Ã  Stanford en 2014. Elle combine les avantages des mÃ©thodes de factorisation matricielle et des modÃ¨les de contexte local comme Word2Vec.

#### ğŸ’¡ L'innovation Stanford

Alors que Word2Vec n'utilise que l'information locale (fenÃªtre glissante), GloVe exploite les **statistiques globales** de co-occurrence de tout le corpus. L'idÃ©e : utiliser les informations de frÃ©quence globale pour amÃ©liorer l'apprentissage des embeddings.

### ğŸ¯ Pourquoi GloVe ?

*   **Vision globale :** Exploite toutes les statistiques du corpus
*   **EfficacitÃ© :** PrÃ©-calcul de la matrice de co-occurrence
*   **ParallÃ©lisation :** Plus facile Ã  parallÃ©liser que Word2Vec
*   **Performance :** Souvent supÃ©rieur sur les tÃ¢ches d'analogies

## ğŸ§  Principe Fondamental

GloVe repose sur une observation simple mais puissante : *les ratios de probabilitÃ©s de co-occurrence rÃ©vÃ¨lent les relations sÃ©mantiques*.

#### ğŸ’¡ Exemple concret

ConsidÃ©rons les mots "glace" et "vapeur" avec le mot de contexte "solide" :

*   P(solide | glace) sera Ã©levÃ© (la glace est solide)
*   P(solide | vapeur) sera faible (la vapeur n'est pas solide)
*   **Le ratio P(solide | glace) / P(solide | vapeur)** sera trÃ¨s Ã©levÃ©

Ce ratio capture une relation sÃ©mantique importante que GloVe exploite.

**Objectif de GloVe :**  
F(wáµ¢, wâ±¼, wÌƒâ‚–) = Páµ¢â‚– / Pâ±¼â‚–  
*oÃ¹ F est une fonction des vecteurs de mots qui reproduit les ratios de co-occurrence*

## ğŸ“Š Matrice de Co-occurrence Globale

Contrairement Ã  Word2Vec qui traite les paires de mots une par une, GloVe commence par construire une matrice de co-occurrence sur tout le corpus.

#### Exemple de matrice de co-occurrence

*Phrase : "Le roi rÃ¨gne sur son royaume et la reine gouverne"*

roi

rÃ¨gne

reine

royaume

roi

0

2

1

3

rÃ¨gne

2

0

0

1

reine

1

0

0

1

royaume

3

1

1

0

**Xáµ¢â±¼** = nombre de fois que le mot j apparaÃ®t dans le contexte du mot i

1

#### Construction

Parcourir tout le corpus pour compter les co-occurrences

2

#### PondÃ©ration

Appliquer des poids selon la distance dans la fenÃªtre

3

#### Optimisation

Factoriser la matrice pour obtenir les embeddings

## ğŸ¯ Fonction de CoÃ»t GloVe

GloVe optimise une fonction de coÃ»t qui force les produits scalaires des embeddings Ã  reproduire les logarithmes des probabilitÃ©s de co-occurrence.

**Fonction de coÃ»t :**  
J = Î£áµ¢,â±¼ f(Xáµ¢â±¼) (wáµ¢áµ€ wÌƒâ±¼ + báµ¢ + bÌƒâ±¼ - log Xáµ¢â±¼)Â²  
  
**oÃ¹ :**  
â€¢ wáµ¢, wÌƒâ±¼ = vecteurs d'embeddings  
â€¢ báµ¢, bÌƒâ±¼ = termes de biais  
â€¢ f(Xáµ¢â±¼) = fonction de pondÃ©ration  
â€¢ Xáµ¢â±¼ = co-occurrence observÃ©e

#### ğŸ¯ Fonction de PondÃ©ration f(x)

Donne moins d'importance aux co-occurrences trÃ¨s rares ou trÃ¨s frÃ©quentes :

**f(x) = (x/xâ‚˜â‚â‚“)^Î±** si x < xâ‚˜â‚â‚“, sinon 1

Typiquement : xâ‚˜â‚â‚“ = 100, Î± = 0.75

#### ğŸ“ Factorisation Matricielle

L'objectif est de factoriser la matrice log(X) :

**log(Xáµ¢â±¼) â‰ˆ wáµ¢áµ€ wÌƒâ±¼ + báµ¢ + bÌƒâ±¼**

Les embeddings finaux combinent W et WÌƒ

## âš–ï¸ GloVe vs Word2Vec

Aspect

Word2Vec

GloVe

**Approche**

PrÃ©dictive locale

Factorisation de matrice globale

**Information utilisÃ©e**

Contexte local (fenÃªtre)

Statistiques globales du corpus

**PrÃ©-calcul**

âŒ Traitement sÃ©quentiel

âœ… Matrice de co-occurrence

**ParallÃ©lisation**

âš ï¸ LimitÃ©e

âœ… Excellente

**Performance analogies**

âœ… TrÃ¨s bonne

âœ… Souvent supÃ©rieure

**MÃ©moire requise**

âœ… ModÃ©rÃ©e

âš ï¸ Ã‰levÃ©e (matrice)

**Convergence**

âš ï¸ Plus lente

âœ… Plus rapide

## âš–ï¸ Avantages et Limites

#### âœ… Avantages de GloVe

*   **Vision globale :** Exploite toutes les statistiques du corpus
*   **EfficacitÃ© :** ParallÃ©lisation facile, convergence rapide
*   **Performance :** Excellent sur les tÃ¢ches d'analogies
*   **ReproductibilitÃ© :** RÃ©sultats dÃ©terministes
*   **ScalabilitÃ© :** Fonctionne bien sur de gros corpus

#### âš ï¸ Limites de GloVe

*   **MÃ©moire :** Stockage de la matrice de co-occurrence
*   **Preprocessing :** Construction de la matrice coÃ»teuse
*   **Vocabulaire :** Taille limitÃ©e par la mÃ©moire
*   **Nouveaux mots :** Pas de gestion des mots OOV
*   **ComplexitÃ© :** Plus de paramÃ¨tres Ã  ajuster

## ğŸš€ Applications et Usage

GloVe est particuliÃ¨rement efficace pour :

#### ğŸ§© Analogies Complexes

Excelle sur les benchmarks d'analogies grÃ¢ce Ã  sa vision globale des relations sÃ©mantiques.

#### ğŸ“Š Analyse SÃ©mantique

Capture efficacement les relations sÃ©mantiques subtiles entre concepts.

#### ğŸ” Recherche Documentaire

AmÃ©liore la recherche sÃ©mantique grÃ¢ce Ã  des embeddings de haute qualitÃ©.

#### ğŸ¯ Transfer Learning

Embeddings prÃ©-entraÃ®nÃ©s utilisables comme features pour diverses tÃ¢ches NLP.

#### ğŸ“š Sources et Ressources Officielles

Pour approfondir vos connaissances sur GloVe :

[ğŸ“„ Paper Original - GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) [ğŸ›ï¸ Site Officiel Stanford NLP](https://nlp.stanford.edu/projects/glove/) [ğŸ’» Code Source Officiel (GitHub)](https://github.com/stanfordnlp/GloVe) [ğŸ“¦ Embeddings PrÃ©-entraÃ®nÃ©s](https://nlp.stanford.edu/data/glove.6B.zip) [ğŸ Utilisation avec Gensim](https://radimrehurek.com/gensim/scripts/glove2word2vec.html)

[â† Index Module 4](index.html)

**PrÃªt pour FastText ?**  
DÃ©couvrez la gestion des sous-mots

[FastText â†’](module4_fasttext.html)
