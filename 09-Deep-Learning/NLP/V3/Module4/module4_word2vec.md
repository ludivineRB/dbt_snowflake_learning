---
title: 'Module 4 - Word2Vec : Comprendre les Embeddings de Mots'
description: 'Formation NLP - Module 4 - Word2Vec : Comprendre les Embeddings de Mots'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ§  Word2Vec : Comprendre les Embeddings de Mots

La rÃ©volution qui a transformÃ© le traitement du langage naturel

**ğŸ¯ L'idÃ©e gÃ©niale de Word2Vec :**  
"Les mots qui apparaissent dans des contextes similaires ont des significations similaires"

[â† Index Module 4](index.html)

**Word2Vec - De dÃ©butant Ã  expert**  
Google Research, 2013

[Notebook Pratique â†’](notebook/module4_word2vec_demo.ipynb)

## 1\. ğŸŒŸ Qu'est-ce que Word2Vec ?

#### ğŸ’¡ Le problÃ¨me avant Word2Vec

Imaginez que vous devez expliquer Ã  un ordinateur ce qu'est un "chat". Avant Word2Vec, on reprÃ©sentait chaque mot par un numÃ©ro unique :

*   chat = \[0, 0, 1, 0, 0, ...\] (un seul 1 parmi des milliers de 0)
*   chien = \[0, 0, 0, 1, 0, ...\] (un 1 Ã  une position diffÃ©rente)

**ProblÃ¨me :** L'ordinateur ne sait pas que "chat" et "chien" sont similaires !

#### âœ¨ La solution Word2Vec

**Word2Vec** transforme chaque mot en un petit vecteur dense qui capture sa signification :

*   chat = \[0.2, -0.5, 0.8, 0.1, ...\] (100 dimensions par exemple)
*   chien = \[0.3, -0.4, 0.7, 0.2, ...\] (vecteur similaire car sens proche)

**RÃ©volution :** Maintenant l'ordinateur "comprend" que chat et chien sont proches !

#### ğŸ” Exemple concret

**Phrase 1 :** "Le chat dort sur le canapÃ©"

**Phrase 2 :** "Le chien dort sur le canapÃ©"

**RÃ©sultat :** Word2Vec remarque que "chat" et "chien" apparaissent dans le mÃªme contexte, donc leurs vecteurs deviennent similaires !

### ğŸ¯ Pourquoi c'est rÃ©volutionnaire ?

*   **Capture la sÃ©mantique :** Les mots similaires ont des vecteurs proches
*   **Relations mathÃ©matiques :** roi - homme + femme â‰ˆ reine
*   **Efficace :** Vecteurs compacts (100-300 dimensions)
*   **Non supervisÃ© :** Apprend tout seul Ã  partir de texte brut

## 2\. ğŸ—ï¸ Les Deux Architectures

Word2Vec propose deux mÃ©thodes pour apprendre ces vecteurs magiques :

#### ğŸ“– CBOW (Continuous Bag of Words)

**Mission :** "Devine le mot manquant"

**Exemple :**  
Contexte : \["Le", "\_\_\_", "mange", "des", "croquettes"\]  
PrÃ©diction : "chat"

Plus rapide

Bon pour mots frÃ©quents

**Architecture :** Plusieurs mots en entrÃ©e â†’ Un mot en sortie

#### ğŸ¯ Skip-gram

**Mission :** "Devine le contexte"

**Exemple :**  
Mot central : "chat"  
PrÃ©diction : \["Le", "mange", "des", "croquettes"\]

Meilleur qualitÃ©

Excellent pour mots rares

**Architecture :** Un mot en entrÃ©e â†’ Plusieurs mots en sortie

## 3\. ğŸ¬ Comment Ã§a fonctionne ?

### ğŸ“š Le processus d'apprentissage Ã©tape par Ã©tape

**Ã‰tape 1 - PrÃ©paration :** Transforme "Le chat mange" en \["le", "chat", "mange"\]

**Ã‰tape 2 - FenÃªtre glissante :** Regarde 2 mots avant et aprÃ¨s chaque mot central

#### ğŸªŸ Exemple avec fenÃªtre de taille 2

```
[le, chat, mange, des, croquettes]
     â†‘
   mot central

Contexte de "chat" : [le, mange]
```

**Ã‰tape 3 - Apprentissage :** Le modÃ¨le ajuste les vecteurs pour que les mots qui apparaissent ensemble aient des vecteurs proches

**Ã‰tape 4 - RÃ©pÃ©tition :** Fait cela pour des millions de phrases jusqu'Ã  capturer tous les patterns

#### âœ¨ La magie des vecteurs

AprÃ¨s l'entraÃ®nement, on obtient des propriÃ©tÃ©s Ã©tonnantes :

*   **SimilaritÃ©s :** distance(chat, chien) < distance(chat, voiture)
*   **Analogies :** roi - homme + femme â‰ˆ reine
*   **Clustering :** Les mots du mÃªme domaine se regroupent

## 4\. ğŸ”§ Les ParamÃ¨tres ClÃ©s

Pour configurer Word2Vec, voici les paramÃ¨tres importants Ã  comprendre :

#### ğŸ“ vector\_size

Dimension des vecteurs de mots

RecommandÃ© : 100-300

**Impact :** Plus grand = plus prÃ©cis mais plus lent

#### ğŸªŸ window

Taille de la fenÃªtre de contexte

RecommandÃ© : 5-10

**Impact :** Plus grand = relations plus larges

#### ğŸ¯ sg (Skip-gram)

Choix de l'architecture

0 = CBOW, 1 = Skip-gram

**Impact :** Skip-gram gÃ©nÃ©ralement meilleur

#### ğŸ“Š min\_count

FrÃ©quence minimale des mots

RecommandÃ© : 5-10

**Impact :** Ignore les mots trop rares

#### ğŸ”„ epochs

Nombre d'itÃ©rations d'entraÃ®nement

RecommandÃ© : 50-200

**Impact :** Plus = meilleur apprentissage

#### ğŸ‘¥ workers

Nombre de processeurs utilisÃ©s

RecommandÃ© : 4-8

**Impact :** AccÃ©lÃ¨re l'entraÃ®nement

**\# Configuration recommandÃ©e :**  
model = Word2Vec(  
Â Â Â Â vector\_size=100,Â Â Â Â # Dimension des vecteurs  
Â Â Â Â window=5,Â Â Â Â Â Â Â Â Â Â Â # Taille du contexte  
Â Â Â Â min\_count=5,Â Â Â Â Â Â Â Â # Ignorer les mots rares  
Â Â Â Â sg=1,Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â # Skip-gram  
Â Â Â Â epochs=100Â Â Â Â Â Â Â Â Â Â # Nombre d'itÃ©rations  
)

## 5\. ğŸš€ Applications Pratiques

Word2Vec n'est pas juste thÃ©orique - voici comment on l'utilise dans la vraie vie :

#### ğŸ” Recherche SÃ©mantique

**Usage :** Trouver des documents similaires mÃªme s'ils n'utilisent pas les mÃªmes mots

**Exemple :** Chercher "voiture" trouve aussi "automobile", "vÃ©hicule", "auto"

#### ğŸ’¡ SystÃ¨me de Recommandation

**Usage :** SuggÃ©rer des produits ou contenus similaires

**Exemple :** Si vous aimez "iPhone", proposer "Samsung", "smartphone", "mobile"

#### ğŸ¯ Analyse de Sentiment

**Usage :** Comprendre les Ã©motions dans les textes

**Exemple :** DÃ©tecter que "gÃ©nial", "fantastique", "super" sont positifs

#### ğŸŒ Traduction Automatique

**Usage :** Aligner les mots entre langues

**Exemple :** "chat" en franÃ§ais correspond Ã  "cat" en anglais

#### ğŸ“Š Classification de Texte

**Usage :** CatÃ©goriser automatiquement des documents

**Exemple :** Trier des emails en "important", "spam", "personnel"

#### ğŸ¤– Chatbots

**Usage :** Comprendre les intentions des utilisateurs

**Exemple :** "RÃ©server", "Commander", "Acheter" = mÃªme intention

#### ğŸ§® Les analogies magiques

Voici les exemples les plus impressionnants de Word2Vec :

*   **roi - homme + femme â‰ˆ reine** (relations de genre)
*   **Paris - France + Italie â‰ˆ Rome** (capitales)
*   **marcher - marche + cours â‰ˆ courir** (conjugaisons)
*   **grand - plus grand + plus petit â‰ˆ petit** (comparatifs)

## 6\. âš–ï¸ Avantages et Limitations

#### âœ… Avantages

*   **Capture la sÃ©mantique :** Comprend le sens des mots
*   **Efficace :** Vecteurs compacts et rapides
*   **Relations mathÃ©matiques :** Analogies fonctionnent
*   **PrÃ©-entraÃ®nÃ© disponible :** ModÃ¨les prÃªts Ã  utiliser
*   **Non supervisÃ© :** Apprend sans annotations
*   **Polyvalent :** Fonctionne sur tous domaines

#### âŒ Limitations

*   **PolysÃ©mie :** "avocat" (fruit) = "avocat" (mÃ©tier)
*   **Mots nouveaux :** Ne gÃ¨re pas les mots non vus
*   **Besoin de donnÃ©es :** Millions de mots nÃ©cessaires
*   **Contexte fixe :** FenÃªtre limitÃ©e
*   **Pas de composition :** Difficile pour phrases complÃ¨tes
*   **Biais :** Reproduit les stÃ©rÃ©otypes du texte

#### ğŸ’¡ Exemple de limitation : la polysÃ©mie

Word2Vec donne **un seul vecteur** par mot, donc :

*   "J'ai mangÃ© un avocat" (fruit) ğŸ¥‘
*   "Mon avocat dÃ©fend mon dossier" (mÃ©tier) âš–ï¸

â†’ Le mÃªme vecteur pour les deux sens ! C'est pourquoi on a crÃ©Ã© des modÃ¨les plus rÃ©cents.

## 7\. ğŸš€ Ã‰volution vers les ModÃ¨les Modernes

Word2Vec a ouvert la voie Ã  une rÃ©volution en NLP. Voici l'Ã©volution :

2013

**Word2Vec** - Le pionnier des embeddings denses

2014

**GloVe** - Combine approche locale et globale

2016

**FastText** - GÃ¨re les mots hors vocabulaire

2018

**ELMo** - Premiers embeddings contextuels

2018

**BERT** - RÃ©volution des Transformers bidirectionnels

2019+

**GPT, T5, etc.** - ModÃ¨les de langage gÃ©ants

#### ğŸ¯ Pourquoi Word2Vec reste important ?

*   **Fondamental :** Base pour comprendre les embeddings modernes
*   **Efficace :** Parfait pour applications simples et rapides
*   **Ressources limitÃ©es :** Fonctionne sur un ordinateur portable
*   **PÃ©dagogique :** Excellent pour apprendre les concepts
*   **Toujours utilisÃ© :** Dans beaucoup d'applications industrielles

[â† Index Module 4](index.html)

**PrÃªt pour la pratique ?**  
Essayez le notebook interactif

[Notebook Pratique â†’](notebook/module4_word2vec_demo.ipynb)
