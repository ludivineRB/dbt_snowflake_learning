---
title: 'Module 5 - LeÃ§on 4 : Applications Pratiques des RNN'
description: 'Formation NLP - Module 5 - LeÃ§on 4 : Applications Pratiques des RNN'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸš€ LeÃ§on 4 : Applications Pratiques des RNN

Passons de la thÃ©orie Ã  la pratique ! DÃ©couvrez comment implÃ©menter des RNN pour rÃ©soudre des problÃ¨mes rÃ©els de NLP, avec des exemples concrets et du code prÃªt Ã  l'emploi.

## ğŸ¯ Vue d'ensemble des applications

ğŸ“

GÃ©nÃ©ration de texte

ğŸ˜Š

Analyse de sentiment

ğŸŒ

Traduction

ğŸ’¬

Chatbots

ğŸ“Š

Classification

ğŸ”®

PrÃ©diction

## ğŸ“ Projet 1 : GÃ©nÃ©ration de texte

âœï¸

### GÃ©nÃ©rateur de texte style Shakespeare

Apprenez Ã  un RNN Ã  Ã©crire comme Shakespeare !

Moyen

#### Ã‰tapes du projet :

1

**PrÃ©paration des donnÃ©es**

Charger et prÃ©parer le texte de Shakespeare

2

**CrÃ©ation des sÃ©quences**

Transformer le texte en sÃ©quences d'entraÃ®nement

3

**Construction du modÃ¨le**

LSTM avec couche d'embedding

4

**GÃ©nÃ©ration**

GÃ©nÃ©rer du nouveau texte caractÃ¨re par caractÃ¨re

```
import tensorflow as tf
from tensorflow.keras import layers

# ModÃ¨le simple de gÃ©nÃ©ration de texte
def create_text_generator(vocab_size, embedding_dim=256):
    model = tf.keras.Sequential([
        # Couche d'embedding pour convertir les caractÃ¨res en vecteurs
        layers.Embedding(vocab_size, embedding_dim),
        
        # LSTM pour apprendre les patterns
        layers.LSTM(512, return_sequences=True),
        layers.Dropout(0.3),
        
        # DeuxiÃ¨me LSTM pour plus de complexitÃ©
        layers.LSTM(512, return_sequences=True),
        layers.Dropout(0.3),
        
        # Sortie : probabilitÃ© pour chaque caractÃ¨re
        layers.Dense(vocab_size, activation='softmax')
    ])
    
    return model

# Fonction de gÃ©nÃ©ration
def generate_text(model, start_string, num_chars=100):
    # Convertir le texte de dÃ©part en nombres
    input_eval = [char_to_idx[c] for c in start_string]
    input_eval = tf.expand_dims(input_eval, 0)
    
    generated_text = []
    
    for i in range(num_chars):
        predictions = model(input_eval)
        # Prendre le dernier caractÃ¨re prÃ©dit
        predictions = tf.squeeze(predictions, 0)
        
        # Ã‰chantillonner selon les probabilitÃ©s
        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()
        
        # Ajouter Ã  notre texte gÃ©nÃ©rÃ©
        generated_text.append(idx_to_char[predicted_id])
        
        # Utiliser pour la prochaine prÃ©diction
        input_eval = tf.expand_dims([predicted_id], 0)
    
    return start_string + ''.join(generated_text)
```

#### ğŸ“Š RÃ©sultats attendus

**EntrÃ©e :** "To be or not to be"

**Sortie gÃ©nÃ©rÃ©e :** "To be or not to be, that is the question that doth make cowards of us all..."

~85%

Mots valides

~70%

Grammaire correcte

## ğŸ˜Š Projet 2 : Analyse de sentiment

ğŸ’­

### Classificateur de sentiment pour avis produits

DÃ©terminer si un avis est positif ou nÃ©gatif

Facile

#### GRU (RecommandÃ©)

Rapide et efficace pour cette tÃ¢che

#### LSTM

Si vous avez beaucoup de donnÃ©es

#### Bidirectionnel

Pour capturer le contexte complet

```
def create_sentiment_classifier():
    model = tf.keras.Sequential([
        # Embedding pour les mots
        layers.Embedding(vocab_size, 128),
        
        # GRU bidirectionnel pour capturer le contexte
        layers.Bidirectional(layers.GRU(64, return_sequences=True)),
        layers.Dropout(0.5),
        
        # Attention simple pour se concentrer sur les mots importants
        layers.GlobalMaxPooling1D(),
        
        # Classification finale
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')  # 0 = nÃ©gatif, 1 = positif
    ])
    
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Exemple d'utilisation
reviews = [
    "Ce produit est fantastique, je le recommande !",
    "TrÃ¨s dÃ©Ã§u, ne fonctionne pas comme prÃ©vu.",
    "Correct mais sans plus, prix trop Ã©levÃ©."
]

# PrÃ©diction
predictions = model.predict(preprocess_texts(reviews))
for review, pred in zip(reviews, predictions):
    sentiment = "Positif" if pred > 0.5 else "NÃ©gatif"
    print(f"'{review}' â†’ {sentiment} ({pred[0]:.2%})")
```

#### ğŸ’¡ Conseils pour l'analyse de sentiment

*   **PrÃ©traitement :** Gardez la ponctuation ! Elle contient des informations Ã©motionnelles
*   **Ã‰quilibrage :** Assurez-vous d'avoir autant d'exemples positifs que nÃ©gatifs
*   **Augmentation :** Utilisez des synonymes pour enrichir vos donnÃ©es
*   **Validation :** Testez sur des domaines diffÃ©rents (films, produits, restaurants)

## ğŸŒ Projet 3 : Traduction automatique (Seq2Seq)

ğŸ—£ï¸

### Traducteur FranÃ§ais â†’ Anglais

Architecture Encoder-Decoder avec LSTM

AvancÃ©

![Architecture Seq2Seq](https://miro.medium.com/max/1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg)

Architecture Seq2Seq : L'encodeur lit la phrase source, le dÃ©codeur gÃ©nÃ¨re la traduction

```
class Seq2SeqTranslator:
    def __init__(self, src_vocab_size, tgt_vocab_size, latent_dim=256):
        # Encodeur
        encoder_inputs = layers.Input(shape=(None,))
        encoder_embedding = layers.Embedding(src_vocab_size, latent_dim)(encoder_inputs)
        encoder_lstm = layers.LSTM(latent_dim, return_state=True)
        _, state_h, state_c = encoder_lstm(encoder_embedding)
        encoder_states = [state_h, state_c]
        
        # DÃ©codeur
        decoder_inputs = layers.Input(shape=(None,))
        decoder_embedding = layers.Embedding(tgt_vocab_size, latent_dim)
        decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)
        decoder_dense = layers.Dense(tgt_vocab_size, activation='softmax')
        
        # Connecter encodeur et dÃ©codeur
        decoder_embed = decoder_embedding(decoder_inputs)
        decoder_outputs, _, _ = decoder_lstm(decoder_embed, initial_state=encoder_states)
        decoder_outputs = decoder_dense(decoder_outputs)
        
        self.model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Exemple de traduction
translator = Seq2SeqTranslator(fr_vocab_size, en_vocab_size)
french_text = "Bonjour, comment allez-vous ?"
english_translation = translator.translate(french_text)
# RÃ©sultat : "Hello, how are you?"
```

#### ğŸ“Š MÃ©triques de performance typiques

~25

Score BLEU

85%

Phrases courtes correctes

60%

Phrases longues correctes

## ğŸ› ï¸ Conseils pratiques pour tous les projets

#### âš¡ Optimisation des performances

*   **Batch size :** Commencez avec 32, augmentez si votre GPU le permet
*   **Learning rate :** Utilisez un scheduler (ReduceLROnPlateau)
*   **Early stopping :** ArrÃªtez l'entraÃ®nement si pas d'amÃ©lioration aprÃ¨s 5 epochs
*   **Gradient clipping :** Ã‰vitez l'explosion des gradients avec clip\_norm=1.0

#### ğŸ› Debugging courant

*   **Overfitting :** Ajoutez plus de dropout (0.3-0.5)
*   **Underfitting :** Augmentez la taille du modÃ¨le ou les donnÃ©es
*   **Gradients qui disparaissent :** Utilisez LSTM/GRU au lieu de RNN vanilla
*   **MÃ©moire insuffisante :** RÃ©duisez la longueur des sÃ©quences ou le batch size

## ğŸ“š Ressources pour aller plus loin

ğŸ“–

#### Documentation

TensorFlow Text Tutorials

PyTorch RNN Examples

ğŸ“

#### Cours avancÃ©s

Attention Mechanisms

Transformer Architecture

ğŸ’»

#### Datasets

IMDB Reviews

Multi30k Translation

ğŸ”§

#### Outils

TensorBoard

Weights & Biases

## ğŸ¯ Projet final : Votre propre application

ğŸ†

### Challenge : CrÃ©ez votre propre projet RNN

Mettez en pratique tout ce que vous avez appris !

#### IdÃ©es de projets :

*   ğŸ“° **GÃ©nÃ©rateur de titres d'articles** - CrÃ©ez des titres accrocheurs
*   ğŸµ **GÃ©nÃ©rateur de paroles de chansons** - Dans le style de votre artiste prÃ©fÃ©rÃ©
*   ğŸ“§ **Classificateur de spam** - Filtrez les emails indÃ©sirables
*   ğŸ¤– **Chatbot simple** - RÃ©pondez Ã  des questions basiques
*   ğŸ“ **Auto-complÃ©tion de code** - PrÃ©disez la suite du code Python

## ğŸ“ RÃ©sumÃ© du module

#### ğŸ‰ FÃ©licitations ! Vous maÃ®trisez maintenant :

*   âœ… Les concepts fondamentaux des RNN
*   âœ… L'architecture et le fonctionnement des LSTM
*   âœ… Les avantages des GRU
*   âœ… L'implÃ©mentation pratique pour diverses applications NLP
*   âœ… Les bonnes pratiques et l'optimisation

ğŸš€ Prochaine Ã©tape : Les Transformers et l'attention mechanism !

[â† LeÃ§on 3 : GRU](module5_lesson3.html) [Module 6 : Transformers â†’](../module6/index.html)
