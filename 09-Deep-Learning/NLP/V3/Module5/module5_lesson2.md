---
title: 'Module 5 - LeÃ§on 2 : Architecture LSTM'
description: 'Formation NLP - Module 5 - LeÃ§on 2 : Architecture LSTM'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸš€ LeÃ§on 2 : LSTM - Long Short-Term Memory

Si les RNN sont comme une personne avec une mÃ©moire Ã  court terme, les LSTM sont comme une personne avec un **carnet de notes** ! Ils peuvent dÃ©cider quoi Ã©crire, quoi effacer et quoi lire.

## ğŸ¤” Pourquoi les LSTM ?

### Le problÃ¨me des RNN classiques

Rappelez-vous : les RNN simples ont du mal Ã  se souvenir d'informations sur de longues sÃ©quences. C'est le problÃ¨me du gradient qui disparaÃ®t.

#### Exemple du problÃ¨me :

**Texte :** "Alice, qui habite Ã  Paris et travaille comme mÃ©decin depuis 10 ans, *\[30 mots...\]* a dÃ©cidÃ© de dÃ©mÃ©nager."

Un RNN simple pourrait oublier qu'on parle d'Alice et de Paris !

**RNN classique :** Comme essayer de retenir un numÃ©ro de tÃ©lÃ©phone en faisant autre chose - vous l'oubliez rapidement !

**LSTM :** Comme Ã©crire le numÃ©ro sur un papier - vous pouvez le consulter quand vous en avez besoin !

## ğŸ—ï¸ L'architecture LSTM expliquÃ©e simplement

### L'idÃ©e gÃ©niale des LSTM

Les LSTM utilisent un systÃ¨me de **portes** (gates) qui contrÃ´lent le flux d'information. Imaginez un chÃ¢teau avec plusieurs portes : certaines laissent entrer des informations, d'autres les laissent sortir, et d'autres dÃ©cident ce qu'on garde en mÃ©moire.

![Architecture complÃ¨te LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

Architecture complÃ¨te d'un LSTM - Regardons chaque Ã©lÃ©ment en dÃ©tail !

## ğŸ” DÃ©cryptage complet du diagramme LSTM

### ğŸ—ºï¸ Guide de lecture du schÃ©ma

Le diagramme peut sembler complexe, mais chaque symbole a une signification prÃ©cise :

**ğŸŸ¡ Couches neuronales**  
Les rectangles jaunes = opÃ©rations mathÃ©matiques (sigmoid, tanh)

**ğŸ”´ OpÃ©rations pointwise**  
Les cercles roses = multiplication ou addition Ã©lÃ©ment par Ã©lÃ©ment

**â¡ï¸ Flux de vecteurs**  
Les flÃ¨ches = transfert d'information entre composants

**ğŸ“‹ ConcatÃ©nation**  
Les jonctions = fusion de plusieurs vecteurs en un seul

### ğŸ¯ Les deux "autoroutes" de l'information

**ğŸ›£ï¸ Cell State (C\_t) - L'autoroute principale :**

Cette ligne horizontale en haut traverse tout le LSTM. C'est la mÃ©moire Ã  long terme. L'information peut circuler facilement avec trÃ¨s peu de modifications.

**ğŸš— Hidden State (h\_t) - La route locale :**

Cette ligne en bas contient l'information immÃ©diatement utile. C'est ce qui est "sorti" Ã  chaque Ã©tape.

![LSTM Forget Gate dÃ©taillÃ©](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

Zoom sur la porte d'oubli - Elle regarde h\_{t-1} et x\_t pour dÃ©cider quoi oublier

## ğŸ” Analyse Ã©tape par Ã©tape du fonctionnement

1ï¸âƒ£

### Ã‰tape 1 : Porte d'oubli (Forget Gate)

f\_t = Ïƒ(W\_f Â· \[h\_{t-1}, x\_t\] + b\_f)

**Ce qui se passe :**

*   ğŸ“¥ **EntrÃ©es :** Ã‰tat prÃ©cÃ©dent h\_{t-1} + nouvelle entrÃ©e x\_t
*   ğŸ”„ **Processus :** ConcatÃ©nation â†’ Multiplication par poids W\_f â†’ Fonction sigmoid
*   ğŸ“¤ **Sortie :** Valeurs entre 0 et 1 (0 = oublier complÃ¨tement, 1 = garder complÃ¨tement)
*   ğŸ¯ **RÃ©sultat :** Ces valeurs sont multipliÃ©es avec l'ancien Cell State

#### Exemple concret :

**Texte :** "Le chat Ã©tait noir. Le chien..."

â†’ Quand on arrive Ã  "chien", la porte d'oubli pourrait dÃ©cider d'oublier partiellement les infos sur le chat (couleur, etc.)

![LSTM Input Gate dÃ©taillÃ©](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)

Porte d'entrÃ©e - Elle crÃ©e de nouvelles informations candidates et dÃ©cide lesquelles ajouter

2ï¸âƒ£

### Ã‰tape 2 : Porte d'entrÃ©e + Candidats (Input Gate)

**DÃ©cision d'ajout :**  
i\_t = Ïƒ(W\_i Â· \[h\_{t-1}, x\_t\] + b\_i)

**Nouvelles infos :**  
CÌƒ\_t = tanh(W\_C Â· \[h\_{t-1}, x\_t\] + b\_C)

**Double processus :**

1.  **ğŸšª Input Gate (i\_t) :** Utilise sigmoid â†’ dÃ©cide *quelles* parties mettre Ã  jour
2.  **ğŸ“ Candidats (CÌƒ\_t) :** Utilise tanh â†’ crÃ©e *les nouvelles valeurs* possibles
3.  **ğŸ¤ Combinaison :** i\_t Ã— CÌƒ\_t = les nouvelles infos qui seront vraiment ajoutÃ©es

#### Pourquoi deux fonctions ?

**Sigmoid (0 Ã  1) :** "Ã€ quel point cette info est-elle importante ?"

**Tanh (-1 Ã  1) :** "Quelle est la valeur de cette nouvelle information ?"

â†’ SÃ©paration entre *l'importance* et *la valeur* !

![LSTM Cell State Update](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)

Mise Ã  jour du Cell State - L'ancien state est filtrÃ© puis enrichi de nouvelles informations

3ï¸âƒ£

### Ã‰tape 3 : Mise Ã  jour du Cell State

C\_t = f\_t Ã— C\_{t-1} + i\_t Ã— CÌƒ\_t

**La formule magique :**

ğŸ§® **Partie 1 :** f\_t Ã— C\_{t-1} = "Ancien Cell State filtrÃ© par la porte d'oubli"

â• **Plus :**

ğŸ§® **Partie 2 :** i\_t Ã— CÌƒ\_t = "Nouvelles informations filtrÃ©es par la porte d'entrÃ©e"

\= Nouveau Cell State qui combine ancien (filtrÃ©) + nouveau (sÃ©lectionnÃ©)

**Analogie :** Imaginez votre bureau :

*   ğŸ“„ f\_t Ã— C\_{t-1} = Garder certains vieux documents (les importants)
*   ğŸ“„ i\_t Ã— CÌƒ\_t = Ajouter de nouveaux documents (les pertinents)
*   ğŸ—‚ï¸ C\_t = Votre bureau mis Ã  jour avec l'essentiel de l'ancien + le pertinent du nouveau

![LSTM Output Gate dÃ©taillÃ©](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)

Porte de sortie - Elle filtre le Cell State pour ne donner que l'information pertinente maintenant

4ï¸âƒ£

### Ã‰tape 4 : Porte de sortie (Output Gate)

**DÃ©cision de sortie :**  
o\_t = Ïƒ(W\_o Â· \[h\_{t-1}, x\_t\] + b\_o)

**Hidden State :**  
h\_t = o\_t Ã— tanh(C\_t)

**Processus final :**

1.  **ğŸšª Output Gate (o\_t) :** Regarde le contexte actuel, dÃ©cide quoi montrer
2.  **ğŸ­ tanh(C\_t) :** "Normalise" le Cell State entre -1 et 1
3.  **ğŸ¯ h\_t :** Le rÃ©sultat final = Cell State filtrÃ© et adaptÃ© au contexte

#### Pourquoi tanh(C\_t) ?

Le Cell State peut contenir des valeurs trÃ¨s grandes ou trÃ¨s petites. tanh() les "normalise" pour que le rÃ©sultat soit utilisable par les couches suivantes.

**Analogie :** C'est comme ajuster le volume de votre musique selon le contexte (bureau vs fÃªte) !

## ğŸšª Les 3 portes magiques du LSTM

ğŸ—‘ï¸

### Porte d'oubli (Forget Gate)

DÃ©cide quelles informations oublier

Cette porte regarde l'Ã©tat prÃ©cÃ©dent et l'entrÃ©e actuelle, puis dÃ©cide quelles informations ne sont plus utiles.

#### Exemple :

**Texte :** "Le chat Ã©tait sur le tapis. Le chien..."

â†’ On peut "oublier" certaines infos sur le chat car on parle maintenant du chien.

â•

### Porte d'entrÃ©e (Input Gate)

DÃ©cide quelles nouvelles informations stocker

Cette porte dÃ©termine quelles nouvelles informations sont importantes et doivent Ãªtre ajoutÃ©es Ã  la mÃ©moire.

#### Exemple :

**Texte :** "Marie est **mÃ©decin**."

â†’ L'information "mÃ©decin" est importante et sera stockÃ©e pour comprendre la suite.

ğŸ“¤

### Porte de sortie (Output Gate)

DÃ©cide quelles informations utiliser maintenant

Cette porte filtre la mÃ©moire pour ne donner que les informations pertinentes pour la tÃ¢che actuelle.

#### Exemple :

**Contexte mÃ©morisÃ© :** "Marie, mÃ©decin, Paris, 35 ans"

**Phrase actuelle :** "Elle soigne..."

â†’ La porte de sortie active l'info "mÃ©decin" car c'est pertinent ici.

## ğŸ”„ Le flux d'information dans un LSTM

1

**Ã‰tat prÃ©cÃ©dent**  
\+ Nouvelle entrÃ©e

â†’

2

**Forget Gate**  
Oublier l'inutile

â†’

3

**Input Gate**  
Ajouter le nouveau

â†’

4

**Output Gate**  
Filtrer la sortie

![LSTM Forget Gate](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

Zoom sur la porte d'oubli - Elle utilise une fonction sigmoÃ¯de (0 = tout oublier, 1 = tout garder)

## ğŸ’¡ Comprendre avec une analogie complÃ¨te

### L'LSTM comme un Ã©tudiant qui prend des notes

Imaginez un Ã©tudiant en cours :

*   **ğŸ—‘ï¸ Forget Gate :** Il efface les infos du tableau prÃ©cÃ©dent qui ne sont plus utiles
*   **â• Input Gate :** Il dÃ©cide quelles nouvelles informations noter dans son cahier
*   **ğŸ“¤ Output Gate :** Il choisit quelles notes consulter pour rÃ©pondre Ã  une question
*   **ğŸ“ Cell State (Ã‰tat de cellule) :** Son cahier de notes complet
*   **ğŸ‘ï¸ Hidden State (Ã‰tat cachÃ©) :** Ce qu'il a actuellement en tÃªte

## âš–ï¸ Avantages et inconvÃ©nients des LSTM

Avantages âœ…

InconvÃ©nients âŒ

Excellente mÃ©moire Ã  long terme

Plus complexe et lent Ã  entraÃ®ner

RÃ©sout le problÃ¨me du gradient qui disparaÃ®t

NÃ©cessite plus de mÃ©moire (RAM)

TrÃ¨s efficace pour les longues sÃ©quences

Plus de paramÃ¨tres Ã  optimiser

Flexible et adaptatif

Peut Ãªtre "overkill" pour des tÃ¢ches simples

## ğŸ’» ImplÃ©mentation simplifiÃ©e

```
# Pseudo-code conceptuel d'un LSTM
class LSTM:
    def process_sequence(self, sequence):
        cell_state = 0  # MÃ©moire Ã  long terme
        hidden_state = 0  # MÃ©moire Ã  court terme
        
        for word in sequence:
            # 1. Forget Gate : dÃ©cider quoi oublier
            forget = sigmoid(word + hidden_state)
            cell_state = cell_state * forget
            
            # 2. Input Gate : dÃ©cider quoi ajouter
            input_gate = sigmoid(word + hidden_state)
            new_info = tanh(word + hidden_state)
            cell_state = cell_state + (input_gate * new_info)
            
            # 3. Output Gate : dÃ©cider quoi sortir
            output_gate = sigmoid(word + hidden_state)
            hidden_state = output_gate * tanh(cell_state)
            
        return hidden_state
```

## ğŸ¯ Applications pratiques des LSTM

### OÃ¹ les LSTM excellent particuliÃ¨rement

*   **ğŸˆ‚ï¸ Traduction automatique :** Garder le contexte de phrases longues
*   **ğŸµ GÃ©nÃ©ration de musique :** Se souvenir des motifs musicaux
*   **ğŸ’¬ Chatbots :** Maintenir le contexte d'une conversation
*   **ğŸ“ RÃ©sumÃ© de texte :** Identifier les informations importantes sur de longs documents
*   **ğŸ—£ï¸ Reconnaissance vocale :** Comprendre des phrases complÃ¨tes

### Conseil pratique

Utilisez les LSTM quand :

*   âœ“ Vos sÃ©quences sont longues (> 100 Ã©lÃ©ments)
*   âœ“ Les dÃ©pendances Ã  long terme sont importantes
*   âœ“ La performance prime sur la vitesse

PrÃ©fÃ©rez des architectures plus simples (RNN vanilla ou GRU) pour des tÃ¢ches plus basiques.

## ğŸ“ RÃ©sumÃ© de la leÃ§on

### Points clÃ©s Ã  retenir :

*   âœ… Les LSTM rÃ©solvent le problÃ¨me de mÃ©moire Ã  court terme des RNN
*   âœ… Ils utilisent 3 portes : Forget, Input et Output
*   âœ… Le Cell State agit comme une "autoroute" pour l'information
*   âœ… Parfaits pour les tÃ¢ches nÃ©cessitant une mÃ©moire Ã  long terme
*   âœ… Plus complexes mais plus puissants que les RNN simples

[â† LeÃ§on 1 : Introduction RNN](module5_lesson1.html) [LeÃ§on 3 : GRU â†’](module5_lesson3.html)
