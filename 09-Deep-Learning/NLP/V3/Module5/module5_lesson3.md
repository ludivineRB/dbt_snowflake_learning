---
title: 'Module 5 - LeÃ§on 3 : GRU et Comparaisons'
description: 'Formation NLP - Module 5 - LeÃ§on 3 : GRU et Comparaisons'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# âš¡ LeÃ§on 3 : GRU et Comparaisons des Architectures

Si les LSTM sont comme un chÃ¢teau avec plusieurs portes complexes, les GRU sont comme une maison moderne avec moins de portes mais tout aussi efficace ! DÃ©couvrons cette architecture Ã©lÃ©gante et comparons toutes nos options.

## ðŸŽ¯ Qu'est-ce qu'un GRU ?

### GRU : Gated Recurrent Unit

Le GRU est une version simplifiÃ©e du LSTM crÃ©Ã©e en 2014. Il accomplit essentiellement la mÃªme tÃ¢che (garder une mÃ©moire Ã  long terme) mais avec moins de complexitÃ©.

**LSTM :** Comme un smartphone avec plein de fonctionnalitÃ©s

**GRU :** Comme un tÃ©lÃ©phone bien conÃ§u avec juste les fonctions essentielles

â†’ Plus simple, plus rapide, mais tout aussi efficace dans la plupart des cas !

## ðŸšª Les 2 portes du GRU

#### 

ðŸ”„

Update Gate (Porte de mise Ã  jour)

DÃ©cide **combien** d'information du passÃ© garder vs combien de nouvelle information accepter.

Combine les rÃ´les des portes Input et Forget du LSTM !

#### 

ðŸŽ¯

Reset Gate (Porte de rÃ©initialisation)

DÃ©cide **quelles** parties du passÃ© sont pertinentes pour calculer la nouvelle information candidate.

Permet d'oublier sÃ©lectivement certains aspects.

![Architecture GRU](https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png)

Architecture du GRU - Plus simple que le LSTM avec seulement 2 portes

## ðŸ†š Comparaison dÃ©taillÃ©e : RNN vs LSTM vs GRU

#### ðŸ”µ RNN Vanilla

**ComplexitÃ© :** â­

**MÃ©moire :** Court terme uniquement

**Portes :** Aucune

**ParamÃ¨tres :** Peu

Simple mais limitÃ©

#### ðŸ”´ LSTM

**ComplexitÃ© :** â­â­â­

**MÃ©moire :** Long terme excellent

**Portes :** 3 (Forget, Input, Output)

**ParamÃ¨tres :** Beaucoup

Puissant mais complexe

#### ðŸŸ¢ GRU

**ComplexitÃ© :** â­â­

**MÃ©moire :** Long terme trÃ¨s bon

**Portes :** 2 (Update, Reset)

**ParamÃ¨tres :** Moyennement

Ã‰quilibre optimal

## ðŸ“Š Tableau comparatif dÃ©taillÃ©

CritÃ¨re

RNN Vanilla

LSTM

GRU

**AnnÃ©e d'invention**

1986

1997

2014

**Nombre de portes**

0

3

2

**Vitesse d'entraÃ®nement**

TrÃ¨s rapide

Lente

Rapide

**MÃ©moire requise**

Faible

Ã‰levÃ©e

Moyenne

**Performance (longues sÃ©quences)**

Mauvaise

Excellente

TrÃ¨s bonne

**ComplexitÃ© d'implÃ©mentation**

Simple

Complexe

Moyenne

## ðŸ¤” LSTM vs GRU : Le match en dÃ©tail

#### LSTM - Forces et Faiblesses

**âœ… Avantages**

*   Meilleure sur trÃ¨s longues sÃ©quences
*   Plus de contrÃ´le fin
*   Ã‰tat de cellule sÃ©parÃ©

**âŒ InconvÃ©nients**

*   Plus lent Ã  entraÃ®ner
*   Plus de paramÃ¨tres
*   Sur-apprentissage possible

#### GRU - Forces et Faiblesses

**âœ… Avantages**

*   Plus rapide Ã  entraÃ®ner
*   Moins de paramÃ¨tres
*   Souvent aussi efficace

**âŒ InconvÃ©nients**

*   Moins de flexibilitÃ©
*   Pas d'Ã©tat de cellule
*   Peut Ãªtre moins bon sur certaines tÃ¢ches

## ðŸŽ¯ Guide de dÃ©cision : Quelle architecture choisir ?

**Votre sÃ©quence fait moins de 50 Ã©lÃ©ments ?**

â†“

**OUI â†’ ConsidÃ©rez un RNN simple**  
Rapide et suffisant pour des tÃ¢ches simples

â†“ NON

**Avez-vous beaucoup de donnÃ©es d'entraÃ®nement ?**

â†“

**OUI + Besoin max performance â†’ LSTM**  
Exploite au mieux les grandes quantitÃ©s de donnÃ©es

â†“ NON

**GRU est votre meilleur choix !**  
Bon Ã©quilibre performance/complexitÃ©

## ðŸ’» Comparaison pratique du code

### Nombre de paramÃ¨tres Ã  apprendre

**RNN :** 3 Ã— (taille\_entrÃ©e + taille\_cachÃ©e) Ã— taille\_cachÃ©e

**GRU :** 3 Ã— (taille\_entrÃ©e + taille\_cachÃ©e) Ã— taille\_cachÃ©e

**LSTM :** 4 Ã— (taille\_entrÃ©e + taille\_cachÃ©e) Ã— taille\_cachÃ©e

â†’ LSTM a 33% de paramÃ¨tres en plus !

## ðŸ“ˆ Performances en pratique

![Comparaison performances](https://miro.medium.com/max/1400/1*yBXV9o5q7L_CvY7quJt3WQ.png)

Comparaison typique des performances - GRU souvent proche de LSTM avec moins de complexitÃ©

### Conseil d'expert

**Commencez toujours par un GRU !**

*   Si les performances sont insuffisantes â†’ essayez LSTM
*   Si c'est trop lent â†’ revenez au RNN simple
*   Si vous avez des contraintes mÃ©moire â†’ GRU ou RNN

Dans 90% des cas, le GRU sera le meilleur compromis.

## ðŸš€ Applications spÃ©cifiques

Application

Architecture recommandÃ©e

Pourquoi ?

Classification de tweets

GRU

Textes courts, besoin de rapiditÃ©

Traduction de documents

LSTM

Longues dÃ©pendances importantes

PrÃ©diction mot suivant

RNN simple

Contexte local suffisant

Analyse de sentiment (reviews)

GRU

Bon Ã©quilibre longueur/performance

GÃ©nÃ©ration de musique

LSTM

Patterns complexes sur longue durÃ©e

## ðŸ“ RÃ©sumÃ© de la leÃ§on

### Points clÃ©s Ã  retenir :

*   âœ… **GRU** = Version simplifiÃ©e et efficace du LSTM (2 portes au lieu de 3)
*   âœ… **Performance** : LSTM â‰¥ GRU >> RNN simple (dans la plupart des cas)
*   âœ… **Vitesse** : RNN > GRU > LSTM
*   âœ… **ComplexitÃ©** : RNN < GRU < LSTM
*   âœ… **RÃ¨gle d'or** : Commencez par GRU, ajustez selon vos besoins

[â† LeÃ§on 2 : LSTM](module5_lesson2.html) [LeÃ§on 4 : Applications â†’](module5_lesson4.html)
