---
title: 'Module 5 - LeÃ§on 1 : Introduction aux RNN'
description: 'Formation NLP - Module 5 - LeÃ§on 1 : Introduction aux RNN'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ§  LeÃ§on 1 : Introduction aux RÃ©seaux de Neurones RÃ©currents (RNN)

Imaginez que vous lisez un livre. Pour comprendre chaque phrase, vous vous souvenez de ce qui s'est passÃ© avant. C'est exactement ce que font les RNN : ils possÃ¨dent une **mÃ©moire** qui leur permet de comprendre les sÃ©quences !

## ğŸ“š Pourquoi avons-nous besoin des RNN ?

### Le problÃ¨me des rÃ©seaux de neurones classiques

Les rÃ©seaux de neurones traditionnels (comme ceux que vous avez vus dans les modules prÃ©cÃ©dents) traitent chaque entrÃ©e indÃ©pendamment. Ils n'ont aucune notion de ce qui s'est passÃ© avant.

#### Exemple concret :

**Phrase 1 :** "Le chat est sur le..."

**Phrase 2 :** "J'ai oubliÃ© mes clÃ©s sur le..."

Un humain devine facilement : "tapis" pour la premiÃ¨re, "bureau" pour la seconde. Mais un rÃ©seau classique ne peut pas utiliser le contexte prÃ©cÃ©dent pour prÃ©dire !

![Architecture RNN enroulÃ©e](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png)

Vue simplifiÃ©e d'un RNN - La boucle reprÃ©sente la connexion rÃ©currente

## ğŸ”„ Comment fonctionnent les RNN ?

### L'idÃ©e clÃ© : La mÃ©moire

Un RNN est comme un rÃ©seau de neurones avec une **mÃ©moire Ã  court terme**. Ã€ chaque Ã©tape, il :

*   ğŸ”¸ ReÃ§oit une nouvelle entrÃ©e (ex: un mot)
*   ğŸ”¸ Combine cette entrÃ©e avec sa mÃ©moire prÃ©cÃ©dente
*   ğŸ”¸ Produit une sortie
*   ğŸ”¸ Met Ã  jour sa mÃ©moire pour l'Ã©tape suivante

![Architecture RNN dÃ©roulÃ©e](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)

RNN "dÃ©roulÃ©" dans le temps - Chaque Ã©tape utilise la mÃ©moire de l'Ã©tape prÃ©cÃ©dente

### Point clÃ© Ã  retenir

Un RNN est le **mÃªme rÃ©seau** utilisÃ© plusieurs fois de suite. C'est comme si vous aviez un seul cerveau qui traite les mots un par un, en gardant en mÃ©moire ce qu'il a vu avant.

## ğŸ¯ Applications concrÃ¨tes des RNN

Application

Description

Exemple concret

**Traduction automatique**

Traduire une phrase en gardant le contexte

"I love you" â†’ "Je t'aime" (pas "J'aime tu")

**Analyse de sentiment**

Comprendre l'Ã©motion d'un texte complet

"Le film Ã©tait long mais finalement gÃ©nial !" â†’ Positif

**GÃ©nÃ©ration de texte**

Ã‰crire du texte cohÃ©rent mot par mot

ComplÃ©ter "Il Ã©tait une fois..." â†’ "...un prince dans un chÃ¢teau"

**Reconnaissance vocale**

Transcrire la parole en tenant compte du contexte

Distinguer "verre" de "vers" selon le contexte

## ğŸ’» Architecture technique simplifiÃ©e

```
# Pseudo-code simplifiÃ© d'un RNN
pour chaque mot dans la phrase:
    Ã©tat_cachÃ© = fonction(mot_actuel, Ã©tat_cachÃ©_prÃ©cÃ©dent)
    sortie = fonction(Ã©tat_cachÃ©)
    
# Ã©tat_cachÃ© = la "mÃ©moire" du rÃ©seau
# Il contient les informations importantes des mots prÃ©cÃ©dents
```

### Les composants essentiels

*   **Input (EntrÃ©e) :** Le mot actuel (souvent sous forme de vecteur)
*   **Hidden State (Ã‰tat cachÃ©) :** La mÃ©moire du rÃ©seau
*   **Output (Sortie) :** La prÃ©diction ou reprÃ©sentation actuelle
*   **Poids partagÃ©s :** Les mÃªmes paramÃ¨tres sont utilisÃ©s Ã  chaque Ã©tape

## âš ï¸ Les limitations des RNN simples

### Le problÃ¨me de la mÃ©moire Ã  court terme

Les RNN "vanilla" (basiques) ont tendance Ã  oublier les informations anciennes. C'est comme essayer de se souvenir du dÃ©but d'un livre en arrivant Ã  la fin !

#### Exemple du problÃ¨me :

**Phrase longue :** "Le chat *\[50 mots au milieu\]* Ã©tait noir."

Un RNN simple pourrait oublier que nous parlons d'un chat !

![ProblÃ¨me du gradient qui disparaÃ®t](https://miro.medium.com/max/1400/1*AKpT7aXLCCNSB5nqTqFfmg.png)

Le problÃ¨me du "vanishing gradient" - Les informations anciennes s'estompent

## ğŸš€ Vers des architectures plus avancÃ©es

### La solution : LSTM et GRU

Pour rÃ©soudre ces problÃ¨mes, des chercheurs ont inventÃ© des architectures plus sophistiquÃ©es :

*   **LSTM (Long Short-Term Memory) :** Comme un RNN avec une mÃ©moire Ã  long terme
*   **GRU (Gated Recurrent Unit) :** Une version simplifiÃ©e mais efficace du LSTM

Nous les Ã©tudierons en dÃ©tail dans la prochaine leÃ§on !

#### ğŸ¯ Quiz rapide - Testez votre comprÃ©hension

**Question 1 :** Quelle est la principale diffÃ©rence entre un rÃ©seau de neurones classique et un RNN ? Voir la rÃ©ponse

Un RNN possÃ¨de une **mÃ©moire** qui lui permet de se souvenir des entrÃ©es prÃ©cÃ©dentes, contrairement Ã  un rÃ©seau classique qui traite chaque entrÃ©e indÃ©pendamment.

**Question 2 :** Pourquoi dit-on que les RNN partagent leurs poids ? Voir la rÃ©ponse

Parce que c'est le **mÃªme rÃ©seau** (avec les mÃªmes paramÃ¨tres) qui est utilisÃ© Ã  chaque Ã©tape temporelle. Il n'y a pas un rÃ©seau diffÃ©rent pour chaque mot !

## ğŸ“ RÃ©sumÃ© de la leÃ§on

### Ce qu'il faut retenir :

*   âœ… Les RNN sont conÃ§us pour traiter des **sÃ©quences** (texte, audio, etc.)
*   âœ… Ils possÃ¨dent une **mÃ©moire** qui conserve les informations passÃ©es
*   âœ… Le mÃªme rÃ©seau est utilisÃ© Ã  chaque Ã©tape (poids partagÃ©s)
*   âœ… Ils sont parfaits pour les tÃ¢ches nÃ©cessitant du contexte
*   âœ… Les RNN simples ont des limitations (mÃ©moire courte)

[â† Retour au Module 5](index.html) [LeÃ§on 2 : LSTM â†’](module5_lesson2.html)
