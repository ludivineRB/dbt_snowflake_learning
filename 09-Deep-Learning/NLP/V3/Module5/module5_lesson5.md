---
title: 'Module 5 - LeÃ§on 5 : Bonnes Pratiques et Debugging'
description: 'Formation NLP - Module 5 - LeÃ§on 5 : Bonnes Pratiques et Debugging'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ› ï¸ LeÃ§on 5 : Bonnes Pratiques et Debugging RNN

MÃªme les meilleurs modÃ¨les peuvent mal fonctionner ! Apprenez Ã  diagnostiquer et rÃ©soudre les problÃ¨mes courants avec les RNN : overfitting, underfitting, gradients qui explosent, et bien plus.

## ğŸ¯ Les 3 problÃ¨mes principaux et leurs solutions

#### ğŸš¨ Overfitting

**SymptÃ´mes :**

*   Loss d'entraÃ®nement â†“
*   Loss de validation â†‘
*   Ã‰cart grandissant
*   Accuracy train >> val

#### ğŸ’Š Solutions

*   **Dropout :** 0.2-0.5 sur les couches LSTM
*   **Regularization :** L1/L2 sur les poids
*   **Early stopping :** Patience de 5-10 epochs
*   **Plus de donnÃ©es :** Augmentation
*   **ModÃ¨le plus simple :** Moins de couches

#### ğŸŒ Underfitting

**SymptÃ´mes :**

*   Loss train et val hautes
*   Pas d'amÃ©lioration
*   Predictions alÃ©atoires
*   Courbe plate

#### ğŸš€ Solutions

*   **ModÃ¨le plus complexe :** Plus de neurones/couches
*   **Learning rate :** Augmenter lÃ©gÃ¨rement
*   **Features :** AmÃ©liorer le preprocessing
*   **Architecture :** LSTM au lieu de RNN
*   **EntraÃ®nement :** Plus d'epochs

#### ğŸ’¥ Gradient Problems

**SymptÃ´mes :**

*   Loss â†’ NaN ou Inf
*   Gradients trÃ¨s grands
*   InstabilitÃ© d'entraÃ®nement
*   Convergence impossible

#### ğŸ¯ Solutions

*   **Gradient clipping :** clip\_norm=1.0
*   **Learning rate :** Diminuer Ã  0.001
*   **Batch normalization :** Stabiliser
*   **LSTM/GRU :** Remplacer RNN vanilla
*   **Initialisation :** Xavier/He uniform

## ğŸ“Š Dashboard de monitoring

![Courbes d'apprentissage typiques](https://miro.medium.com/max/1400/1*a61lM8xKF6aF5z3HSpnMHg.png)

Courbes d'apprentissage : Normal vs Overfitting vs Underfitting

Train Loss

0.45

â†“ Diminue bien

Val Loss

0.52

â†— LÃ©gÃ¨re augmentation

Gap Train/Val

0.07

âœ“ Acceptable

Gradient Norm

2.1

âœ“ Stable

## ğŸ”§ Configuration optimale selon le cas

ParamÃ¨tre

Dataset petit (<1M)

Dataset moyen (1-10M)

Dataset large (>10M)

**Architecture**

GRU simple

LSTM 1-2 couches

LSTM bidirectionnel

**Hidden Units**

64-128

256-512

512-1024

**Dropout**

0.3-0.5

0.2-0.4

0.1-0.3

**Learning Rate**

0.01

0.001

0.0001

**Batch Size**

16-32

32-64

64-128

**SÃ©quence Max**

50-100

100-200

200-500

## ğŸ’» Code pour le monitoring et debugging

```
import tensorflow as tf
from tensorflow.keras import callbacks
import matplotlib.pyplot as plt

# Callback personnalisÃ© pour monitorer les gradients
class GradientMonitor(callbacks.Callback):
    def on_batch_end(self, batch, logs=None):
        # Calculer la norme des gradients
        gradients = []
        for layer in self.model.layers:
            if hasattr(layer, 'kernel'):
                grad = tf.keras.backend.gradients(self.model.total_loss, layer.kernel)
                if grad[0] is not None:
                    grad_norm = tf.norm(grad[0])
                    gradients.append(grad_norm)
        
        if gradients:
            avg_grad_norm = tf.reduce_mean(gradients)
            if avg_grad_norm > 10.0:  # Seuil d'alerte
                print(f"âš ï¸ Gradient explosif dÃ©tectÃ©: {avg_grad_norm:.2f}")

# Configuration complÃ¨te avec bonnes pratiques
def create_robust_rnn(vocab_size, max_length, embedding_dim=128):
    model = tf.keras.Sequential([
        # Embedding avec masking pour gÃ©rer les sÃ©quences variables
        tf.keras.layers.Embedding(vocab_size, embedding_dim, 
                                 mask_zero=True, input_length=max_length),
        
        # Dropout sur l'embedding
        tf.keras.layers.Dropout(0.2),
        
        # LSTM avec dropout intÃ©grÃ©
        tf.keras.layers.LSTM(256, 
                           dropout=0.3,          # Dropout sur les entrÃ©es
                           recurrent_dropout=0.3, # Dropout sur les connexions rÃ©currentes
                           return_sequences=False),
        
        # Couche dense avec rÃ©gularisation
        tf.keras.layers.Dense(64, activation='relu',
                            kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        tf.keras.layers.Dropout(0.5),
        
        # Sortie
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    # Optimiseur avec gradient clipping
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# Callbacks essentiels
callbacks_list = [
    # Early stopping avec patience
    tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=7,
        restore_best_weights=True
    ),
    
    # RÃ©duction du learning rate
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7
    ),
    
    # Sauvegarde du meilleur modÃ¨le
    tf.keras.callbacks.ModelCheckpoint(
        'best_model.h5',
        monitor='val_loss',
        save_best_only=True
    ),
    
    # Monitoring des gradients
    GradientMonitor()
]
```

## ğŸ“‹ Checklist de debugging

#### âœ… Avant l'entraÃ®nement

*   VÃ©rifier la forme des donnÃ©es (batch\_size, seq\_length, features)
*   Tester le modÃ¨le sur un petit batch
*   Valider le preprocessing (tokenisation, padding)
*   S'assurer que les labels sont corrects
*   VÃ©rifier l'Ã©quilibrage des classes

#### âš¡ Pendant l'entraÃ®nement

*   Monitorer train/val loss en temps rÃ©el
*   Surveiller la norme des gradients
*   VÃ©rifier l'utilisation mÃ©moire GPU
*   Observer les prÃ©dictions sur quelques exemples
*   Ajuster les hyperparamÃ¨tres si nÃ©cessaire

#### ğŸ¯ AprÃ¨s l'entraÃ®nement

*   Analyser la matrice de confusion
*   Tester sur des exemples hors distribution
*   VÃ©rifier les prÃ©dictions sur des cas limites
*   Ã‰valuer la robustesse aux variations
*   Documenter les rÃ©sultats et insights

## ğŸš¨ Flowchart de rÃ©solution de problÃ¨mes

**Mon modÃ¨le ne converge pas du tout ?**

â†“

1\. RÃ©duire le learning rate (Ã·10)  
2\. VÃ©rifier les donnÃ©es d'entrÃ©e  
3\. Simplifier l'architecture

â†“

**Overfitting (gap train/val > 0.1) ?**

â†“

1\. Augmenter dropout  
2\. Ajouter rÃ©gularisation  
3\. Plus de donnÃ©es / Early stopping

â†“

**Underfitting (loss plateau haut) ?**

â†“

1\. Augmenter la capacitÃ© du modÃ¨le  
2\. RÃ©duire la rÃ©gularisation  
3\. AmÃ©liorer les features

## ğŸ¯ Techniques avancÃ©es

### Augmentation de donnÃ©es pour RNN

```
# Techniques d'augmentation spÃ©cifiques au texte
def augment_text_data(texts, labels):
    augmented_texts = []
    augmented_labels = []
    
    for text, label in zip(texts, labels):
        # Original
        augmented_texts.append(text)
        augmented_labels.append(label)
        
        # Synonyme replacement (using nltk or spacy)
        synonym_text = replace_with_synonyms(text, prob=0.1)
        augmented_texts.append(synonym_text)
        augmented_labels.append(label)
        
        # Random word deletion
        deleted_text = random_word_deletion(text, prob=0.1)
        augmented_texts.append(deleted_text)
        augmented_labels.append(label)
        
        # Word order swap
        swapped_text = random_word_swap(text, n=2)
        augmented_texts.append(swapped_text)
        augmented_labels.append(label)
    
    return augmented_texts, augmented_labels
```

### Learning Rate Scheduling optimal

```
# Warm-up + Cosine Annealing
def create_lr_schedule(initial_lr=0.001, warmup_epochs=5, total_epochs=50):
    def lr_schedule(epoch):
        if epoch < warmup_epochs:
            # Phase de warm-up
            return initial_lr * (epoch + 1) / warmup_epochs
        else:
            # Cosine annealing
            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
            return initial_lr * 0.5 * (1 + np.cos(np.pi * progress))
    
    return tf.keras.callbacks.LearningRateScheduler(lr_schedule)
```

### âš ï¸ Erreurs communes Ã  Ã©viter

*   **Padding sans masking :** Les tokens de padding influencent l'apprentissage
*   **SÃ©quences trop longues :** CoÃ»t computationnel et problÃ¨mes de mÃ©moire
*   **Pas de validation holdout :** Impossible de dÃ©tecter l'overfitting
*   **Learning rate trop Ã©levÃ© :** Le modÃ¨le "saute" par-dessus les minima
*   **Batch size trop petit :** Gradients bruitÃ©s et entraÃ®nement instable
*   **Pas de seed random :** RÃ©sultats non reproductibles

## ğŸ“Š MÃ©triques avancÃ©es pour RNN

```
# MÃ©triques personnalisÃ©es pour mieux Ã©valuer
def perplexity(y_true, y_pred):
    """PerplexitÃ© pour les tÃ¢ches de langage"""
    cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
    return tf.exp(tf.reduce_mean(cross_entropy))

def sequence_accuracy(y_true, y_pred):
    """PrÃ©cision au niveau sÃ©quence complÃ¨te"""
    predicted_sequences = tf.argmax(y_pred, axis=-1)
    correct_sequences = tf.reduce_all(tf.equal(y_true, predicted_sequences), axis=-1)
    return tf.reduce_mean(tf.cast(correct_sequences, tf.float32))

# Compilation avec mÃ©triques avancÃ©es
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy', perplexity, sequence_accuracy]
)
```

## ğŸ“ RÃ©sumÃ© des bonnes pratiques

### ğŸ¯ Points clÃ©s Ã  retenir :

*   âœ… **Monitoring :** Surveillez train/val loss en permanence
*   âœ… **RÃ©gularisation :** Dropout + Early stopping sont essentiels
*   âœ… **Gradients :** Utilisez gradient clipping avec LSTM/GRU
*   âœ… **Learning rate :** Commencez petit et utilisez des schedulers
*   âœ… **Architecture :** Commencez simple, complexifiez si nÃ©cessaire
*   âœ… **DonnÃ©es :** QualitÃ© > QuantitÃ©, Ã©quilibrage important
*   âœ… **ReproductibilitÃ© :** Fixez les seeds pour comparer les expÃ©riences

[â† LeÃ§on 4 : Applications](module5_lesson4.html) [Module 6 : Transformers â†’](../module6/index.html)
