{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Basiques avec TensorFlow\n",
    "\n",
    "Ce notebook explore les concepts fondamentaux des Réseaux de Neurones Récurrents (RNN) en utilisant TensorFlow et Keras.\n",
    "\n",
    "## Objectifs du notebook\n",
    "\n",
    "1. Comprendre l'architecture des RNN\n",
    "2. Implémenter un RNN simple avec TensorFlow/Keras\n",
    "3. Explorer les différents types d'architectures RNN\n",
    "4. Entraîner un RNN pour l'analyse de sentiment\n",
    "5. Visualiser le fonctionnement interne d'un RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances\n",
    "!pip install tensorflow numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Vérifier la version de TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprendre les RNN : Théorie et Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du concept de RNN\n",
    "def visualize_rnn_concept():\n",
    "    \"\"\"\n",
    "    Visualise le déroulement temporel d'un RNN.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # RNN replié\n",
    "    ax1.set_title(\"RNN Replié\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Dessiner la cellule RNN\n",
    "    circle = plt.Circle((0.5, 0.5), 0.3, color='lightblue', ec='darkblue', linewidth=2)\n",
    "    ax1.add_patch(circle)\n",
    "    ax1.text(0.5, 0.5, 'RNN', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Flèches\n",
    "    ax1.arrow(0.5, 0.9, 0, -0.08, head_width=0.05, head_length=0.02, fc='green', ec='green')\n",
    "    ax1.text(0.5, 0.95, 'x_t', ha='center', fontsize=12, color='green')\n",
    "    \n",
    "    ax1.arrow(0.5, 0.18, 0, -0.08, head_width=0.05, head_length=0.02, fc='red', ec='red')\n",
    "    ax1.text(0.5, 0.05, 'h_t', ha='center', fontsize=12, color='red')\n",
    "    \n",
    "    # Boucle récurrente\n",
    "    ax1.annotate('', xy=(0.2, 0.5), xytext=(0.8, 0.5),\n",
    "                arrowprops=dict(arrowstyle='<->', connectionstyle='arc3,rad=.5', \n",
    "                              color='purple', linewidth=2))\n",
    "    ax1.text(0.5, 0.3, 'h_{t-1}', ha='center', fontsize=12, color='purple')\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # RNN déroulé\n",
    "    ax2.set_title(\"RNN Déroulé dans le Temps\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Dessiner plusieurs cellules RNN\n",
    "    positions = [0.2, 0.4, 0.6, 0.8]\n",
    "    times = ['t-1', 't', 't+1', 't+2']\n",
    "    \n",
    "    for i, (pos, time) in enumerate(zip(positions, times)):\n",
    "        # Cellule\n",
    "        circle = plt.Circle((pos, 0.5), 0.08, color='lightblue', ec='darkblue', linewidth=2)\n",
    "        ax2.add_patch(circle)\n",
    "        ax2.text(pos, 0.5, 'RNN', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Input\n",
    "        ax2.arrow(pos, 0.7, 0, -0.08, head_width=0.02, head_length=0.02, fc='green', ec='green')\n",
    "        ax2.text(pos, 0.75, f'x_{time}', ha='center', fontsize=10, color='green')\n",
    "        \n",
    "        # Output\n",
    "        ax2.arrow(pos, 0.4, 0, -0.08, head_width=0.02, head_length=0.02, fc='red', ec='red')\n",
    "        ax2.text(pos, 0.25, f'h_{time}', ha='center', fontsize=10, color='red')\n",
    "        \n",
    "        # Connexions horizontales\n",
    "        if i < len(positions) - 1:\n",
    "            ax2.arrow(pos + 0.08, 0.5, 0.12, 0, head_width=0.03, head_length=0.02, \n",
    "                     fc='purple', ec='purple')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rnn_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation d'un RNN Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation d'un RNN simple avec Keras\n",
    "def create_simple_rnn(input_size, hidden_size, output_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Crée un modèle RNN simple avec TensorFlow/Keras.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Dimension des features d'entrée\n",
    "        hidden_size: Dimension de l'état caché\n",
    "        output_size: Dimension de la sortie\n",
    "        sequence_length: Longueur des séquences\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Couche RNN\n",
    "        layers.SimpleRNN(hidden_size, \n",
    "                        activation='tanh',\n",
    "                        return_sequences=False,  # Ne retourner que la dernière sortie\n",
    "                        input_shape=(sequence_length, input_size)),\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.Dense(output_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer un modèle exemple\n",
    "model = create_simple_rnn(\n",
    "    input_size=300,      # Dimension des embeddings\n",
    "    hidden_size=128,     # Taille de l'état caché\n",
    "    output_size=2,       # Classification binaire\n",
    "    sequence_length=10   # Longueur des séquences\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec des données synthétiques\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "input_dim = 300\n",
    "\n",
    "# Générer des données d'exemple\n",
    "sample_input = np.random.randn(batch_size, sequence_length, input_dim)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nPremières prédictions:\\n{output[:5].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Différentes Architectures RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-Many : Génération de séquence\n",
    "def create_one_to_many_rnn(input_size, hidden_size, output_length, vocab_size):\n",
    "    \"\"\"\n",
    "    RNN pour génération de séquence (ex: description d'image).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input unique transformé en état initial\n",
    "        layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "        layers.RepeatVector(output_length),  # Répéter pour chaque timestep\n",
    "        \n",
    "        # RNN pour générer la séquence\n",
    "        layers.SimpleRNN(hidden_size, return_sequences=True),\n",
    "        \n",
    "        # Sortie pour chaque timestep\n",
    "        layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Many-to-One : Classification de séquence\n",
    "def create_many_to_one_rnn(sequence_length, input_size, hidden_size, num_classes):\n",
    "    \"\"\"\n",
    "    RNN pour classification de séquence (ex: analyse de sentiment).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.SimpleRNN(hidden_size, \n",
    "                        return_sequences=False,\n",
    "                        input_shape=(sequence_length, input_size)),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Many-to-Many : Séquence à séquence\n",
    "def create_many_to_many_rnn(sequence_length, input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    RNN pour transformation de séquence (ex: NER, POS tagging).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.SimpleRNN(hidden_size, \n",
    "                        return_sequences=True,  # Retourner toutes les sorties\n",
    "                        input_shape=(sequence_length, input_size)),\n",
    "        layers.TimeDistributed(layers.Dense(output_size, activation='softmax'))\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer et afficher les différentes architectures\n",
    "print(\"=== One-to-Many (Génération) ===\")\n",
    "model_1tom = create_one_to_many_rnn(512, 256, 20, 10000)\n",
    "print(f\"Input: (batch, 512) → Output: (batch, 20, 10000)\\n\")\n",
    "\n",
    "print(\"=== Many-to-One (Classification) ===\")\n",
    "model_mto1 = create_many_to_one_rnn(50, 300, 128, 5)\n",
    "print(f\"Input: (batch, 50, 300) → Output: (batch, 5)\\n\")\n",
    "\n",
    "print(\"=== Many-to-Many (Tagging) ===\")\n",
    "model_mtom = create_many_to_many_rnn(50, 300, 128, 10)\n",
    "print(f\"Input: (batch, 50, 300) → Output: (batch, 50, 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Projet Pratique : Analyse de Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dataset synthétique pour l'analyse de sentiment\n",
    "def create_sentiment_dataset():\n",
    "    \"\"\"\n",
    "    Crée un dataset simple pour l'analyse de sentiment.\n",
    "    \"\"\"\n",
    "    # Phrases positives\n",
    "    positive_texts = [\n",
    "        \"Ce film est absolument fantastique, je le recommande vivement\",\n",
    "        \"J'ai adoré ce livre, une histoire captivante du début à la fin\",\n",
    "        \"Excellent service, personnel très sympathique et professionnel\",\n",
    "        \"Produit de qualité exceptionnelle, très satisfait de mon achat\",\n",
    "        \"Une expérience merveilleuse, je reviendrai sans hésiter\",\n",
    "        \"Superbe performance, les acteurs sont brillants\",\n",
    "        \"Un chef-d'œuvre, vraiment impressionnant\",\n",
    "        \"Service client au top, problème résolu rapidement\",\n",
    "        \"Qualité irréprochable, conforme à la description\",\n",
    "        \"Je suis ravi de cette découverte, un vrai coup de cœur\"\n",
    "    ]\n",
    "    \n",
    "    # Phrases négatives\n",
    "    negative_texts = [\n",
    "        \"Film décevant, je me suis ennuyé du début à la fin\",\n",
    "        \"Très mauvaise expérience, service déplorable\",\n",
    "        \"Produit de mauvaise qualité, ne correspond pas à la description\",\n",
    "        \"Je suis très déçu, je ne recommande absolument pas\",\n",
    "        \"Perte de temps et d'argent, à éviter\",\n",
    "        \"Service client inexistant, aucune réponse à mes questions\",\n",
    "        \"Qualité médiocre, très en dessous de mes attentes\",\n",
    "        \"Histoire ennuyeuse et prévisible, sans intérêt\",\n",
    "        \"Arnaque totale, ne faites pas la même erreur que moi\",\n",
    "        \"Catastrophique, je regrette mon achat\"\n",
    "    ]\n",
    "    \n",
    "    # Combiner les données\n",
    "    texts = positive_texts + negative_texts\n",
    "    labels = [1] * len(positive_texts) + [0] * len(negative_texts)\n",
    "    \n",
    "    # Ajouter plus de variété\n",
    "    for _ in range(5):\n",
    "        texts.extend(texts)\n",
    "        labels.extend(labels)\n",
    "    \n",
    "    return texts, np.array(labels)\n",
    "\n",
    "# Créer le dataset\n",
    "texts, labels = create_sentiment_dataset()\n",
    "print(f\"Nombre d'exemples: {len(texts)}\")\n",
    "print(f\"Distribution des labels: {np.bincount(labels)}\")\n",
    "print(f\"\\nExemples:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {texts[i][:50]}... → {'Positif' if labels[i] else 'Négatif'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "# Tokenisation et padding\n",
    "max_words = 1000\n",
    "max_length = 20\n",
    "\n",
    "# Créer le tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convertir les textes en séquences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(f\"Exemple de séquence: {sequences[0]}\")\n",
    "\n",
    "# Padding des séquences\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "y = labels\n",
    "\n",
    "# Diviser en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nFormes des données:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle RNN pour l'analyse de sentiment\n",
    "def create_sentiment_rnn(vocab_size, embedding_dim, hidden_dim, max_length):\n",
    "    \"\"\"\n",
    "    Crée un modèle RNN pour l'analyse de sentiment.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Couche d'embedding\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        \n",
    "        # Couche RNN\n",
    "        layers.SimpleRNN(hidden_dim, dropout=0.3, recurrent_dropout=0.3),\n",
    "        \n",
    "        # Couche de dropout\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compiler le modèle\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "sentiment_model = create_sentiment_rnn(\n",
    "    vocab_size=max_words,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "history = sentiment_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'entraînement\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Visualise l'historique d'entraînement.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Époque')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Évolution de la Loss', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    ax2.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Époque')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Évolution de l\\'Accuracy', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle\n",
    "test_loss, test_accuracy = sentiment_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Prédictions sur de nouvelles phrases\n",
    "def predict_sentiment(text, model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Prédit le sentiment d'un texte.\n",
    "    \"\"\"\n",
    "    # Prétraitement\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length)\n",
    "    \n",
    "    # Prédiction\n",
    "    prediction = model.predict(padded, verbose=0)[0, 0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Tester sur de nouvelles phrases\n",
    "test_phrases = [\n",
    "    \"Ce produit est vraiment excellent, je le recommande\",\n",
    "    \"Très déçu de cet achat, qualité médiocre\",\n",
    "    \"Pas mal mais peut mieux faire\",\n",
    "    \"Une catastrophe totale, à éviter absolument\",\n",
    "    \"Superbe expérience, service impeccable\"\n",
    "]\n",
    "\n",
    "print(\"\\nPrédictions sur de nouvelles phrases:\")\n",
    "print(\"=\" * 60)\n",
    "for phrase in test_phrases:\n",
    "    score = predict_sentiment(phrase, sentiment_model, tokenizer, max_length)\n",
    "    sentiment = \"Positif\" if score > 0.5 else \"Négatif\"\n",
    "    print(f\"'{phrase}'\")\n",
    "    print(f\"  → {sentiment} (score: {score:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation de l'État Caché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un modèle qui retourne les états cachés\n",
    "def create_rnn_with_states(vocab_size, embedding_dim, hidden_dim, max_length):\n",
    "    \"\"\"\n",
    "    Crée un RNN qui retourne tous les états cachés.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding\n",
    "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    \n",
    "    # RNN avec return_sequences=True pour obtenir tous les états\n",
    "    hidden_states = layers.SimpleRNN(hidden_dim, return_sequences=True)(x)\n",
    "    \n",
    "    # Sortie finale (dernier état caché)\n",
    "    output = layers.Dense(1, activation='sigmoid')(hidden_states[:, -1, :])\n",
    "    \n",
    "    # Modèle pour prédiction\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    # Modèle pour obtenir les états cachés\n",
    "    state_model = models.Model(inputs=inputs, outputs=hidden_states)\n",
    "    \n",
    "    return model, state_model\n",
    "\n",
    "# Créer les modèles\n",
    "_, state_model = create_rnn_with_states(\n",
    "    vocab_size=max_words,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# Copier les poids du modèle entraîné\n",
    "state_model.layers[1].set_weights(sentiment_model.layers[0].get_weights())  # Embedding\n",
    "state_model.layers[2].set_weights(sentiment_model.layers[1].get_weights())  # RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'évolution des états cachés\n",
    "def visualize_hidden_states(text, model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Visualise l'évolution des états cachés pour une phrase.\n",
    "    \"\"\"\n",
    "    # Prétraitement\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length)\n",
    "    \n",
    "    # Obtenir les états cachés\n",
    "    hidden_states = model.predict(padded, verbose=0)[0]\n",
    "    \n",
    "    # Obtenir les mots\n",
    "    words = text.split()\n",
    "    sequence_words = []\n",
    "    for idx in sequence[0]:\n",
    "        for word, word_idx in tokenizer.word_index.items():\n",
    "            if word_idx == idx:\n",
    "                sequence_words.append(word)\n",
    "                break\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Heatmap des états cachés\n",
    "    im = ax1.imshow(hidden_states.T, aspect='auto', cmap='coolwarm')\n",
    "    ax1.set_xlabel('Position dans la séquence')\n",
    "    ax1.set_ylabel('Dimension de l\\'état caché')\n",
    "    ax1.set_title(f'États cachés RNN pour: \"{text}\"', fontsize=14)\n",
    "    \n",
    "    # Ajouter les mots\n",
    "    if len(sequence_words) <= max_length:\n",
    "        ax1.set_xticks(range(len(sequence_words)))\n",
    "        ax1.set_xticklabels(sequence_words, rotation=45, ha='right')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax1)\n",
    "    \n",
    "    # Norme des états cachés\n",
    "    norms = np.linalg.norm(hidden_states, axis=1)\n",
    "    ax2.plot(norms, 'o-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Position dans la séquence')\n",
    "    ax2.set_ylabel('Norme de l\\'état caché')\n",
    "    ax2.set_title('Évolution de la norme des états cachés', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(sequence_words) <= max_length:\n",
    "        ax2.set_xticks(range(len(sequence_words)))\n",
    "        ax2.set_xticklabels(sequence_words, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser pour différentes phrases\n",
    "test_phrases_viz = [\n",
    "    \"Film absolument fantastique\",\n",
    "    \"Très mauvais produit décevant\"\n",
    "]\n",
    "\n",
    "for phrase in test_phrases_viz:\n",
    "    visualize_hidden_states(phrase, state_model, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison : RNN vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un modèle baseline (sans récurrence)\n",
    "def create_baseline_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"\n",
    "    Modèle baseline : moyenne des embeddings + dense.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.GlobalAveragePooling1D(),  # Moyenne des embeddings\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Créer et entraîner le modèle baseline\n",
    "baseline_model = create_baseline_model(max_words, 100, max_length)\n",
    "print(\"Modèle Baseline:\")\n",
    "baseline_model.summary()\n",
    "\n",
    "# Entraîner\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Évaluer\n",
    "baseline_loss, baseline_acc = baseline_model.evaluate(X_test, y_test, verbose=0)\n",
    "rnn_loss, rnn_acc = sentiment_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\nComparaison des performances:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Baseline - Accuracy: {baseline_acc:.4f}, Loss: {baseline_loss:.4f}\")\n",
    "print(f\"RNN      - Accuracy: {rnn_acc:.4f}, Loss: {rnn_loss:.4f}\")\n",
    "print(f\"\\nAmélioration RNN: +{(rnn_acc - baseline_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limitations des RNN et Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration du problème du gradient qui disparaît\n",
    "def demonstrate_vanishing_gradient():\n",
    "    \"\"\"\n",
    "    Montre comment le gradient diminue avec la longueur de la séquence.\n",
    "    \"\"\"\n",
    "    sequence_lengths = [10, 20, 50, 100]\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        # Créer un mini modèle\n",
    "        model = create_many_to_one_rnn(seq_len, 50, 64, 2)\n",
    "        \n",
    "        # Données synthétiques\n",
    "        x = tf.random.normal((32, seq_len, 50))\n",
    "        y = tf.random.uniform((32,), maxval=2, dtype=tf.int32)\n",
    "        \n",
    "        # Calculer les gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Calculer la norme moyenne des gradients\n",
    "        grad_norm = np.mean([tf.norm(g).numpy() for g in gradients if g is not None])\n",
    "        gradient_norms.append(grad_norm)\n",
    "    \n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sequence_lengths, gradient_norms, 'o-', linewidth=2, markersize=10)\n",
    "    plt.xlabel('Longueur de la séquence')\n",
    "    plt.ylabel('Norme moyenne des gradients')\n",
    "    plt.title('Problème du Gradient qui Disparaît dans les RNN', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')  # Échelle logarithmique\n",
    "    \n",
    "    # Ajouter des annotations\n",
    "    for i, (length, norm) in enumerate(zip(sequence_lengths, gradient_norms)):\n",
    "        plt.annotate(f'{norm:.2e}', \n",
    "                    (length, norm), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_vanishing_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des limitations et solutions\n",
    "limitations_data = {\n",
    "    'Limitation': [\n",
    "        'Gradient qui disparaît',\n",
    "        'Mémoire à court terme',\n",
    "        'Pas de parallélisation',\n",
    "        'Sensibilité à l\\'ordre'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Les gradients deviennent très petits pour les longues séquences',\n",
    "        'Difficulté à retenir les informations sur de longues distances',\n",
    "        'Traitement séquentiel obligatoire, lent à entraîner',\n",
    "        'Performance dégradée si l\\'ordre des mots change'\n",
    "    ],\n",
    "    'Solution': [\n",
    "        'LSTM/GRU avec portes',\n",
    "        'Mécanismes d\\'attention',\n",
    "        'Transformers',\n",
    "        'Embeddings positionnels'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_limitations = pd.DataFrame(limitations_data)\n",
    "\n",
    "# Afficher le tableau\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_limitations.values,\n",
    "                colLabels=df_limitations.columns,\n",
    "                cellLoc='left',\n",
    "                loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 2)\n",
    "\n",
    "# Style du tableau\n",
    "for i in range(len(df_limitations.columns)):\n",
    "    table[(0, i)].set_facecolor('#1E90FF')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.title('Limitations des RNN et Solutions Proposées', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion et Points Clés\n",
    "\n",
    "### Ce que nous avons appris :\n",
    "\n",
    "1. **Architecture RNN** : Comment les RNN traitent les séquences de manière récurrente\n",
    "2. **Implémentation** : Création de RNN avec TensorFlow/Keras\n",
    "3. **Types d'architectures** : One-to-One, One-to-Many, Many-to-One, Many-to-Many\n",
    "4. **Application pratique** : Analyse de sentiment avec RNN\n",
    "5. **Visualisation** : Comment observer les états cachés évoluent\n",
    "\n",
    "### Points clés à retenir :\n",
    "\n",
    "- ✅ Les RNN excellent pour les données séquentielles\n",
    "- ✅ Ils maintiennent un état caché qui capture le contexte\n",
    "- ✅ Simples à implémenter avec les frameworks modernes\n",
    "- ❌ Souffrent du problème du gradient qui disparaît\n",
    "- ❌ Limités pour les très longues séquences\n",
    "- ❌ Lents à entraîner (pas de parallélisation)\n",
    "\n",
    "### Prochaine étape : LSTM et GRU\n",
    "\n",
    "Les LSTM (Long Short-Term Memory) et GRU (Gated Recurrent Unit) résolvent plusieurs limitations des RNN vanilla en introduisant des mécanismes de portes qui permettent de mieux contrôler le flux d'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle pour une utilisation future\n",
    "sentiment_model.save('rnn_sentiment_model.h5')\n",
    "print(\"Modèle sauvegardé sous 'rnn_sentiment_model.h5'\")\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer sauvegardé sous 'tokenizer.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
