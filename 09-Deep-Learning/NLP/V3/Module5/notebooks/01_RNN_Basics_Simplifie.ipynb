{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (R√©seaux de Neurones R√©currents) - Version Simplifi√©e\n",
    "\n",
    "## üéØ Objectifs de ce notebook\n",
    "\n",
    "1. **Comprendre** ce qu'est un RNN et pourquoi on l'utilise\n",
    "2. **Visualiser** comment un RNN traite les donn√©es s√©quentielles\n",
    "3. **Impl√©menter** un RNN simple avec TensorFlow\n",
    "4. **Appliquer** les RNN √† un probl√®me concret : l'analyse de sentiment\n",
    "5. **Identifier** les limites des RNN basiques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ü§î Qu'est-ce qu'un RNN ?\n",
    "\n",
    "### Analogie simple :\n",
    "Imaginez que vous lisez une phrase mot par mot. √Ä chaque mot, vous :\n",
    "- **Regardez** le mot actuel\n",
    "- **Vous souvenez** de ce que vous avez lu avant\n",
    "- **Formez** une nouvelle compr√©hension\n",
    "\n",
    "C'est exactement ce que fait un RNN !\n",
    "\n",
    "### Diff√©rence avec les r√©seaux classiques :\n",
    "- **R√©seau classique** : Une image ‚Üí Une pr√©diction\n",
    "- **RNN** : Une s√©quence (texte, audio, s√©rie temporelle) ‚Üí Une ou plusieurs pr√©dictions\n",
    "\n",
    "### Exemples d'applications :\n",
    "- üì± **Traduction automatique** : \"Hello\" ‚Üí \"Bonjour\"\n",
    "- üòä **Analyse de sentiment** : \"J'adore ce film\" ‚Üí Positif\n",
    "- üìà **Pr√©diction de s√©ries temporelles** : Prix des actions\n",
    "- üéµ **G√©n√©ration de musique** : Composer de nouvelles m√©lodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üì¶ Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "!pip install tensorflow numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration pour l'affichage\n",
    "plt.style.use('default')\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ Imports termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üß† Comment fonctionne un RNN ?\n",
    "\n",
    "### Visualisation conceptuelle\n",
    "\n",
    "Un RNN traite une s√©quence √©tape par √©tape :\n",
    "\n",
    "```\n",
    "Mot 1: \"J'\"     ‚Üí √âtat cach√© h1\n",
    "Mot 2: \"adore\"  ‚Üí √âtat cach√© h2 (utilise h1 + \"adore\")\n",
    "Mot 3: \"ce\"     ‚Üí √âtat cach√© h3 (utilise h2 + \"ce\")\n",
    "Mot 4: \"film\"   ‚Üí √âtat cach√© h4 (utilise h3 + \"film\")\n",
    "                ‚Üí Pr√©diction finale: POSITIF\n",
    "```\n",
    "\n",
    "### Formule math√©matique (simplifi√©e) :\n",
    "```\n",
    "√âtat cach√©(t) = tanh(Poids √ó [√âtat pr√©c√©dent, Mot actuel])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation simple du fonctionnement d'un RNN\n",
    "def visualiser_rnn():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Positions des √©l√©ments\n",
    "    positions = [1, 3, 5, 7]\n",
    "    mots = [\"J'\", \"adore\", \"ce\", \"film\"]\n",
    "    \n",
    "    # Dessiner les cellules RNN\n",
    "    for i, (pos, mot) in enumerate(zip(positions, mots)):\n",
    "        # Cellule RNN\n",
    "        rect = plt.Rectangle((pos-0.3, 2), 0.6, 1, \n",
    "                           facecolor='lightblue', edgecolor='darkblue', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(pos, 2.5, 'RNN', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Mot d'entr√©e\n",
    "        ax.text(pos, 1, mot, ha='center', va='center', fontsize=12, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen'))\n",
    "        \n",
    "        # Fl√®che d'entr√©e\n",
    "        ax.arrow(pos, 1.5, 0, 0.4, head_width=0.1, head_length=0.1, fc='green', ec='green')\n",
    "        \n",
    "        # √âtat cach√©\n",
    "        ax.text(pos, 3.5, f'h{i+1}', ha='center', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='orange'))\n",
    "        \n",
    "        # Fl√®che de sortie\n",
    "        ax.arrow(pos, 3.1, 0, 0.3, head_width=0.1, head_length=0.1, fc='orange', ec='orange')\n",
    "        \n",
    "        # Connexion horizontale (sauf pour le dernier)\n",
    "        if i < len(positions) - 1:\n",
    "            ax.arrow(pos + 0.3, 2.5, 1.4, 0, head_width=0.1, head_length=0.1, \n",
    "                    fc='red', ec='red', linewidth=2)\n",
    "    \n",
    "    # Pr√©diction finale\n",
    "    ax.text(8.5, 2.5, 'POSITIF', ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='gold'))\n",
    "    ax.arrow(7.3, 2.5, 0.9, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "    \n",
    "    # L√©gendes\n",
    "    ax.text(4, 0.2, 'Mots d\\'entr√©e', ha='center', fontsize=10, color='green')\n",
    "    ax.text(4, 4.2, '√âtats cach√©s', ha='center', fontsize=10, color='orange')\n",
    "    ax.text(4, 1.8, 'Connexions r√©currentes', ha='center', fontsize=10, color='red')\n",
    "    \n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_title('üß† Comment un RNN traite la phrase \"J\\'adore ce film\"', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualiser_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üõ†Ô∏è Premier RNN avec TensorFlow\n",
    "\n",
    "Cr√©ons notre premier RNN √©tape par √©tape :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 1: Cr√©er un RNN simple\n",
    "def creer_rnn_simple():\n",
    "    \"\"\"\n",
    "    Cr√©e un RNN basique pour comprendre la structure\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Couche RNN simple\n",
    "        # - 64: nombre de neurones dans l'√©tat cach√©\n",
    "        # - input_shape: (longueur_sequence, dimensions_features)\n",
    "        layers.SimpleRNN(64, input_shape=(10, 1)),\n",
    "        \n",
    "        # Couche de sortie pour classification binaire\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "mon_premier_rnn = creer_rnn_simple()\n",
    "\n",
    "# Voir la structure\n",
    "print(\"üìã Structure de notre premier RNN :\")\n",
    "mon_premier_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 2: Tester avec des donn√©es factices\n",
    "print(\"üß™ Test avec des donn√©es exemple :\")\n",
    "\n",
    "# Cr√©er des donn√©es d'exemple\n",
    "# Format: (nombre_exemples, longueur_sequence, features)\n",
    "donnees_test = np.random.randn(5, 10, 1)  # 5 exemples, s√©quences de 10, 1 feature\n",
    "\n",
    "print(f\"üìä Forme des donn√©es d'entr√©e: {donnees_test.shape}\")\n",
    "\n",
    "# Faire une pr√©diction\n",
    "predictions = mon_premier_rnn.predict(donnees_test, verbose=0)\n",
    "\n",
    "print(f\"üìà Forme des pr√©dictions: {predictions.shape}\")\n",
    "print(f\"üìã Premi√®res pr√©dictions: {predictions.flatten()[:3]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Notre RNN fonctionne ! Il prend des s√©quences et produit des pr√©dictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìù Projet pratique : Analyse de sentiment\n",
    "\n",
    "Maintenant, appliquons les RNN √† un probl√®me r√©el : d√©terminer si un commentaire est positif ou n√©gatif.\n",
    "\n",
    "### √âtapes :\n",
    "1. **Cr√©er** des donn√©es d'exemple\n",
    "2. **Preprocesser** le texte\n",
    "3. **Construire** le mod√®le RNN\n",
    "4. **Entra√Æner** le mod√®le\n",
    "5. **Tester** sur de nouveaux exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 1: Cr√©er nos donn√©es d'entra√Ænement\n",
    "def creer_donnees_sentiment():\n",
    "    \"\"\"\n",
    "    Cr√©e un petit dataset pour l'analyse de sentiment\n",
    "    \"\"\"\n",
    "    # Commentaires POSITIFS\n",
    "    positifs = [\n",
    "        \"Ce film est absolument fantastique\",\n",
    "        \"J'ai ador√© cette exp√©rience\",\n",
    "        \"Excellent service, tr√®s satisfait\",\n",
    "        \"Produit de qualit√© exceptionnelle\",\n",
    "        \"Une exp√©rience merveilleuse\",\n",
    "        \"Superbe performance des acteurs\",\n",
    "        \"Je recommande vivement\",\n",
    "        \"Tr√®s bonne qualit√©\",\n",
    "        \"Service client parfait\",\n",
    "        \"Vraiment impressionnant\"\n",
    "    ]\n",
    "    \n",
    "    # Commentaires N√âGATIFS\n",
    "    negatifs = [\n",
    "        \"Film tr√®s d√©cevant et ennuyeux\",\n",
    "        \"Tr√®s mauvaise exp√©rience\",\n",
    "        \"Produit de mauvaise qualit√©\",\n",
    "        \"Je suis tr√®s d√©√ßu\",\n",
    "        \"√Ä √©viter absolument\",\n",
    "        \"Service client inexistant\",\n",
    "        \"Qualit√© vraiment m√©diocre\",\n",
    "        \"Perte de temps total\",\n",
    "        \"Je regrette cet achat\",\n",
    "        \"Vraiment catastrophique\"\n",
    "    ]\n",
    "    \n",
    "    # Combiner les donn√©es\n",
    "    tous_textes = positifs + negatifs\n",
    "    \n",
    "    # Labels: 1 = positif, 0 = n√©gatif\n",
    "    labels = [1] * len(positifs) + [0] * len(negatifs)\n",
    "    \n",
    "    # Multiplier les donn√©es pour avoir plus d'exemples\n",
    "    tous_textes = tous_textes * 5  # 100 exemples au total\n",
    "    labels = labels * 5\n",
    "    \n",
    "    return tous_textes, np.array(labels)\n",
    "\n",
    "# Cr√©er les donn√©es\n",
    "textes, sentiments = creer_donnees_sentiment()\n",
    "\n",
    "print(f\"üìä Nombre total d'exemples: {len(textes)}\")\n",
    "print(f\"üìä Positifs: {np.sum(sentiments)}, N√©gatifs: {len(sentiments) - np.sum(sentiments)}\")\n",
    "print(\"\\nüìã Exemples:\")\n",
    "for i in range(3):\n",
    "    emoji = \"üòä\" if sentiments[i] == 1 else \"üòû\"\n",
    "    print(f\"  {emoji} {textes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 2: Preprocesser les textes\n",
    "print(\"üîß Pr√©paration des donn√©es...\")\n",
    "\n",
    "# Configuration\n",
    "MAX_MOTS = 1000  # Vocabulaire maximum\n",
    "MAX_LONGUEUR = 15  # Longueur maximale des phrases\n",
    "\n",
    "# Cr√©er le tokenizer (convertit les mots en nombres)\n",
    "tokenizer = Tokenizer(num_words=MAX_MOTS)\n",
    "tokenizer.fit_on_texts(textes)\n",
    "\n",
    "print(f\"üìö Taille du vocabulaire: {len(tokenizer.word_index)}\")\n",
    "\n",
    "# Convertir les textes en s√©quences de nombres\n",
    "sequences = tokenizer.texts_to_sequences(textes)\n",
    "\n",
    "print(f\"üìã Exemple de conversion:\")\n",
    "print(f\"  Texte: '{textes[0]}'\")\n",
    "print(f\"  S√©quence: {sequences[0]}\")\n",
    "\n",
    "# √âgaliser la longueur des s√©quences (padding)\n",
    "X = pad_sequences(sequences, maxlen=MAX_LONGUEUR)\n",
    "y = sentiments\n",
    "\n",
    "print(f\"\\nüìä Forme finale des donn√©es: {X.shape}\")\n",
    "print(f\"üìä Exemple apr√®s padding: {X[0]}\")\n",
    "\n",
    "# Diviser en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Donn√©es pr√™tes !\")\n",
    "print(f\"   Train: {X_train.shape[0]} exemples\")\n",
    "print(f\"   Test: {X_test.shape[0]} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 3: Construire le mod√®le RNN pour l'analyse de sentiment\n",
    "def creer_rnn_sentiment():\n",
    "    \"\"\"\n",
    "    Cr√©e un RNN sp√©cialis√© pour l'analyse de sentiment\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # 1. Couche d'embedding (convertit les nombres en vecteurs)\n",
    "        layers.Embedding(\n",
    "            input_dim=MAX_MOTS,      # Taille du vocabulaire\n",
    "            output_dim=50,           # Dimension des vecteurs de mots\n",
    "            input_length=MAX_LONGUEUR # Longueur des s√©quences\n",
    "        ),\n",
    "        \n",
    "        # 2. Couche RNN\n",
    "        layers.SimpleRNN(\n",
    "            64,                      # 64 neurones dans l'√©tat cach√©\n",
    "            dropout=0.3              # Pour √©viter le surapprentissage\n",
    "        ),\n",
    "        \n",
    "        # 3. Couche de sortie (positif ou n√©gatif)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compiler le mod√®le\n",
    "    model.compile(\n",
    "        optimizer='adam',                    # Algorithme d'optimisation\n",
    "        loss='binary_crossentropy',         # Fonction de perte pour classification binaire\n",
    "        metrics=['accuracy']                # M√©trique √† suivre\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "modele_sentiment = creer_rnn_sentiment()\n",
    "\n",
    "print(\"üèóÔ∏è Architecture du mod√®le RNN pour sentiment:\")\n",
    "modele_sentiment.summary()\n",
    "\n",
    "print(\"\\nüìù Explication des couches:\")\n",
    "print(\"  üîπ Embedding: Convertit les mots (nombres) en vecteurs\")\n",
    "print(\"  üîπ SimpleRNN: Traite la s√©quence et m√©morise le contexte\")\n",
    "print(\"  üîπ Dense: Pr√©diction finale (0=n√©gatif, 1=positif)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 4: Entra√Æner le mod√®le\n",
    "print(\"üöÄ D√©but de l'entra√Ænement...\")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "historique = modele_sentiment.fit(\n",
    "    X_train, y_train,           # Donn√©es d'entra√Ænement\n",
    "    batch_size=16,              # Traiter 16 exemples √† la fois\n",
    "    epochs=10,                  # 10 passages sur toutes les donn√©es\n",
    "    validation_split=0.2,       # 20% des donn√©es pour validation\n",
    "    verbose=1                   # Afficher les d√©tails\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'entra√Ænement\n",
    "def visualiser_entrainement(historique):\n",
    "    \"\"\"\n",
    "    Affiche l'√©volution de l'accuracy et de la loss pendant l'entra√Ænement\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Graphique de l'accuracy\n",
    "    ax1.plot(historique.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "    ax1.plot(historique.history['val_accuracy'], 'r-', label='Validation', linewidth=2)\n",
    "    ax1.set_title('üìà Pr√©cision du mod√®le', fontsize=14)\n",
    "    ax1.set_xlabel('√âpoque')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Graphique de la loss\n",
    "    ax2.plot(historique.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "    ax2.plot(historique.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    ax2.set_title('üìâ Erreur du mod√®le', fontsize=14)\n",
    "    ax2.set_xlabel('√âpoque')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualiser_entrainement(historique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtape 5: Tester le mod√®le\n",
    "print(\"üß™ √âvaluation sur les donn√©es de test:\")\n",
    "\n",
    "# √âvaluer sur les donn√©es de test\n",
    "test_loss, test_accuracy = modele_sentiment.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"üìä Pr√©cision sur le test: {test_accuracy:.1%}\")\n",
    "\n",
    "# Fonction pour pr√©dire le sentiment d'une nouvelle phrase\n",
    "def predire_sentiment(phrase):\n",
    "    \"\"\"\n",
    "    Pr√©dit si une phrase est positive ou n√©gative\n",
    "    \"\"\"\n",
    "    # Preprocesser la phrase\n",
    "    sequence = tokenizer.texts_to_sequences([phrase])\n",
    "    sequence_paddee = pad_sequences(sequence, maxlen=MAX_LONGUEUR)\n",
    "    \n",
    "    # Faire la pr√©diction\n",
    "    score = modele_sentiment.predict(sequence_paddee, verbose=0)[0, 0]\n",
    "    \n",
    "    # Interpr√©ter le r√©sultat\n",
    "    if score > 0.5:\n",
    "        return \"üòä POSITIF\", score\n",
    "    else:\n",
    "        return \"üòû N√âGATIF\", score\n",
    "\n",
    "# Tester sur de nouvelles phrases\n",
    "nouvelles_phrases = [\n",
    "    \"Ce restaurant est formidable\",\n",
    "    \"Je deteste ce produit\",\n",
    "    \"Tr√®s bonne qualit√©\",\n",
    "    \"Service d√©cevant\",\n",
    "    \"Pas mal mais peut mieux faire\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîÆ Pr√©dictions sur de nouvelles phrases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for phrase in nouvelles_phrases:\n",
    "    sentiment, score = predire_sentiment(phrase)\n",
    "    print(f\"'{phrase}'\")\n",
    "    print(f\"  ‚Üí {sentiment} (confiance: {score:.1%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîç Comprendre les limites des RNN\n",
    "\n",
    "Les RNN basiques ont quelques probl√®mes importants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration du probl√®me de m√©moire courte\n",
    "def tester_memoire_rnn():\n",
    "    \"\"\"\n",
    "    Montre comment les RNN ont du mal avec les longues s√©quences\n",
    "    \"\"\"\n",
    "    # Phrase courte vs phrase longue\n",
    "    phrase_courte = \"Ce film est g√©nial\"\n",
    "    phrase_longue = \"Ce film, malgr√© quelques d√©fauts mineurs dans le sc√©nario et des moments qui tra√Ænent un peu en longueur, reste g√©nial\"\n",
    "    \n",
    "    print(\"üî¨ Test de m√©moire des RNN:\")\n",
    "    print(\"\\nüìù Phrase courte:\")\n",
    "    print(f\"  '{phrase_courte}'\")\n",
    "    sentiment, score = predire_sentiment(phrase_courte)\n",
    "    print(f\"  ‚Üí {sentiment} (confiance: {score:.1%})\")\n",
    "    \n",
    "    print(\"\\nüìù Phrase longue (m√™me sentiment final):\")\n",
    "    print(f\"  '{phrase_longue}'\")\n",
    "    sentiment, score = predire_sentiment(phrase_longue)\n",
    "    print(f\"  ‚Üí {sentiment} (confiance: {score:.1%})\")\n",
    "    \n",
    "    print(\"\\nüí° Observation:\")\n",
    "    print(\"   Les RNN ont plus de difficult√© avec les phrases longues\")\n",
    "    print(\"   car ils 'oublient' le d√©but de la phrase.\")\n",
    "\n",
    "tester_memoire_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des limitations\n",
    "def afficher_limitations():\n",
    "    \"\"\"\n",
    "    Montre les principales limitations des RNN\n",
    "    \"\"\"\n",
    "    limitations = {\n",
    "        \"üß† M√©moire courte\": \"Oublient les informations lointaines dans la s√©quence\",\n",
    "        \"‚è≥ Lenteur d'entra√Ænement\": \"Doivent traiter les mots un par un (pas de parall√©lisation)\",\n",
    "        \"üìâ Gradient qui dispara√Æt\": \"Difficult√© √† apprendre sur de longues s√©quences\",\n",
    "        \"üîÑ Pas de vision d'ensemble\": \"Ne voient que ce qui pr√©c√®de, pas la suite\"\n",
    "    }\n",
    "    \n",
    "    solutions = {\n",
    "        \"üß† M√©moire courte\": \"‚Üí LSTM et GRU (avec portes de m√©moire)\",\n",
    "        \"‚è≥ Lenteur d'entra√Ænement\": \"‚Üí Transformers (traitement parall√®le)\",\n",
    "        \"üìâ Gradient qui dispara√Æt\": \"‚Üí LSTM/GRU avec connexions r√©siduelles\",\n",
    "        \"üîÑ Pas de vision d'ensemble\": \"‚Üí Attention mechanisms et Transformers\"\n",
    "    }\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Limitations des RNN basiques:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for limitation, description in limitations.items():\n",
    "        print(f\"\\n{limitation}\")\n",
    "        print(f\"  {description}\")\n",
    "        if limitation in solutions:\n",
    "            print(f\"  {solutions[limitation]}\")\n",
    "\n",
    "afficher_limitations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üéØ R√©sum√© et Points Cl√©s\n",
    "\n",
    "### ‚úÖ Ce que nous avons appris :\n",
    "\n",
    "1. **Les RNN sont parfaits pour les donn√©es s√©quentielles** (texte, audio, s√©ries temporelles)\n",
    "2. **Ils maintiennent un \"√©tat cach√©\"** qui se souvient du contexte pr√©c√©dent\n",
    "3. **TensorFlow/Keras rend l'impl√©mentation simple** avec quelques lignes de code\n",
    "4. **Les applications sont nombreuses** : sentiment, traduction, g√©n√©ration de texte\n",
    "\n",
    "### ‚ö†Ô∏è Limitations importantes :\n",
    "\n",
    "1. **M√©moire courte** pour les longues s√©quences\n",
    "2. **Entra√Ænement lent** (traitement s√©quentiel)\n",
    "3. **Probl√®me du gradient qui dispara√Æt**\n",
    "\n",
    "### üöÄ Prochaines √©tapes :\n",
    "\n",
    "- **LSTM et GRU** : Versions am√©lior√©es des RNN\n",
    "- **Transformers** : Architecture moderne qui r√©sout la plupart des probl√®mes\n",
    "- **Applications avanc√©es** : Traduction automatique, chatbots\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Exercices pour aller plus loin :\n",
    "\n",
    "1. **Modifiez le dataset** : Ajoutez vos propres phrases positives/n√©gatives\n",
    "2. **Changez l'architecture** : Testez diff√©rents nombres de neurones\n",
    "3. **Essayez d'autres probl√®mes** : Classification de genres de films, d√©tection de spam\n",
    "4. **Comparez avec un mod√®le simple** : Bag of Words vs RNN\n",
    "\n",
    "F√©licitations ! üéâ Vous ma√Ætrisez maintenant les bases des RNN !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
