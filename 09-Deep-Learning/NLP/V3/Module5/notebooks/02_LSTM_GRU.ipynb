{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM et GRU avec TensorFlow\n",
    "\n",
    "Ce notebook explore les architectures LSTM (Long Short-Term Memory) et GRU (Gated Recurrent Unit), qui r√©solvent les limitations des RNN vanilla.\n",
    "\n",
    "## Objectifs du notebook\n",
    "\n",
    "1. Comprendre les m√©canismes de portes dans LSTM et GRU\n",
    "2. Impl√©menter LSTM et GRU avec TensorFlow/Keras\n",
    "3. Comparer les performances RNN vs LSTM vs GRU\n",
    "4. Visualiser le fonctionnement des portes\n",
    "5. Applications pratiques avanc√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install tensorflow numpy pandas matplotlib seaborn scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprendre les LSTM : Architecture et Portes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des portes LSTM\n",
    "def visualize_lstm_gates():\n",
    "    \"\"\"\n",
    "    Visualise les diff√©rentes portes d'une cellule LSTM.\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Forget Gate', 'Input Gate', 'Output Gate', 'Cell State Update'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Simuler des valeurs de portes au fil du temps\n",
    "    time_steps = np.arange(1, 11)\n",
    "    \n",
    "    # Forget Gate (oublie progressivement l'ancienne information)\n",
    "    forget_gate = np.array([0.9, 0.8, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01])\n",
    "    \n",
    "    # Input Gate (accepte la nouvelle information)\n",
    "    input_gate = np.array([0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.98, 0.99])\n",
    "    \n",
    "    # Output Gate (contr√¥le ce qui est expos√©)\n",
    "    output_gate = np.array([0.5, 0.6, 0.7, 0.8, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65])\n",
    "    \n",
    "    # Cell State (combinaison des trois)\n",
    "    cell_state = np.array([0.2, 0.3, 0.5, 0.7, 0.8, 0.85, 0.9, 0.88, 0.85, 0.82])\n",
    "    \n",
    "    # Forget Gate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_steps, y=forget_gate, mode='lines+markers', \n",
    "                  name='Forget', line=dict(color='red', width=3)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Input Gate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_steps, y=input_gate, mode='lines+markers', \n",
    "                  name='Input', line=dict(color='green', width=3)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Output Gate\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_steps, y=output_gate, mode='lines+markers', \n",
    "                  name='Output', line=dict(color='blue', width=3)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Cell State\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=time_steps, y=cell_state, mode='lines+markers', \n",
    "                  name='Cell State', line=dict(color='purple', width=3)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"√âvolution des Portes LSTM au Fil du Temps\",\n",
    "        showlegend=False,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Mise √† jour des axes\n",
    "    for row in range(1, 3):\n",
    "        for col in range(1, 3):\n",
    "            fig.update_xaxes(title_text=\"Time Step\", row=row, col=col)\n",
    "            fig.update_yaxes(title_text=\"Activation\", range=[0, 1], row=row, col=col)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "visualize_lstm_gates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison architecturale RNN vs LSTM vs GRU\n",
    "def compare_architectures():\n",
    "    \"\"\"\n",
    "    Compare les architectures des diff√©rents types de RNN.\n",
    "    \"\"\"\n",
    "    architectures = {\n",
    "        'Caract√©ristique': [\n",
    "            'Nombre de portes',\n",
    "            '√âtat interne',\n",
    "            'Complexit√©',\n",
    "            'M√©moire',\n",
    "            'Vitesse d\\'entra√Ænement',\n",
    "            'Gradient qui dispara√Æt',\n",
    "            'Capacit√© √† long terme'\n",
    "        ],\n",
    "        'RNN Vanilla': [\n",
    "            '0',\n",
    "            '√âtat cach√© simple',\n",
    "            'Faible',\n",
    "            'Limit√©e',\n",
    "            'Rapide',\n",
    "            'Probl√©matique',\n",
    "            'Faible'\n",
    "        ],\n",
    "        'LSTM': [\n",
    "            '3 (forget, input, output)',\n",
    "            '√âtat cach√© + √âtat de cellule',\n",
    "            '√âlev√©e',\n",
    "            'Excellente',\n",
    "            'Lent',\n",
    "            'R√©solu',\n",
    "            'Excellente'\n",
    "        ],\n",
    "        'GRU': [\n",
    "            '2 (reset, update)',\n",
    "            '√âtat cach√© uniquement',\n",
    "            'Moyenne',\n",
    "            'Tr√®s bonne',\n",
    "            'Moyennement rapide',\n",
    "            'Largement r√©solu',\n",
    "            'Tr√®s bonne'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_arch = pd.DataFrame(architectures)\n",
    "    \n",
    "    # Affichage du tableau\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df_arch.values,\n",
    "                    colLabels=df_arch.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2.5)\n",
    "    \n",
    "    # Style du tableau\n",
    "    for i in range(len(df_arch.columns)):\n",
    "        table[(0, i)].set_facecolor('#1E90FF')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Colorer les cellules selon le type\n",
    "    colors = ['#FFE6E6', '#E6F3FF', '#E6FFE6']  # Rouge clair, Bleu clair, Vert clair\n",
    "    for i in range(1, 4):  # Colonnes RNN, LSTM, GRU\n",
    "        for j in range(1, len(df_arch) + 1):\n",
    "            table[(j, i)].set_facecolor(colors[i-1])\n",
    "    \n",
    "    plt.title('Comparaison des Architectures RNN', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.show()\n",
    "\n",
    "compare_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impl√©mentation des Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions pour cr√©er diff√©rents types de mod√®les\n",
    "def create_rnn_model(vocab_size, embedding_dim, hidden_dim, max_length, num_classes=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le RNN vanilla.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.SimpleRNN(hidden_dim, dropout=0.3, recurrent_dropout=0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(vocab_size, embedding_dim, hidden_dim, max_length, num_classes=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le LSTM.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_gru_model(vocab_size, embedding_dim, hidden_dim, max_length, num_classes=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le GRU.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.GRU(hidden_dim, dropout=0.3, recurrent_dropout=0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_bidirectional_lstm(vocab_size, embedding_dim, hidden_dim, max_length, num_classes=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le LSTM bidirectionnel.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.Bidirectional(layers.LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3)),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_stacked_lstm(vocab_size, embedding_dim, hidden_dim, max_length, num_classes=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le LSTM empil√© (2 couches).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        layers.LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3, return_sequences=True),\n",
    "        layers.LSTM(hidden_dim, dropout=0.3, recurrent_dropout=0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Afficher les architectures\n",
    "params = {\n",
    "    'vocab_size': 1000,\n",
    "    'embedding_dim': 100,\n",
    "    'hidden_dim': 128,\n",
    "    'max_length': 50,\n",
    "    'num_classes': 2\n",
    "}\n",
    "\n",
    "models_dict = {\n",
    "    'RNN': create_rnn_model(**params),\n",
    "    'LSTM': create_lstm_model(**params),\n",
    "    'GRU': create_gru_model(**params),\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm(**params),\n",
    "    'Stacked LSTM': create_stacked_lstm(**params)\n",
    "}\n",
    "\n",
    "# Comparer le nombre de param√®tres\n",
    "for name, model in models_dict.items():\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    total_params = model.count_params()\n",
    "    print(f\"{name:20s}: {total_params:,} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset pour Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un dataset plus complexe pour tester les capacit√©s de m√©moire\n",
    "def create_complex_sentiment_dataset():\n",
    "    \"\"\"\n",
    "    Cr√©e un dataset avec des phrases complexes n√©cessitant une m√©moire √† long terme.\n",
    "    \"\"\"\n",
    "    # Phrases avec sentiment au d√©but et n√©gation/confirmation √† la fin\n",
    "    complex_positive = [\n",
    "        \"Ce film √©tait vraiment fantastique au d√©but mais finalement c'√©tait encore mieux √† la fin\",\n",
    "        \"J'ai d'abord pens√© que ce produit √©tait moyen mais apr√®s utilisation je suis tr√®s satisfait\",\n",
    "        \"Le service semblait d√©cevant au premier abord mais l'√©quipe a su me convaincre de leur professionnalisme\",\n",
    "        \"Bien que le prix soit √©lev√© cette exp√©rience vaut vraiment le d√©tour\",\n",
    "        \"Malgr√© quelques d√©fauts mineurs je recommande vivement cet article\",\n",
    "        \"Au d√©but j'√©tais sceptique mais maintenant je peux affirmer que c'est excellent\",\n",
    "        \"Ce restaurant avait mauvaise r√©putation mais la r√©alit√© d√©passe les attentes\",\n",
    "        \"L'interface para√Æt compliqu√©e mais une fois ma√Ætris√©e elle est tr√®s efficace\",\n",
    "        \"Contrairement aux avis n√©gatifs mon exp√©rience a √©t√© formidable\",\n",
    "        \"Bien que critiqu√© par certains je trouve ce produit absolument parfait\"\n",
    "    ]\n",
    "    \n",
    "    complex_negative = [\n",
    "        \"Ce film commen√ßait bien mais s'est r√©v√©l√© √™tre une grande d√©ception\",\n",
    "        \"J'avais de grandes attentes mais le r√©sultat est vraiment d√©cevant\",\n",
    "        \"Le produit semblait prometteur mais la qualit√© n'est pas au rendez-vous\",\n",
    "        \"Malgr√© de bonnes critiques mon exp√©rience personnelle a √©t√© catastrophique\",\n",
    "        \"Bien que recommand√© par mes amis je ne peux pas partager leur enthousiasme\",\n",
    "        \"Au d√©but tout allait bien mais les probl√®mes se sont accumul√©s\",\n",
    "        \"Ce service avait bonne r√©putation mais la r√©alit√© est tout autre\",\n",
    "        \"L'√©quipe semblait comp√©tente mais leur travail est b√¢cl√©\",\n",
    "        \"Contrairement aux promesses publicitaires le produit est d√©faillant\",\n",
    "        \"Bien que cher ce produit ne vaut absolument pas son prix\"\n",
    "    ]\n",
    "    \n",
    "    # Phrases simples pour contraste\n",
    "    simple_positive = [\n",
    "        \"Excellent produit je recommande\",\n",
    "        \"Tr√®s satisfait de mon achat\",\n",
    "        \"Service parfait et rapide\",\n",
    "        \"Qualit√© exceptionnelle bravo\",\n",
    "        \"Exp√©rience merveilleuse et agr√©able\"\n",
    "    ]\n",
    "    \n",
    "    simple_negative = [\n",
    "        \"Produit d√©cevant tr√®s mauvais\",\n",
    "        \"Service client inexistant\",\n",
    "        \"Qualit√© m√©diocre √† √©viter\",\n",
    "        \"Exp√©rience d√©sastreuse et frustrante\",\n",
    "        \"Arnaque totale ne pas acheter\"\n",
    "    ]\n",
    "    \n",
    "    # Combiner tous les textes\n",
    "    all_texts = (\n",
    "        complex_positive * 3 + complex_negative * 3 +  # Plus de phrases complexes\n",
    "        simple_positive * 2 + simple_negative * 2\n",
    "    )\n",
    "    \n",
    "    all_labels = (\n",
    "        [1] * (len(complex_positive) * 3) +\n",
    "        [0] * (len(complex_negative) * 3) +\n",
    "        [1] * (len(simple_positive) * 2) +\n",
    "        [0] * (len(simple_negative) * 2)\n",
    "    )\n",
    "    \n",
    "    return all_texts, np.array(all_labels)\n",
    "\n",
    "# Cr√©er le dataset\n",
    "texts, labels = create_complex_sentiment_dataset()\n",
    "\n",
    "# Ajouter du bruit et m√©langer\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(texts))\n",
    "texts = [texts[i] for i in indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "print(f\"Dataset cr√©√©: {len(texts)} exemples\")\n",
    "print(f\"Distribution: {np.bincount(labels)}\")\n",
    "print(\"\\nExemples de phrases complexes:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {texts[i]} ‚Üí {'Positif' if labels[i] else 'N√©gatif'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es\n",
    "max_words = 2000\n",
    "max_length = 30  # Plus long pour les phrases complexes\n",
    "\n",
    "# Tokenisation\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Conversion en s√©quences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "y = labels\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Donn√©es d'entra√Ænement: {X_train.shape}\")\n",
    "print(f\"Donn√©es de test: {X_test.shape}\")\n",
    "print(f\"Longueur moyenne des s√©quences: {np.mean([len(s) for s in sequences]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison Exp√©rimentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entra√Æner et √©valuer un mod√®le\n",
    "def train_and_evaluate(model, model_name, X_train, y_train, X_test, y_test, epochs=15):\n",
    "    \"\"\"\n",
    "    Entra√Æne et √©value un mod√®le.\n",
    "    \"\"\"\n",
    "    # Compilation\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=3, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    print(f\"\\nEntra√Ænement du mod√®le {model_name}...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # √âvaluation\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'name': model_name\n",
    "    }\n",
    "\n",
    "# Cr√©er et entra√Æner tous les mod√®les\n",
    "model_params = {\n",
    "    'vocab_size': max_words,\n",
    "    'embedding_dim': 100,\n",
    "    'hidden_dim': 128,\n",
    "    'max_length': max_length,\n",
    "    'num_classes': 2\n",
    "}\n",
    "\n",
    "model_creators = {\n",
    "    'RNN': create_rnn_model,\n",
    "    'LSTM': create_lstm_model,\n",
    "    'GRU': create_gru_model,\n",
    "    'Bidirectional LSTM': create_bidirectional_lstm,\n",
    "    'Stacked LSTM': create_stacked_lstm\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, creator in model_creators.items():\n",
    "    model = creator(**model_params)\n",
    "    result = train_and_evaluate(model, name, X_train, y_train, X_test, y_test)\n",
    "    results[name] = result\n",
    "    print(f\"{name}: Accuracy = {result['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des r√©sultats de comparaison\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"\n",
    "    Compare les performances des diff√©rents mod√®les.\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[name]['test_accuracy'] for name in model_names]\n",
    "    losses = [results[name]['test_loss'] for name in model_names]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "    bars1 = ax1.bar(model_names, accuracies, color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Test Accuracy')\n",
    "    ax1.set_title('Comparaison des Performances', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{acc:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # Loss comparison\n",
    "    bars2 = ax2.bar(model_names, losses, color=colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Test Loss')\n",
    "    ax2.set_title('Comparaison des Loss', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Ajouter les valeurs sur les barres\n",
    "    for bar, loss in zip(bars2, losses):\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{loss:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # Rotation des labels\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvolution de l'entra√Ænement pour les meilleurs mod√®les\n",
    "def plot_training_histories(results, models_to_plot=['LSTM', 'GRU', 'Bidirectional LSTM']):\n",
    "    \"\"\"\n",
    "    Affiche l'√©volution de l'entra√Ænement pour les mod√®les s√©lectionn√©s.\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    for i, model_name in enumerate(models_to_plot):\n",
    "        if model_name in results:\n",
    "            history = results[model_name]['history']\n",
    "            color = colors[i]\n",
    "            \n",
    "            # Training accuracy\n",
    "            ax1.plot(history.history['accuracy'], label=f'{model_name}', color=color, linewidth=2)\n",
    "            \n",
    "            # Validation accuracy\n",
    "            ax2.plot(history.history['val_accuracy'], label=f'{model_name}', color=color, linewidth=2)\n",
    "            \n",
    "            # Training loss\n",
    "            ax3.plot(history.history['loss'], label=f'{model_name}', color=color, linewidth=2)\n",
    "            \n",
    "            # Validation loss\n",
    "            ax4.plot(history.history['val_loss'], label=f'{model_name}', color=color, linewidth=2)\n",
    "    \n",
    "    # Configuration des axes\n",
    "    ax1.set_title('Training Accuracy', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.set_title('Validation Accuracy', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3.set_title('Training Loss', fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4.set_title('Validation Loss', fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('√âvolution de l\\'Entra√Ænement par Mod√®le', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_histories(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des Erreurs et Phrases Complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les pr√©dictions sur des phrases complexes\n",
    "def analyze_complex_predictions(models_dict, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Analyse les pr√©dictions des mod√®les sur des phrases complexes.\n",
    "    \"\"\"\n",
    "    # Phrases test sp√©cifiques\n",
    "    test_sentences = [\n",
    "        \"Au d√©but ce film semblait ennuyeux mais finalement c'√©tait absolument fantastique\",  # Positif complexe\n",
    "        \"J'avais de grandes attentes pour ce produit mais il s'est r√©v√©l√© tr√®s d√©cevant\",      # N√©gatif complexe\n",
    "        \"Malgr√© les critiques n√©gatives mon exp√©rience a √©t√© merveilleuse\",                   # Positif complexe\n",
    "        \"Bien que recommand√© par tous mes amis je trouve ce service catastrophique\",          # N√©gatif complexe\n",
    "        \"Excellent produit parfait\",                                                          # Positif simple\n",
    "        \"Tr√®s mauvais d√©cevant\"                                                               # N√©gatif simple\n",
    "    ]\n",
    "    \n",
    "    true_labels = [1, 0, 1, 0, 1, 0]  # 1=Positif, 0=N√©gatif\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "    X_test_complex = pad_sequences(sequences, maxlen=max_length)\n",
    "    \n",
    "    # S√©lectionner les meilleurs mod√®les\n",
    "    best_models = ['RNN', 'LSTM', 'GRU']\n",
    "    \n",
    "    results_analysis = []\n",
    "    \n",
    "    for sentence, true_label in zip(test_sentences, true_labels):\n",
    "        row = {'Phrase': sentence[:60] + '...', 'Vrai': 'Pos' if true_label else 'Neg'}\n",
    "        \n",
    "        for model_name in best_models:\n",
    "            if model_name in models_dict:\n",
    "                # Pr√©diction\n",
    "                idx = test_sentences.index(sentence)\n",
    "                pred_proba = models_dict[model_name]['model'].predict(\n",
    "                    X_test_complex[idx:idx+1], verbose=0\n",
    "                )[0]\n",
    "                pred_label = np.argmax(pred_proba)\n",
    "                confidence = pred_proba[pred_label]\n",
    "                \n",
    "                # Ajouter au r√©sultat\n",
    "                pred_text = 'Pos' if pred_label == 1 else 'Neg'\n",
    "                correct = '‚úì' if pred_label == true_label else '‚úó'\n",
    "                row[f'{model_name}'] = f'{pred_text} {correct} ({confidence:.2f})'\n",
    "        \n",
    "        results_analysis.append(row)\n",
    "    \n",
    "    # Cr√©er un DataFrame pour l'affichage\n",
    "    df_analysis = pd.DataFrame(results_analysis)\n",
    "    \n",
    "    # Affichage styl√©\n",
    "    print(\"Analyse des Pr√©dictions sur Phrases Complexes\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in df_analysis.iterrows():\n",
    "        print(f\"\\nüìù {row['Phrase']}\")\n",
    "        print(f\"   V√©rit√©: {row['Vrai']}\")\n",
    "        for model_name in best_models:\n",
    "            if model_name in row:\n",
    "                print(f\"   {model_name:4s}: {row[model_name]}\")\n",
    "    \n",
    "    return df_analysis\n",
    "\n",
    "# Effectuer l'analyse\n",
    "analysis_results = analyze_complex_predictions(results, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisation des √âtats Cach√©s LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un mod√®le LSTM qui retourne les √©tats cach√©s\n",
    "def create_lstm_with_states(vocab_size, embedding_dim, hidden_dim, max_length):\n",
    "    \"\"\"\n",
    "    Cr√©e un LSTM qui retourne tous les √©tats cach√©s et √©tats de cellule.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding\n",
    "    x = layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    \n",
    "    # LSTM avec return_sequences=True et return_state=True\n",
    "    lstm_out, final_h, final_c = layers.LSTM(\n",
    "        hidden_dim, \n",
    "        return_sequences=True, \n",
    "        return_state=True\n",
    "    )(x)\n",
    "    \n",
    "    # Classification\n",
    "    output = layers.Dense(2, activation='softmax')(final_h)\n",
    "    \n",
    "    # Mod√®le principal\n",
    "    main_model = models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    # Mod√®le pour extraire les √©tats\n",
    "    state_model = models.Model(inputs=inputs, outputs=[lstm_out, final_h, final_c])\n",
    "    \n",
    "    return main_model, state_model\n",
    "\n",
    "# Cr√©er le mod√®le avec √©tats\n",
    "_, lstm_state_model = create_lstm_with_states(\n",
    "    vocab_size=max_words,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=128,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# Copier les poids du LSTM entra√Æn√©\n",
    "lstm_trained = results['LSTM']['model']\n",
    "lstm_state_model.layers[1].set_weights(lstm_trained.layers[0].get_weights())  # Embedding\n",
    "lstm_state_model.layers[2].set_weights(lstm_trained.layers[1].get_weights())  # LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les √©tats LSTM pour une phrase complexe\n",
    "def visualize_lstm_states(text, model, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Visualise l'√©volution des √©tats cach√©s et √©tats de cellule LSTM.\n",
    "    \"\"\"\n",
    "    # Pr√©paration\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=max_length)\n",
    "    \n",
    "    # Obtenir les √©tats\n",
    "    all_hidden, final_h, final_c = model.predict(padded, verbose=0)\n",
    "    all_hidden = all_hidden[0]  # Premier (et seul) exemple du batch\n",
    "    \n",
    "    # Obtenir les mots de la s√©quence\n",
    "    words = []\n",
    "    for idx in sequence[0]:\n",
    "        for word, word_idx in tokenizer.word_index.items():\n",
    "            if word_idx == idx:\n",
    "                words.append(word)\n",
    "                break\n",
    "    \n",
    "    # Cr√©er les visualisations\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=[\n",
    "            f'√âtats Cach√©s LSTM - \"{text}\"',\n",
    "            'Norme des √âtats Cach√©s',\n",
    "            '√âtats de Cellule (√©chantillon)'\n",
    "        ],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 1. Heatmap des √©tats cach√©s\n",
    "    seq_length = min(len(words), all_hidden.shape[0])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=all_hidden[:seq_length].T,\n",
    "            x=words[:seq_length],\n",
    "            colorscale='RdBu',\n",
    "            showscale=True\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Norme des √©tats cach√©s\n",
    "    norms = np.linalg.norm(all_hidden[:seq_length], axis=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=words[:seq_length],\n",
    "            y=norms,\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='blue', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 3. Quelques dimensions des √©tats de cellule\n",
    "    for i in range(0, min(5, all_hidden.shape[1]), 2):  # Prendre quelques dimensions\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=words[:seq_length],\n",
    "                y=final_c[0][i:i+1].numpy() if i < len(final_c[0]) else [0] * seq_length,\n",
    "                mode='lines',\n",
    "                name=f'Cellule dim {i}',\n",
    "                line=dict(width=2)\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # Mise en forme\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Analyse des √âtats LSTM\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Rotation des labels x\n",
    "    for row in range(1, 4):\n",
    "        fig.update_xaxes(tickangle=45, row=row, col=1)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Tester sur une phrase complexe\n",
    "complex_sentence = \"Au d√©but ce film semblait ennuyeux mais finalement c'√©tait absolument fantastique\"\n",
    "visualize_lstm_states(complex_sentence, lstm_state_model, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Benchmarking Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tableau de comparaison complet\n",
    "def create_comprehensive_comparison(results):\n",
    "    \"\"\"\n",
    "    Cr√©e un tableau de comparaison complet des mod√®les.\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        model = result['model']\n",
    "        \n",
    "        # Calculer les m√©triques\n",
    "        total_params = model.count_params()\n",
    "        test_acc = result['test_accuracy']\n",
    "        test_loss = result['test_loss']\n",
    "        \n",
    "        # Estimer le temps d'entra√Ænement (bas√© sur les epochs r√©ellement utilis√©es)\n",
    "        epochs_used = len(result['history'].history['loss'])\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Mod√®le': name,\n",
    "            'Param√®tres': f\"{total_params:,}\",\n",
    "            'Test Accuracy': f\"{test_acc:.4f}\",\n",
    "            'Test Loss': f\"{test_loss:.4f}\",\n",
    "            'Epochs': epochs_used,\n",
    "            'Complexit√©': 'Faible' if 'RNN' in name and 'LSTM' not in name else \n",
    "                         '√âlev√©e' if 'Stacked' in name or 'Bidirectional' in name else 'Moyenne'\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Trier par accuracy\n",
    "    df_comparison['Accuracy_num'] = df_comparison['Test Accuracy'].astype(float)\n",
    "    df_comparison = df_comparison.sort_values('Accuracy_num', ascending=False)\n",
    "    df_comparison = df_comparison.drop('Accuracy_num', axis=1)\n",
    "    \n",
    "    # Affichage styl√©\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df_comparison.values,\n",
    "                    colLabels=df_comparison.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1.2, 3)\n",
    "    \n",
    "    # Style du header\n",
    "    for i in range(len(df_comparison.columns)):\n",
    "        table[(0, i)].set_facecolor('#1E90FF')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Colorer les lignes selon la performance\n",
    "    accuracies = [float(row[2]) for row in df_comparison.values]\n",
    "    max_acc = max(accuracies)\n",
    "    \n",
    "    for i, (_, row) in enumerate(df_comparison.iterrows(), 1):\n",
    "        accuracy = float(row['Test Accuracy'])\n",
    "        \n",
    "        if accuracy == max_acc:\n",
    "            color = '#E6FFE6'  # Vert clair pour le meilleur\n",
    "        elif accuracy >= max_acc - 0.02:\n",
    "            color = '#FFF8E6'  # Jaune clair pour les bons\n",
    "        else:\n",
    "            color = '#FFE6E6'  # Rouge clair pour les moins bons\n",
    "        \n",
    "        for j in range(len(df_comparison.columns)):\n",
    "            table[(i, j)].set_facecolor(color)\n",
    "    \n",
    "    plt.title('Comparaison Compl√®te des Mod√®les RNN/LSTM/GRU', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_comparison\n",
    "\n",
    "# Cr√©er la comparaison\n",
    "comparison_df = create_comprehensive_comparison(results)\n",
    "\n",
    "# Afficher les conclusions\n",
    "print(\"\\nüèÜ CONCLUSIONS DE L'EXP√âRIENCE\")\n",
    "print(\"=\" * 50)\n",
    "best_model = comparison_df.iloc[0]['Mod√®le']\n",
    "best_acc = comparison_df.iloc[0]['Test Accuracy']\n",
    "print(f\"ü•á Meilleur mod√®le: {best_model} (Accuracy: {best_acc})\")\n",
    "\n",
    "print(f\"\\nüìä Observations:\")\n",
    "print(f\"   ‚Ä¢ Les LSTM surpassent g√©n√©ralement les RNN vanilla\")\n",
    "print(f\"   ‚Ä¢ Les GRU offrent un bon compromis performance/complexit√©\")\n",
    "print(f\"   ‚Ä¢ Les mod√®les bidirectionnels et empil√©s peuvent am√©liorer les performances\")\n",
    "print(f\"   ‚Ä¢ Le co√ªt en param√®tres augmente significativement avec la complexit√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Application Avanc√©e : G√©n√©ration de Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un mod√®le de g√©n√©ration de texte avec LSTM\n",
    "def create_text_generation_model(vocab_size, embedding_dim, lstm_units, sequence_length):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le LSTM pour la g√©n√©ration de texte.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
    "        layers.LSTM(lstm_units, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(lstm_units),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Pr√©parer des donn√©es pour la g√©n√©ration (utiliser nos textes existants)\n",
    "def prepare_generation_data(texts, tokenizer, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Pr√©pare les donn√©es pour l'entra√Ænement de g√©n√©ration de texte.\n",
    "    \"\"\"\n",
    "    # Joindre tous les textes\n",
    "    full_text = ' '.join(texts)\n",
    "    \n",
    "    # Tokeniser\n",
    "    sequences = tokenizer.texts_to_sequences([full_text])[0]\n",
    "    \n",
    "    # Cr√©er les s√©quences d'entra√Ænement\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(sequences) - sequence_length):\n",
    "        X.append(sequences[i:i + sequence_length])\n",
    "        y.append(sequences[i + sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Fonction de g√©n√©ration de texte\n",
    "def generate_text(model, tokenizer, seed_text, num_words=10, sequence_length=10):\n",
    "    \"\"\"\n",
    "    G√©n√®re du texte √† partir d'un seed.\n",
    "    \"\"\"\n",
    "    # Tokeniser le seed\n",
    "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    \n",
    "    # S'assurer que la s√©quence a la bonne longueur\n",
    "    if len(seed_sequence) < sequence_length:\n",
    "        seed_sequence = [0] * (sequence_length - len(seed_sequence)) + seed_sequence\n",
    "    else:\n",
    "        seed_sequence = seed_sequence[-sequence_length:]\n",
    "    \n",
    "    generated = seed_sequence.copy()\n",
    "    \n",
    "    # Cr√©er un mapping inverse pour les mots\n",
    "    reverse_word_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Pr√©dire le prochain mot\n",
    "        input_seq = np.array([generated[-sequence_length:]])\n",
    "        predictions = model.predict(input_seq, verbose=0)[0]\n",
    "        \n",
    "        # √âchantillonner avec temp√©rature\n",
    "        temperature = 0.8\n",
    "        predictions = np.log(predictions + 1e-8) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Choisir le prochain mot\n",
    "        next_word_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        generated.append(next_word_idx)\n",
    "    \n",
    "    # Convertir en texte\n",
    "    generated_text = []\n",
    "    for idx in generated:\n",
    "        if idx in reverse_word_map:\n",
    "            generated_text.append(reverse_word_map[idx])\n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Cr√©er et entra√Æner un mini mod√®le de g√©n√©ration\n",
    "sequence_length_gen = 8\n",
    "X_gen, y_gen = prepare_generation_data(texts, tokenizer, sequence_length_gen)\n",
    "\n",
    "print(f\"Donn√©es de g√©n√©ration: {X_gen.shape}, {y_gen.shape}\")\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "gen_model = create_text_generation_model(\n",
    "    vocab_size=max_words,\n",
    "    embedding_dim=50,\n",
    "    lstm_units=64,\n",
    "    sequence_length=sequence_length_gen\n",
    ")\n",
    "\n",
    "gen_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Entra√Æner rapidement\n",
    "print(\"Entra√Ænement du mod√®le de g√©n√©ration...\")\n",
    "gen_model.fit(X_gen, y_gen, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Tester la g√©n√©ration\n",
    "seeds = [\"ce produit\", \"tr√®s bon\", \"je recommande\"]\n",
    "\n",
    "print(\"\\nüé® G√âN√âRATION DE TEXTE\")\n",
    "print(\"=\" * 40)\n",
    "for seed in seeds:\n",
    "    generated = generate_text(gen_model, tokenizer, seed, num_words=8, sequence_length=sequence_length_gen)\n",
    "    print(f\"Seed: '{seed}'\")\n",
    "    print(f\"G√©n√©r√©: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et Bonnes Pratiques\n",
    "\n",
    "### üèÜ R√©sultats de nos Exp√©riences\n",
    "\n",
    "√Ä travers ce notebook, nous avons explor√© les diff√©rences fondamentales entre RNN, LSTM et GRU :\n",
    "\n",
    "### üìä Points Cl√©s\n",
    "\n",
    "1. **LSTM vs RNN** : Les LSTM r√©solvent le probl√®me du gradient qui dispara√Æt gr√¢ce √† leurs portes\n",
    "2. **GRU vs LSTM** : Les GRU offrent des performances similaires avec moins de param√®tres\n",
    "3. **Mod√®les Bidirectionnels** : Am√©liorent les performances en regardant dans les deux directions\n",
    "4. **Mod√®les Empil√©s** : Peuvent capturer des patterns plus complexes au prix de plus de param√®tres\n",
    "\n",
    "### üõ†Ô∏è Bonnes Pratiques\n",
    "\n",
    "- **Commencer simple** : Tester d'abord un GRU simple avant les architectures complexes\n",
    "- **Dropout** : Utiliser dropout et recurrent_dropout pour √©viter l'overfitting\n",
    "- **Early Stopping** : Arr√™ter l'entra√Ænement quand la validation loss stagne\n",
    "- **Longueur des s√©quences** : Adapter max_length selon votre domaine\n",
    "- **Embedding pr√©-entra√Æn√©** : Utiliser Word2Vec/GloVe/FastText quand possible\n",
    "\n",
    "### ‚ö° Optimisations\n",
    "\n",
    "- **Batch Size** : Augmenter si vous avez assez de GPU/RAM\n",
    "- **Learning Rate** : Utiliser un scheduler pour ajuster dynamiquement\n",
    "- **Gradient Clipping** : √âviter l'explosion des gradients\n",
    "- **Mixed Precision** : Acc√©l√©rer l'entra√Ænement sur GPU modernes\n",
    "\n",
    "### üöÄ Prochaines √âtapes\n",
    "\n",
    "1. **Attention Mechanisms** : M√©canismes d'attention pour am√©liorer les performances\n",
    "2. **Transformers** : Architecture plus moderne qui remplace souvent les RNN\n",
    "3. **Transfer Learning** : Utiliser des mod√®les pr√©-entra√Æn√©s comme BERT\n",
    "4. **Applications Sp√©cifiques** : NER, POS tagging, traduction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le meilleur mod√®le\n",
    "best_model_name = comparison_df.iloc[0]['Mod√®le']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "best_model.save('best_lstm_gru_model.h5')\n",
    "print(f\"‚úÖ Meilleur mod√®le ({best_model_name}) sauvegard√© sous 'best_lstm_gru_model.h5'\")\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "import pickle\n",
    "with open('lstm_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"‚úÖ Tokenizer sauvegard√© sous 'lstm_tokenizer.pkl'\")\n",
    "\n",
    "print(\"\\nüéâ Notebook termin√© ! Vous ma√Ætrisez maintenant LSTM et GRU avec TensorFlow.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}