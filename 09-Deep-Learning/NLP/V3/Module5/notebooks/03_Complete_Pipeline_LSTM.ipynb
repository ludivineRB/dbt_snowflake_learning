{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Pipeline Complet : Du Texte Brut √† la Pr√©diction LSTM\n",
    "\n",
    "Ce notebook combine toutes les techniques apprises :\n",
    "- Nettoyage de texte (Module 2)\n",
    "- BOW et TF-IDF (Module 3)\n",
    "- Word Embeddings (Module 4)\n",
    "- LSTM pour la classification (Module 5)\n",
    "- Sauvegarde et r√©utilisation du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 1. Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Imports standards\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport pickle\nimport json\n\n# NLP et preprocessing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# T√©l√©charger les ressources NLTK n√©cessaires\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nprint(\"‚úÖ Toutes les biblioth√®ques sont import√©es !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Chargement et exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ons un dataset d'exemple pour la classification de sentiment\n",
    "# Dans la pratique, vous chargeriez vos propres donn√©es\n",
    "\n",
    "# Exemples de critiques de films (positives et n√©gatives)\n",
    "texts = [\n",
    "    \"This movie was absolutely fantastic! Best film I've seen all year.\",\n",
    "    \"Terrible waste of time. The plot made no sense at all.\",\n",
    "    \"Amazing performances by all actors. Highly recommend!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\",\n",
    "    \"A masterpiece of cinema. Every scene was perfectly crafted.\",\n",
    "    \"Worst movie ever. Complete disaster from start to finish.\",\n",
    "    \"Brilliant storytelling and stunning visuals. Oscar-worthy!\",\n",
    "    \"Disappointing sequel. Nothing like the original.\",\n",
    "    \"Incredible movie! Had me on the edge of my seat.\",\n",
    "    \"Total garbage. Want my money back.\",\n",
    "    \"Outstanding film with great character development.\",\n",
    "    \"Awful acting and terrible dialogue. Skip this one.\",\n",
    "    \"Exceptional movie that touches your heart.\",\n",
    "    \"Mediocre at best. Nothing special about it.\",\n",
    "    \"Phenomenal! Best movie of the decade.\",\n",
    "    \"Horrible experience. Left the theater early.\"\n",
    "]\n",
    "\n",
    "# Labels : 1 = positif, 0 = n√©gatif\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Cr√©er un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'sentiment': labels\n",
    "})\n",
    "\n",
    "print(f\"üìä Dataset cr√©√© avec {len(df)} exemples\")\n",
    "print(f\"\\nüîç Distribution des sentiments:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\nüìù Exemples de textes:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ 3. Nettoyage de texte (Module 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des outils\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Pipeline de nettoyage complet pour le texte\n",
    "    \"\"\"\n",
    "    # 1. Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Supprimer les URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # 3. Supprimer les mentions et hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # 4. Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 5. Supprimer les chiffres\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 6. Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 7. Supprimer les stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 8. Lemmatization ou Stemming\n",
    "    if use_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    else:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # 9. Rejoindre les tokens\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"üßπ Nettoyage termin√© !\")\n",
    "print(\"\\nüìù Comparaison avant/apr√®s :\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['text'].iloc[i]}\")\n",
    "    print(f\"Nettoy√©:  {df['cleaned_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Feature Engineering : BOW et TF-IDF (Module 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=100)\n",
    "bow_features = bow_vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "print(\"üéí Bag of Words:\")\n",
    "print(f\"Forme de la matrice BOW: {bow_features.shape}\")\n",
    "print(f\"Vocabulaire (10 premiers mots): {list(bow_vectorizer.vocabulary_.keys())[:10]}\")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "print(\"\\nüìà TF-IDF:\")\n",
    "print(f\"Forme de la matrice TF-IDF: {tfidf_features.shape}\")\n",
    "\n",
    "# Visualiser les mots les plus importants selon TF-IDF\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = np.mean(tfidf_features, axis=0)\n",
    "top_indices = np.argsort(tfidf_scores)[::-1][:10]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_words = [feature_names[i] for i in top_indices]\n",
    "top_scores = [tfidf_scores[i] for i in top_indices]\n",
    "plt.barh(top_words, top_scores)\n",
    "plt.xlabel('Score TF-IDF moyen')\n",
    "plt.title('Top 10 mots selon TF-IDF')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 5. Pr√©paration pour LSTM avec Embeddings (Module 4 & 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres pour le mod√®le LSTM\n",
    "MAX_WORDS = 1000  # Taille du vocabulaire\n",
    "MAX_SEQUENCE_LENGTH = 100  # Longueur maximale des s√©quences\n",
    "EMBEDDING_DIM = 100  # Dimension des embeddings\n",
    "\n",
    "# Tokenizer de Keras pour pr√©parer les s√©quences\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "# Convertir les textes en s√©quences d'indices\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "\n",
    "# Padding des s√©quences\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "y = np.array(df['sentiment'])\n",
    "\n",
    "print(f\"üî¢ Taille du vocabulaire: {len(tokenizer.word_index)}\")\n",
    "print(f\"üìè Forme des donn√©es X: {X.shape}\")\n",
    "print(f\"\\nüìù Exemple de conversion:\")\n",
    "print(f\"Texte original: {df['cleaned_text'].iloc[0]}\")\n",
    "print(f\"S√©quence: {sequences[0]}\")\n",
    "print(f\"Apr√®s padding: {X[0][:20]}...\")\n",
    "\n",
    "# Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 6. Construction du mod√®le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Construction du mod√®le LSTM\nmodel = Sequential([\n    # Couche d'embedding\n    Embedding(input_dim=MAX_WORDS, \n              output_dim=EMBEDDING_DIM,\n              name='embedding_layer'),\n    \n    # LSTM bidirectionnel pour capturer le contexte dans les deux sens\n    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)),\n    \n    # Deuxi√®me couche LSTM\n    LSTM(32, dropout=0.3, recurrent_dropout=0.3),\n    \n    # Couches denses\n    Dense(32, activation='relu'),\n    Dropout(0.5),\n    \n    # Couche de sortie\n    Dense(1, activation='sigmoid')\n])\n\n# Compilation\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Construire le mod√®le avec la forme des donn√©es\nmodel.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n\n# Afficher l'architecture\nmodel.summary()\n\n# Visualisation de l'architecture (apr√®s construction)\ntry:\n    tf.keras.utils.plot_model(model, to_file='lstm_model.png', show_shapes=True, show_layer_names=True)\n    print(\"‚úÖ Diagramme du mod√®le sauvegard√© dans 'lstm_model.png'\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Impossible de cr√©er le diagramme: {e}\")\n    print(\"Le mod√®le est quand m√™me pr√™t pour l'entra√Ænement !\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 7. Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks pour l'optimisation\n",
    "callbacks = [\n",
    "    # Early stopping pour √©viter l'overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # R√©duction du learning rate\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le\n",
    "    ModelCheckpoint(\n",
    "        'best_lstm_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entra√Ænement\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Visualisation de l'entra√Ænement\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Evolution de la Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Evolution de l\\'Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. √âvaluation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur le test set\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# M√©triques\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üéØ Accuracy sur le test set: {accuracy:.2%}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nüìä Rapport de classification:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['N√©gatif', 'Positif']))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['N√©gatif', 'Positif'],\n",
    "            yticklabels=['N√©gatif', 'Positif'])\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe pr√©dite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 9. Sauvegarde du mod√®le et des pr√©processeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sauvegarder le mod√®le Keras\n",
    "model.save('sentiment_lstm_model.h5')\n",
    "print(\"‚úÖ Mod√®le LSTM sauvegard√© dans 'sentiment_lstm_model.h5'\")\n",
    "\n",
    "# 2. Sauvegarder le tokenizer\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "print(\"‚úÖ Tokenizer sauvegard√© dans 'tokenizer.json'\")\n",
    "\n",
    "# 3. Sauvegarder les param√®tres du mod√®le\n",
    "model_config = {\n",
    "    'max_words': MAX_WORDS,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'model_type': 'LSTM_Bidirectional',\n",
    "    'task': 'sentiment_analysis'\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=4)\n",
    "print(\"‚úÖ Configuration sauvegard√©e dans 'model_config.json'\")\n",
    "\n",
    "# 4. Sauvegarder aussi les vectorizers (optionnel)\n",
    "with open('bow_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_vectorizer, f)\n",
    "    \n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "print(\"‚úÖ Vectorizers BOW et TF-IDF sauvegard√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ 10. Fonction de pr√©diction pour nouveaux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPredictor:\n",
    "    def __init__(self, model_path, tokenizer_path, config_path):\n",
    "        \"\"\"Charger le mod√®le et les pr√©processeurs\"\"\"\n",
    "        # Charger le mod√®le\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # Charger le tokenizer\n",
    "        with open(tokenizer_path, 'r', encoding='utf-8') as f:\n",
    "            tokenizer_json = f.read()\n",
    "        self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
    "        \n",
    "        # Charger la configuration\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "            \n",
    "        # Initialiser les outils de nettoyage\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Nettoyer le texte\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Pr√©dire le sentiment d'un texte\"\"\"\n",
    "        # Nettoyer le texte\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Convertir en s√©quence\n",
    "        sequence = self.tokenizer.texts_to_sequences([cleaned])\n",
    "        padded = pad_sequences(sequence, \n",
    "                             maxlen=self.config['max_sequence_length'],\n",
    "                             padding='post',\n",
    "                             truncating='post')\n",
    "        \n",
    "        # Pr√©diction\n",
    "        prediction = self.model.predict(padded, verbose=0)[0][0]\n",
    "        \n",
    "        # Interpr√©tation\n",
    "        sentiment = 'Positif' if prediction > 0.5 else 'N√©gatif'\n",
    "        confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'cleaned_text': cleaned,\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': float(confidence),\n",
    "            'raw_score': float(prediction)\n",
    "        }\n",
    "\n",
    "# Instancier le pr√©dicteur\n",
    "predictor = SentimentPredictor(\n",
    "    model_path='sentiment_lstm_model.h5',\n",
    "    tokenizer_path='tokenizer.json',\n",
    "    config_path='model_config.json'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pr√©dicteur charg√© et pr√™t !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 11. Test sur de nouveaux textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textes de test\n",
    "test_texts = [\n",
    "    \"This product is amazing! I love it so much.\",\n",
    "    \"Terrible quality. Complete waste of money.\",\n",
    "    \"Not bad, but could be better. Average product.\",\n",
    "    \"Absolutely fantastic! Exceeded all my expectations!\",\n",
    "    \"Disappointing. Does not work as advertised.\"\n",
    "]\n",
    "\n",
    "print(\"üîÆ Pr√©dictions sur de nouveaux textes:\\n\")\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predictor.predict(text)\n",
    "    print(f\"üìù Texte: '{text}'\")\n",
    "    print(f\"üßπ Nettoy√©: '{result['cleaned_text']}'\")\n",
    "    print(f\"üòä Sentiment: {result['sentiment']} (Confiance: {result['confidence']:.2%})\")\n",
    "    print(f\"üìä Score brut: {result['raw_score']:.3f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª 12. Interface interactive pour tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_sentiment_interactif():\n",
    "    \"\"\"Interface interactive pour tester le mod√®le\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ü§ñ Analyseur de Sentiment LSTM\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Entrez du texte pour analyser le sentiment (ou 'quit' pour arr√™ter)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nüí¨ Votre texte: \")\n",
    "        \n",
    "        if text.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nüëã Au revoir !\")\n",
    "            break\n",
    "        \n",
    "        if text.strip():\n",
    "            result = predictor.predict(text)\n",
    "            \n",
    "            # Affichage avec emojis selon le sentiment\n",
    "            emoji = \"üòä\" if result['sentiment'] == 'Positif' else \"üò¢\"\n",
    "            \n",
    "            print(f\"\\n{emoji} Sentiment: {result['sentiment']}\")\n",
    "            print(f\"üìä Confiance: {result['confidence']:.2%}\")\n",
    "            \n",
    "            # Barre de progression visuelle\n",
    "            bar_length = 30\n",
    "            filled = int(bar_length * result['raw_score'])\n",
    "            bar = \"‚ñà\" * filled + \"‚ñë\" * (bar_length - filled)\n",
    "            print(f\"\\nN√©gatif [{bar}] Positif\")\n",
    "            print(f\"Score: {result['raw_score']:.3f}\")\n",
    "\n",
    "# D√©commenter pour utiliser l'interface interactive\n",
    "# analyse_sentiment_interactif()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ 13. Script Python standalone pour utilisation future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un script Python r√©utilisable\n",
    "script_content = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script d'analyse de sentiment avec LSTM\n",
    "Utilisation: python sentiment_analyzer.py \"Votre texte ici\"\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Charger le mod√®le\n",
    "        self.model = load_model('sentiment_lstm_model.h5')\n",
    "        \n",
    "        # Charger le tokenizer\n",
    "        with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "            tokenizer_json = f.read()\n",
    "        self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
    "        \n",
    "        # Charger la config\n",
    "        with open('model_config.json', 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "            \n",
    "        # Outils NLP\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', '', text)\n",
    "        text = re.sub(r'@\\\\w+|#\\\\w+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\\\d+', '', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        sequence = self.tokenizer.texts_to_sequences([cleaned])\n",
    "        padded = pad_sequences(sequence, \n",
    "                             maxlen=self.config['max_sequence_length'],\n",
    "                             padding='post')\n",
    "        \n",
    "        prediction = self.model.predict(padded, verbose=0)[0][0]\n",
    "        sentiment = 'Positif' if prediction > 0.5 else 'N√©gatif'\n",
    "        confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "        \n",
    "        return sentiment, confidence, prediction\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python sentiment_analyzer.py \\\"Votre texte\\\"\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    text = ' '.join(sys.argv[1:])\n",
    "    \n",
    "    analyzer = SentimentAnalyzer()\n",
    "    sentiment, confidence, score = analyzer.analyze(text)\n",
    "    \n",
    "    emoji = \"üòä\" if sentiment == 'Positif' else \"üò¢\"\n",
    "    print(f\"\\n{emoji} Sentiment: {sentiment}\")\n",
    "    print(f\"üìä Confiance: {confidence:.2%}\")\n",
    "    print(f\"üìà Score: {score:.3f}\")\n",
    "'''\n",
    "\n",
    "# Sauvegarder le script\n",
    "with open('sentiment_analyzer.py', 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"‚úÖ Script standalone cr√©√© : 'sentiment_analyzer.py'\")\n",
    "print(\"\\nüìå Utilisation:\")\n",
    "print(\"   python sentiment_analyzer.py \\\"Votre texte √† analyser\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "F√©licitations ! Vous avez cr√©√© un pipeline complet de NLP qui combine :\n",
    "\n",
    "1. **Nettoyage de texte** (Module 2)\n",
    "2. **Feature engineering** avec BOW et TF-IDF (Module 3)\n",
    "3. **Word embeddings** (Module 4)\n",
    "4. **LSTM** pour la classification (Module 5)\n",
    "\n",
    "### üìÅ Fichiers sauvegard√©s :\n",
    "- `sentiment_lstm_model.h5` : Le mod√®le LSTM entra√Æn√©\n",
    "- `tokenizer.json` : Le tokenizer pour pr√©parer les textes\n",
    "- `model_config.json` : La configuration du mod√®le\n",
    "- `sentiment_analyzer.py` : Script Python standalone\n",
    "- `bow_vectorizer.pkl` et `tfidf_vectorizer.pkl` : Vectorizers (optionnel)\n",
    "\n",
    "### üöÄ Prochaines √©tapes :\n",
    "- Entra√Æner sur un dataset plus large\n",
    "- Essayer d'autres architectures (GRU, Transformers)\n",
    "- D√©ployer le mod√®le dans une API\n",
    "- Am√©liorer le preprocessing\n",
    "- Ajouter plus de classes (sentiment neutre, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}