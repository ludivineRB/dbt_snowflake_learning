---
title: 'Module 3 - Bag of Words : Concepts'
description: 'Formation NLP - Module 3 - Bag of Words : Concepts'
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# üéí Bag of Words : Concepts

Transformer le texte en nombres : la m√©thode fondamentale

[üè† Index Module 3](index.html) [‚Üê Introduction](module3_intro.html) [D√©monstrations ‚Üí](module3_bow_demo.html)

## üß† Le Concept Fondamental

### üéØ L'Id√©e Simple mais Puissante

Le **Bag of Words** (sac de mots) est la technique la plus intuitive pour vectoriser du texte :

#### **Principe Central :**

Compter combien de fois chaque mot appara√Æt dans chaque document

Document ‚Üí Vecteur de comptages

**üè† Analogie simple :**  
Imaginez que vous triez vos courses dans des paniers √©tiquet√©s.  
‚Ä¢ Chaque panier = un mot du vocabulaire  
‚Ä¢ Chaque produit dans le panier = une occurrence du mot  
‚Üí Votre liste de courses devient un vecteur de comptages !

### üìä Transformation Math√©matique

#### Exemple Concret

**Documents :**

*   Doc 1: "Le chat mange des croquettes"
*   Doc 2: "Le chien mange aussi des croquettes"
*   Doc 3: "Le chat boit de l'eau"

**Vocabulaire cr√©√© :**

le chat mange des croquettes chien aussi boit de l'eau

**Matrice Bag of Words :**

Document

le

chat

mange

des

croquettes

chien

aussi

boit

de

l'eau

**Doc 1**

1

1

1

1

1

0

0

0

0

0

**Doc 2**

1

0

1

1

1

1

1

0

0

0

**Doc 3**

1

1

0

0

0

0

0

1

1

1

## üî¢ Formalisation Math√©matique

### üìê Algorithme D√©taill√©

**√âtape 1 : Construction du Vocabulaire**  
V = {w‚ÇÅ, w‚ÇÇ, ..., w‚Çô} o√π n = |vocabulaire|

**Explication :** On collecte tous les mots uniques de tous les documents. Chaque mot distinct devient une dimension dans notre espace vectoriel. Si nous avons 10 000 mots uniques, nous travaillons dans un espace √† 10 000 dimensions !

**Exemple concret :**  
Corpus : \["Le chat dort", "Le chien mange", "Le chat mange"\]  
‚Üí V = {le, chat, dort, chien, mange} ‚Üí n = 5 dimensions

**√âtape 2 : Matrice de Comptage**  
BoW\[i,j\] = count(w‚±º in d·µ¢)  
o√π d·µ¢ = document i, w‚±º = mot j du vocabulaire

**Explication :** Pour chaque document (ligne i) et chaque mot du vocabulaire (colonne j), on compte combien de fois ce mot appara√Æt. Cela cr√©e une matrice documents √ó mots.

**Matrice r√©sultante :**

Doc\\Mot

le

chat

dort

chien

mange

Doc 1

1

1

1

0

0

Doc 2

1

0

0

1

1

Doc 3

1

1

0

0

1

### üìä Propri√©t√©s Math√©matiques

#### Dimension de l'Espace

dim(BoW) = |V| = nombre de mots uniques

**Implications :**  
‚Ä¢ Corpus anglais typique : 10 000 - 50 000 dimensions  
‚Ä¢ Wikipedia : > 1 000 000 dimensions  
‚Ä¢ Espace tr√®s sparse (creux) : 95-99% de z√©ros

#### Distance et Similarit√©

**Distance Euclidienne :**  
d(d‚ÇÅ, d‚ÇÇ) = ||BoW\[d‚ÇÅ\] - BoW\[d‚ÇÇ\]||‚ÇÇ  
  
**Similarit√© Cosinus :**  
sim(d‚ÇÅ, d‚ÇÇ) = (BoW\[d‚ÇÅ\] ¬∑ BoW\[d‚ÇÇ\]) / (||BoW\[d‚ÇÅ\]||‚ÇÇ √ó ||BoW\[d‚ÇÇ\]||‚ÇÇ)

La similarit√© cosinus mesure l'angle entre deux vecteurs (0 = orthogonaux, 1 = identiques). Plus adapt√©e que la distance euclidienne pour des documents de longueurs diff√©rentes.

## ‚öñÔ∏è Avantages vs Limitations

#### ‚úÖ Avantages

*   **Simplicit√© :** Facile √† comprendre et impl√©menter
*   **Rapidit√© :** Calcul tr√®s rapide
*   **Interpr√©tabilit√© :** R√©sultats explicables
*   **Baseline solide :** Point de d√©part efficace
*   **Peu de param√®tres :** Pas d'hyperparam√®tres complexes
*   **Robustesse :** Fonctionne sur tout type de texte

#### ‚ùå Limitations

*   **Perte d'ordre :** Ignore la s√©quence des mots
*   **Sparsit√© :** Beaucoup de z√©ros dans la matrice
*   **Dimension √©lev√©e :** Vocabulaire peut √™tre tr√®s grand
*   **Pas de s√©mantique :** "bon" ‚â† "excellent"
*   **Sensible au bruit :** Fautes de frappe probl√©matiques
*   **Mots fr√©quents :** "le", "de" dominent

### üéØ Quand Utiliser BoW ?

**‚úÖ Cas d'usage recommand√©s :**

*   **Classification de documents :** Spam/Ham, th√©matiques
*   **Analyse de sentiment basique :** Positif/N√©gatif
*   **Recherche par mots-cl√©s :** Moteurs de recherche simples
*   **Baseline pour comparaison :** Avant mod√®les complexes
*   **Datasets petits/moyens :** < 100k documents
*   **Applications temps r√©el :** Rapidit√© prioritaire

**‚ùå √âviter BoW pour :**

*   **Traduction automatique :** Ordre crucial
*   **G√©n√©ration de texte :** S√©quence n√©cessaire
*   **Analyse syntaxique :** Structure grammaticale
*   **Similarit√© s√©mantique :** Sens des mots important
*   **Textes longs :** Livres, articles complets

### Navigation

[üè† Index Module 3](index.html) [‚Üê Introduction](module3_intro.html) [D√©monstrations ‚Üí](module3_bow_demo.html) [üè† Accueil Formation](../index.html)
