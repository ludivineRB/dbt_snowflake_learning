{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âœï¸ Module 7.2 - GPT et GÃ©nÃ©ration de Texte avec TensorFlow\n",
    "\n",
    "## ğŸ¯ Objectifs\n",
    "- Comprendre l'architecture GPT et la gÃ©nÃ©ration autoregressive\n",
    "- ImplÃ©menter GPT from scratch avec TensorFlow\n",
    "- Fine-tuner GPT pour gÃ©nÃ©ration de contenu spÃ©cialisÃ©\n",
    "- Explorer les techniques de gÃ©nÃ©ration avancÃ©es\n",
    "\n",
    "## ğŸ“š Contenu\n",
    "1. **Architecture GPT** - DÃ©codeur Transformer autorÃ©gressif\n",
    "2. **GÃ©nÃ©ration autoregressive** - MÃ©canismes et techniques\n",
    "3. **Fine-tuning GPT** - Adaptation pour contenu spÃ©cialisÃ©\n",
    "4. **Techniques avancÃ©es** - Sampling, beam search, contrÃ´le\n",
    "5. **Applications pratiques** - Chatbot, gÃ©nÃ©ration crÃ©ative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Installation des dÃ©pendances\n",
    "!pip install tensorflow transformers datasets tokenizers matplotlib plotly openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š Imports\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"ğŸ”¥ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ¤— Transformers library loaded\")\n",
    "print(f\"ğŸ¯ GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 1. Architecture GPT - ImplÃ©mentation DÃ©taillÃ©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTCausalSelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"ğŸ‘ï¸ Attention causale (masquÃ©e) pour GPT\"\"\"\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "        \n",
    "        # ğŸ¯ Projections Q, K, V combinÃ©es pour efficacitÃ©\n",
    "        self.c_attn = tf.keras.layers.Dense(\n",
    "            3 * self.embed_dim, name=\"c_attn\"\n",
    "        )\n",
    "        self.c_proj = tf.keras.layers.Dense(\n",
    "            self.embed_dim, name=\"c_proj\"\n",
    "        )\n",
    "        \n",
    "        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # ğŸ­ Masque causal (triangulaire infÃ©rieur)\n",
    "        max_len = 1024  # Longueur max pour le masque\n",
    "        causal_mask = tf.linalg.band_part(\n",
    "            tf.ones((max_len, max_len)), -1, 0\n",
    "        )\n",
    "        self.causal_mask = tf.Variable(\n",
    "            causal_mask, trainable=False, name=\"causal_mask\"\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        batch_size, seq_len, embed_dim = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        \n",
    "        # ğŸ¯ Calcul Q, K, V\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)\n",
    "        \n",
    "        # ğŸ”„ Reshape pour multi-head\n",
    "        q = tf.reshape(q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        k = tf.reshape(k, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        v = tf.reshape(v, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        \n",
    "        q = tf.transpose(q, [0, 2, 1, 3])  # (batch, heads, seq, head_dim)\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # ğŸ“Š Scores d'attention\n",
    "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_scores = attn_scores / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        \n",
    "        # ğŸ­ Application du masque causal\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        attn_scores = attn_scores * mask + (1.0 - mask) * (-1e9)\n",
    "        \n",
    "        # ğŸ”¥ Softmax\n",
    "        attn_probs = tf.nn.softmax(attn_scores, axis=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs, training=training)\n",
    "        \n",
    "        # ğŸ¯ Application attention\n",
    "        attn_output = tf.matmul(attn_probs, v)\n",
    "        attn_output = tf.transpose(attn_output, [0, 2, 1, 3])\n",
    "        attn_output = tf.reshape(attn_output, (batch_size, seq_len, embed_dim))\n",
    "        \n",
    "        # ğŸ”„ Projection finale\n",
    "        output = self.c_proj(attn_output)\n",
    "        output = self.resid_dropout(output, training=training)\n",
    "        \n",
    "        return output, attn_probs\n",
    "\n",
    "print(\"âœ… GPTCausalSelfAttention implÃ©mentÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"ğŸ—ï¸ Bloc Transformer GPT complet\"\"\"\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attn = GPTCausalSelfAttention(config)\n",
    "        \n",
    "        # ğŸ”„ Layer Normalization (prÃ©-attention)\n",
    "        self.ln_1 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=config.layer_norm_epsilon, name=\"ln_1\"\n",
    "        )\n",
    "        \n",
    "        # ğŸ§  Feed Forward Network\n",
    "        self.mlp_c_fc = tf.keras.layers.Dense(\n",
    "            4 * config.hidden_size, activation=\"gelu\", name=\"mlp_c_fc\"\n",
    "        )\n",
    "        self.mlp_c_proj = tf.keras.layers.Dense(\n",
    "            config.hidden_size, name=\"mlp_c_proj\"\n",
    "        )\n",
    "        self.mlp_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n",
    "        \n",
    "        # ğŸ”„ Layer Normalization (prÃ©-MLP)\n",
    "        self.ln_2 = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=config.layer_norm_epsilon, name=\"ln_2\"\n",
    "        )\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # ğŸ‘ï¸ Self-attention avec connexion rÃ©siduelle\n",
    "        attn_output, attn_probs = self.attn(self.ln_1(x), training=training)\n",
    "        x = x + attn_output\n",
    "        \n",
    "        # ğŸ§  MLP avec connexion rÃ©siduelle  \n",
    "        mlp_output = self.mlp_c_fc(self.ln_2(x))\n",
    "        mlp_output = self.mlp_c_proj(mlp_output)\n",
    "        mlp_output = self.mlp_dropout(mlp_output, training=training)\n",
    "        x = x + mlp_output\n",
    "        \n",
    "        return x, attn_probs\n",
    "\n",
    "print(\"âœ… GPTBlock implÃ©mentÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT(tf.keras.Model):\n",
    "    \"\"\"ğŸ¤– ModÃ¨le GPT simplifiÃ© pour dÃ©monstration\"\"\"\n",
    "    \n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        \n",
    "        # ğŸ“ Embeddings\n",
    "        self.wte = tf.keras.layers.Embedding(\n",
    "            config.vocab_size, config.hidden_size, name=\"wte\"\n",
    "        )\n",
    "        self.wpe = tf.keras.layers.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size, name=\"wpe\"\n",
    "        )\n",
    "        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n",
    "        \n",
    "        # ğŸ—ï¸ Couches Transformer\n",
    "        self.h = [GPTBlock(config, name=f\"h_{i}\") \n",
    "                  for i in range(config.num_hidden_layers)]\n",
    "        \n",
    "        # ğŸ”„ Layer norm finale\n",
    "        self.ln_f = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=config.layer_norm_epsilon, name=\"ln_f\"\n",
    "        )\n",
    "        \n",
    "        # ğŸ“Š TÃªte de gÃ©nÃ©ration (Language Modeling Head)\n",
    "        self.lm_head = tf.keras.layers.Dense(\n",
    "            config.vocab_size, use_bias=False, name=\"lm_head\"\n",
    "        )\n",
    "    \n",
    "    def call(self, input_ids, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        \n",
    "        # ğŸ“ Position IDs\n",
    "        position_ids = tf.range(seq_len, dtype=tf.int32)[tf.newaxis, :]\n",
    "        \n",
    "        # ğŸ“ Embeddings\n",
    "        token_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        \n",
    "        hidden_states = token_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states, training=training)\n",
    "        \n",
    "        # ğŸ—ï¸ Passage dans les couches Transformer\n",
    "        attention_weights = []\n",
    "        for layer in self.h:\n",
    "            hidden_states, attn_probs = layer(hidden_states, training=training)\n",
    "            attention_weights.append(attn_probs)\n",
    "        \n",
    "        # ğŸ”„ Layer norm finale\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        \n",
    "        # ğŸ“Š GÃ©nÃ©ration des logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "print(\"âœ… SimpleGPT implÃ©mentÃ©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 2. Chargement et Configuration de GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– Chargement de GPT-2 prÃ©-entraÃ®nÃ©\n",
    "model_name = \"gpt2\"  # Ou \"gpt2-medium\", \"gpt2-large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# ğŸ”§ Configuration du tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"ğŸ¤– ModÃ¨le GPT-2: {model_name}\")\n",
    "print(f\"ğŸ“ Vocabulaire: {len(tokenizer)} tokens\")\n",
    "print(f\"ğŸ“Š ParamÃ¨tres: {model.count_params():,}\")\n",
    "print(f\"ğŸ¯ Max longueur: {tokenizer.model_max_length}\")\n",
    "\n",
    "# ğŸ“Š Configuration dÃ©taillÃ©e\n",
    "config = model.config\n",
    "print(f\"\\nğŸ—ï¸ Architecture GPT-2:\")\n",
    "print(f\"  - Couches: {config.n_layer}\")\n",
    "print(f\"  - TÃªtes attention: {config.n_head}\")\n",
    "print(f\"  - Dimension cachÃ©e: {config.n_embd}\")\n",
    "print(f\"  - Contexte max: {config.n_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœï¸ 3. GÃ©nÃ©ration de Texte - Techniques et StratÃ©gies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=100, temperature=0.7, \n",
    "                  top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"âœï¸ GÃ©nÃ©ration de texte avec contrÃ´les avancÃ©s\"\"\"\n",
    "    \n",
    "    # ğŸ”„ Tokenisation du prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    \n",
    "    # âš™ï¸ Configuration de gÃ©nÃ©ration\n",
    "    generation_config = {\n",
    "        'max_length': max_length,\n",
    "        'temperature': temperature,\n",
    "        'top_k': top_k,\n",
    "        'top_p': top_p,\n",
    "        'repetition_penalty': repetition_penalty,\n",
    "        'do_sample': True,\n",
    "        'pad_token_id': tokenizer.eos_token_id,\n",
    "        'eos_token_id': tokenizer.eos_token_id,\n",
    "        'return_dict_in_generate': True,\n",
    "        'output_scores': True\n",
    "    }\n",
    "    \n",
    "    # ğŸš€ GÃ©nÃ©ration\n",
    "    with tf.device('/CPU:0'):  # CPU pour Ã©viter les problÃ¨mes de mÃ©moire\n",
    "        outputs = model.generate(input_ids, **generation_config)\n",
    "    \n",
    "    # ğŸ“ DÃ©codage\n",
    "    generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'generated_text': generated_text,\n",
    "        'new_text': generated_text[len(prompt):],\n",
    "        'total_length': len(outputs.sequences[0]),\n",
    "        'config': generation_config\n",
    "    }\n",
    "\n",
    "# ğŸ§ª Test de gÃ©nÃ©ration\n",
    "prompt = \"L'intelligence artificielle va rÃ©volutionner\"\n",
    "result = generate_text(prompt, model, tokenizer, max_length=150)\n",
    "\n",
    "print(\"ğŸ§ª Test de gÃ©nÃ©ration GPT-2:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“ Prompt: {result['prompt']}\")\n",
    "print(f\"âœï¸ Texte gÃ©nÃ©rÃ©: {result['new_text']}\")\n",
    "print(f\"ğŸ“Š Longueur totale: {result['total_length']} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ Comparaison des techniques de gÃ©nÃ©ration\n",
    "def compare_generation_strategies(prompt, model, tokenizer):\n",
    "    \"\"\"âš–ï¸ Comparaison de diffÃ©rentes stratÃ©gies de gÃ©nÃ©ration\"\"\"\n",
    "    \n",
    "    strategies = [\n",
    "        {'name': 'Greedy', 'params': {'temperature': 0.1, 'top_k': 1, 'top_p': 1.0}},\n",
    "        {'name': 'Low Temp', 'params': {'temperature': 0.3, 'top_k': 50, 'top_p': 0.95}},\n",
    "        {'name': 'Balanced', 'params': {'temperature': 0.7, 'top_k': 50, 'top_p': 0.95}},\n",
    "        {'name': 'Creative', 'params': {'temperature': 1.0, 'top_k': 100, 'top_p': 0.9}},\n",
    "        {'name': 'Very Creative', 'params': {'temperature': 1.3, 'top_k': 0, 'top_p': 0.8}}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        result = generate_text(\n",
    "            prompt, model, tokenizer, \n",
    "            max_length=100,\n",
    "            **strategy['params']\n",
    "        )\n",
    "        results.append({\n",
    "            'strategy': strategy['name'],\n",
    "            'text': result['new_text'],\n",
    "            'params': strategy['params']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ğŸ§ª Test des stratÃ©gies\n",
    "prompt = \"Dans le futur, les robots\"\n",
    "comparison_results = compare_generation_strategies(prompt, model, tokenizer)\n",
    "\n",
    "print(\"âš–ï¸ Comparaison des stratÃ©gies de gÃ©nÃ©ration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“ Prompt: {prompt}\")\n",
    "print()\n",
    "\n",
    "for i, result in enumerate(comparison_results, 1):\n",
    "    print(f\"ğŸ¯ StratÃ©gie {i}: {result['strategy']}\")\n",
    "    print(f\"   ParamÃ¨tres: {result['params']}\")\n",
    "    print(f\"   Texte: {result['text'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ 4. Visualisation des Patterns d'Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(text, model, tokenizer, layer_idx=5, head_idx=0):\n",
    "    \"\"\"ğŸ¨ Visualisation des patterns d'attention GPT\"\"\"\n",
    "    \n",
    "    # ğŸ”„ Tokenisation\n",
    "    inputs = tokenizer(text, return_tensors='tf', max_length=50, truncation=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # ğŸ¯ Forward pass avec attention\n",
    "    outputs = model(inputs['input_ids'], output_attentions=True)\n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    # ğŸ“Š Extraction de la matrice d'attention\n",
    "    attention_matrix = attentions[layer_idx][0, head_idx].numpy()\n",
    "    \n",
    "    # ğŸ¨ Visualisation interactive\n",
    "    fig = px.imshow(\n",
    "        attention_matrix,\n",
    "        x=tokens,\n",
    "        y=tokens,\n",
    "        title=f\"ğŸ¯ Attention Patterns - Couche {layer_idx}, TÃªte {head_idx}\",\n",
    "        labels=dict(x=\"Tokens (To)\", y=\"Tokens (From)\", color=\"Attention\"),\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis={'side': 'top'},\n",
    "        font_size=10\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return attention_matrix, tokens\n",
    "\n",
    "# ğŸ§ª Test de visualisation\n",
    "test_text = \"The artificial intelligence will transform our society in many ways\"\n",
    "attention_matrix, tokens = visualize_attention_patterns(test_text, model, tokenizer)\n",
    "\n",
    "print(f\"ğŸ¨ Visualisation pour: {test_text}\")\n",
    "print(f\"ğŸ“Š Matrice d'attention: {attention_matrix.shape}\")\n",
    "print(f\"ğŸ”¤ Tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 5. Fine-tuning GPT pour GÃ©nÃ©ration SpÃ©cialisÃ©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PrÃ©paration d'un dataset pour fine-tuning\n",
    "# Exemple: gÃ©nÃ©ration de descriptions de produits\n",
    "\n",
    "def create_product_descriptions_dataset():\n",
    "    \"\"\"ğŸ“¦ CrÃ©ation d'un dataset synthÃ©tique pour descriptions produits\"\"\"\n",
    "    \n",
    "    products = [\n",
    "        \"Smartphone Samsung Galaxy S23 Ultra 256GB\",\n",
    "        \"MacBook Pro 16 pouces M2 Max\", \n",
    "        \"Casque audio Sony WH-1000XM5\",\n",
    "        \"Nike Air Max 270 React\",\n",
    "        \"Parfum Chanel NÂ°5 Eau de Toilette\",\n",
    "        \"Montre Apple Watch Series 8\",\n",
    "        \"Cuisine Ã©quipÃ©e IKEA moderne\",\n",
    "        \"Tesla Model 3 Performance\"\n",
    "    ]\n",
    "    \n",
    "    descriptions = [\n",
    "        \"Un smartphone rÃ©volutionnaire avec appareil photo 200MP, Ã©cran Dynamic AMOLED et processeur Snapdragon ultra-puissant. Parfait pour la photographie professionnelle et les performances gaming.\",\n",
    "        \"L'ordinateur portable le plus puissant d'Apple avec puce M2 Max, Ã©cran Liquid Retina XDR et autonomie exceptionnelle. IdÃ©al pour les crÃ©atifs et professionnels.\",\n",
    "        \"Casque Ã  rÃ©duction de bruit leader du marchÃ© offrant une qualitÃ© audio exceptionnelle et un confort toute la journÃ©e. Technology LDAC pour audio haute rÃ©solution.\",\n",
    "        \"Chaussures de sport alliant style et performance avec technologie React pour un amorti optimal. Design moderne et coloris tendance pour tous styles.\",\n",
    "        \"Le parfum iconique franÃ§ais aux notes florales aldehydÃ©es. Une fragrance intemporelle qui incarne l'Ã©lÃ©gance et la sophistication fÃ©minine.\",\n",
    "        \"Montre connectÃ©e la plus avancÃ©e avec capteurs de santÃ© complets, GPS intÃ©grÃ© et rÃ©sistance Ã  l'eau. Compatible avec tout l'Ã©cosystÃ¨me Apple.\",\n",
    "        \"Solution cuisine complÃ¨te avec Ã©lectromÃ©nager intÃ©grÃ©, plan de travail quartz et rangements optimisÃ©s. Design scandinave et fonctionnalitÃ© maximale.\",\n",
    "        \"VÃ©hicule Ã©lectrique haute performance avec autonomie 500km, accÃ©lÃ©ration 0-100 en 3.3s et conduite autonome. L'avenir de la mobilitÃ© premium.\"\n",
    "    ]\n",
    "    \n",
    "    # ğŸ”„ Format pour fine-tuning\n",
    "    training_data = []\n",
    "    for product, description in zip(products, descriptions):\n",
    "        # Format: \"Produit: [PRODUIT] Description: [DESCRIPTION]\"\n",
    "        text = f\"Produit: {product}\\nDescription: {description}<|endoftext|>\"\n",
    "        training_data.append(text)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# ğŸ“¦ CrÃ©ation du dataset\n",
    "training_texts = create_product_descriptions_dataset()\n",
    "\n",
    "print(\"ğŸ“¦ Dataset de descriptions produits crÃ©Ã©:\")\n",
    "print(f\"ğŸ“Š Nombre d'exemples: {len(training_texts)}\")\n",
    "print(\"\\nğŸ“ Exemple:\")\n",
    "print(training_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Fine-tuning GPT-2 (version simplifiÃ©e pour dÃ©monstration)\n",
    "def prepare_fine_tuning_data(texts, tokenizer, max_length=512):\n",
    "    \"\"\"ğŸ”„ PrÃ©paration des donnÃ©es pour fine-tuning\"\"\"\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'][0])\n",
    "        attention_masks.append(encoded['attention_mask'][0])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tf.stack(input_ids),\n",
    "        'attention_mask': tf.stack(attention_masks)\n",
    "    }\n",
    "\n",
    "# ğŸ”„ PrÃ©paration donnÃ©es\n",
    "fine_tune_data = prepare_fine_tuning_data(training_texts, tokenizer, max_length=256)\n",
    "\n",
    "print(\"ğŸ”„ DonnÃ©es de fine-tuning prÃ©parÃ©es:\")\n",
    "print(f\"ğŸ“Š Shape input_ids: {fine_tune_data['input_ids'].shape}\")\n",
    "print(f\"ğŸ“Š Shape attention_mask: {fine_tune_data['attention_mask'].shape}\")\n",
    "\n",
    "# âš™ï¸ Configuration pour fine-tuning\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=5e-5,  # Learning rate plus Ã©levÃ© que BERT\n",
    "    epsilon=1e-08\n",
    ")\n",
    "\n",
    "# ğŸ“Š Loss pour language modeling\n",
    "def compute_loss(labels, logits):\n",
    "    \"\"\"ğŸ“Š Calcul de la loss pour language modeling\"\"\"\n",
    "    # Shift labels for causal LM\n",
    "    shift_labels = labels[..., 1:]\n",
    "    shift_logits = logits[..., :-1, :]\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none'\n",
    "    )\n",
    "    \n",
    "    loss = loss_fn(shift_labels, shift_logits)\n",
    "    \n",
    "    # Masquer les tokens de padding\n",
    "    mask = tf.cast(shift_labels != tokenizer.pad_token_id, tf.float32)\n",
    "    loss = loss * mask\n",
    "    \n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "print(\"âœ… Configuration fine-tuning prÃªte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 6. Applications Pratiques - GÃ©nÃ©ration ContrÃ´lÃ©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_description(product_name, model, tokenizer):\n",
    "    \"\"\"ğŸ“¦ GÃ©nÃ©rateur de descriptions produits\"\"\"\n",
    "    \n",
    "    prompt = f\"Produit: {product_name}\\nDescription:\"\n",
    "    \n",
    "    result = generate_text(\n",
    "        prompt, model, tokenizer,\n",
    "        max_length=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    \n",
    "    # ğŸ” Extraction de la description\n",
    "    full_text = result['generated_text']\n",
    "    if \"Description:\" in full_text:\n",
    "        description = full_text.split(\"Description:\")[1].strip()\n",
    "        # Nettoyer et limiter\n",
    "        description = description.split('\\n')[0]  # PremiÃ¨re ligne\n",
    "        description = description.replace('<|endoftext|>', '').strip()\n",
    "    else:\n",
    "        description = \"Description non gÃ©nÃ©rÃ©e\"\n",
    "    \n",
    "    return {\n",
    "        'product': product_name,\n",
    "        'description': description,\n",
    "        'prompt': prompt,\n",
    "        'full_generation': full_text\n",
    "    }\n",
    "\n",
    "# ğŸ§ª Tests de gÃ©nÃ©ration de descriptions\n",
    "test_products = [\n",
    "    \"iPhone 15 Pro Max 512GB\",\n",
    "    \"Gaming Chair DXRacer Formula\",\n",
    "    \"Machine Ã  cafÃ© Nespresso Vertuo\",\n",
    "    \"Drone DJI Mini 3 Pro\",\n",
    "    \"Guitare Ã©lectrique Fender Stratocaster\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ GÃ©nÃ©ration de descriptions produits:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, product in enumerate(test_products, 1):\n",
    "    result = generate_product_description(product, model, tokenizer)\n",
    "    print(f\"\\nğŸ›ï¸ Produit {i}: {result['product']}\")\n",
    "    print(f\"ğŸ“ Description: {result['description']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creative_story_generator(theme, style, model, tokenizer):\n",
    "    \"\"\"ğŸ“š GÃ©nÃ©rateur d'histoires crÃ©atives\"\"\"\n",
    "    \n",
    "    style_prompts = {\n",
    "        \"science-fiction\": \"Dans un futur lointain oÃ¹ la technologie a transformÃ© l'humanitÃ©,\",\n",
    "        \"fantasy\": \"Dans un royaume magique peuplÃ© de crÃ©atures fantastiques,\",\n",
    "        \"mystery\": \"Par une nuit sombre et pluvieuse, l'inspecteur dÃ©couvrit un mystÃ¨re troublant:\",\n",
    "        \"romance\": \"C'Ã©tait un matin de printemps quand leurs regards se croisÃ¨rent pour la premiÃ¨re fois,\",\n",
    "        \"adventure\": \"L'aventure commenÃ§a quand ils trouvÃ¨rent la carte ancienne dans le grenier,\"\n",
    "    }\n",
    "    \n",
    "    base_prompt = style_prompts.get(style, \"Il Ã©tait une fois,\")\n",
    "    prompt = f\"{base_prompt} {theme}\"\n",
    "    \n",
    "    # ğŸ¨ ParamÃ¨tres crÃ©atifs\n",
    "    result = generate_text(\n",
    "        prompt, model, tokenizer,\n",
    "        max_length=300,\n",
    "        temperature=0.8,  # Plus crÃ©atif\n",
    "        top_p=0.9,\n",
    "        top_k=80,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'theme': theme,\n",
    "        'style': style,\n",
    "        'story': result['generated_text'],\n",
    "        'word_count': len(result['generated_text'].split())\n",
    "    }\n",
    "\n",
    "# ğŸ­ Tests de gÃ©nÃ©ration crÃ©ative\n",
    "creative_tests = [\n",
    "    {\"theme\": \"un robot qui dÃ©couvre l'amour\", \"style\": \"science-fiction\"},\n",
    "    {\"theme\": \"un dragon qui a peur du feu\", \"style\": \"fantasy\"},\n",
    "    {\"theme\": \"une disparition mystÃ©rieuse dans un petit village\", \"style\": \"mystery\"}\n",
    "]\n",
    "\n",
    "print(\"ğŸ­ GÃ©nÃ©ration d'histoires crÃ©atives:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test in enumerate(creative_tests, 1):\n",
    "    story = creative_story_generator(test['theme'], test['style'], model, tokenizer)\n",
    "    print(f\"\\nğŸ“š Histoire {i} ({story['style']})\")\n",
    "    print(f\"ğŸ¯ ThÃ¨me: {story['theme']}\")\n",
    "    print(f\"ğŸ“ DÃ©but: {story['story'][:200]}...\")\n",
    "    print(f\"ğŸ“Š Mots: {story['word_count']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ 7. ContrÃ´le AvancÃ© de la GÃ©nÃ©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generation_quality(texts, original_prompt):\n",
    "    \"\"\"ğŸ“Š Analyse de la qualitÃ© des gÃ©nÃ©rations\"\"\"\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        # ğŸ“ MÃ©triques basiques\n",
    "        word_count = len(text.split())\n",
    "        char_count = len(text)\n",
    "        avg_word_length = np.mean([len(word) for word in text.split()])\n",
    "        \n",
    "        # ğŸ”„ RÃ©pÃ©tition de mots\n",
    "        words = text.lower().split()\n",
    "        unique_words = len(set(words))\n",
    "        repetition_ratio = unique_words / len(words) if words else 0\n",
    "        \n",
    "        # ğŸ“ CohÃ©rence avec le prompt\n",
    "        prompt_words = set(original_prompt.lower().split())\n",
    "        text_words = set(words)\n",
    "        coherence = len(prompt_words & text_words) / len(prompt_words) if prompt_words else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'index': i,\n",
    "            'word_count': word_count,\n",
    "            'char_count': char_count,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'repetition_ratio': repetition_ratio,\n",
    "            'coherence': coherence,\n",
    "            'text': text[:100] + \"...\" if len(text) > 100 else text\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ğŸ§ª Test avec diffÃ©rentes tempÃ©ratures\n",
    "prompt = \"L'avenir de l'intelligence artificielle\"\n",
    "temperatures = [0.3, 0.7, 1.0, 1.3]\n",
    "\n",
    "generation_results = []\n",
    "for temp in temperatures:\n",
    "    result = generate_text(\n",
    "        prompt, model, tokenizer,\n",
    "        max_length=150,\n",
    "        temperature=temp,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    generation_results.append(result['new_text'])\n",
    "\n",
    "# ğŸ“Š Analyse des rÃ©sultats\n",
    "quality_metrics = analyze_generation_quality(generation_results, prompt)\n",
    "\n",
    "print(\"ğŸ“Š Analyse de qualitÃ© selon la tempÃ©rature:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (temp, metrics) in enumerate(zip(temperatures, quality_metrics)):\n",
    "    print(f\"\\nğŸŒ¡ï¸ TempÃ©rature: {temp}\")\n",
    "    print(f\"  ğŸ“ Mots: {metrics['word_count']}\")\n",
    "    print(f\"  ğŸ”„ Ratio diversitÃ©: {metrics['repetition_ratio']:.2f}\")\n",
    "    print(f\"  ğŸ¯ CohÃ©rence: {metrics['coherence']:.2f}\")\n",
    "    print(f\"  ğŸ“– Texte: {metrics['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ Visualisation des mÃ©triques de qualitÃ©\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ“Š CrÃ©ation du DataFrame\n",
    "df_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'Temperature': temp,\n",
    "        'Word Count': metrics['word_count'],\n",
    "        'Diversity Ratio': metrics['repetition_ratio'],\n",
    "        'Coherence': metrics['coherence'],\n",
    "        'Avg Word Length': metrics['avg_word_length']\n",
    "    }\n",
    "    for temp, metrics in zip(temperatures, quality_metrics)\n",
    "])\n",
    "\n",
    "# ğŸ“ˆ Graphiques interactifs\n",
    "fig = go.Figure()\n",
    "\n",
    "# DiversitÃ©\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_metrics['Temperature'],\n",
    "    y=df_metrics['Diversity Ratio'],\n",
    "    mode='lines+markers',\n",
    "    name='DiversitÃ©',\n",
    "    line=dict(color='blue', width=3),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "# CohÃ©rence\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_metrics['Temperature'],\n",
    "    y=df_metrics['Coherence'],\n",
    "    mode='lines+markers',\n",
    "    name='CohÃ©rence',\n",
    "    line=dict(color='red', width=3),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ğŸ“Š Impact de la TempÃ©rature sur la QualitÃ© de GÃ©nÃ©ration\",\n",
    "    xaxis_title=\"TempÃ©rature\",\n",
    "    yaxis_title=\"Score (0-1)\",\n",
    "    hovermode='x unified',\n",
    "    width=800,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"ğŸ“Š Observations:\")\n",
    "print(f\"  ğŸ¯ Meilleure diversitÃ© Ã  T={temperatures[df_metrics['Diversity Ratio'].idxmax()]}\")\n",
    "print(f\"  ğŸ¯ Meilleure cohÃ©rence Ã  T={temperatures[df_metrics['Coherence'].idxmax()]}\")\n",
    "print(f\"  ğŸ’¡ Compromis optimal autour de T=0.7-1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion et Applications Pratiques\n",
    "\n",
    "### âœ… Ce que nous avons accompli:\n",
    "1. **Architecture GPT** - ImplÃ©mentation dÃ©taillÃ©e avec attention causale\n",
    "2. **GÃ©nÃ©ration autoregressive** - Techniques et paramÃ¨tres de contrÃ´le\n",
    "3. **Fine-tuning** - Adaptation pour gÃ©nÃ©ration spÃ©cialisÃ©e\n",
    "4. **Visualisation** - Patterns d'attention et mÃ©triques de qualitÃ©\n",
    "5. **Applications** - GÃ©nÃ©ration crÃ©ative et contrÃ´lÃ©e\n",
    "\n",
    "### ğŸ¯ Techniques de gÃ©nÃ©ration maÃ®trisÃ©es:\n",
    "- **Temperature control** - Balance crÃ©ativitÃ©/cohÃ©rence\n",
    "- **Top-k/Top-p sampling** - ContrÃ´le de la diversitÃ©\n",
    "- **Repetition penalty** - Ã‰viter les rÃ©pÃ©titions\n",
    "- **Prompt engineering** - Guide la gÃ©nÃ©ration\n",
    "\n",
    "### ğŸš€ Applications pratiques:\n",
    "1. **GÃ©nÃ©ration de contenu** - Marketing, descriptions produits\n",
    "2. **Assistant crÃ©atif** - Ã‰criture, brainstorming\n",
    "3. **Chatbot intelligent** - Conversations naturelles\n",
    "4. **ComplÃ©tion de code** - Assistance programmation\n",
    "\n",
    "### ğŸ’¡ Bonnes pratiques:\n",
    "- **TempÃ©rature 0.7-1.0** pour Ã©quilibre crÃ©ativitÃ©/cohÃ©rence\n",
    "- **Top-p 0.9** pour diversitÃ© contrÃ´lÃ©e\n",
    "- **Repetition penalty 1.1-1.2** pour Ã©viter les boucles\n",
    "- **Prompt engineering** crucial pour rÃ©sultats qualitÃ©\n",
    "- **Fine-tuning** sur donnÃ©es spÃ©cialisÃ©es amÃ©liore drastiquement\n",
    "\n",
    "### ğŸ”® Ã‰volutions futures:\n",
    "- **ModÃ¨les plus gros** (GPT-4, Claude, LLaMA)\n",
    "- **MultimodalitÃ©** (texte + images)\n",
    "- **Agents autonomes** avec tools/APIs\n",
    "- **Optimisations** (quantification, distillation)\n",
    "- **ContrÃ´le Ã©thique** renforcÃ©"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}