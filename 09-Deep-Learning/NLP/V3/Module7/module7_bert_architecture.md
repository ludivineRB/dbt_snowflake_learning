---
title: Module 7 - Architecture BERT
description: Formation NLP - Module 7 - Architecture BERT
tags:
  - NLP
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ğŸ§  Architecture BERT

Bidirectional Encoder Representations from Transformers

## ğŸ¯ Qu'est-ce que BERT ?

### ğŸ”„ La RÃ©volution Bidirectionnelle

BERT (Bidirectional Encoder Representations from Transformers) a rÃ©volutionnÃ© le NLP en 2018 en introduisant la **bidirectionnalitÃ©** dans le prÃ©-entraÃ®nement des modÃ¨les de langage.

**ğŸ’¡ Innovation ClÃ© :**  
Contrairement aux modÃ¨les prÃ©cÃ©dents qui lisaient le texte dans une seule direction, BERT lit **simultanÃ©ment** de gauche Ã  droite ET de droite Ã  gauche, capturant ainsi le contexte complet de chaque mot.

#### ğŸ—ï¸ Architecture BERT SimplifiÃ©e

**ğŸ¯ Couche de Classification/PrÃ©diction**  
TÃ¢che spÃ©cifique (sentiment, Q&A, NER...)

â¬‡ï¸

**ğŸ”„ Pooler (optionnel)**  
ReprÃ©sentation globale de la sÃ©quence \[CLS\]

â¬‡ï¸

**ğŸ—ï¸ 12 Couches Transformer Encoder**  
Multi-Head Self-Attention + Feed-Forward (BERT-Base)

â¬‡ï¸

**â• Embeddings = Token + Position + Segment**  
ReprÃ©sentation vectorielle complÃ¨te

â¬‡ï¸

**ğŸ“ Input : \[CLS\] Tokens \[SEP\]**  
Tokens spÃ©ciaux + WordPiece tokenization

**ğŸ” DiffÃ©rence Fondamentale :**  
â€¢ GPT : "Le chat mange des" â†’ prÃ©dit "croquettes" (unidirectionnel)  
â€¢ BERT : "Le chat \[MASK\] des croquettes" â†’ devine "mange" (bidirectionnel)  
  
**ğŸ’¡ RÃ©sultat :** BERT comprend le contexte complet !

## ğŸ­ EntraÃ®nement BERT : Masked LM

### ğŸ¯ Masked Language Modeling (MLM)

BERT apprend en jouant Ã  un jeu de **"cache-cache avec les mots"** ! 15% des mots sont masquÃ©s et BERT doit les deviner.

#### ğŸ² DÃ©monstration du Masquage

Phrase originale : "BERT rÃ©volutionne le traitement du langage naturel"

AprÃ¨s masquage (15%) : "BERT \[MASK\] le \[MASK\] du langage naturel"

PrÃ©dictions BERT : "BERT rÃ©volutionne le traitement du langage naturel"

**ğŸ¯ StratÃ©gie de Masquage (15% des tokens) :**  
â€¢ **80%** â†’ remplacÃ©s par \[MASK\]  
â€¢ **10%** â†’ remplacÃ©s par un token alÃ©atoire  
â€¢ **10%** â†’ gardÃ©s inchangÃ©s  
  
**ğŸ’¡ Pourquoi ?** Ã‰viter l'overfitting sur \[MASK\] qui n'existe pas en production !

### ğŸ”— Next Sentence Prediction (NSP)

BERT apprend aussi les relations entre phrases en prÃ©disant si la phrase B suit logiquement la phrase A.

#### ğŸ§ª Testeur MLM Interactif

DÃ©monstration MLM apparaÃ®tra ici...

## ğŸŒŸ L'Ã‰cosystÃ¨me BERT

### ğŸš€ Les Variantes SpÃ©cialisÃ©es

ğŸ‡«ğŸ‡·

CamemBERT

BERT spÃ©cialement entraÃ®nÃ© sur du franÃ§ais. Performance native exceptionnelle sur les tÃ¢ches franÃ§aises.

âš¡

DistilBERT

Version compressÃ©e de BERT. 60% plus rapide, 40% plus petit, conserve 95% des performances.

ğŸŒ

Multilingual BERT

EntraÃ®nÃ© sur 104 langues simultanÃ©ment. Transfert zero-shot entre langues possible.

ğŸš€

RoBERTa

BERT optimisÃ© par Facebook. EntraÃ®nement plus long, donnÃ©es plus nombreuses, meilleures performances.

ğŸ“

Longformer

GÃ¨re des sÃ©quences trÃ¨s longues (4096+ tokens). Parfait pour documents entiers.

ğŸ”¬

SciBERT

SpÃ©cialisÃ© pour la littÃ©rature scientifique. Vocabulaire et corpus scientifiques.

## âš”ï¸ BERT vs Alternatives

### ğŸ“Š Comparaison Architecturale

ModÃ¨le

Architecture

Objectif d'entraÃ®nement

Forces

Cas d'usage

**BERT**

Encoder seulement

Masked LM + NSP

ComprÃ©hension bidirectionnelle

Classification, Q&A, NER

**GPT**

Decoder seulement

Causal LM

GÃ©nÃ©ration fluide

GÃ©nÃ©ration, complÃ©tion

**T5**

Encoder-Decoder

Text-to-Text

VersatilitÃ© des tÃ¢ches

Traduction, rÃ©sumÃ©

**RoBERTa**

Encoder seulement

Masked LM optimisÃ©

Performance supÃ©rieure

Toutes tÃ¢ches BERT

**ğŸ¯ Choisir le bon modÃ¨le :**  
â€¢ ComprÃ©hension â†’ BERT/RoBERTa  
â€¢ GÃ©nÃ©ration â†’ GPT  
â€¢ Traduction â†’ T5/mBART  
â€¢ FranÃ§ais â†’ CamemBERT  
â€¢ Vitesse â†’ DistilBERT

## ğŸ¯ Applications de BERT

### ğŸš€ Domaines d'Application

ğŸ˜Š

Analyse de Sentiment

Classification des Ã©motions dans les textes avec comprÃ©hension du contexte et du sarcasme.

â“

Question-RÃ©ponse

Extraction de rÃ©ponses prÃ©cises Ã  partir de documents. Performance niveau humain sur SQuAD.

ğŸ·ï¸

Named Entity Recognition

Identification automatique de personnes, lieux, organisations avec comprÃ©hension contextuelle.

ğŸ“Š

Classification de Texte

CatÃ©gorisation automatique de documents : spam, sujet, intention, urgence.

ğŸ”—

SimilaritÃ© SÃ©mantique

Mesure de similaritÃ© entre textes pour moteurs de recherche et recommandations.

ğŸ­

InfÃ©rence Textuelle

DÃ©terminer si une phrase implique, contredit ou est neutre par rapport Ã  une autre.

#### ğŸ§ª DÃ©monstration Application BERT

Analyse BERT apparaÃ®tra ici...

## ğŸ“Š Performance BERT

### ğŸ† Records Ã‰tablis

BERT a Ã©tabli de nouveaux records sur pratiquement tous les benchmarks NLP lors de sa sortie :

**ğŸ¯ Benchmarks GLUE (General Language Understanding) :**  
â€¢ **GLUE Score :** 80.5 â†’ 88.5 (+8 points)  
â€¢ **SQuAD 2.0 :** Performance humaine dÃ©passÃ©e  
â€¢ **SWAG :** 86.3% accuracy (nouveau record)  
â€¢ **MultiNLI :** 86.7% accuracy  
  
**ğŸ’¡ Impact :** Premier modÃ¨le Ã  surpasser les humains sur plusieurs tÃ¢ches !

### âš¡ Optimisations et Variantes

Variante ParamÃ¨tres Vitesse Performance Avantage BERT-Base 110M 1x Base Ã‰quilibre BERT-Large 340M 0.3x +2-3% Maximum performance DistilBERT 66M 1.6x -3% RapiditÃ© RoBERTa 125M 0.9x +1-2% Optimisation poussÃ©e

[â† Index Module 7](index.html)

**Architecture BERT**  
ComprÃ©hension bidirectionnelle rÃ©volutionnaire

[Architecture GPT â†’](module7_gpt_architecture.html)

// Animation de la barre de progression window.addEventListener('load', function() { setTimeout(() => { document.getElementById('progressBar').style.width = '100%'; }, 1000); }); // DÃ©monstration MLM function demonstrateMLM() { const input = document.getElementById('mlmInput').value.trim(); if (!input) { document.getElementById('mlmOutput').textContent = 'DÃ©monstration MLM apparaÃ®tra ici...'; return; } const words = input.split(' '); const numToMask = Math.max(1, Math.floor(words.length \* 0.15)); // SÃ©lectionner des mots Ã  masquer alÃ©atoirement const maskedIndices = new Set(); while (maskedIndices.size < numToMask) { maskedIndices.add(Math.floor(Math.random() \* words.length)); } let resultHTML = '<strong>ğŸ­ DÃ©monstration Masked Language Modeling</strong><br><br>'; // Phrase originale resultHTML += \`<div style="margin: 10px 0; padding: 10px; background: #F0F8FF; border-radius: 5px;">\`; resultHTML += \`<strong>ğŸ“ Original :</strong> "${input}"</div>\`; // Phrase masquÃ©e const maskedWords = words.map((word, i) => { if (maskedIndices.has(i)) { const rand = Math.random(); if (rand < 0.8) return '<span style="background: #FF5722; color: white; padding: 2px 4px; border-radius: 3px;">\[MASK\]</span>'; else if (rand < 0.9) return '<span style="background: #FFA500; color: white; padding: 2px 4px; border-radius: 3px;">mot\_alÃ©atoire</span>'; else return word; } else { return word; } }); resultHTML += \`<div style="margin: 10px 0; padding: 10px; background: #FFF8DC; border-radius: 5px;">\`; resultHTML += \`<strong>ğŸ¯ MasquÃ© (15%) :</strong> ${maskedWords.join(' ')}</div>\`; // PrÃ©dictions const predictions = words.map((word, i) => { if (maskedIndices.has(i)) { return \`<span style="background: #4CAF50; color: white; padding: 2px 4px; border-radius: 3px;">${word}</span>\`; } else { return word; } }); resultHTML += \`<div style="margin: 10px 0; padding: 10px; background: #F0FFF0; border-radius: 5px;">\`; resultHTML += \`<strong>ğŸ”® PrÃ©dictions BERT :</strong> ${predictions.join(' ')}</div>\`; resultHTML += \`<br><small style="color: #666;">ğŸ’¡ BERT utilise le contexte bidirectionnel pour deviner les mots masquÃ©s !</small>\`; document.getElementById('mlmOutput').innerHTML = resultHTML; } // DÃ©monstration BERT function demonstrateBERT() { const input = document.getElementById('bertDemo').value.trim(); if (!input) { document.getElementById('bertOutput').textContent = 'Analyse BERT apparaÃ®tra ici...'; return; } // Simulation d'analyse BERT multi-tÃ¢ches const positiveWords = \['fantastique', 'excellent', 'parfait', 'gÃ©nial', 'super', 'magnifique', 'merveilleux', 'incroyable'\]; const negativeWords = \['nul', 'horrible', 'mauvais', 'dÃ©cevant', 'affreux', 'catastrophique', 'terrible'\]; const words = input.toLowerCase().split(/\\W+/); const posCount = words.filter(word => positiveWords.some(pw => word.includes(pw))).length; const negCount = words.filter(word => negativeWords.some(nw => word.includes(nw))).length; let sentiment, sentimentScore; if (posCount > negCount) { sentiment = 'ğŸ˜Š POSITIF'; sentimentScore = Math.min(95, 75 + posCount \* 8); } else if (negCount > posCount) { sentiment = 'ğŸ˜ NÃ‰GATIF'; sentimentScore = Math.min(95, 75 + negCount \* 8); } else { sentiment = 'ğŸ˜ NEUTRE'; sentimentScore = 70; } // DÃ©tection d'entitÃ©s simulÃ©e const entities = \[\]; if (input.match(/\\b\[A-Z\]\[a-z\]+\\b/g)) { const matches = input.match(/\\b\[A-Z\]\[a-z\]+\\b/g); matches.forEach(match => { if (\['Paris', 'France', 'Google', 'Apple'\].includes(match)) { entities.push(\`${match} (ORGANISATION/LIEU)\`); } else { entities.push(\`${match} (PERSONNE)\`); } }); } const result = \` <strong>ğŸ§  Analyse Multi-TÃ¢ches BERT</strong><br><br> <div style="background: #FFF3E0; padding: 15px; border-radius: 8px; margin: 10px 0;"> <strong>ğŸ“ Texte :</strong> "${input}"<br><br> <strong>ğŸ˜Š Analyse de Sentiment :</strong> ${sentiment} (${sentimentScore}%)<br> <strong>ğŸ·ï¸ EntitÃ©s NommÃ©es :</strong> ${entities.length > 0 ? entities.join(', ') : 'Aucune dÃ©tectÃ©e'}<br> <strong>ğŸ“Š Longueur :</strong> ${words.length} mots<br> <strong>ğŸ¯ ComplexitÃ© :</strong> ${words.length > 10 ? 'Ã‰levÃ©e' : words.length > 5 ? 'Moyenne' : 'Simple'} </div> <div style="background: #F0F8FF; padding: 10px; border-radius: 5px; margin: 10px 0;"> <small> âš¡ <strong>Temps de traitement :</strong> 0.045s<br> ğŸ¤– <strong>ModÃ¨le :</strong> BERT-Base fine-tunÃ©<br> ğŸšï¸ <strong>Couches utilisÃ©es :</strong> 12 Transformer layers<br> ğŸ“Š <strong>ParamÃ¨tres :</strong> 110M </small> </div> \`; document.getElementById('bertOutput').innerHTML = result; } // Animation des couches BERT document.querySelectorAll('.bert-layer').forEach((layer, index) => { layer.addEventListener('click', function() { this.style.animation = 'none'; setTimeout(() => { this.style.animation = 'pulse 0.8s ease-in-out'; this.style.background = 'linear-gradient(135deg, #FFEB3B, #FFC107)'; setTimeout(() => { this.style.background = 'linear-gradient(135deg, #FFF3E0, #FFCC80)'; }, 800); }, 10); }); });
