# 04 - Word Embeddings

[â† 03 - ReprÃ©sentations](03-representations-textuelles.md) | [ğŸ  Accueil](README.md) | [05 - RÃ©seaux RÃ©currents â†’](05-reseaux-recurrents.md)

---

## ğŸ§  ReprÃ©sentation SÃ©mantique

Les **word embeddings** sont des vecteurs denses qui capturent le sens des mots. Les mots similaires sont proches dans l'espace vectoriel.

### ğŸ§® Word2Vec
Algorithme de Google (2013) basÃ© sur l'idÃ©e que le sens d'un mot dÃ©pend de ses voisins.
- **Skip-gram** : PrÃ©dit les voisins Ã  partir du mot.
- **CBOW** : PrÃ©dit le mot Ã  partir de ses voisins.

### ğŸŒ GloVe (Stanford)
Combine les statistiques globales du corpus pour construire les vecteurs.

### âš¡ FastText (Facebook)
Traite les mots comme des sacs de sous-mots (n-grams de caractÃ¨res), idÃ©al pour les mots inconnus ou les fautes de frappe.

---

[â† 03 - ReprÃ©sentations](03-representations-textuelles.md) | [ğŸ  Accueil](README.md) | [05 - RÃ©seaux RÃ©currents â†’](05-reseaux-recurrents.md)
