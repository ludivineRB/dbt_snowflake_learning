# 06 - Attention et Transformers

[â† 05 - RNN](05-reseaux-recurrents.md) | [ğŸ  Accueil](README.md) | [07 - BERT et GPT â†’](07-bert-gpt.md)

---

## ğŸ‘ï¸ La RÃ©volution de l'Attention

### âœ¨ MÃ©canisme d'Attention
Permet au modÃ¨le de se focaliser sur les mots les plus pertinents d'une phrase pour comprendre le contexte, sans traiter l'information de maniÃ¨re sÃ©quentielle (mot par mot).

### ğŸ—ï¸ Architecture Transformer (2017)
BasÃ©e uniquement sur l'attention ("Attention is all you need"). Elle est massivement parallÃ©lisable et beaucoup plus puissante que les RNN.
- **Encoder** : ComprÃ©hension.
- **Decoder** : GÃ©nÃ©ration.

---

[â† 05 - RNN](05-reseaux-recurrents.md) | [ğŸ  Accueil](README.md) | [07 - BERT et GPT â†’](07-bert-gpt.md)
