---
title: 'Module 4: Architectures CÃ©lÃ¨bres des CNN'
description: 'Formation CNN - Module 4: Architectures CÃ©lÃ¨bres des CNN'
tags:
  - CNN
  - 09-Deep-Learning
category: 09-Deep-Learning
---

# ðŸ—ï¸ Module 4: Architectures CÃ©lÃ¨bres des CNN

ðŸ“š Niveau: IntermÃ©diaire | â±ï¸ DurÃ©e: 1h45 | ðŸŽ¯ Objectif: Comprendre l'Ã©volution des architectures

## ðŸŽ¯ Ã‰volution Historique des CNN

Ce module explore les architectures qui ont marquÃ© l'histoire des CNN. Chaque architecture a apportÃ© des innovations rÃ©volutionnaires qui ont faÃ§onnÃ© la computer vision moderne.

1998 - LeNet-5

PremiÃ¨re architecture CNN pratique de Yann LeCun pour reconnaÃ®tre les chiffres manuscrits.

2012 - AlexNet

RÃ©volution du deep learning en remportant ImageNet avec ReLU, Dropout et GPU.

2014 - VGG & GoogLeNet

Deux approches : simplicitÃ© avec filtres 3Ã—3 vs efficacitÃ© avec modules Inception.

2015 - ResNet

Skip connections rÃ©volutionnaires permettant des rÃ©seaux de 100+ couches.

ðŸ§  LeNet-5 (1998) - Le Pionnier

**LeNet-5** est la premiÃ¨re architecture CNN pratique, dÃ©veloppÃ©e par Yann LeCun pour la reconnaissance de chiffres manuscrits sur le dataset MNIST.

CrÃ©ateur

Yann LeCun (Bell Labs)

ParamÃ¨tres

~60,000

Performance

99.2% sur MNIST

Innovation

PremiÃ¨re CNN pratique

#### Architecture LeNet-5

Input 32Ã—32Ã—1

â†’

Conv 5Ã—5

â†’

Pool 2Ã—2

â†’

Conv 5Ã—5

â†’

Pool 2Ã—2

â†’

Conv 5Ã—5

â†’

FC 84

â†’

Output 10

#### ðŸ’¡ Innovations Fondatrices

*   **PremiÃ¨re utilisation systÃ©matique des convolutions**
*   **Partage de paramÃ¨tres sur toute l'image**
*   **Architecture hiÃ©rarchique simple**
*   **Sous-Ã©chantillonnage progressif**

ðŸš€ AlexNet (2012) - La RÃ©volution

**AlexNet** a rÃ©volutionnÃ© la computer vision en remportant ImageNet 2012 avec une marge Ã©crasante (15.3% vs 26.2%), relanÃ§ant l'intÃ©rÃªt mondial pour le deep learning.

CrÃ©ateurs

Krizhevsky, Sutskever, Hinton

ParamÃ¨tres

~60 millions

Performance

15.3% top-5 error

Innovation

ReLU + GPU + Dropout

#### Architecture AlexNet

Input 227Ã—227Ã—3

â†’

Conv 11Ã—11

â†’

MaxPool

â†’

Conv 5Ã—5

â†’

MaxPool

â†’

3Ã—Conv 3Ã—3

â†’

FC 4096

â†’

FC 1000

#### ðŸ’¡ Innovations RÃ©volutionnaires

*   **ReLU :** PremiÃ¨re utilisation massive (remplace tanh/sigmoid)
*   **Dropout :** RÃ©gularisation pour Ã©viter le sur-apprentissage
*   **GPU Training :** PremiÃ¨re utilisation des GPU
*   **Data Augmentation :** Augmentation artificielle du dataset

ðŸ“ VGG (2014) - SimplicitÃ© et Profondeur

**VGG** a dÃ©montrÃ© qu'une architecture simple utilisant exclusivement des filtres 3Ã—3 pouvait atteindre d'excellentes performances grÃ¢ce Ã  la profondeur.

CrÃ©ateurs

Simonyan & Zisserman (Oxford)

ParamÃ¨tres

~138M (VGG-16)

Performance

7.3% top-5 error

Innovation

Filtres 3Ã—3 uniquement

#### Architecture VGG-16

Input 224Ã—224Ã—3

â†’

2Ã—Conv3Ã—3 (64)

â†’

MaxPool

â†’

2Ã—Conv3Ã—3 (128)

â†’

MaxPool

â†’

3Ã—Conv3Ã—3 (256)

â†’

3Ã—Conv3Ã—3 (512)

â†’

FC 4096

â†’

FC 1000

#### ðŸ’¡ Philosophie VGG

*   **SimplicitÃ© :** Filtres 3Ã—3 exclusivement
*   **Profondeur :** DÃ©monstration que "plus profond = mieux"
*   **ModularitÃ© :** Blocs rÃ©pÃ©titifs faciles Ã  comprendre
*   **Transfer Learning :** Excellent extracteur de features

ðŸ”— ResNet (2015) - Skip Connections

**ResNet** a rÃ©volutionnÃ© l'entraÃ®nement de rÃ©seaux profonds avec les skip connections, permettant des architectures de 100+ couches sans gradient vanishing.

CrÃ©ateurs

Kaiming He et al. (Microsoft)

ParamÃ¨tres

~25M (ResNet-50)

Performance

3.6% top-5 error

Innovation

Skip connections

#### Bloc RÃ©siduel

Input x

â†“

Conv + BN + ReLU

â†“

Conv + BN

â†“

+

â†“

ReLU

Formule: y = F(x) + x

#### ðŸ’¡ RÃ©volution ResNet

*   **Skip Connections :** RÃ©sout le gradient vanishing
*   **RÃ©seaux ultra-profonds :** Jusqu'Ã  152 couches
*   **Residual Learning :** Apprendre F(x) au lieu de H(x)
*   **Batch Normalization :** IntÃ©gration systÃ©matique

## ðŸ“Š Comparaison des Architectures

Architecture

AnnÃ©e

ParamÃ¨tres

ImageNet Top-5

Innovation Principale

**LeNet-5**

1998

60K

N/A (MNIST)

PremiÃ¨re CNN pratique

**AlexNet**

2012

60M

15.3%

ReLU + GPU + Dropout

**VGG-16**

2014

138M

7.3%

Filtres 3Ã—3 uniquement

**ResNet-50**

2015

25M

3.6%

Skip connections

## ðŸ“‹ RÃ©sumÃ© du Module 4

### ðŸŽ¯ Ce que vous avez appris :

*   **LeNet-5 (1998) :** Architecture pionniÃ¨re, base des CNN modernes
*   **AlexNet (2012) :** RÃ©volution ReLU + GPU + Dropout
*   **VGG (2014) :** SimplicitÃ© avec filtres 3Ã—3 exclusifs
*   **ResNet (2015) :** Skip connections pour rÃ©seaux ultra-profonds
*   **CritÃ¨res de choix :** Adapter selon contraintes et objectifs

### ðŸš€ Prochaine Ã©tape :

Le Module 5 explore les applications pratiques : classification, dÃ©tection d'objets, segmentation, mÃ©dical, vÃ©hicules autonomes, et les dÃ©fis du dÃ©ploiement en production.

[â† Module 3: Techniques AvancÃ©es](cnn_module3.html)

**Module 4 / 6**  
Architectures CÃ©lÃ¨bres

[Module 5: Applications Pratiques â†’](cnn_module5.html)

// Animation d'entrÃ©e document.addEventListener('DOMContentLoaded', () => { const elements = document.querySelectorAll('.architecture-box, .timeline-item'); elements.forEach((element, index) => { element.style.opacity = '0'; element.style.transform = 'translateY(30px)'; element.style.transition = 'all 0.6s ease'; setTimeout(() => { element.style.opacity = '1'; element.style.transform = 'translateY(0)'; }, index \* 200); }); });
