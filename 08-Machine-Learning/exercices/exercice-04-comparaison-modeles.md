# Exercice 4 : Comparaison de Mod√®les ‚Äî Le Shootout des Algorithmes

**Phase 3 ‚Äî Chapitres 9, 10 & 11** | Dur√©e estim√©e : 3h | Niveau : Interm√©diaire

---

## üéØ Objectifs

- Entra√Æner et comparer au moins 6 algorithmes de classification
- Utiliser une m√©thodologie rigoureuse (m√™me pipeline, m√™me split, m√™mes m√©triques)
- Tuner les hyperparam√®tres des meilleurs mod√®les
- Interpr√©ter les r√©sultats et choisir le meilleur mod√®le

---

## üìã Contexte

Votre √©quipe doit choisir le meilleur mod√®le pour pr√©dire le churn client. La direction veut un **tableau comparatif** et une **recommandation argument√©e**. Le co√ªt d'un faux n√©gatif (client churn√© non d√©tect√©) est 5 fois plus √©lev√© que celui d'un faux positif (offre de r√©tention envoy√©e √† un client fid√®le).

---

## üìù Instructions

### Partie 1 : Pr√©paration (30 min)

1. Chargez `data/clients_churn.csv`
2. R√©utilisez le Pipeline de l'exercice 3 pour le preprocessing
3. Faites un train/test split (80/20, stratifi√©, random_state=42)
4. **Question** : quelle m√©trique principale utiliser vu le contexte m√©tier (FN 5x plus co√ªteux que FP) ? Justifiez.

### Partie 2 : Le Shootout (1h)

5. Entra√Ænez les 6 mod√®les suivants avec leurs param√®tres par d√©faut :

| # | Mod√®le | Classe sklearn |
|---|--------|---------------|
| 1 | R√©gression Logistique | `LogisticRegression(max_iter=1000)` |
| 2 | KNN | `KNeighborsClassifier(n_neighbors=5)` |
| 3 | Arbre de D√©cision | `DecisionTreeClassifier(max_depth=5)` |
| 4 | Random Forest | `RandomForestClassifier(n_estimators=100)` |
| 5 | Gradient Boosting | `GradientBoostingClassifier(n_estimators=100)` |
| 6 | XGBoost | `XGBClassifier(n_estimators=100, use_label_encoder=False)` |

6. Pour chaque mod√®le, calculez :
   - Accuracy
   - Precision
   - Recall
   - F1-Score
   - AUC-ROC

7. Affichez un tableau comparatif tri√© par la m√©trique principale choisie

### Partie 3 : Tuning des top 3 (1h)

8. Prenez les 3 meilleurs mod√®les et tuner leurs hyperparam√®tres avec `GridSearchCV` ou `RandomizedSearchCV` :

   **Pour Random Forest** :
   ```python
   param_grid_rf = {
       'n_estimators': [100, 200, 500],
       'max_depth': [5, 10, 20, None],
       'min_samples_split': [2, 5, 10]
   }
   ```

   **Pour XGBoost** :
   ```python
   param_grid_xgb = {
       'n_estimators': [100, 200, 300],
       'max_depth': [3, 5, 7],
       'learning_rate': [0.01, 0.1, 0.3]
   }
   ```

9. Apr√®s tuning, refaites le tableau comparatif

### Partie 4 : Analyse et recommandation (30 min)

10. Pour le meilleur mod√®le :
    - Tracez la matrice de confusion
    - Tracez la courbe ROC
    - Affichez le feature importance (top 10)

11. R√©digez une recommandation de 5-10 lignes pour la direction :
    - Quel mod√®le recommandez-vous ?
    - Quelle est sa performance ?
    - Quelles sont les features les plus importantes ?
    - Quelles sont les limites ?

---

## üí° Indices

```python
from sklearn.model_selection import GridSearchCV, cross_val_score
import pandas as pd

# Pour un beau tableau comparatif
resultats = []
for nom, modele in modeles.items():
    modele.fit(X_train, y_train)
    y_pred = modele.predict(X_test)
    resultats.append({
        'Mod√®le': nom,
        'Recall': recall_score(y_test, y_pred),
        # ...
    })
df_resultats = pd.DataFrame(resultats).sort_values('Recall', ascending=False)
```

---

## ‚úÖ Crit√®res de r√©ussite

- [ ] Au moins 6 mod√®les sont compar√©s avec les m√™mes donn√©es et m√©triques
- [ ] La m√©trique principale est justifi√©e par le contexte m√©tier
- [ ] Le tuning est fait sur les top 3 mod√®les avec cross-validation
- [ ] Les r√©sultats avant/apr√®s tuning sont pr√©sent√©s
- [ ] La recommandation est argument√©e (pas juste "c'est le meilleur score")
- [ ] Les feature importances du meilleur mod√®le sont analys√©es
