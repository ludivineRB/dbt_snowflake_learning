# Exercice 2 : Distances, Vecteurs et KNN from Scratch

**Phase 1 ‚Äî Chapitres 3 & 4** | Dur√©e estim√©e : 2h30 | Niveau : D√©butant-Interm√©diaire

---

## üéØ Objectifs

- Calculer des distances entre points (euclidienne, Manhattan)
- Impl√©menter KNN from scratch
- Comprendre la descente de gradient visuellement
- Coder une r√©gression lin√©aire simple from scratch

---

## üìã Contexte

Avant d'utiliser scikit-learn, vous allez **coder les algorithmes vous-m√™me**. L'objectif n'est pas de r√©inventer la roue, mais de comprendre ce qui se passe sous le capot.

---

## üìù Instructions

### Partie 1 : Calcul de distances (30 min)

Voici 5 clients repr√©sent√©s par 2 features (normalis√©es entre 0 et 1) :

| Client | Anciennet√© (norm.) | Montant mensuel (norm.) | Churn ? |
|--------|-------------------|------------------------|---------|
| A | 0.8 | 0.2 | Non |
| B | 0.3 | 0.7 | Oui |
| C | 0.9 | 0.3 | Non |
| D | 0.1 | 0.9 | Oui |
| E | 0.5 | 0.5 | Non |

1. **√Ä la main** : calculez la distance euclidienne entre le nouveau client `X = [0.4, 0.6]` et chacun des 5 clients
2. **En Python** : v√©rifiez vos calculs avec NumPy
3. Avec K=3, quel serait le vote majoritaire pour X ?
4. R√©p√©tez avec la distance de Manhattan. Le r√©sultat change-t-il ?

### Partie 2 : KNN from scratch (45 min)

5. Impl√©mentez une fonction `knn_predict(X_train, y_train, x_new, k)` qui :
   - Calcule la distance entre `x_new` et tous les points de `X_train`
   - Trie par distance croissante
   - Prend les K plus proches voisins
   - Retourne la classe majoritaire

6. Testez votre fonction sur les donn√©es de la Partie 1
7. Comparez avec `KNeighborsClassifier` de scikit-learn ‚Äî les r√©sultats sont-ils identiques ?

### Partie 3 : Descente de gradient (45 min)

8. Soit la fonction `f(x) = (x - 3)¬≤ + 2` (une parabole dont le minimum est en x=3)
   - Tracez cette fonction avec matplotlib
   - Calculez la d√©riv√©e `f'(x) = 2*(x - 3)`
   - Impl√©mentez la descente de gradient :
     ```
     x = point_depart (ex: 10)
     pour chaque it√©ration:
         gradient = 2 * (x - 3)
         x = x - learning_rate * gradient
     ```
   - Tracez la trajectoire de x sur la courbe
   - Testez avec `learning_rate = 0.01`, `0.1`, `0.5`, `1.1` ‚Äî que se passe-t-il ?

### Partie 4 : R√©gression lin√©aire from scratch (30 min)

9. G√©n√©rez des donn√©es synth√©tiques :
   ```python
   import numpy as np
   np.random.seed(42)
   X = np.random.rand(50) * 10  # Surface (m¬≤)
   y = 2.5 * X + 10 + np.random.randn(50) * 3  # Prix
   ```

10. Impl√©mentez la r√©gression lin√©aire par descente de gradient :
    - Initialisez `a = 0`, `b = 0`
    - Pour chaque it√©ration :
      - Calculez les pr√©dictions : `y_pred = a * X + b`
      - Calculez l'erreur MSE
      - Calculez les gradients de a et b
      - Mettez √† jour a et b
    - Tracez la droite de r√©gression sur le nuage de points

11. Comparez avec `LinearRegression` de scikit-learn

---

## üí° Indices

```python
# Distance euclidienne
def distance_euclidienne(p1, p2):
    return np.sqrt(np.sum((p1 - p2) ** 2))

# Gradient pour r√©gression lin√©aire
# dMSE/da = -2/n * sum(X * (y - y_pred))
# dMSE/db = -2/n * sum(y - y_pred)
```

---

## ‚úÖ Crit√®res de r√©ussite

- [ ] Les distances sont calcul√©es correctement (√† la main ET en Python)
- [ ] KNN from scratch donne les m√™mes r√©sultats que scikit-learn
- [ ] La descente de gradient converge vers le minimum de la parabole
- [ ] L'impact du learning rate est compris et document√©
- [ ] La r√©gression lin√©aire from scratch donne des r√©sultats proches de scikit-learn
