# Chapitre 2 : Environnement et Outils

## üéØ Objectifs

- Configurer un environnement ML complet et reproductible
- Ma√Ætriser Jupyter Notebook pour l'exp√©rimentation
- Savoir charger et explorer des datasets
- R√©aliser une Analyse Exploratoire des Donn√©es (EDA) efficace
- Structurer un projet ML proprement

---

## 1. ‚öôÔ∏è Installation de l'environnement

### 1.1 Installer uv (gestionnaire de packages moderne)

**uv** est un gestionnaire de packages Python ultra-rapide (√©crit en Rust) qui remplace `pip`, `venv` et `pip-tools`. Il est **10 √† 100x plus rapide** que pip.

```bash
# Installer uv (une seule fois)
curl -LsSf https://astral.sh/uv/install.sh | sh

# V√©rifier l'installation
uv --version
```

> üí° **Conseil** : "uv est le futur de la gestion de packages Python. Il remplace pip, venv, pip-tools et m√™me pyenv en un seul outil ultra-rapide."

### 1.2 Initialiser un projet ML

```bash
# Cr√©er et initialiser un projet avec uv
uv init mon-projet-ml
cd mon-projet-ml

# uv cr√©e automatiquement :
# - pyproject.toml (description du projet + d√©pendances)
# - .venv/ (environnement virtuel)
# - .python-version (version de Python)
```

### 1.3 Installer les packages essentiels

```bash
# Installation de tous les packages ML essentiels
uv add numpy pandas matplotlib seaborn scikit-learn jupyter ipykernel

# Optionnel mais recommand√©
uv add missingno  # Visualisation des valeurs manquantes
uv add xgboost    # Algorithme de boosting performant
uv add plotly     # Visualisations interactives
```

> üí° **Conseil de pro** : "Avec uv, pas besoin d'activer manuellement l'environnement virtuel. Utilisez `uv run python script.py` ou `uv run jupyter lab` pour ex√©cuter dans le bon environnement automatiquement."

### 1.4 Gestion des d√©pendances avec `pyproject.toml`

uv g√®re les d√©pendances dans le fichier `pyproject.toml` (standard Python moderne) et g√©n√®re automatiquement un `uv.lock` pour le verrouillage exact des versions :

```toml
# pyproject.toml (g√©n√©r√© et maintenu par uv)
[project]
name = "mon-projet-ml"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "numpy>=1.26.4",
    "pandas>=2.2.0",
    "matplotlib>=3.8.2",
    "seaborn>=0.13.1",
    "scikit-learn>=1.4.0",
    "jupyter>=1.0.0",
    "ipykernel>=6.29.0",
    "missingno>=0.5.2",
]
```

```bash
# Pour reproduire l'environnement sur une autre machine
uv sync  # Installe exactement les m√™mes versions gr√¢ce √† uv.lock
```

> üí° **Conseil de pro** : "Committez **toujours** votre `pyproject.toml` ET `uv.lock` dans votre d√©p√¥t Git. Un coll√®gue peut reproduire votre environnement exact avec `uv sync`."

> ‚ö†Ô∏è **Attention** : "Ne pas verrouiller les versions est une source fr√©quente de bugs. Le code qui marche aujourd'hui peut casser demain si une biblioth√®que est mise √† jour avec des changements incompatibles. Le fichier `uv.lock` garantit la reproductibilit√©."

---

## 2. üìì Jupyter Notebook : votre laboratoire

### 2.1 Pourquoi Jupyter pour le ML ?

Jupyter Notebook est l'outil de pr√©dilection des data scientists pour plusieurs raisons :

| Avantage | Description |
|----------|-------------|
| **It√©ration rapide** | Ex√©cuter cellule par cellule, pas besoin de relancer tout le script |
| **Visualisation inline** | Les graphiques s'affichent directement dans le notebook |
| **Documentation int√©gr√©e** | M√©langer code, texte Markdown, formules LaTeX |
| **Exploration** | Parfait pour l'EDA et l'exp√©rimentation |
| **Partage** | Les notebooks `.ipynb` contiennent code + r√©sultats + explications |

### 2.2 Lancer Jupyter

```bash
# Lancer Jupyter Notebook (interface classique)
jupyter notebook

# Ou Jupyter Lab (interface moderne, recommand√©e)
jupyter lab
```

### 2.3 Raccourcis essentiels

| Raccourci | Action | Mode |
|-----------|--------|------|
| `Shift + Enter` | Ex√©cuter la cellule et passer √† la suivante | √âdition |
| `Ctrl + Enter` | Ex√©cuter la cellule sans avancer | √âdition |
| `A` | Ins√©rer une cellule au-dessus | Commande |
| `B` | Ins√©rer une cellule en-dessous | Commande |
| `DD` | Supprimer la cellule | Commande |
| `M` | Convertir en Markdown | Commande |
| `Y` | Convertir en Code | Commande |
| `Esc` | Passer en mode commande | √âdition |
| `Enter` | Passer en mode √©dition | Commande |
| `Tab` | Autocompl√©tion | √âdition |
| `Shift + Tab` | Afficher la documentation | √âdition |

> üí° **Conseil** : "Utilisez des cellules **Markdown** pour documenter vos hypoth√®ses, vos observations et vos d√©cisions. Un notebook bien document√© est un notebook r√©utilisable."

### 2.4 Bonnes pratiques Jupyter

```python
# Toujours commencer un notebook avec ces imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration pour de jolis graphiques
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)
plt.rcParams['font.size'] = 12

# Afficher toutes les colonnes dans pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

# Afficher les graphiques inline
%matplotlib inline

# Recharger automatiquement les modules modifi√©s
%load_ext autoreload
%autoreload 2
```

> üí° **Conseil de pro** : "Cr√©ez un template de notebook avec ces imports et configurations. Vous gagnerez du temps √† chaque nouveau projet."

### 2.5 Extensions utiles

| Extension | Utilit√© |
|-----------|---------|
| **Table of Contents** | Navigation dans les longs notebooks |
| **Variable Inspector** | Voir les variables en m√©moire |
| **ExecuteTime** | Temps d'ex√©cution de chaque cellule |
| **Collapsible Headings** | Plier/d√©plier des sections |
| **Nbextensions** | Collection d'extensions (Jupyter Notebook classique) |

---

## 3. üìä Les datasets de d√©monstration

### 3.1 Datasets int√©gr√©s √† scikit-learn

scikit-learn inclut des datasets classiques, parfaits pour apprendre :

```python
from sklearn import datasets

# --- Datasets pour la CLASSIFICATION ---

# Iris : classification de fleurs (3 classes, 4 features)
iris = datasets.load_iris()
X_iris, y_iris = iris.data, iris.target
print(f"Iris - Shape: {X_iris.shape}, Classes: {iris.target_names}")

# Digits : reconnaissance de chiffres manuscrits (10 classes, 64 features)
digits = datasets.load_digits()
X_digits, y_digits = digits.data, digits.target
print(f"Digits - Shape: {X_digits.shape}, Classes: {np.unique(y_digits)}")

# Breast Cancer : classification tumeur b√©nigne/maligne (2 classes, 30 features)
cancer = datasets.load_breast_cancer()
X_cancer, y_cancer = cancer.data, cancer.target
print(f"Cancer - Shape: {X_cancer.shape}, Classes: {cancer.target_names}")

# --- Datasets pour la R√âGRESSION ---

# California Housing : pr√©dire le prix m√©dian des maisons
housing = datasets.fetch_california_housing()
X_housing, y_housing = housing.data, housing.target
print(f"Housing - Shape: {X_housing.shape}, Target: prix m√©dian")

# Diabetes : pr√©dire la progression du diab√®te
diabetes = datasets.load_diabetes()
X_diabetes, y_diabetes = diabetes.data, diabetes.target
print(f"Diabetes - Shape: {X_diabetes.shape}")
```

### 3.2 G√©n√©rer des datasets synth√©tiques

```python
from sklearn.datasets import make_classification, make_regression, make_blobs

# G√©n√©rer un dataset de classification
X_classif, y_classif = make_classification(
    n_samples=1000,        # Nombre d'√©chantillons
    n_features=10,         # Nombre de features
    n_informative=5,       # Features r√©ellement informatives
    n_redundant=2,         # Features redondantes
    n_classes=2,           # Nombre de classes
    random_state=42        # Reproductibilit√©
)

# G√©n√©rer un dataset de r√©gression
X_reg, y_reg = make_regression(
    n_samples=1000,
    n_features=5,
    noise=10,              # Niveau de bruit
    random_state=42
)

# G√©n√©rer des clusters (pour le non-supervis√©)
X_blobs, y_blobs = make_blobs(
    n_samples=500,
    centers=4,             # Nombre de clusters
    cluster_std=1.0,       # √âcart-type des clusters
    random_state=42
)
```

> üí° **Conseil** : "Les datasets synth√©tiques sont parfaits pour **comprendre** un algorithme car vous contr√¥lez les param√®tres. Utilisez-les avant de passer √† des donn√©es r√©elles."

### 3.3 Charger ses propres donn√©es

```python
# Depuis un fichier CSV
df = pd.read_csv("donnees.csv")

# Depuis un fichier Excel
df = pd.read_excel("donnees.xlsx", sheet_name="Feuille1")

# Depuis une base de donn√©es SQL
import sqlite3
conn = sqlite3.connect("base.db")
df = pd.read_sql("SELECT * FROM clients", conn)

# Depuis une URL
url = "https://raw.githubusercontent.com/datasets/iris/master/data/iris.csv"
df = pd.read_csv(url)
```

### 3.4 Tableau r√©capitulatif des datasets sklearn

| Dataset | Type | Samples | Features | Classes | Difficult√© |
|---------|------|---------|----------|---------|------------|
| `load_iris` | Classification | 150 | 4 | 3 | ‚≠ê |
| `load_digits` | Classification | 1 797 | 64 | 10 | ‚≠ê‚≠ê |
| `load_breast_cancer` | Classification | 569 | 30 | 2 | ‚≠ê‚≠ê |
| `load_diabetes` | R√©gression | 442 | 10 | - | ‚≠ê‚≠ê |
| `fetch_california_housing` | R√©gression | 20 640 | 8 | - | ‚≠ê‚≠ê‚≠ê |
| `make_classification` | Classification | Configurable | Configurable | Configurable | Variable |
| `make_regression` | R√©gression | Configurable | Configurable | - | Variable |

---

## 4. üîç Exploration de donn√©es (EDA)

L'Analyse Exploratoire des Donn√©es (EDA) est l'√©tape la plus importante avant toute mod√©lisation.

> üí° **Conseil de pro** : "Toujours visualiser vos donn√©es AVANT de mod√©liser. L'EDA permet de d√©tecter des probl√®mes (valeurs manquantes, outliers, d√©s√©quilibres) et de formuler des hypoth√®ses."

### 4.1 Vue d'ensemble du dataset

```python
import pandas as pd
import numpy as np

# Charger le dataset
df = pd.read_csv("donnees.csv")

# --- Informations g√©n√©rales ---
print("=== FORME DU DATASET ===")
print(f"Nombre de lignes : {df.shape[0]}")
print(f"Nombre de colonnes : {df.shape[1]}")

print("\n=== TYPES DE DONN√âES ===")
print(df.dtypes)

print("\n=== INFORMATIONS COMPL√àTES ===")
df.info()

print("\n=== PREMI√àRES LIGNES ===")
df.head(10)
```

### 4.2 Statistiques descriptives

```python
# Statistiques des variables num√©riques
print("=== STATISTIQUES NUM√âRIQUES ===")
print(df.describe())

# Statistiques des variables cat√©gorielles
print("\n=== STATISTIQUES CAT√âGORIELLES ===")
print(df.describe(include='object'))

# Distribution de la variable cible
print("\n=== DISTRIBUTION DE LA CIBLE ===")
print(df['target'].value_counts())
print(f"\nPourcentages :")
print(df['target'].value_counts(normalize=True) * 100)
```

### 4.3 Valeurs manquantes

```python
# Compter les valeurs manquantes
print("=== VALEURS MANQUANTES ===")
missing = df.isnull().sum()
missing_pct = (df.isnull().sum() / len(df)) * 100
missing_df = pd.DataFrame({'Manquantes': missing, 'Pourcentage': missing_pct})
print(missing_df[missing_df['Manquantes'] > 0].sort_values('Pourcentage', ascending=False))

# Visualisation avec missingno
import missingno as msno
msno.matrix(df, figsize=(12, 6))
plt.title("Matrice des valeurs manquantes")
plt.show()
```

> ‚ö†Ô∏è **Attention** : "Des valeurs manquantes sup√©rieures √† 50% sur une colonne sont souvent un signal pour la supprimer. Entre 5% et 50%, l'imputation est g√©n√©ralement appropri√©e."

### 4.4 Visualisations cl√©s

```python
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Distributions des variables num√©riques ---
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
for i, col in enumerate(df.select_dtypes(include=[np.number]).columns[:6]):
    ax = axes[i // 3, i % 3]
    df[col].hist(bins=30, ax=ax, edgecolor='black')
    ax.set_title(f'Distribution de {col}')
plt.tight_layout()
plt.show()

# --- 2. Boxplots pour d√©tecter les outliers ---
fig, axes = plt.subplots(1, 4, figsize=(16, 4))
for i, col in enumerate(df.select_dtypes(include=[np.number]).columns[:4]):
    sns.boxplot(y=df[col], ax=axes[i])
    axes[i].set_title(f'Boxplot de {col}')
plt.tight_layout()
plt.show()

# --- 3. Matrice de corr√©lation ---
plt.figure(figsize=(12, 8))
corr_matrix = df.select_dtypes(include=[np.number]).corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            fmt='.2f', linewidths=0.5)
plt.title("Matrice de corr√©lation")
plt.tight_layout()
plt.show()

# --- 4. Scatter plots des features les plus corr√©l√©es ---
# Trouver les paires les plus corr√©l√©es avec la cible
if 'target' in df.columns:
    correlations = df.corr()['target'].drop('target').abs().sort_values(ascending=False)
    top_features = correlations.head(4).index

    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    for i, feat in enumerate(top_features):
        axes[i].scatter(df[feat], df['target'], alpha=0.3)
        axes[i].set_xlabel(feat)
        axes[i].set_ylabel('target')
        axes[i].set_title(f'Corr√©lation: {correlations[feat]:.2f}')
    plt.tight_layout()
    plt.show()

# --- 5. Distribution de la target (classification) ---
plt.figure(figsize=(8, 5))
sns.countplot(x='target', data=df)
plt.title("Distribution des classes")
plt.xlabel("Classe")
plt.ylabel("Nombre d'√©chantillons")
plt.show()
```

### 4.5 Checklist EDA

| V√©rification | Code | Objectif |
|-------------|------|----------|
| Forme du dataset | `df.shape` | Combien de lignes/colonnes ? |
| Types de donn√©es | `df.dtypes` | Num√©rique vs cat√©goriel |
| Valeurs manquantes | `df.isnull().sum()` | Colonnes √† imputer/supprimer |
| Statistiques | `df.describe()` | Moyennes, √©carts-types, min/max |
| Distribution cible | `df['target'].value_counts()` | Classes d√©s√©quilibr√©es ? |
| Corr√©lations | `df.corr()` | Features li√©es entre elles ? |
| Outliers | Boxplots, IQR | Valeurs extr√™mes √† traiter ? |
| Distributions | Histogrammes | Normalit√©, asym√©trie ? |

> üí° **Conseil de pro** : "Documentez chaque observation de votre EDA dans des cellules Markdown. 'La colonne X a 15% de manquantes', 'La target est d√©s√©quilibr√©e 80/20', etc. Ce journal vous sera utile lors de la mod√©lisation."

---

## 5. üìÅ Structure d'un projet ML

### 5.1 Arborescence recommand√©e

```
mon-projet-ml/
‚îÇ
‚îú‚îÄ‚îÄ data/                   # Donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Donn√©es brutes (jamais modifi√©es)
‚îÇ   ‚îú‚îÄ‚îÄ processed/          # Donn√©es nettoy√©es/transform√©es
‚îÇ   ‚îî‚îÄ‚îÄ external/           # Donn√©es de sources externes
‚îÇ
‚îú‚îÄ‚îÄ notebooks/              # Notebooks d'exploration
‚îÇ   ‚îú‚îÄ‚îÄ 01-eda.ipynb        # Analyse exploratoire
‚îÇ   ‚îú‚îÄ‚îÄ 02-preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03-modeling.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04-evaluation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ src/                    # Code source r√©utilisable
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py    # Fonctions de preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ features.py         # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Entra√Ænement des mod√®les
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py       # Fonctions d'√©valuation
‚îÇ
‚îú‚îÄ‚îÄ models/                 # Mod√®les sauvegard√©s (.pkl, .joblib)
‚îÇ   ‚îî‚îÄ‚îÄ model_v1.joblib
‚îÇ
‚îú‚îÄ‚îÄ tests/                  # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessing.py
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py
‚îÇ
‚îú‚îÄ‚îÄ reports/                # Rapports et visualisations
‚îÇ   ‚îî‚îÄ‚îÄ figures/
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml          # D√©pendances Python (g√©r√© par uv)
‚îú‚îÄ‚îÄ .gitignore              # Fichiers √† exclure de Git
‚îî‚îÄ‚îÄ README.md               # Description du projet
```

### 5.2 Convention de nommage

| √âl√©ment | Convention | Exemple |
|---------|-----------|---------|
| Fichiers Python | snake_case | `data_preprocessing.py` |
| Notebooks | Num√©rot√©s + descriptif | `01-exploration-donnees.ipynb` |
| Variables | snake_case | `train_data`, `n_features` |
| Classes | PascalCase | `DataPreprocessor` |
| Constantes | UPPER_SNAKE_CASE | `RANDOM_STATE = 42` |
| Mod√®les sauvegard√©s | Version + date | `model_v2_2024-01-15.joblib` |

### 5.3 Bonnes pratiques de reproductibilit√©

```python
# 1. TOUJOURS fixer le random_state
RANDOM_STATE = 42

# Partout dans le code :
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=RANDOM_STATE)

# 2. Sauvegarder les mod√®les
import joblib
joblib.dump(model, 'models/model_v1.joblib')
model_charge = joblib.load('models/model_v1.joblib')

# 3. Logger les r√©sultats
resultats = {
    'modele': 'RandomForest',
    'parametres': model.get_params(),
    'accuracy_test': 0.95,
    'f1_test': 0.93,
    'date': '2024-01-15'
}
```

> üí° **Conseil de pro** : "Un projet ML reproductible doit permettre √† n'importe qui de cloner le d√©p√¥t, installer les d√©pendances et obtenir **les m√™mes r√©sultats**. Le `random_state` et le `uv.lock` sont vos meilleurs alli√©s."

### 5.4 Fichier `.gitignore` pour un projet ML

```gitignore
# Environnement virtuel
.venv/
.env

# Donn√©es volumineuses
data/raw/*.csv
data/raw/*.parquet
*.h5

# Mod√®les volumineux
models/*.pkl
models/*.joblib

# Jupyter checkpoints
.ipynb_checkpoints/

# Python cache
__pycache__/
*.pyc

# OS
.DS_Store
Thumbs.db
```

> ‚ö†Ô∏è **Attention** : "Ne versionnez **jamais** de fichiers de donn√©es volumineux dans Git. Utilisez Git LFS, DVC ou stockez-les sur un bucket cloud (S3, GCS)."

---

## üéØ Points cl√©s √† retenir

1. **Environnement virtuel** obligatoire pour chaque projet (`uv init` + `pyproject.toml`)
2. **Jupyter Notebook** est l'outil id√©al pour l'exploration et l'exp√©rimentation
3. **scikit-learn** fournit des datasets de d√©mo parfaits pour apprendre
4. **L'EDA** est une √©tape **non-n√©gociable** avant toute mod√©lisation
5. **Visualisez** toujours vos donn√©es : distributions, corr√©lations, outliers, manquantes
6. **Structurez** votre projet avec une arborescence claire et des conventions de nommage
7. **Reproductibilit√©** : `random_state`, `uv.lock`, sauvegarde des mod√®les
8. **Documentez** vos observations dans les notebooks (cellules Markdown)

---

## ‚úÖ Checklist de validation

- [ ] J'ai cr√©√© un environnement virtuel pour mon projet ML
- [ ] J'ai install√© numpy, pandas, matplotlib, seaborn, scikit-learn et jupyter
- [ ] J'ai un `pyproject.toml` et un `uv.lock` dans mon projet
- [ ] Je ma√Ætrise les raccourcis de base de Jupyter Notebook
- [ ] Je sais charger un dataset sklearn (`load_iris`, `load_breast_cancer`, etc.)
- [ ] Je sais charger mes propres donn√©es (CSV, Excel, SQL)
- [ ] Je sais r√©aliser une EDA compl√®te : shape, types, manquantes, statistiques, visualisations
- [ ] Je sais cr√©er une matrice de corr√©lation avec seaborn
- [ ] Mon projet suit une arborescence propre (data/, notebooks/, src/, models/)
- [ ] J'ai un `.gitignore` adapt√© √† un projet ML

---

**Pr√©c√©dent** : [Chapitre 1 : Introduction au Machine Learning](01-introduction-ml.md)

**Suivant** : [Chapitre 3 : Preprocessing ‚Äì Pr√©parer ses Donn√©es](03-preprocessing.md)
