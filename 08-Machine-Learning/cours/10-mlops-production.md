# Chapitre 10 : MLOps et Mise en Production

## üéØ Objectifs

- Comprendre le MLOps et pourquoi il est indispensable pour industrialiser le ML
- Savoir tracker des exp√©riences avec MLflow (logging, comparaison, registry)
- Ma√Ætriser la sauvegarde et le versioning de mod√®les (joblib, pickle, ONNX)
- Cr√©er une API de pr√©diction avec FastAPI et la conteneuriser avec Docker
- Mettre en place un pipeline CI/CD pour le ML avec GitHub Actions
- D√©tecter le data drift et le model drift en production
- Appliquer les bonnes pratiques de mise en production

---

## 1. üß† Introduction au MLOps

### 1.1 Qu'est-ce que le MLOps ?

Le **MLOps** (Machine Learning Operations) est l'ensemble des pratiques qui visent √† **d√©ployer et maintenir des mod√®les ML en production de mani√®re fiable et efficace**. C'est la rencontre entre le Machine Learning, le DevOps et le Data Engineering.

> üí° **Conseil de pro** : "Un mod√®le qui tourne dans un notebook Jupyter n'a AUCUNE valeur business. La valeur commence quand le mod√®le est en production, accessible, monitor√© et maintenu. Le MLOps, c'est le pont entre l'exp√©rimentation et la valeur business."

### 1.2 Pourquoi c'est important ?

| Probl√®me sans MLOps | Solution avec MLOps |
|---|---|
| "√áa marchait sur mon laptop" | Environnements reproductibles (Docker, uv) |
| Pas de tra√ßabilit√© des exp√©riences | Experiment tracking (MLflow) |
| Mod√®le d√©ploy√© √† la main | CI/CD automatis√© |
| Aucune id√©e si le mod√®le est encore bon | Monitoring et alerting |
| Impossible de revenir en arri√®re | Versioning des mod√®les et des donn√©es |
| Code spaghetti dans des notebooks | Pipelines structur√©s et test√©s |

**Statistiques alarmantes :**

- **87%** des projets ML n'atteignent jamais la production (Gartner)
- **55%** des entreprises n'ont jamais d√©ploy√© un mod√®le ML (Algorithmia)
- Le temps moyen de d√©ploiement d'un mod√®le est de **31 jours** sans MLOps, **7 jours** avec

> ‚ö†Ô∏è **Attention** : "Le MLOps n'est pas un luxe r√©serv√© aux grandes entreprises. M√™me pour un side-project ou une startup, les bonnes pratiques d√®s le d√©but vous √©viteront des mois de dette technique."

### 1.3 Le cycle de vie ML

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CYCLE DE VIE ML                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ 1. Cadrer‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ 2. Donn√©es‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ 3. Mod√®le‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ4. √âvaluer‚îÇ ‚îÇ
‚îÇ  ‚îÇle probl√®me‚îÇ  ‚îÇ & Features‚îÇ   ‚îÇ Training ‚îÇ   ‚îÇ& Valider‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ       ‚ñ≤                                             ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ                                             ‚ñº      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ8. R√©entr.‚îÇ‚óÑ‚îÄ‚îÄ‚îÇ7. Monitor‚îÇ‚óÑ‚îÄ‚îÄ‚îÇ6. Op√©rer ‚îÇ‚óÑ‚îÄ‚îÄ‚îÇ5. Deploy ‚îÇ ‚îÇ
‚îÇ  ‚îÇ& It√©rer  ‚îÇ   ‚îÇ& Alerter ‚îÇ   ‚îÇ& Servir  ‚îÇ   ‚îÇ& Livrer ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.4 Les niveaux de maturit√© MLOps

| Niveau | Description | Caract√©ristiques |
|---|---|---|
| **0 - Manuel** | Tout est fait √† la main | Notebooks, pas de versioning, pas de monitoring |
| **1 - Pipeline ML** | Pipeline automatis√© | Entra√Ænement automatis√©, experiment tracking |
| **2 - CI/CD ML** | Automatisation compl√®te | Tests auto, d√©ploiement auto, monitoring |
| **3 - Full MLOps** | Optimisation continue | R√©entra√Ænement auto, A/B testing, feature store |

> üí° **Conseil de pro** : "Visez le niveau 2 comme objectif r√©aliste. Le niveau 3 n'est pertinent que pour les entreprises qui ont des dizaines de mod√®les en production avec des donn√©es qui changent rapidement."

---

## 2. üìä Experiment Tracking avec MLflow

### 2.1 Pourquoi tracker ses exp√©riences ?

Sans tracking, vous allez forc√©ment vous retrouver dans cette situation :

```
modele_v1.pkl
modele_v2.pkl
modele_v2_final.pkl
modele_v2_final_FINAL.pkl
modele_v2_final_FINAL_OK.pkl    # ‚Üê Lequel est le bon ?
```

MLflow r√©sout ce probl√®me en enregistrant **automatiquement** :
- Les **param√®tres** (hyperparam√®tres, features utilis√©es)
- Les **m√©triques** (accuracy, F1, RMSE, etc.)
- Les **artefacts** (mod√®le s√©rialis√©, graphiques, donn√©es)
- L'**environnement** (versions des packages)

### 2.2 Installation et configuration

```bash
# Installation avec uv
uv add mlflow scikit-learn pandas

# Lancer l'interface MLflow
uv run mlflow ui --port 5000
```

> üí° **Conseil de pro** : "Lancez l'interface MLflow dans un terminal d√©di√©. Ouvrez http://localhost:5000 dans votre navigateur et gardez-le ouvert pendant vos exp√©rimentations. Vous verrez vos runs appara√Ætre en temps r√©el."

### 2.3 Logging des exp√©riences

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.datasets import load_iris
import pandas as pd

# Charger les donn√©es
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Configurer l'exp√©rience MLflow
mlflow.set_experiment("classification-iris")

# D√©finir les hyperparam√®tres √† tester
configs = [
    {"n_estimators": 50,  "max_depth": 3,  "min_samples_split": 2},
    {"n_estimators": 100, "max_depth": 5,  "min_samples_split": 5},
    {"n_estimators": 200, "max_depth": 10, "min_samples_split": 10},
    {"n_estimators": 100, "max_depth": None, "min_samples_split": 2},
]

for params in configs:
    # Chaque run est isol√© dans un contexte MLflow
    with mlflow.start_run(run_name=f"rf_depth{params['max_depth']}_est{params['n_estimators']}"):

        # 1. Logger les param√®tres
        mlflow.log_params(params)
        mlflow.log_param("random_state", 42)
        mlflow.log_param("test_size", 0.2)

        # 2. Entra√Æner le mod√®le
        model = RandomForestClassifier(**params, random_state=42)
        model.fit(X_train, y_train)

        # 3. Pr√©dire et √©valuer
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average="weighted")
        precision = precision_score(y_test, y_pred, average="weighted")
        recall = recall_score(y_test, y_pred, average="weighted")

        # 4. Logger les m√©triques
        mlflow.log_metrics({
            "accuracy": accuracy,
            "f1_weighted": f1,
            "precision_weighted": precision,
            "recall_weighted": recall,
        })

        # 5. Logger le mod√®le comme artefact
        mlflow.sklearn.log_model(model, "random_forest_model")

        # 6. Logger des m√©tadonn√©es suppl√©mentaires
        mlflow.set_tag("auteur", "equipe-ml")
        mlflow.set_tag("type", "classification")
        mlflow.set_tag("dataset", "iris")

        print(f"Params: {params} ‚Üí Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
```

> ‚ö†Ô∏è **Attention** : "N'oubliez jamais de logger le `random_state` et le `test_size`. Sans √ßa, vous ne pourrez pas reproduire vos r√©sultats, m√™me avec les m√™mes hyperparam√®tres."

### 2.4 Comparaison de runs

```python
import mlflow

# R√©cup√©rer toutes les runs d'une exp√©rience
experiment = mlflow.get_experiment_by_name("classification-iris")
runs = mlflow.search_runs(
    experiment_ids=[experiment.experiment_id],
    order_by=["metrics.f1_weighted DESC"]
)

# Afficher le top 5 des meilleurs runs
print("=== Top 5 des meilleurs mod√®les ===")
colonnes = ["run_id", "params.n_estimators", "params.max_depth",
            "metrics.accuracy", "metrics.f1_weighted"]
print(runs[colonnes].head(5).to_string(index=False))

# Trouver le meilleur run
best_run = runs.iloc[0]
print(f"\nMeilleur run : {best_run['run_id']}")
print(f"  Accuracy : {best_run['metrics.accuracy']:.4f}")
print(f"  F1 Score : {best_run['metrics.f1_weighted']:.4f}")
```

### 2.5 Model Registry

Le **Model Registry** est un registre centralis√© pour g√©rer le cycle de vie des mod√®les : staging, production, archivage.

```python
import mlflow
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Enregistrer le meilleur mod√®le dans le registry
best_run_id = best_run["run_id"]
model_uri = f"runs:/{best_run_id}/random_forest_model"

# Cr√©er ou mettre √† jour le mod√®le dans le registry
result = mlflow.register_model(
    model_uri=model_uri,
    name="iris-classifier"
)
print(f"Mod√®le enregistr√© : version {result.version}")

# Passer le mod√®le en staging (pour validation)
client.transition_model_version_stage(
    name="iris-classifier",
    version=result.version,
    stage="Staging"
)
print(f"Mod√®le v{result.version} ‚Üí Staging")

# Apr√®s validation, passer en production
client.transition_model_version_stage(
    name="iris-classifier",
    version=result.version,
    stage="Production"
)
print(f"Mod√®le v{result.version} ‚Üí Production")

# Charger le mod√®le de production
model_prod = mlflow.sklearn.load_model("models:/iris-classifier/Production")
print(f"Mod√®le de production charg√© : {type(model_prod).__name__}")
```

> üí° **Conseil de pro** : "Utilisez syst√©matiquement les stages Staging/Production. Avant de passer un mod√®le en Production, validez-le sur un jeu de donn√©es de staging avec des crit√®res de performance clairs (ex: F1 > 0.95)."

---

## 3. üíæ Sauvegarde et Versioning des Mod√®les

### 3.1 S√©rialisation avec joblib et pickle

```python
import joblib
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Cr√©er un pipeline complet
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestClassifier(n_estimators=100, random_state=42))
])
pipeline.fit(X_train, y_train)

# === M√©thode 1 : joblib (RECOMMAND√â pour sklearn) ===
joblib.dump(pipeline, "model/pipeline_v1.joblib")
pipeline_charge = joblib.load("model/pipeline_v1.joblib")
print(f"joblib - Accuracy : {pipeline_charge.score(X_test, y_test):.4f}")

# === M√©thode 2 : pickle (standard Python) ===
with open("model/pipeline_v1.pkl", "wb") as f:
    pickle.dump(pipeline, f)

with open("model/pipeline_v1.pkl", "rb") as f:
    pipeline_pickle = pickle.load(f)
print(f"pickle - Accuracy : {pipeline_pickle.score(X_test, y_test):.4f}")
```

| Format | Avantages | Inconv√©nients | Cas d'usage |
|---|---|---|---|
| **joblib** | Rapide pour gros arrays numpy | Sp√©cifique Python | Mod√®les sklearn |
| **pickle** | Standard Python | Lent pour gros objets, vuln√©rable | Objets Python simples |
| **ONNX** | Multi-plateforme, performant | Complexe √† configurer | Production cross-language |
| **MLflow** | Versioning int√©gr√©, metadata | D√©pendance MLflow | Projets avec tracking |

> ‚ö†Ô∏è **Attention** : "Ne chargez JAMAIS un fichier pickle provenant d'une source non fiable. Pickle peut ex√©cuter du code arbitraire lors du chargement. En production, pr√©f√©rez ONNX ou les formats MLflow."

### 3.2 Export ONNX pour la production

```python
# Installation
# uv add skl2onnx onnxruntime

from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import onnxruntime as rt
import numpy as np

# Convertir le pipeline sklearn en ONNX
initial_type = [("float_input", FloatTensorType([None, X_train.shape[1]]))]
onnx_model = convert_sklearn(pipeline, initial_types=initial_type)

# Sauvegarder le mod√®le ONNX
with open("model/pipeline_v1.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())

# Charger et inf√©rer avec ONNX Runtime (beaucoup plus rapide)
session = rt.InferenceSession("model/pipeline_v1.onnx")
input_name = session.get_inputs()[0].name

# Pr√©diction ONNX
onnx_pred = session.run(
    None,
    {input_name: X_test.values.astype(np.float32)}
)
print(f"Pr√©dictions ONNX : {onnx_pred[0][:5]}")
```

> üí° **Conseil de pro** : "ONNX est le format id√©al pour la production. Il est 2 √† 10x plus rapide que sklearn pour l'inf√©rence, il est ind√©pendant du langage (Python, C++, Java, JavaScript) et il ne n√©cessite pas d'installer sklearn en production."

### 3.3 Versioning avec MLflow

```python
import mlflow
import json
from datetime import datetime

# Sauvegarder un mod√®le avec toutes ses m√©tadonn√©es
with mlflow.start_run(run_name="production-v1.2"):
    # Logger le mod√®le
    mlflow.sklearn.log_model(pipeline, "model")

    # Logger les m√©tadonn√©es de versioning
    mlflow.log_params({
        "model_type": "RandomForest",
        "n_features": X_train.shape[1],
        "n_samples_train": X_train.shape[0],
        "feature_names": json.dumps(list(X_train.columns)),
    })

    mlflow.log_metrics({
        "accuracy": pipeline.score(X_test, y_test),
        "n_classes": len(set(y_test)),
    })

    mlflow.set_tags({
        "version": "1.2",
        "date_training": datetime.now().isoformat(),
        "deploye_par": "equipe-ml",
        "environnement": "production",
    })

    print("Mod√®le versionn√© et sauvegard√© dans MLflow")
```

---

## 4. üöÄ Serving de Mod√®les avec FastAPI

### 4.1 Pourquoi FastAPI ?

| Framework | Performance | Documentation auto | Validation | Async |
|---|---|---|---|---|
| Flask | Moyenne | Non | Non | Non |
| **FastAPI** | **Excellente** | **Oui (Swagger)** | **Oui (Pydantic)** | **Oui** |
| Django REST | Bonne | Plugin | Plugin | Plugin |

FastAPI est le choix id√©al pour servir des mod√®les ML gr√¢ce √† sa **performance**, sa **validation automatique** et sa **documentation Swagger g√©n√©r√©e automatiquement**.

### 4.2 Installation

```bash
# Installer les d√©pendances
uv add fastapi uvicorn joblib scikit-learn pydantic
```

### 4.3 API de pr√©diction compl√®te

```python
# fichier : app/main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import joblib
import numpy as np
import logging
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger le mod√®le au d√©marrage de l'API
MODEL_PATH = "model/pipeline_v1.joblib"
try:
    model = joblib.load(MODEL_PATH)
    logger.info(f"Mod√®le charg√© depuis {MODEL_PATH}")
except FileNotFoundError:
    logger.error(f"Mod√®le introuvable : {MODEL_PATH}")
    raise

# Initialiser l'application FastAPI
app = FastAPI(
    title="API de Pr√©diction ML",
    description="API pour servir un mod√®le de classification Iris",
    version="1.0.0"
)

# === Sch√©mas Pydantic pour la validation ===

class PredictionInput(BaseModel):
    """Sch√©ma d'entr√©e pour une pr√©diction."""
    sepal_length: float = Field(..., ge=0, le=10, description="Longueur du s√©pale (cm)")
    sepal_width: float = Field(..., ge=0, le=10, description="Largeur du s√©pale (cm)")
    petal_length: float = Field(..., ge=0, le=10, description="Longueur du p√©tale (cm)")
    petal_width: float = Field(..., ge=0, le=10, description="Largeur du p√©tale (cm)")

    class Config:
        json_schema_extra = {
            "example": {
                "sepal_length": 5.1,
                "sepal_width": 3.5,
                "petal_length": 1.4,
                "petal_width": 0.2
            }
        }

class PredictionOutput(BaseModel):
    """Sch√©ma de sortie pour une pr√©diction."""
    prediction: int
    label: str
    probabilites: List[float]
    timestamp: str

class BatchInput(BaseModel):
    """Sch√©ma d'entr√©e pour des pr√©dictions en lot."""
    instances: List[PredictionInput]

class HealthResponse(BaseModel):
    """Sch√©ma de r√©ponse pour le health check."""
    status: str
    model_loaded: bool
    timestamp: str

# === Mapping des classes ===
CLASSES = {0: "setosa", 1: "versicolor", 2: "virginica"}

# === Endpoints ===

@app.get("/health", response_model=HealthResponse)
def health_check():
    """V√©rifier que l'API et le mod√®le fonctionnent."""
    return HealthResponse(
        status="healthy",
        model_loaded=model is not None,
        timestamp=datetime.now().isoformat()
    )

@app.post("/predict", response_model=PredictionOutput)
def predict(input_data: PredictionInput):
    """Pr√©dire la classe d'une fleur Iris."""
    try:
        # Convertir l'entr√©e en array numpy
        features = np.array([[
            input_data.sepal_length,
            input_data.sepal_width,
            input_data.petal_length,
            input_data.petal_width
        ]])

        # Pr√©diction et probabilit√©s
        prediction = int(model.predict(features)[0])
        probabilites = model.predict_proba(features)[0].tolist()

        # Logger la pr√©diction (utile pour le monitoring)
        logger.info(
            f"Pr√©diction: {CLASSES[prediction]} "
            f"(proba: {max(probabilites):.2%}) "
            f"| Input: {input_data.model_dump()}"
        )

        return PredictionOutput(
            prediction=prediction,
            label=CLASSES[prediction],
            probabilites=[round(p, 4) for p in probabilites],
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.error(f"Erreur de pr√©diction : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de pr√©diction : {str(e)}")

@app.post("/predict/batch", response_model=List[PredictionOutput])
def predict_batch(batch: BatchInput):
    """Pr√©dire en lot pour plusieurs instances."""
    if len(batch.instances) > 1000:
        raise HTTPException(
            status_code=400,
            detail="Maximum 1000 instances par requ√™te batch"
        )

    resultats = []
    for instance in batch.instances:
        resultat = predict(instance)
        resultats.append(resultat)
    return resultats
```

### 4.4 Lancer l'API

```bash
# D√©marrer le serveur de d√©veloppement
uv run uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Acc√©der √† la documentation Swagger automatique
# http://localhost:8000/docs

# Tester avec curl
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'
```

### 4.5 Tester l'API avec Python

```python
import requests

# Test du health check
response = requests.get("http://localhost:8000/health")
print(f"Statut : {response.json()['status']}")

# Test d'une pr√©diction unitaire
data = {
    "sepal_length": 5.1,
    "sepal_width": 3.5,
    "petal_length": 1.4,
    "petal_width": 0.2
}
response = requests.post("http://localhost:8000/predict", json=data)
result = response.json()
print(f"Pr√©diction : {result['label']} (confiance : {max(result['probabilites']):.2%})")

# Test en lot (batch)
batch_data = {
    "instances": [
        {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2},
        {"sepal_length": 6.7, "sepal_width": 3.0, "petal_length": 5.2, "petal_width": 2.3},
        {"sepal_length": 5.9, "sepal_width": 3.0, "petal_length": 4.2, "petal_width": 1.5},
    ]
}
response = requests.post("http://localhost:8000/predict/batch", json=batch_data)
for r in response.json():
    print(f"  {r['label']} (proba max : {max(r['probabilites']):.2%})")
```

> üí° **Conseil de pro** : "Testez TOUJOURS votre API avec des donn√©es r√©alistes ET des donn√©es aberrantes (valeurs n√©gatives, nulles, tr√®s grandes). La validation Pydantic vous prot√®ge, mais v√©rifiez que les messages d'erreur sont clairs pour les consommateurs de l'API."

---

## 5. üê≥ Docker pour le ML

### 5.1 Pourquoi Docker ?

Docker garantit que votre mod√®le tourne **exactement de la m√™me mani√®re** sur votre laptop, en staging et en production. Plus de "√ßa marchait chez moi".

### 5.2 Dockerfile pour un mod√®le ML

```dockerfile
# fichier : Dockerfile

# Image de base Python l√©g√®re
FROM python:3.11-slim

# M√©tadonn√©es
LABEL maintainer="equipe-ml"
LABEL description="API de pr√©diction ML - Iris Classifier"
LABEL version="1.0.0"

# Variables d'environnement
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    UV_SYSTEM_PYTHON=1

# Installer uv (gestionnaire de paquets rapide)
RUN pip install uv

# R√©pertoire de travail
WORKDIR /app

# Copier les fichiers de d√©pendances en premier (cache Docker)
COPY pyproject.toml uv.lock* ./

# Installer les d√©pendances avec uv
RUN uv sync --frozen --no-dev

# Copier le code et le mod√®le
COPY app/ ./app/
COPY model/ ./model/

# Exposer le port
EXPOSE 8000

# Health check int√©gr√©
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Lancer l'API avec uvicorn
CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.3 Le fichier .dockerignore

```
# fichier : .dockerignore
__pycache__
*.pyc
.git
.gitignore
.env
notebooks/
data/raw/
*.ipynb
.venv/
mlruns/
```

### 5.4 Docker Compose pour l'environnement complet

```yaml
# fichier : docker-compose.yml
version: "3.8"

services:
  # API de pr√©diction
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/model/pipeline_v1.joblib
      - LOG_LEVEL=INFO
    volumes:
      - ./model:/app/model:ro    # Mod√®le en lecture seule
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MLflow pour le tracking (optionnel en prod)
  mlflow:
    image: python:3.11-slim
    command: >
      bash -c "pip install uv && uv pip install mlflow --system &&
      mlflow server --host 0.0.0.0 --port 5000
      --backend-store-uri sqlite:///mlflow.db
      --default-artifact-root ./mlruns"
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/app/mlruns
      - mlflow_db:/app
    restart: unless-stopped

volumes:
  mlflow_data:
  mlflow_db:
```

### 5.5 Commandes Docker essentielles

```bash
# Construire l'image
docker build -t ml-api:v1.0 .

# Lancer le conteneur
docker run -d -p 8000:8000 --name ml-api ml-api:v1.0

# V√©rifier que √ßa tourne
docker logs ml-api
curl http://localhost:8000/health

# Lancer l'environnement complet
docker compose up -d

# V√©rifier les services
docker compose ps

# Voir les logs en temps r√©el
docker compose logs -f api

# Arr√™ter tout
docker compose down
```

> ‚ö†Ô∏è **Attention** : "Ne mettez JAMAIS vos donn√©es d'entra√Ænement dans l'image Docker. L'image ne doit contenir que le code, les d√©pendances et le mod√®le s√©rialis√©. Les donn√©es restent dans des volumes ou des services externes (S3, GCS, BigQuery)."

---

## 6. üîÑ CI/CD pour le ML

### 6.1 Pourquoi CI/CD pour le ML ?

Le CI/CD (Continuous Integration / Continuous Deployment) automatise les tests et le d√©ploiement. Pour le ML, il y a des sp√©cificit√©s :

| CI/CD classique | CI/CD ML |
|---|---|
| Tests unitaires du code | Tests unitaires + tests du mod√®le |
| Linting du code | Linting + validation des donn√©es |
| Build de l'application | Build + entra√Ænement du mod√®le |
| D√©ploiement de l'app | D√©ploiement du mod√®le + de l'API |

### 6.2 Pipeline GitHub Actions

```yaml
# fichier : .github/workflows/ml-pipeline.yml

name: ML Pipeline CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  # √âtape 1 : Tests du code et du mod√®le
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Installer Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Installer uv
        run: pip install uv

      - name: Installer les d√©pendances
        run: uv sync

      - name: Linting avec ruff
        run: uv run ruff check .

      - name: Tests unitaires
        run: uv run pytest tests/ -v --tb=short

      - name: Tests du mod√®le (performance minimale)
        run: uv run pytest tests/test_model.py -v

  # √âtape 2 : Build et push de l'image Docker
  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4

      - name: Connexion au registre Docker
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build et push de l'image
        uses: docker/build-push-action@v5
        with:
          push: true
          tags: |
            ghcr.io/${{ github.repository }}/ml-api:latest
            ghcr.io/${{ github.repository }}/ml-api:${{ github.sha }}

  # √âtape 3 : D√©ploiement
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - name: D√©ployer sur le serveur
        run: |
          echo "D√©ploiement de l'image ghcr.io/${{ github.repository }}/ml-api:${{ github.sha }}"
          # Ici : ssh, kubectl apply, gcloud run deploy, etc.
```

### 6.3 Tests sp√©cifiques au ML

```python
# fichier : tests/test_model.py

import pytest
import joblib
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Seuils de performance minimale
ACCURACY_MINIMALE = 0.90
F1_MINIMAL = 0.88

@pytest.fixture
def model():
    """Charger le mod√®le de production."""
    return joblib.load("model/pipeline_v1.joblib")

@pytest.fixture
def test_data():
    """Pr√©parer les donn√©es de test."""
    iris = load_iris()
    _, X_test, _, y_test = train_test_split(
        iris.data, iris.target, test_size=0.2, random_state=42, stratify=iris.target
    )
    return X_test, y_test

class TestModelPerformance:
    """Tests de performance du mod√®le."""

    def test_accuracy_minimale(self, model, test_data):
        """Le mod√®le doit avoir une accuracy sup√©rieure au seuil."""
        X_test, y_test = test_data
        accuracy = model.score(X_test, y_test)
        assert accuracy >= ACCURACY_MINIMALE, (
            f"Accuracy {accuracy:.4f} < seuil {ACCURACY_MINIMALE}"
        )

    def test_f1_minimal(self, model, test_data):
        """Le mod√®le doit avoir un F1 sup√©rieur au seuil."""
        from sklearn.metrics import f1_score
        X_test, y_test = test_data
        y_pred = model.predict(X_test)
        f1 = f1_score(y_test, y_pred, average="weighted")
        assert f1 >= F1_MINIMAL, (
            f"F1 {f1:.4f} < seuil {F1_MINIMAL}"
        )

    def test_prediction_shape(self, model, test_data):
        """Les pr√©dictions doivent avoir la bonne forme."""
        X_test, _ = test_data
        predictions = model.predict(X_test)
        assert predictions.shape == (X_test.shape[0],)

    def test_classes_valides(self, model, test_data):
        """Les pr√©dictions doivent √™tre dans les classes connues."""
        X_test, _ = test_data
        predictions = model.predict(X_test)
        classes_attendues = {0, 1, 2}
        assert set(predictions).issubset(classes_attendues)

    def test_probabilites(self, model, test_data):
        """Les probabilit√©s doivent sommer √† 1."""
        X_test, _ = test_data
        probas = model.predict_proba(X_test)
        # Chaque ligne doit sommer √† ~1.0
        sommes = probas.sum(axis=1)
        np.testing.assert_allclose(sommes, 1.0, atol=1e-6)

class TestModelRobustesse:
    """Tests de robustesse du mod√®le."""

    def test_prediction_unitaire(self, model):
        """Le mod√®le doit g√©rer une seule instance."""
        instance = np.array([[5.1, 3.5, 1.4, 0.2]])
        prediction = model.predict(instance)
        assert len(prediction) == 1

    def test_valeurs_extremes(self, model):
        """Le mod√®le ne doit pas planter avec des valeurs extr√™mes."""
        extreme = np.array([[0.0, 0.0, 0.0, 0.0]])
        prediction = model.predict(extreme)
        assert prediction is not None

    def test_reproductibilite(self, model, test_data):
        """Deux appels identiques doivent donner le m√™me r√©sultat."""
        X_test, _ = test_data
        pred1 = model.predict(X_test)
        pred2 = model.predict(X_test)
        np.testing.assert_array_equal(pred1, pred2)
```

> üí° **Conseil de pro** : "Les tests de performance du mod√®le sont aussi importants que les tests unitaires du code. Si un nouveau commit d√©grade l'accuracy en dessous du seuil, le pipeline doit BLOQUER le d√©ploiement."

---

## 7. üìà Monitoring en Production

### 7.1 Pourquoi monitorer ?

Un mod√®le ML en production se **d√©grade in√©vitablement** avec le temps. Les donn√©es changent, les comportements utilisateurs √©voluent, le monde change. C'est ce qu'on appelle le **drift**.

| Type de drift | Description | Exemple |
|---|---|---|
| **Data drift** | La distribution des donn√©es d'entr√©e change | Les clients sont plus jeunes qu'avant |
| **Concept drift** | La relation entre features et target change | Le COVID change les habitudes d'achat |
| **Model drift** | Les performances du mod√®le se d√©gradent | L'accuracy passe de 95% √† 80% |

### 7.2 D√©tection du data drift

```python
# uv add scipy numpy pandas

import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, Tuple

class DataDriftDetector:
    """D√©tecteur de data drift bas√© sur des tests statistiques."""

    def __init__(self, reference_data: pd.DataFrame, seuil_pvalue: float = 0.05):
        """
        Initialiser le d√©tecteur avec les donn√©es de r√©f√©rence (entra√Ænement).

        Args:
            reference_data: Donn√©es utilis√©es lors de l'entra√Ænement
            seuil_pvalue: Seuil en dessous duquel on consid√®re qu'il y a drift
        """
        self.reference = reference_data
        self.seuil = seuil_pvalue

    def test_drift_numerique(self, production_data: pd.DataFrame) -> Dict[str, dict]:
        """
        Tester le drift sur les colonnes num√©riques avec le test de Kolmogorov-Smirnov.

        Le test KS compare deux distributions : si la p-value est basse,
        les distributions sont significativement diff√©rentes ‚Üí il y a drift.
        """
        resultats = {}

        colonnes_num = self.reference.select_dtypes(include=[np.number]).columns

        for col in colonnes_num:
            # Test de Kolmogorov-Smirnov : compare deux distributions
            statistic, p_value = stats.ks_2samp(
                self.reference[col].dropna(),
                production_data[col].dropna()
            )

            drift_detecte = p_value < self.seuil

            resultats[col] = {
                "statistic": round(statistic, 4),
                "p_value": round(p_value, 4),
                "drift": drift_detecte,
                "severite": "CRITIQUE" if p_value < 0.001 else "ALERTE" if drift_detecte else "OK",
                "ref_mean": round(self.reference[col].mean(), 4),
                "prod_mean": round(production_data[col].mean(), 4),
            }

        return resultats

    def test_drift_categoriel(self, production_data: pd.DataFrame) -> Dict[str, dict]:
        """
        Tester le drift sur les colonnes cat√©gorielles avec le test du Chi-2.
        """
        resultats = {}

        colonnes_cat = self.reference.select_dtypes(include=["object", "category"]).columns

        for col in colonnes_cat:
            # Distributions des cat√©gories
            ref_counts = self.reference[col].value_counts(normalize=True)
            prod_counts = production_data[col].value_counts(normalize=True)

            # Aligner les cat√©gories
            toutes_categories = set(ref_counts.index) | set(prod_counts.index)
            ref_aligned = [ref_counts.get(c, 0) for c in toutes_categories]
            prod_aligned = [prod_counts.get(c, 0) for c in toutes_categories]

            # Test du Chi-2
            statistic, p_value = stats.chisquare(prod_aligned, ref_aligned)

            resultats[col] = {
                "statistic": round(statistic, 4),
                "p_value": round(p_value, 4),
                "drift": p_value < self.seuil,
            }

        return resultats

    def rapport_complet(self, production_data: pd.DataFrame) -> dict:
        """G√©n√©rer un rapport complet de drift."""
        drift_num = self.test_drift_numerique(production_data)
        drift_cat = self.test_drift_categoriel(production_data)

        # Compter les colonnes avec drift
        nb_drift_num = sum(1 for v in drift_num.values() if v["drift"])
        nb_drift_cat = sum(1 for v in drift_cat.values() if v["drift"])
        total_colonnes = len(drift_num) + len(drift_cat)
        total_drift = nb_drift_num + nb_drift_cat

        return {
            "resume": {
                "total_colonnes": total_colonnes,
                "colonnes_avec_drift": total_drift,
                "pourcentage_drift": round(total_drift / max(total_colonnes, 1) * 100, 1),
                "action_requise": total_drift > total_colonnes * 0.3,
            },
            "drift_numerique": drift_num,
            "drift_categoriel": drift_cat,
        }


# === Exemple d'utilisation ===

# Donn√©es de r√©f√©rence (entra√Ænement)
np.random.seed(42)
ref_data = pd.DataFrame({
    "age": np.random.normal(35, 10, 1000),
    "revenu": np.random.normal(45000, 15000, 1000),
    "nb_achats": np.random.poisson(5, 1000),
})

# Donn√©es de production (avec drift sur l'√¢ge)
prod_data = pd.DataFrame({
    "age": np.random.normal(28, 8, 500),       # Drift : clients plus jeunes
    "revenu": np.random.normal(44000, 15000, 500),  # Pas de drift significatif
    "nb_achats": np.random.poisson(5, 500),     # Pas de drift
})

# D√©tecter le drift
detecteur = DataDriftDetector(ref_data, seuil_pvalue=0.05)
rapport = detecteur.rapport_complet(prod_data)

print("=== RAPPORT DE DATA DRIFT ===")
print(f"Colonnes analys√©es : {rapport['resume']['total_colonnes']}")
print(f"Drift d√©tect√© sur  : {rapport['resume']['colonnes_avec_drift']} colonnes")
print(f"Pourcentage drift  : {rapport['resume']['pourcentage_drift']}%")
print(f"Action requise     : {'OUI' if rapport['resume']['action_requise'] else 'Non'}")
print()

for col, info in rapport["drift_numerique"].items():
    status = f"{'DRIFT' if info['drift'] else 'OK':>8}"
    print(f"  {col:15} : {status} (p={info['p_value']:.4f}, "
          f"ref={info['ref_mean']:.1f}, prod={info['prod_mean']:.1f})")
```

> ‚ö†Ô∏è **Attention** : "Un data drift ne signifie pas toujours que le mod√®le est mauvais. Parfois les donn√©es changent mais le mod√®le reste performant. C'est pourquoi il faut monitorer AUSSI les m√©triques de performance du mod√®le, pas seulement les distributions des donn√©es."

### 7.3 Monitoring des m√©triques en production

```python
import json
import logging
from datetime import datetime
from collections import deque
from typing import Optional

logger = logging.getLogger(__name__)

class ModelMonitor:
    """Moniteur de performance du mod√®le en production."""

    def __init__(self, nom_modele: str, taille_fenetre: int = 1000):
        """
        Args:
            nom_modele: Nom du mod√®le monitor√©
            taille_fenetre: Nombre de pr√©dictions √† garder en m√©moire
        """
        self.nom_modele = nom_modele
        self.predictions = deque(maxlen=taille_fenetre)
        self.feedbacks = deque(maxlen=taille_fenetre)
        self.alertes = []

    def enregistrer_prediction(self, input_data: dict, prediction: int,
                                probabilite: float):
        """Enregistrer une pr√©diction pour le monitoring."""
        self.predictions.append({
            "timestamp": datetime.now().isoformat(),
            "input": input_data,
            "prediction": prediction,
            "probabilite": probabilite,
        })

    def enregistrer_feedback(self, prediction_id: str, vrai_label: int):
        """Enregistrer le vrai label quand il est disponible (feedback loop)."""
        self.feedbacks.append({
            "timestamp": datetime.now().isoformat(),
            "prediction_id": prediction_id,
            "vrai_label": vrai_label,
        })

    def calculer_metriques(self) -> dict:
        """Calculer les m√©triques sur la fen√™tre glissante."""
        if not self.predictions:
            return {"erreur": "Aucune pr√©diction enregistr√©e"}

        probas = [p["probabilite"] for p in self.predictions]
        preds = [p["prediction"] for p in self.predictions]

        metriques = {
            "nb_predictions": len(self.predictions),
            "proba_moyenne": round(sum(probas) / len(probas), 4),
            "proba_min": round(min(probas), 4),
            "proba_max": round(max(probas), 4),
            "distribution_classes": {
                cls: preds.count(cls) for cls in set(preds)
            },
        }

        # Alerte si la confiance moyenne baisse
        if metriques["proba_moyenne"] < 0.7:
            alerte = {
                "type": "CONFIANCE_BASSE",
                "message": f"Confiance moyenne √† {metriques['proba_moyenne']:.2%}",
                "timestamp": datetime.now().isoformat(),
            }
            self.alertes.append(alerte)
            logger.warning(f"ALERTE : {alerte['message']}")

        return metriques

    def verifier_sante(self) -> dict:
        """V√©rification compl√®te de la sant√© du mod√®le."""
        metriques = self.calculer_metriques()
        return {
            "modele": self.nom_modele,
            "timestamp": datetime.now().isoformat(),
            "metriques": metriques,
            "alertes_recentes": self.alertes[-5:],
            "sante": "OK" if not self.alertes else "DEGRADEE",
        }
```

### 7.4 Dashboard de monitoring (int√©gration FastAPI)

```python
# Ajouter ces endpoints √† app/main.py

monitor = ModelMonitor("iris-classifier")

@app.get("/monitoring/metriques")
def get_metriques():
    """R√©cup√©rer les m√©triques de monitoring."""
    return monitor.calculer_metriques()

@app.get("/monitoring/sante")
def get_sante():
    """V√©rifier la sant√© du mod√®le."""
    return monitor.verifier_sante()

@app.get("/monitoring/alertes")
def get_alertes():
    """R√©cup√©rer les alertes r√©centes."""
    return {"alertes": monitor.alertes[-20:]}
```

> üí° **Conseil de pro** : "En production, connectez ces m√©triques √† un outil de visualisation comme Grafana ou Datadog. Configurez des alertes automatiques (Slack, email) quand la confiance moyenne passe sous un seuil ou quand du drift est d√©tect√©."

---

## 8. üìã Bonnes Pratiques MLOps

### 8.1 Structure de projet recommand√©e

```
mon-projet-ml/
‚îú‚îÄ‚îÄ app/                        # Code de l'API
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # Endpoints FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py              # Sch√©mas Pydantic
‚îú‚îÄ‚îÄ src/                        # Code ML
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ train.py                # Script d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ predict.py              # Logique de pr√©diction
‚îÇ   ‚îú‚îÄ‚îÄ features.py             # Feature engineering
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py             # √âvaluation du mod√®le
‚îú‚îÄ‚îÄ tests/                      # Tests
‚îÇ   ‚îú‚îÄ‚îÄ test_model.py           # Tests du mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py             # Tests de l'API
‚îÇ   ‚îî‚îÄ‚îÄ test_features.py        # Tests du feature engineering
‚îú‚îÄ‚îÄ model/                      # Mod√®les s√©rialis√©s
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_v1.joblib
‚îú‚îÄ‚îÄ data/                       # Donn√©es (PAS dans git)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ features/
‚îú‚îÄ‚îÄ notebooks/                  # Notebooks d'exploration
‚îÇ   ‚îî‚îÄ‚îÄ exploration.ipynb
‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ ml-pipeline.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ pyproject.toml              # D√©pendances (uv)
‚îú‚îÄ‚îÄ uv.lock                     # Lockfile uv
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .env.example                # Variables d'environnement (template)
‚îî‚îÄ‚îÄ README.md
```

### 8.2 Le .gitignore pour le ML

```
# Donn√©es (trop volumineuses pour git)
data/
*.csv
*.parquet
*.h5

# Mod√®les s√©rialis√©s (versionn√©s avec MLflow ou DVC)
model/*.joblib
model/*.pkl
model/*.onnx

# MLflow
mlruns/

# Environnement
.env
.venv/

# Python
__pycache__/
*.pyc

# Notebooks checkpoints
.ipynb_checkpoints/

# IDE
.vscode/
.idea/
```

### 8.3 Checklist de mise en production

| Etape | Action | Fait ? |
|---|---|---|
| **Code** | Code versionn√© (git), pas de notebooks en prod | ‚òê |
| **Donn√©es** | Pipeline de donn√©es reproductible | ‚òê |
| **Mod√®le** | Mod√®le versionn√© (MLflow/DVC), m√©triques document√©es | ‚òê |
| **Tests** | Tests unitaires + tests de performance du mod√®le | ‚òê |
| **API** | Endpoint `/predict` + `/health` + validation Pydantic | ‚òê |
| **Docker** | Dockerfile optimis√©, `.dockerignore` pr√©sent | ‚òê |
| **CI/CD** | Pipeline automatis√© (build, test, deploy) | ‚òê |
| **Monitoring** | Data drift + model drift + alerting | ‚òê |
| **Logging** | Toutes les pr√©dictions logg√©es | ‚òê |
| **S√©curit√©** | Pas de secrets dans le code, HTTPS activ√© | ‚òê |
| **Documentation** | API document√©e (Swagger), README √† jour | ‚òê |
| **Rollback** | Proc√©dure de rollback test√©e | ‚òê |

### 8.4 Les erreurs classiques √† √©viter

| Erreur | Cons√©quence | Solution |
|---|---|---|
| Pas de versioning du mod√®le | Impossible de reproduire ou rollback | MLflow Model Registry |
| Preprocessing s√©par√© du mod√®le | Bugs subtils en production | Pipeline sklearn complet |
| Pas de monitoring | Le mod√®le se d√©grade en silence | Data drift + m√©triques |
| Tests manuels seulement | R√©gressions non d√©tect√©es | CI/CD avec tests auto |
| Secrets dans le code | Fuite de donn√©es | Variables d'env + `.env` |
| Docker image trop grosse | D√©ploiement lent | Image `slim` + `.dockerignore` |
| Pas de health check | Impossible de d√©tecter les pannes | Endpoint `/health` |
| Donn√©es d'entra√Ænement dans Docker | Image de 10 Go | Donn√©es dans S3/GCS |

> üí° **Conseil de pro** : "La r√®gle d'or du MLOps : automatisez tout ce qui peut l'√™tre, documentez le reste. Si une t√¢che est faite plus de deux fois manuellement, elle doit √™tre automatis√©e."

---

## üéØ Points cl√©s √† retenir

1. Le **MLOps** est essentiel : 87% des projets ML √©chouent sans bonnes pratiques de d√©ploiement
2. **MLflow** permet de tracker les exp√©riences, comparer les runs et g√©rer le cycle de vie des mod√®les
3. **Sauvegardez le pipeline complet** (preprocessing + mod√®le), pas juste le mod√®le seul
4. **ONNX** est le format id√©al pour la production (rapide, multi-plateforme)
5. **FastAPI** + Pydantic = API de pr√©diction robuste avec validation et documentation automatiques
6. **Docker** garantit la reproductibilit√© : "√ßa marche sur mon laptop" n'existe plus
7. **CI/CD** avec des tests de performance du mod√®le emp√™che les r√©gressions
8. Le **monitoring** est non-n√©gociable : data drift, model drift, m√©triques de confiance
9. La **structure de projet** doit s√©parer clairement code ML, API, tests et donn√©es
10. **Automatisez tout**, documentez le reste

## ‚úÖ Checklist de validation

- [ ] Je comprends le cycle de vie ML et les niveaux de maturit√© MLOps
- [ ] Je sais configurer MLflow et logger des exp√©riences (param√®tres, m√©triques, mod√®les)
- [ ] Je sais comparer des runs et utiliser le Model Registry
- [ ] Je ma√Ætrise la sauvegarde de mod√®les (joblib, pickle, ONNX)
- [ ] Je sais cr√©er une API FastAPI avec `/predict`, `/health` et validation Pydantic
- [ ] Je sais conteneuriser un mod√®le ML avec Docker et Docker Compose
- [ ] Je sais √©crire des tests de performance du mod√®le (accuracy minimale, robustesse)
- [ ] Je sais mettre en place un pipeline CI/CD avec GitHub Actions
- [ ] Je sais d√©tecter le data drift avec des tests statistiques (KS, Chi-2)
- [ ] Je connais les bonnes pratiques de mise en production et la checklist de d√©ploiement

---

[‚¨ÖÔ∏è Chapitre 9 : Feature Engineering](09-feature-engineering.md) | [‚û°Ô∏è Cheatsheet ML](CHEATSHEET-ml.md)
