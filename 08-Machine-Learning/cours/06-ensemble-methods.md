# Chapitre 6 : M√©thodes d'Ensemble ‚Äì La Force du Collectif

## üéØ Objectifs

- Comprendre le principe fondamental des m√©thodes d'ensemble
- Ma√Ætriser le fonctionnement de Random Forest et ses hyperparam√®tres
- Comprendre la famille du Boosting : AdaBoost, Gradient Boosting, XGBoost
- Savoir interpr√©ter les feature importances de mani√®re fiable
- Ma√Ætriser le tuning d'hyperparam√®tres avec GridSearchCV et RandomizedSearchCV
- Savoir choisir la bonne m√©thode d'ensemble selon le contexte

---

## 1. üß† Principe des m√©thodes d'ensemble

### 1.1 La sagesse des foules

Le principe des m√©thodes d'ensemble repose sur une id√©e simple mais puissante : **combiner plusieurs mod√®les faibles pour obtenir un mod√®le fort**. C'est le m√™me principe que la ¬´ sagesse des foules ¬ª : si vous demandez √† 1 000 personnes d'estimer le poids d'un b≈ìuf, la moyenne des estimations sera remarquablement proche du poids r√©el, m√™me si chaque estimation individuelle est impr√©cise.

En Machine Learning, cela se traduit par :

- Chaque mod√®le individuel (appel√© **learner faible**) fait des erreurs
- Ces erreurs sont **diff√©rentes** d'un mod√®le √† l'autre
- En **combinant** les pr√©dictions, les erreurs se compensent
- Le mod√®le final est **plus performant** et **plus robuste** que chaque mod√®le individuel

### 1.2 Bagging vs Boosting

Les deux grandes familles de m√©thodes d'ensemble se distinguent par leur strat√©gie de combinaison :

| Caract√©ristique | **Bagging** | **Boosting** |
|---|---|---|
| Principe | Mod√®les en **parall√®le** | Mod√®les en **s√©quence** |
| √âchantillonnage | Bootstrap (avec remise) | Pond√©ration des erreurs |
| Objectif principal | R√©duire la **variance** | R√©duire le **biais** |
| Risque d'overfitting | Faible | Mod√©r√© √† √©lev√© |
| Vitesse d'entra√Ænement | Parall√©lisable | S√©quentiel (plus lent) |
| Exemple phare | Random Forest | XGBoost, LightGBM |
| Sensibilit√© au bruit | Robuste | Sensible (apprend le bruit) |

> üí° **Conseil** : "Si vos donn√©es sont bruit√©es (beaucoup d'outliers, labels incertains), pr√©f√©rez le Bagging. Si vos donn√©es sont propres et que vous cherchez la performance maximale, le Boosting sera souvent meilleur."

---

## 2. üå≤ Bagging et Random Forest

### 2.1 Bagging (Bootstrap Aggregating)

Le Bagging, invent√© par Leo Breiman en 1996, suit un processus en trois √©tapes :

1. **Cr√©er N √©chantillons bootstrap** : tirer avec remise depuis le dataset d'entra√Ænement (chaque √©chantillon contient ~63% des donn√©es originales)
2. **Entra√Æner N mod√®les ind√©pendants** : chacun sur son √©chantillon bootstrap
3. **Agr√©ger les pr√©dictions** :
   - Classification ‚Üí **vote majoritaire**
   - R√©gression ‚Üí **moyenne des pr√©dictions**

```python
import numpy as np
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# G√©n√©rer des donn√©es d'exemple
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=10,
    n_redundant=5,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Bagging avec des arbres de d√©cision
bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(),  # mod√®le de base
    n_estimators=100,                     # nombre de mod√®les
    max_samples=0.8,                      # 80% des donn√©es par √©chantillon
    max_features=0.8,                     # 80% des features par mod√®le
    bootstrap=True,                       # tirage avec remise
    random_state=42,
    n_jobs=-1                             # parall√©liser sur tous les c≈ìurs
)

bagging.fit(X_train, y_train)

# √âvaluation
y_pred = bagging.predict(X_test)
print(f"Accuracy du Bagging : {accuracy_score(y_test, y_pred):.4f}")
```

> üí° **Conseil** : "Le Bagging est particuli√®rement efficace avec des mod√®les instables (haute variance) comme les arbres de d√©cision. Il n'apporte presque rien avec des mod√®les stables comme la r√©gression logistique."

### 2.2 Random Forest

Random Forest est l'am√©lioration la plus c√©l√®bre du Bagging. L'id√©e cl√© est d'ajouter une **double source d'al√©atoire** :

1. **√âchantillons bootstrap** (comme le Bagging classique)
2. **S√©lection al√©atoire de features** √† chaque split de chaque arbre

Cette double randomisation rend les arbres plus **d√©corr√©l√©s** entre eux, ce qui am√©liore la qualit√© de l'agr√©gation.

#### Hyperparam√®tres cl√©s

| Hyperparam√®tre | Description | Valeur par d√©faut | Conseil de tuning |
|---|---|---|---|
| `n_estimators` | Nombre d'arbres | 100 | 100-1000, plus = mieux (mais plus lent) |
| `max_depth` | Profondeur max des arbres | None (illimit√©e) | 10-30, ou None |
| `min_samples_split` | √âchantillons min pour splitter | 2 | 2-20 |
| `min_samples_leaf` | √âchantillons min par feuille | 1 | 1-10 |
| `max_features` | Features √† consid√©rer par split | 'sqrt' | 'sqrt', 'log2', 0.3-0.8 |
| `class_weight` | Poids des classes | None | 'balanced' si classes d√©s√©quilibr√©es |
| `n_jobs` | Parall√©lisation | 1 | -1 (tous les c≈ìurs) |

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import pandas as pd

# Cr√©er et entra√Æner le Random Forest
rf = RandomForestClassifier(
    n_estimators=200,        # 200 arbres
    max_depth=20,            # profondeur max de 20
    min_samples_split=5,     # au moins 5 √©chantillons pour splitter
    min_samples_leaf=2,      # au moins 2 √©chantillons par feuille
    max_features='sqrt',     # racine carr√©e du nombre de features
    class_weight='balanced', # gestion des classes d√©s√©quilibr√©es
    random_state=42,
    n_jobs=-1                # utiliser tous les c≈ìurs
)

rf.fit(X_train, y_train)

# Pr√©dictions
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

# Rapport de classification complet
print("=== Rapport de classification ===")
print(classification_report(y_test, y_pred))

# AUC-ROC
auc = roc_auc_score(y_test, y_proba)
print(f"AUC-ROC : {auc:.4f}")

# Score OOB (Out-Of-Bag) - estimation gratuite de la performance
rf_oob = RandomForestClassifier(
    n_estimators=200,
    oob_score=True,      # activer le score OOB
    random_state=42,
    n_jobs=-1
)
rf_oob.fit(X_train, y_train)
print(f"Score OOB : {rf_oob.oob_score_:.4f}")
```

> üí° **Conseil de pro** : "Random Forest est souvent le meilleur premier mod√®le √† essayer : peu de tuning n√©cessaire, g√®re les non-lin√©arit√©s, r√©sistant √† l'overfitting, et fournit des feature importances gratuitement. C'est votre couteau suisse du ML."

> üí° **Conseil** : "Commencez avec 100 arbres et augmentez si le score continue de monter. Au-del√† de 500 arbres, le gain marginal est g√©n√©ralement n√©gligeable. V√©rifiez avec le score OOB."

#### Feature Importance avec Random Forest

Random Forest calcule automatiquement l'importance de chaque feature :

```python
import matplotlib.pyplot as plt

# R√©cup√©rer les importances
importances = rf.feature_importances_
feature_names = [f"feature_{i}" for i in range(X.shape[1])]

# Cr√©er un DataFrame tri√©
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

# Visualiser les top 10 features
plt.figure(figsize=(10, 6))
plt.barh(
    importance_df['feature'][:10][::-1],
    importance_df['importance'][:10][::-1]
)
plt.xlabel('Importance (Gini)')
plt.title('Top 10 Features les plus importantes (Random Forest)')
plt.tight_layout()
plt.show()
```

> ‚ö†Ô∏è **Attention** : "Les feature importances bas√©es sur l'impuret√© (Gini) ont un biais connu : elles surestiment l'importance des features √† haute cardinalit√© (beaucoup de valeurs uniques). Pr√©f√©rez la permutation importance pour des r√©sultats fiables (voir section 6)."

---

## 3. ‚ö° Boosting

### 3.1 AdaBoost (Adaptive Boosting)

AdaBoost, cr√©√© par Freund et Schapire en 1997, est le premier algorithme de Boosting √† avoir connu un succ√®s pratique.

**Principe :**

1. Entra√Æner un premier mod√®le faible (souvent un arbre √† 1 niveau = ¬´ decision stump ¬ª)
2. Identifier les **√©chantillons mal class√©s**
3. **Augmenter le poids** de ces √©chantillons pour le prochain mod√®le
4. Entra√Æner un nouveau mod√®le qui se concentre sur les erreurs
5. R√©p√©ter N fois
6. Combiner les mod√®les avec des **poids proportionnels √† leur performance**

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# AdaBoost avec des stumps (arbres √† 1 niveau)
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # stump
    n_estimators=200,
    learning_rate=0.1,     # contribution de chaque mod√®le
    random_state=42
)

ada.fit(X_train, y_train)

y_pred_ada = ada.predict(X_test)
y_proba_ada = ada.predict_proba(X_test)[:, 1]

print(f"Accuracy AdaBoost : {accuracy_score(y_test, y_pred_ada):.4f}")
print(f"AUC-ROC AdaBoost  : {roc_auc_score(y_test, y_proba_ada):.4f}")
```

> ‚ö†Ô∏è **Attention** : "AdaBoost est tr√®s sensible aux outliers et au bruit. Comme il augmente le poids des √©chantillons mal class√©s, un outlier aberrant aura un impact disproportionn√© sur le mod√®le."

### 3.2 Gradient Boosting

Le Gradient Boosting est une g√©n√©ralisation du Boosting qui utilise la **descente de gradient** pour optimiser n'importe quelle fonction de perte.

**Principe simplifi√© :**

1. Initialiser avec une pr√©diction constante (moyenne pour r√©gression, log-odds pour classification)
2. Calculer les **r√©sidus** (erreurs) du mod√®le actuel
3. Entra√Æner un nouvel arbre pour **pr√©dire les r√©sidus**
4. Ajouter ce nouvel arbre au mod√®le (pond√©r√© par le learning rate)
5. R√©p√©ter N fois

Chaque arbre corrige un peu les erreurs des arbres pr√©c√©dents, comme un sculpteur qui affine progressivement son ≈ìuvre.

```python
from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting
gb = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.1,     # taux d'apprentissage
    max_depth=3,           # arbres peu profonds (stumps am√©lior√©s)
    min_samples_split=5,
    min_samples_leaf=2,
    subsample=0.8,         # sous-√©chantillonnage stochastique
    random_state=42
)

gb.fit(X_train, y_train)

y_pred_gb = gb.predict(X_test)
y_proba_gb = gb.predict_proba(X_test)[:, 1]

print(f"Accuracy Gradient Boosting : {accuracy_score(y_test, y_pred_gb):.4f}")
print(f"AUC-ROC Gradient Boosting  : {roc_auc_score(y_test, y_proba_gb):.4f}")
```

#### Hyperparam√®tres cl√©s du Gradient Boosting

| Hyperparam√®tre | R√¥le | Valeur typique | Impact |
|---|---|---|---|
| `learning_rate` | Contribution de chaque arbre | 0.01 - 0.3 | Plus petit = plus robuste mais plus lent |
| `n_estimators` | Nombre d'arbres | 100 - 5000 | Plus = mieux (avec early stopping) |
| `max_depth` | Profondeur des arbres | 3 - 8 | Plus profond = plus complexe |
| `subsample` | Fraction des donn√©es par arbre | 0.7 - 0.9 | < 1.0 r√©duit l'overfitting |
| `min_samples_leaf` | √âchantillons min par feuille | 1 - 50 | Plus grand = plus r√©gularis√© |

> ‚ö†Ô∏è **Attention** : "Learning rate et n_estimators sont intimement li√©s : un learning rate petit (0.01) n√©cessite beaucoup plus d'arbres (1000+) pour converger. La r√®gle : small LR + many trees = meilleur r√©sultat mais entra√Ænement plus lent. Utilisez toujours l'early stopping pour trouver le bon nombre d'arbres."

> üí° **Conseil** : "Pour le Gradient Boosting sklearn, commencez avec learning_rate=0.1, max_depth=3, n_estimators=200. Puis ajustez le learning_rate vers le bas et augmentez n_estimators proportionnellement."

---

## 4. üèÜ XGBoost ‚Äì Le champion des comp√©titions

### 4.1 Pourquoi XGBoost domine

XGBoost (eXtreme Gradient Boosting) est une impl√©mentation optimis√©e du Gradient Boosting cr√©√©e par Tianqi Chen en 2014. Il domine les comp√©titions Kaggle depuis des ann√©es pour plusieurs raisons :

- **R√©gularisation int√©gr√©e** (L1 et L2) pour √©viter l'overfitting
- **Gestion native des valeurs manquantes**
- **Parall√©lisation** au niveau des splits (pas des arbres)
- **Pruning intelligent** des arbres (contrairement au Gradient Boosting classique qui fait du pr√©-pruning)
- **Cache-aware access** pour des performances CPU optimales
- **Out-of-core computing** pour les datasets tr√®s volumineux

### 4.2 Installation et usage de base

```python
# Installation
# uv add xgboost

import xgboost as xgb
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# Cr√©er le mod√®le XGBoost
xgb_clf = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    min_child_weight=5,      # √©quivalent de min_samples_leaf
    subsample=0.8,           # sous-√©chantillonnage des lignes
    colsample_bytree=0.8,    # sous-√©chantillonnage des colonnes
    reg_alpha=0.1,           # r√©gularisation L1
    reg_lambda=1.0,          # r√©gularisation L2
    scale_pos_weight=1,      # pour classes d√©s√©quilibr√©es
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss'    # m√©trique d'√©valuation
)

# Entra√Æner avec early stopping
xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],  # donn√©es de validation
    verbose=50                     # afficher toutes les 50 it√©rations
)

# Pr√©dictions
y_pred_xgb = xgb_clf.predict(X_test)
y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]

print(f"Accuracy XGBoost : {accuracy_score(y_test, y_pred_xgb):.4f}")
print(f"AUC-ROC XGBoost  : {roc_auc_score(y_test, y_proba_xgb):.4f}")
print("\n", classification_report(y_test, y_pred_xgb))
```

### 4.3 Early Stopping ‚Äì L'arme anti-overfitting

L'early stopping est une technique cruciale : on arr√™te l'entra√Ænement quand la performance sur le set de validation ne s'am√©liore plus.

```python
# XGBoost avec early stopping
xgb_es = xgb.XGBClassifier(
    n_estimators=2000,          # mettre un nombre √©lev√©
    learning_rate=0.01,         # learning rate petit
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    early_stopping_rounds=50    # arr√™ter si pas d'am√©lioration pendant 50 rounds
)

xgb_es.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=100
)

# Voir combien d'arbres ont √©t√© utilis√©s
print(f"Meilleur nombre d'it√©rations : {xgb_es.best_iteration}")
print(f"Meilleur score : {xgb_es.best_score:.4f}")
```

> üí° **Conseil de pro** : "Utilisez early_stopping_rounds=50 pour √©viter l'overfitting automatiquement. Mettez n_estimators √† une valeur tr√®s √©lev√©e (2000-5000) et laissez l'early stopping trouver le bon moment pour s'arr√™ter."

### 4.4 Hyperparam√®tres XGBoost cl√©s

| Hyperparam√®tre | Description | Valeur recommand√©e | Impact |
|---|---|---|---|
| `learning_rate` (eta) | Taux d'apprentissage | 0.01 - 0.1 | Contr√¥le la vitesse de convergence |
| `n_estimators` | Nombre d'arbres | 500 - 5000 (avec early stop) | Plus = plus lent |
| `max_depth` | Profondeur max | 3 - 10 | Plus = plus complexe |
| `min_child_weight` | Poids min par feuille | 1 - 10 | R√©gularisation |
| `subsample` | % lignes par arbre | 0.6 - 0.9 | R√©duction de variance |
| `colsample_bytree` | % features par arbre | 0.6 - 0.9 | R√©duction de variance |
| `reg_alpha` | R√©gularisation L1 | 0 - 1 | Sparsit√© des features |
| `reg_lambda` | R√©gularisation L2 | 0 - 10 | Lissage des poids |
| `gamma` | Min loss reduction pour split | 0 - 5 | Contr√¥le la complexit√© |
| `scale_pos_weight` | Ratio classes | n_n√©gatifs / n_positifs | Classes d√©s√©quilibr√©es |

> üí° **Conseil** : "Pour un premier essai XGBoost, utilisez ces valeurs : learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8. Puis activez l'early stopping et r√©duisez le learning_rate."

---

## 5. üìä Comparaison des performances

### 5.1 Table comparative des m√©thodes d'ensemble

| Algorithme | Complexit√© d'entra√Ænement | Interpr√©tabilit√© | Performance typique | Vitesse d'inf√©rence | Gestion des NaN |
|---|---|---|---|---|---|
| **Random Forest** | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê‚≠ê Rapide | Non |
| **AdaBoost** | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê Correcte | ‚≠ê‚≠ê‚≠ê Rapide | Non |
| **Gradient Boosting** | ‚≠ê‚≠ê‚≠ê √âlev√©e | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s bonne | ‚≠ê‚≠ê Moyenne | Non |
| **XGBoost** | ‚≠ê‚≠ê‚≠ê √âlev√©e | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚≠ê‚≠ê‚≠ê Rapide | Oui |
| **LightGBM** | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s rapide | Oui |

### 5.2 Quand utiliser quoi ?

| Situation | Algorithme recommand√© | Justification |
|---|---|---|
| Premi√®re exploration | Random Forest | Robuste, peu de tuning |
| Performance maximale | XGBoost / LightGBM | Meilleurs scores Kaggle |
| Besoin d'interpr√©tabilit√© | Random Forest | Feature importances intuitives |
| Dataset tr√®s volumineux (>1M lignes) | LightGBM | Rapide, efficace en m√©moire |
| Donn√©es bruit√©es | Random Forest | R√©sistant √† l'overfitting |
| Donn√©es propres + features bien construites | XGBoost | Tire le meilleur parti des donn√©es |
| S√©ries temporelles | Gradient Boosting / XGBoost | Avec feature engineering temporel |
| Domaine r√©glement√© (m√©dical, finance) | Random Forest | Interpr√©tabilit√© + robustesse |

> üí° **Conseil** : "Si vous devez expliquer votre mod√®le √† un m√©tier ou un r√©gulateur (m√©dical, finance, assurance), pr√©f√©rez Random Forest. Si seule la performance compte (comp√©titions, ad ranking), foncez sur XGBoost ou LightGBM."

### 5.3 Benchmark comparatif

```python
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier
)
from sklearn.model_selection import cross_val_score
import xgboost as xgb
import time

# D√©finir les mod√®les √† comparer
modeles = {
    'Random Forest': RandomForestClassifier(
        n_estimators=200, random_state=42, n_jobs=-1
    ),
    'AdaBoost': AdaBoostClassifier(
        n_estimators=200, learning_rate=0.1, random_state=42
    ),
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42
    ),
    'XGBoost': xgb.XGBClassifier(
        n_estimators=200, learning_rate=0.1, max_depth=6,
        random_state=42, n_jobs=-1, eval_metric='logloss'
    )
}

# Comparer avec cross-validation
resultats = {}
for nom, modele in modeles.items():
    debut = time.time()
    scores = cross_val_score(modele, X_train, y_train, cv=5, scoring='roc_auc')
    duree = time.time() - debut
    resultats[nom] = {
        'AUC moyenne': f"{scores.mean():.4f}",
        '√âcart-type': f"{scores.std():.4f}",
        'Temps (s)': f"{duree:.1f}"
    }

# Afficher les r√©sultats
resultats_df = pd.DataFrame(resultats).T
print(resultats_df)
```

---

## 6. üîç Feature Importance ‚Äì Comprendre vos mod√®les

### 6.1 Impurity-based importance (Gini / Entropy)

C'est la m√©thode par d√©faut de Random Forest et Gradient Boosting. Elle mesure la diminution totale de l'impuret√© apport√©e par chaque feature √† travers tous les arbres.

```python
# Feature importance par impuret√© (m√©thode par d√©faut)
importances_gini = rf.feature_importances_

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance_gini': importances_gini
}).sort_values('importance_gini', ascending=False)

print("Top 10 features (impuret√©) :")
print(importance_df.head(10))
```

> ‚ö†Ô∏è **Attention** : "L'importance par impuret√© est **biais√©e** en faveur des features num√©riques √† haute cardinalit√© et des features corr√©l√©es. Une feature al√©atoire avec beaucoup de valeurs uniques peut appara√Ætre comme ¬´ importante ¬ª alors qu'elle ne l'est pas."

### 6.2 Permutation Importance (plus fiable)

La permutation importance mesure la **chute de performance** quand on m√©lange al√©atoirement les valeurs d'une feature. Si le score chute beaucoup, la feature est importante.

```python
from sklearn.inspection import permutation_importance

# Calculer la permutation importance sur le set de test
perm_importance = permutation_importance(
    rf, X_test, y_test,
    n_repeats=10,        # r√©p√©ter 10 fois pour stabilit√©
    random_state=42,
    n_jobs=-1,
    scoring='roc_auc'    # m√©trique utilis√©e
)

# Cr√©er un DataFrame
perm_df = pd.DataFrame({
    'feature': feature_names,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

print("Top 10 features (permutation) :")
print(perm_df.head(10))

# Visualisation avec barres d'erreur
plt.figure(figsize=(10, 6))
top_10 = perm_df.head(10)[::-1]  # inverser pour affichage horizontal
plt.barh(
    top_10['feature'],
    top_10['importance_mean'],
    xerr=top_10['importance_std'],
    color='steelblue'
)
plt.xlabel('Diminution moyenne du score AUC-ROC')
plt.title('Permutation Importance (Top 10)')
plt.tight_layout()
plt.show()
```

> üí° **Conseil de pro** : "Ne faites JAMAIS confiance aux feature importances par impuret√© seules. Utilisez TOUJOURS la permutation importance pour valider. Les deux m√©thodes devraient globalement s'accorder sur les features les plus importantes."

### 6.3 SHAP Values ‚Äì L'√©tat de l'art pour l'interpr√©tabilit√©

SHAP (SHapley Additive exPlanations) est bas√© sur la th√©orie des jeux et fournit une explication **locale** pour chaque pr√©diction.

```python
# Installation : uv add shap
import shap

# Cr√©er un explainer SHAP pour le mod√®le XGBoost
explainer = shap.TreeExplainer(xgb_clf)
shap_values = explainer.shap_values(X_test)

# Graphique global : importance des features
shap.summary_plot(shap_values, X_test, feature_names=feature_names)

# Graphique pour une pr√©diction individuelle
shap.force_plot(
    explainer.expected_value,
    shap_values[0],           # premi√®re observation
    X_test[0],
    feature_names=feature_names
)

# Dependence plot : relation feature <-> impact
shap.dependence_plot("feature_0", shap_values, X_test, feature_names=feature_names)
```

> üí° **Conseil de pro** : "SHAP est LA r√©f√©rence pour l'interpr√©tabilit√©. Il vous dit non seulement quelles features sont importantes, mais COMMENT elles influencent chaque pr√©diction. Indispensable pour les domaines r√©glement√©s."

### 6.4 Comparaison des m√©thodes d'importance

| M√©thode | Fiabilit√© | Vitesse | Scope | Cas d'usage |
|---|---|---|---|---|
| Impurity-based | ‚≠ê‚≠ê Moyenne | ‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s rapide | Global | Exploration rapide |
| Permutation | ‚≠ê‚≠ê‚≠ê‚≠ê Bonne | ‚≠ê‚≠ê Moyenne | Global | Validation, s√©lection |
| SHAP | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellente | ‚≠ê Lente | Global + Local | Production, r√©glement√© |

---

## 7. üìà Tuning avec GridSearchCV et RandomizedSearchCV

### 7.1 GridSearchCV ‚Äì Recherche exhaustive

GridSearchCV teste **toutes les combinaisons** possibles des hyperparam√®tres sp√©cifi√©s. Id√©al pour un espace de recherche restreint.

```python
from sklearn.model_selection import GridSearchCV

# D√©finir la grille d'hyperparam√®tres
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Attention : 3 x 4 x 3 x 3 x 2 = 216 combinaisons x 5 folds = 1080 entra√Ænements !
print(f"Nombre de combinaisons : {3*4*3*3*2}")

# GridSearchCV
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_grid=param_grid,
    cv=5,                    # 5-fold cross-validation
    scoring='roc_auc',       # m√©trique √† optimiser
    n_jobs=-1,               # parall√©liser
    verbose=1,               # afficher la progression
    return_train_score=True  # pour diagnostiquer l'overfitting
)

grid_search.fit(X_train, y_train)

# R√©sultats
print(f"\nMeilleurs hyperparam√®tres : {grid_search.best_params_}")
print(f"Meilleur score AUC-ROC (CV) : {grid_search.best_score_:.4f}")

# Score sur le test set avec le meilleur mod√®le
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
y_proba_best = best_model.predict_proba(X_test)[:, 1]
print(f"Score AUC-ROC (test) : {roc_auc_score(y_test, y_proba_best):.4f}")

# Analyser les r√©sultats
resultats_cv = pd.DataFrame(grid_search.cv_results_)
print("\nTop 5 combinaisons :")
print(resultats_cv.nlargest(5, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score', 'mean_train_score']
])
```

> ‚ö†Ô∏è **Attention** : "GridSearchCV est exhaustif et donc tr√®s lent. Avec 5 hyperparam√®tres √† 4 valeurs chacun, vous avez 4^5 = 1024 combinaisons. Avec 5-fold CV, cela fait 5120 entra√Ænements de mod√®le. R√©fl√©chissez avant de lancer !"

### 7.2 RandomizedSearchCV ‚Äì Recherche al√©atoire

RandomizedSearchCV tire des combinaisons **au hasard** dans des distributions d'hyperparam√®tres. Beaucoup plus efficace pour de grands espaces de recherche.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# D√©finir des distributions (pas juste des listes)
param_distributions = {
    'n_estimators': randint(100, 1000),          # entier entre 100 et 1000
    'max_depth': randint(3, 30),                  # entier entre 3 et 30
    'min_samples_split': randint(2, 20),          # entier entre 2 et 20
    'min_samples_leaf': randint(1, 10),           # entier entre 1 et 10
    'max_features': uniform(0.1, 0.9),            # flottant entre 0.1 et 1.0
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_distributions=param_distributions,
    n_iter=100,              # 100 combinaisons al√©atoires (au lieu de milliers)
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1,
    random_state=42,
    return_train_score=True
)

random_search.fit(X_train, y_train)

print(f"Meilleurs hyperparam√®tres : {random_search.best_params_}")
print(f"Meilleur score AUC-ROC (CV) : {random_search.best_score_:.4f}")
```

> üí° **Conseil** : "Commencez TOUJOURS par RandomizedSearchCV pour explorer largement l'espace des hyperparam√®tres, puis affinez avec GridSearchCV autour des meilleures valeurs trouv√©es. C'est la strat√©gie la plus efficace."

### 7.3 Strat√©gie optimale de tuning

```python
# √âtape 1 : Exploration large avec RandomizedSearchCV
# (voir code ci-dessus)

# √âtape 2 : Affinage cibl√© avec GridSearchCV
# Bas√© sur les meilleurs param√®tres trouv√©s : ex. max_depth=15, n_estimators=400
param_grid_fin = {
    'n_estimators': [350, 400, 450],        # autour de 400
    'max_depth': [12, 15, 18],              # autour de 15
    'min_samples_split': [3, 5, 7],         # autour de 5
    'min_samples_leaf': [1, 2, 3],          # autour de 2
}

grid_fin = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_grid=param_grid_fin,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)

grid_fin.fit(X_train, y_train)
print(f"Score final affin√© : {grid_fin.best_score_:.4f}")
```

> üí° **Conseil de pro** : "Le tuning des hyperparam√®tres donne g√©n√©ralement 2-5% d'am√©lioration. Le feature engineering en donne 10-30%. Ne passez pas des heures √† tuner si vos features sont m√©diocres."

---

## 8. üß† Techniques avanc√©es

### 8.1 Voting Classifier

Combiner plusieurs algorithmes diff√©rents pour un vote final :

```python
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression

# Combiner plusieurs mod√®les diff√©rents
voting = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=200, random_state=42)),
        ('lr', LogisticRegression(max_iter=1000, random_state=42))
    ],
    voting='soft',   # utiliser les probabilit√©s (meilleur que 'hard')
    n_jobs=-1
)

voting.fit(X_train, y_train)
y_proba_voting = voting.predict_proba(X_test)[:, 1]
print(f"AUC-ROC Voting : {roc_auc_score(y_test, y_proba_voting):.4f}")
```

### 8.2 Stacking

Le stacking utilise un **m√©ta-mod√®le** pour combiner les pr√©dictions des mod√®les de base :

```python
from sklearn.ensemble import StackingClassifier

# Stacking : mod√®les de base + m√©ta-mod√®le
stacking = StackingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),
        ('gb', GradientBoostingClassifier(n_estimators=200, random_state=42)),
        ('xgb', xgb.XGBClassifier(n_estimators=200, random_state=42, eval_metric='logloss'))
    ],
    final_estimator=LogisticRegression(max_iter=1000),  # m√©ta-mod√®le
    cv=5,            # cross-validation pour les pr√©dictions interm√©diaires
    n_jobs=-1
)

stacking.fit(X_train, y_train)
y_proba_stack = stacking.predict_proba(X_test)[:, 1]
print(f"AUC-ROC Stacking : {roc_auc_score(y_test, y_proba_stack):.4f}")
```

> üí° **Conseil** : "Le stacking est souvent la m√©thode la plus performante, mais aussi la plus complexe et la plus lente. R√©servez-le pour les comp√©titions ou les cas o√π chaque 0.1% compte."

---

## üéØ Points cl√©s √† retenir

1. **Les m√©thodes d'ensemble** combinent plusieurs mod√®les faibles pour cr√©er un mod√®le fort
2. **Bagging** (Random Forest) r√©duit la variance ‚Üí robuste, peu de tuning
3. **Boosting** (Gradient Boosting, XGBoost) r√©duit le biais ‚Üí performances maximales
4. **Random Forest** est le meilleur premier choix : robuste, interpr√©table, rapide
5. **XGBoost** avec early stopping est l'arme ultime pour la performance
6. **Feature importance** : toujours v√©rifier avec la permutation importance, pas seulement Gini
7. **SHAP** est l'√©tat de l'art pour l'interpr√©tabilit√©
8. **RandomizedSearchCV** d'abord, puis **GridSearchCV** pour affiner
9. Le **tuning** donne 2-5%, le **feature engineering** donne 10-30%
10. **Voting** et **Stacking** pour aller encore plus loin

## ‚úÖ Checklist de validation

- [ ] Je sais expliquer la diff√©rence entre Bagging et Boosting
- [ ] Je sais entra√Æner un Random Forest et interpr√©ter ses feature importances
- [ ] Je comprends le principe du Gradient Boosting (r√©sidus)
- [ ] Je sais utiliser XGBoost avec early stopping
- [ ] Je connais les hyperparam√®tres cl√©s de chaque algorithme
- [ ] Je sais utiliser la permutation importance et SHAP
- [ ] Je ma√Ætrise GridSearchCV et RandomizedSearchCV
- [ ] Je sais choisir entre Random Forest et XGBoost selon le contexte
- [ ] Je comprends le Voting et le Stacking

---

[‚¨ÖÔ∏è Chapitre 5 : SVM et KNN](05-svm-knn.md) | [‚û°Ô∏è Chapitre 7 : Clustering](07-clustering.md)
