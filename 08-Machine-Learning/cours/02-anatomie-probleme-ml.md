# Chapitre 2 : Anatomie d'un ProblÃ¨me ML

## ğŸ¯ Objectifs â€” Phase 0 Â· Semaine 2 Â· Comprendre avant de calculer

- Analyser un cas concret de bout en bout : **prÃ©dire si un client va partir** (churn)
- Explorer un dataset rÃ©el avec Python et Pandas
- Se poser les bonnes questions **avant** de toucher au ML
- Comprendre intuitivement comment mesurer la qualitÃ© d'une prÃ©diction
- MaÃ®triser les commandes Pandas essentielles pour l'exploration
- CrÃ©er des visualisations simples avec Matplotlib et Seaborn
- ConnaÃ®tre le workflow ML complet en 8 Ã©tapes
- Identifier les piÃ¨ges classiques du dÃ©butant

> âš ï¸ **Attention** : "Ce chapitre ne contient **aucun modÃ¨le ML**. ZÃ©ro. On explore, on questionne, on visualise. C'est la partie la plus importante â€” et la plus souvent nÃ©gligÃ©e."

**Livrable attendu** : un rapport d'exploration en franÃ§ais, 0 ligne de ML.

---

## 1. ğŸ¯ Cas concret : prÃ©dire si un client va partir (churn)

### 1.1 Le contexte business

Vous Ãªtes Data Engineer dans une entreprise de tÃ©lÃ©communications. Le directeur commercial vous dit :

> Â« Chaque mois, on perd 15 % de nos clients. Ã‡a nous coÃ»te une fortune en acquisition. Si on pouvait **prÃ©dire** quels clients vont partir **avant** qu'ils partent, on pourrait les contacter et leur proposer une offre de rÃ©tention. Â»

C'est un problÃ¨me de **classification binaire** (supervisÃ©) :
- **Target** : le client va-t-il partir ? â†’ `oui` (1) ou `non` (0)
- **Features** : tout ce qu'on sait sur le client (anciennetÃ©, factures, appels au service client, etc.)

### 1.2 Pourquoi ce cas est parfait pour apprendre

| Raison | DÃ©tail |
|--------|--------|
| **Concret** | Tout le monde a Ã©tÃ© client d'un opÃ©rateur tÃ©lÃ©com |
| **Binaire** | Deux rÃ©ponses possibles seulement (part / ne part pas) |
| **Impact mesurable** | On peut calculer l'argent Ã©conomisÃ© |
| **DonnÃ©es disponibles** | Beaucoup de datasets publics existent |
| **Vraies difficultÃ©s** | DonnÃ©es manquantes, dÃ©sÃ©quilibre, variables catÃ©gorielles |

### 1.3 Ce qu'on va faire dans ce chapitre

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  PLAN DU CHAPITRE                        â•‘
â•‘                                                          â•‘
â•‘  1. Charger les donnÃ©es              (pd.read_csv)       â•‘
â•‘  2. Regarder les donnÃ©es             (head, info)        â•‘
â•‘  3. Comprendre chaque colonne        (describe, unique)  â•‘
â•‘  4. Chercher les problÃ¨mes           (NaN, dÃ©sÃ©quilibre) â•‘
â•‘  5. Visualiser                       (histogrammes)      â•‘
â•‘  6. RÃ©diger un rapport d'exploration                     â•‘
â•‘                                                          â•‘
â•‘  âŒ PAS de modÃ¨le ML                                    â•‘
â•‘  âŒ PAS de prÃ©diction                                   â•‘
â•‘  âœ… 100 % exploration et comprÃ©hension                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 2. ğŸ“‚ Ã€ quoi ressemblent les donnÃ©es ?

### 2.1 Le dataset `clients_churn.csv`

Pour cet exercice, nous allons simuler un dataset rÃ©aliste de clients tÃ©lÃ©com. En production, ces donnÃ©es viendraient de votre data warehouse.

```python
import pandas as pd
import numpy as np

# â”€â”€ CrÃ©er un dataset rÃ©aliste de churn tÃ©lÃ©com â”€â”€
np.random.seed(42)
n = 1000

data = {
    "client_id": range(1, n + 1),
    "anciennete_mois": np.random.randint(1, 72, n),
    "forfait_mensuel": np.round(np.random.uniform(15, 120, n), 2),
    "nb_appels_support": np.random.poisson(1.5, n),
    "data_utilisee_go": np.round(np.random.uniform(0.5, 50, n), 1),
    "contrat": np.random.choice(["mensuel", "annuel", "2 ans"], n, p=[0.5, 0.3, 0.2]),
    "moyen_paiement": np.random.choice(
        ["carte", "virement", "prelevement"], n, p=[0.4, 0.25, 0.35]
    ),
    "satisfaction": np.random.choice([1, 2, 3, 4, 5], n, p=[0.05, 0.1, 0.3, 0.35, 0.2]),
}

df = pd.DataFrame(data)

# CrÃ©er la target (churn) avec une logique rÃ©aliste
proba_churn = (
    0.1
    + 0.3 * (df["contrat"] == "mensuel").astype(int)
    + 0.15 * (df["nb_appels_support"] > 3).astype(int)
    + 0.2 * (df["satisfaction"] <= 2).astype(int)
    - 0.1 * (df["anciennete_mois"] > 24).astype(int)
)
proba_churn = proba_churn.clip(0, 1)
df["churn"] = (np.random.random(n) < proba_churn).astype(int)

# Introduire des valeurs manquantes (rÃ©aliste !)
mask_satisfaction = np.random.random(n) < 0.08
df.loc[mask_satisfaction, "satisfaction"] = np.nan

mask_data = np.random.random(n) < 0.05
df.loc[mask_data, "data_utilisee_go"] = np.nan

# Sauvegarder
df.to_csv("clients_churn.csv", index=False)
print(f"Dataset crÃ©Ã© : {len(df)} lignes, {len(df.columns)} colonnes")
print(f"Fichier sauvegardÃ© : clients_churn.csv")
```

### 2.2 Charger et inspecter les donnÃ©es

```python
import pandas as pd

# â”€â”€ Ã‰tape 1 : Charger les donnÃ©es â”€â”€
df = pd.read_csv("clients_churn.csv")

# â”€â”€ Ã‰tape 2 : PremiÃ¨res lignes â”€â”€
print("=== 5 premiÃ¨res lignes ===")
print(df.head())
```

```
   client_id  anciennete_mois  forfait_mensuel  nb_appels_support  data_utilisee_go  contrat moyen_paiement  satisfaction  churn
0          1               52            89.73                  2              23.4  mensuel          carte           4.0      0
1          2               15            34.56                  0              45.2   annuel     prelevement           3.0      0
2          3               71            78.12                  4              12.8    2 ans       virement           5.0      0
3          4                8           110.45                  3               8.9  mensuel          carte           2.0      1
4          5               33            55.67                  1              31.5  mensuel     prelevement           NaN      1
```

```python
# â”€â”€ Ã‰tape 3 : Structure du dataset â”€â”€
print("\n=== Informations sur le dataset ===")
print(df.info())
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000 entries, 0 to 999
Data columns (total 9 columns):
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   client_id          1000 non-null   int64
 1   anciennete_mois    1000 non-null   int64
 2   forfait_mensuel    1000 non-null   float64
 3   nb_appels_support  1000 non-null   int64
 4   data_utilisee_go   952 non-null    float64    â† 48 valeurs manquantes !
 5   contrat            1000 non-null   object
 6   moyen_paiement     1000 non-null   object
 7   satisfaction       921 non-null    float64    â† 79 valeurs manquantes !
 8   churn              1000 non-null   int64
dtypes: float64(3), int64(4), object(2)
```

> ğŸ’¡ **Conseil** : "`df.info()` est votre meilleur ami. En une commande, vous voyez le nombre de lignes, les types de donnÃ©es et les valeurs manquantes. ExÃ©cutez-le **toujours** en premier."

```python
# â”€â”€ Ã‰tape 4 : Statistiques descriptives â”€â”€
print("\n=== Statistiques numÃ©riques ===")
print(df.describe())
```

```
       client_id  anciennete_mois  forfait_mensuel  nb_appels_support  data_utilisee_go  satisfaction       churn
count    1000.00          1000.00          1000.00            1000.00            952.00        921.00     1000.00
mean      500.50            35.89            67.32               1.52             25.18          3.54        0.28
std       288.82            20.64            30.41               1.23             14.38          1.02        0.45
min         1.00             1.00            15.01               0.00              0.50          1.00        0.00
25%       250.75            18.00            41.23               1.00             13.10          3.00        0.00
50%       500.50            36.00            67.45               1.00             25.20          4.00        0.00
75%       750.25            54.00            93.67               2.00             37.30          4.00        1.00
max      1000.00            71.00           119.98               8.00             49.90          5.00        1.00
```

### 2.3 Comprendre chaque colonne

| Colonne | Type | Description | RÃ´le ML |
|---------|------|-------------|---------|
| `client_id` | int | Identifiant unique du client | **Ã€ exclure** (pas prÃ©dictif) |
| `anciennete_mois` | int | Nombre de mois depuis l'inscription | Feature numÃ©rique |
| `forfait_mensuel` | float | Montant mensuel en euros | Feature numÃ©rique |
| `nb_appels_support` | int | Nombre d'appels au service client | Feature numÃ©rique |
| `data_utilisee_go` | float | Consommation de donnÃ©es en Go | Feature numÃ©rique |
| `contrat` | str | Type de contrat (mensuel/annuel/2 ans) | Feature catÃ©gorielle |
| `moyen_paiement` | str | Carte, virement ou prÃ©lÃ¨vement | Feature catÃ©gorielle |
| `satisfaction` | float | Note de 1 Ã  5 | Feature ordinale |
| `churn` | int | 0 = reste, 1 = parti | **TARGET** |

> âš ï¸ **Attention** : "L'identifiant `client_id` ne doit **jamais** Ãªtre utilisÃ© comme feature. Le modÃ¨le pourrait apprendre que Â« le client 42 part toujours Â» â€” ce qui ne veut rien dire pour un nouveau client."

---

## 3. â“ Quelles questions se poser ?

Avant de toucher au ML, un bon Data Engineer / Data Scientist pose **systÃ©matiquement** ces questions.

### 3.1 Y a-t-il des valeurs manquantes ?

```python
# â”€â”€ Valeurs manquantes par colonne â”€â”€
print("=== Valeurs manquantes ===")
missing = df.isnull().sum()
missing_pct = (df.isnull().sum() / len(df) * 100).round(2)

missing_df = pd.DataFrame({
    "nb_manquantes": missing,
    "pourcentage": missing_pct
})
print(missing_df[missing_df["nb_manquantes"] > 0])
```

```
                  nb_manquantes  pourcentage
data_utilisee_go            48         4.80
satisfaction                79         7.90
```

**Que faire ?** Plusieurs stratÃ©gies existent (on les verra en dÃ©tail au chapitre suivant) :

| StratÃ©gie | Quand l'utiliser | Risque |
|-----------|-----------------|--------|
| Supprimer les lignes | TrÃ¨s peu de valeurs manquantes (< 1 %) | Perte d'information |
| Remplir par la moyenne/mÃ©diane | Valeurs numÃ©riques, peu de manquants | RÃ©duit la variance |
| Remplir par le mode | Valeurs catÃ©gorielles | Introduit un biais |
| CrÃ©er une catÃ©gorie Â« inconnu Â» | Quand le fait d'Ãªtre manquant est informatif | Complexifie le modÃ¨le |
| Utiliser un algorithme robuste | Quand on ne peut pas se permettre de perdre de donnÃ©es | DÃ©pend de l'algorithme |

### 3.2 La target est-elle dÃ©sÃ©quilibrÃ©e ?

```python
# â”€â”€ RÃ©partition de la target â”€â”€
print("=== RÃ©partition du churn ===")
print(df["churn"].value_counts())
print()
print(df["churn"].value_counts(normalize=True).round(3))
```

```
=== RÃ©partition du churn ===
churn
0    720
1    280
Name: count, dtype: int64

churn
0    0.72
1    0.28
Name: proportion, dtype: float64
```

**28 % de churn.** C'est un dÃ©sÃ©quilibre modÃ©rÃ©. Voyons pourquoi c'est important :

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              POURQUOI LE DÃ‰SÃ‰QUILIBRE POSE PROBLÃˆME           â•‘
â•‘                                                               â•‘
â•‘  Si 95 % des clients restent et 5 % partent :                â•‘
â•‘                                                               â•‘
â•‘  Un modÃ¨le qui prÃ©dit TOUJOURS "reste" a 95 % de prÃ©cision ! â•‘
â•‘  Mais il ne dÃ©tecte AUCUN dÃ©part â†’ inutile pour le business. â•‘
â•‘                                                               â•‘
â•‘  C'est comme un dÃ©tecteur d'incendie qui ne sonne jamais :   â•‘
â•‘  il a raison 99.99 % du temps... jusqu'Ã  l'incendie.         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

> ğŸ’¡ **Conseil** : "VÃ©rifiez **toujours** la rÃ©partition de votre target en tout premier. Si elle est trÃ¨s dÃ©sÃ©quilibrÃ©e (ex : 99 % / 1 %), il faudra adapter votre approche."

### 3.3 Quels sont les types de variables ?

```python
# â”€â”€ Types de variables â”€â”€
print("=== Variables numÃ©riques ===")
print(df.select_dtypes(include=["number"]).columns.tolist())

print("\n=== Variables catÃ©gorielles ===")
print(df.select_dtypes(include=["object"]).columns.tolist())

# â”€â”€ Valeurs uniques des catÃ©gorielles â”€â”€
for col in df.select_dtypes(include=["object"]).columns:
    print(f"\n{col} : {df[col].unique()}")
    print(f"  â†’ {df[col].nunique()} valeurs uniques")
```

```
=== Variables numÃ©riques ===
['client_id', 'anciennete_mois', 'forfait_mensuel', 'nb_appels_support',
 'data_utilisee_go', 'satisfaction', 'churn']

=== Variables catÃ©gorielles ===
['contrat', 'moyen_paiement']

contrat : ['mensuel' 'annuel' '2 ans']
  â†’ 3 valeurs uniques

moyen_paiement : ['carte' 'virement' 'prelevement']
  â†’ 3 valeurs uniques
```

| Type de variable | Exemples dans notre dataset | Traitement nÃ©cessaire |
|-----------------|---------------------------|----------------------|
| **NumÃ©rique continue** | forfait_mensuel, data_utilisee_go | Normalisation possible |
| **NumÃ©rique discrÃ¨te** | anciennete_mois, nb_appels_support | Souvent utilisable tel quel |
| **CatÃ©gorielle nominale** | contrat, moyen_paiement | Encodage (one-hot, label) |
| **CatÃ©gorielle ordinale** | satisfaction (1 â†’ 5) | Encodage ordinal |

### 3.4 Y a-t-il des valeurs aberrantes ?

```python
# â”€â”€ DÃ©tecter les valeurs aberrantes (outliers) â”€â”€
print("=== Statistiques pour dÃ©tecter les outliers ===")
for col in ["anciennete_mois", "forfait_mensuel", "nb_appels_support", "data_utilisee_go"]:
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    borne_basse = q1 - 1.5 * iqr
    borne_haute = q3 + 1.5 * iqr
    outliers = df[(df[col] < borne_basse) | (df[col] > borne_haute)]
    print(f"{col:25s} : {len(outliers):3d} outliers  (bornes: [{borne_basse:.1f}, {borne_haute:.1f}])")
```

```
anciennete_mois           :   0 outliers  (bornes: [-36.0, 108.0])
forfait_mensuel           :   0 outliers  (bornes: [-37.4, 172.3])
nb_appels_support         :  23 outliers  (bornes: [-0.5, 3.5])
data_utilisee_go          :   0 outliers  (bornes: [-23.2, 73.6])
```

> ğŸ’¡ **Conseil** : "Un outlier n'est pas forcÃ©ment une erreur. Un client qui appelle 8 fois le support est peut-Ãªtre trÃ¨s mÃ©content â€” et c'est une information **prÃ©cieuse** pour prÃ©dire le churn."

---

## 4. ğŸ“ Comment mesurer si notre prÃ©diction est bonne ?

On ne construit pas encore de modÃ¨le, mais comprenons **intuitivement** comment on jugera sa qualitÃ©.

### 4.1 L'analogie du mÃ©decin

Imaginez un test mÃ©dical pour dÃ©tecter une maladie :

```
                        RÃ‰ALITÃ‰
                   Malade    Pas malade
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   TEST   Positif â”‚  TP âœ…   â”‚  FP âŒ   â”‚   TP = Vrai Positif (bien dÃ©tectÃ©)
   dit :          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   FP = Faux Positif (fausse alarme)
          NÃ©gatif â”‚  FN âŒ   â”‚  TN âœ…   â”‚   FN = Faux NÃ©gatif (ratÃ© !)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   TN = Vrai NÃ©gatif (bien exclu)
```

AppliquÃ© Ã  notre cas de churn :

| Situation | Signification | ConsÃ©quence |
|-----------|--------------|-------------|
| **Vrai Positif (TP)** | On prÃ©dit Â« part Â» et le client part vraiment | On peut le retenir ! |
| **Faux Positif (FP)** | On prÃ©dit Â« part Â» mais le client reste | On dÃ©pense une offre inutilement |
| **Faux NÃ©gatif (FN)** | On prÃ©dit Â« reste Â» mais le client part | On perd le client â€” le pire cas ! |
| **Vrai NÃ©gatif (TN)** | On prÃ©dit Â« reste Â» et le client reste | Tout va bien |

### 4.2 Les mÃ©triques essentielles (sans formule)

| MÃ©trique | Question qu'elle pose | Analogie |
|----------|----------------------|----------|
| **Accuracy** | Â« Sur l'ensemble, combien de prÃ©dictions sont correctes ? Â» | Note globale Ã  un examen |
| **PrÃ©cision** | Â« Parmi ceux qu'on a identifiÃ©s comme "part", combien partent vraiment ? Â» | FiabilitÃ© des alarmes |
| **Rappel** | Â« Parmi tous ceux qui partent, combien a-t-on dÃ©tectÃ©s ? Â» | SensibilitÃ© du radar |
| **F1-score** | Â« Compromis entre prÃ©cision et rappel Â» | Note harmonisÃ©e |

```python
# â”€â”€ Exemple intuitif avec des chiffres â”€â”€

# Imaginons 100 clients : 30 partent, 70 restent
# Notre modÃ¨le prÃ©dit :

predictions = {
    "vrais_positifs":  22,  # on a bien dÃ©tectÃ© 22 des 30 qui partent
    "faux_positifs":   8,   # 8 clients qu'on pensait partants restent
    "faux_negatifs":   8,   # 8 clients partent sans qu'on les ait dÃ©tectÃ©s
    "vrais_negatifs":  62,  # 62 clients restent comme prÃ©vu
}

tp = predictions["vrais_positifs"]
fp = predictions["faux_positifs"]
fn = predictions["faux_negatifs"]
tn = predictions["vrais_negatifs"]

accuracy  = (tp + tn) / (tp + fp + fn + tn)
precision = tp / (tp + fp)
rappel    = tp / (tp + fn)
f1        = 2 * (precision * rappel) / (precision + rappel)

print(f"Accuracy  : {accuracy:.1%}")   # 84.0%
print(f"PrÃ©cision : {precision:.1%}")  # 73.3%
print(f"Rappel    : {rappel:.1%}")     # 73.3%
print(f"F1-score  : {f1:.1%}")         # 73.3%
```

> âš ï¸ **Attention** : "L'accuracy **seule** est souvent trompeuse. Un modÃ¨le qui prÃ©dit toujours Â« reste Â» sur notre dataset aurait 72 % d'accuracy â€” mais dÃ©tecterait 0 % des dÃ©parts. Regardez toujours **plusieurs** mÃ©triques."

---

## 5. ğŸ Premier contact avec Python/Pandas

### 5.1 Les commandes essentielles Ã  connaÃ®tre

```python
import pandas as pd

df = pd.read_csv("clients_churn.csv")

# â”€â”€ Les 10 commandes que vous utiliserez tous les jours â”€â”€

# 1. PremiÃ¨res et derniÃ¨res lignes
df.head(10)        # 10 premiÃ¨res lignes
df.tail(5)         # 5 derniÃ¨res lignes

# 2. Dimensions
print(f"Taille : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")

# 3. Types et valeurs manquantes
df.info()

# 4. Statistiques numÃ©riques
df.describe()

# 5. Valeurs uniques d'une colonne
df["contrat"].value_counts()

# 6. Filtrage
clients_mensuels = df[df["contrat"] == "mensuel"]
print(f"Clients mensuels : {len(clients_mensuels)}")

# 7. Groupement et agrÃ©gation
df.groupby("contrat")["churn"].mean()

# 8. Tri
df.sort_values("forfait_mensuel", ascending=False).head()

# 9. SÃ©lection de colonnes
df[["anciennete_mois", "forfait_mensuel", "churn"]].head()

# 10. CorrÃ©lation entre variables numÃ©riques
df.select_dtypes(include="number").corr()["churn"].sort_values()
```

### 5.2 Calculer des statistiques simples

```python
# â”€â”€ Statistiques par groupe â”€â”€
print("=== Taux de churn par type de contrat ===")
churn_par_contrat = df.groupby("contrat")["churn"].agg(["mean", "count"])
churn_par_contrat.columns = ["taux_churn", "nb_clients"]
churn_par_contrat["taux_churn"] = (churn_par_contrat["taux_churn"] * 100).round(1)
print(churn_par_contrat)
```

```
         taux_churn  nb_clients
contrat
2 ans          12.5         200
annuel         18.7         300
mensuel        40.2         500
```

```python
# â”€â”€ Statistiques descriptives par segment â”€â”€
print("\n=== Forfait mensuel moyen selon le churn ===")
print(df.groupby("churn")["forfait_mensuel"].agg(["mean", "median"]).round(2))

print("\n=== AnciennetÃ© moyenne selon le churn ===")
print(df.groupby("churn")["anciennete_mois"].agg(["mean", "median"]).round(1))

print("\n=== Nombre moyen d'appels support selon le churn ===")
print(df.groupby("churn")["nb_appels_support"].agg(["mean", "median"]).round(2))
```

```
=== Forfait mensuel moyen selon le churn ===
        mean  median
churn
0      66.18   65.34
1      70.25   71.12

=== AnciennetÃ© moyenne selon le churn ===
       mean  median
churn
0      37.2    37.0
1      32.5    31.0

=== Nombre moyen d'appels support selon le churn ===
        mean  median
churn
0       1.38    1.00
1       1.88    2.00
```

> ğŸ’¡ **Conseil** : "Ces statistiques simples donnent dÃ©jÃ  des indices : les clients qui partent ont en moyenne un forfait plus Ã©levÃ©, une anciennetÃ© plus faible et appellent plus souvent le support. On n'a pas eu besoin de ML pour Ã§a !"

### 5.3 Visualiser avec Matplotlib et Seaborn

```python
import matplotlib.pyplot as plt
import seaborn as sns

# â”€â”€ Configuration globale â”€â”€
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_palette("Set2")
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle("Exploration du dataset Churn TÃ©lÃ©com", fontsize=16, fontweight="bold")

# â”€â”€ 1. RÃ©partition de la target â”€â”€
ax = axes[0, 0]
df["churn"].value_counts().plot(kind="bar", ax=ax, color=["#66c2a5", "#fc8d62"])
ax.set_title("RÃ©partition du Churn")
ax.set_xticklabels(["Reste (0)", "Part (1)"], rotation=0)
ax.set_ylabel("Nombre de clients")

# â”€â”€ 2. Distribution de l'anciennetÃ© â”€â”€
ax = axes[0, 1]
sns.histplot(data=df, x="anciennete_mois", hue="churn", bins=20, ax=ax, kde=True)
ax.set_title("AnciennetÃ© par statut de churn")
ax.set_xlabel("AnciennetÃ© (mois)")

# â”€â”€ 3. Forfait mensuel par churn â”€â”€
ax = axes[0, 2]
sns.boxplot(data=df, x="churn", y="forfait_mensuel", ax=ax)
ax.set_title("Forfait mensuel par churn")
ax.set_xticklabels(["Reste", "Part"])

# â”€â”€ 4. Churn par type de contrat â”€â”€
ax = axes[1, 0]
churn_rate = df.groupby("contrat")["churn"].mean().sort_values()
churn_rate.plot(kind="barh", ax=ax, color="#8da0cb")
ax.set_title("Taux de churn par contrat")
ax.set_xlabel("Taux de churn")

# â”€â”€ 5. Nombre d'appels support â”€â”€
ax = axes[1, 1]
sns.countplot(data=df, x="nb_appels_support", hue="churn", ax=ax)
ax.set_title("Appels support par churn")
ax.set_xlabel("Nombre d'appels")

# â”€â”€ 6. Satisfaction vs Churn â”€â”€
ax = axes[1, 2]
df_satisfaction = df.dropna(subset=["satisfaction"])
sns.countplot(data=df_satisfaction, x="satisfaction", hue="churn", ax=ax)
ax.set_title("Satisfaction par churn")
ax.set_xlabel("Note de satisfaction")

plt.tight_layout()
plt.savefig("exploration_churn.png", dpi=150, bbox_inches="tight")
plt.show()
print("Graphique sauvegardÃ© : exploration_churn.png")
```

### 5.4 Matrice de corrÃ©lation

```python
# â”€â”€ Matrice de corrÃ©lation â”€â”€
plt.figure(figsize=(10, 8))
correlation = df.select_dtypes(include="number").corr()

sns.heatmap(
    correlation,
    annot=True,          # afficher les valeurs
    fmt=".2f",           # 2 dÃ©cimales
    cmap="coolwarm",     # palette de couleurs
    center=0,            # centrer sur 0
    square=True,
    linewidths=0.5,
)
plt.title("Matrice de corrÃ©lation â€” Variables numÃ©riques", fontsize=14)
plt.tight_layout()
plt.savefig("correlation_churn.png", dpi=150)
plt.show()
```

> ğŸ’¡ **Conseil** : "La matrice de corrÃ©lation montre les relations **linÃ©aires** entre variables. Une corrÃ©lation de 0 ne signifie pas Â« pas de relation Â» â€” elle peut Ãªtre non-linÃ©aire. Mais c'est un excellent point de dÃ©part."

---

## 6. ğŸ”„ Le workflow ML complet (8 Ã©tapes)

MÃªme si on ne fait pas de ML dans ce chapitre, voici la vue d'ensemble du processus complet. Chaque Ã©tape fera l'objet d'un chapitre dÃ©diÃ©.

### 6.1 Diagramme du workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WORKFLOW ML EN 8 Ã‰TAPES                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 1. DÃ‰FINIR LE        â”‚  Â« Qu'est-ce qu'on veut prÃ©dire ? Â»  â”‚
â”‚  â”‚    PROBLÃˆME          â”‚  Â« Quelle mÃ©trique de succÃ¨s ? Â»      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 2. COLLECTER LES     â”‚  SQL, API, CSV, scraping...           â”‚
â”‚  â”‚    DONNÃ‰ES           â”‚  C'est le rÃ´le du Data Engineer !     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 3. EXPLORER          â”‚  â† VOUS ÃŠTES ICI (ce chapitre)       â”‚
â”‚  â”‚    (EDA)             â”‚  head(), describe(), visualisations   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 4. PRÃ‰PARER LES      â”‚  Nettoyage, encodage, normalisation  â”‚
â”‚  â”‚    DONNÃ‰ES           â”‚  Feature engineering                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 5. SÃ‰PARER           â”‚  Train / Validation / Test            â”‚
â”‚  â”‚    TRAIN/TEST        â”‚  Ne JAMAIS tricher !                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 6. ENTRAÃNER         â”‚  Choisir un algorithme, fit()         â”‚
â”‚  â”‚    LE MODÃˆLE         â”‚  Ajuster les hyperparamÃ¨tres          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 7. Ã‰VALUER           â”‚  Accuracy, prÃ©cision, rappel, F1     â”‚
â”‚  â”‚    LE MODÃˆLE         â”‚  Matrice de confusion                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚             â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚  â”‚ 8. DÃ‰PLOYER          â”‚  API, batch, monitoring               â”‚
â”‚  â”‚    EN PRODUCTION     â”‚  MLOps                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                                                                 â”‚
â”‚  âŸ² ItÃ©ratif : on revient souvent aux Ã©tapes 3-4 !              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Description de chaque Ã©tape

| Ã‰tape | Nom | Qui la fait ? | Temps passÃ© (%) |
|-------|-----|---------------|-----------------|
| 1 | DÃ©finir le problÃ¨me | Business + Data Scientist | 5 % |
| 2 | Collecter les donnÃ©es | **Data Engineer** | 20 % |
| 3 | Explorer (EDA) | Data Scientist / Data Analyst | 15 % |
| 4 | PrÃ©parer les donnÃ©es | **Data Engineer** + Data Scientist | 30 % |
| 5 | SÃ©parer train/test | Data Scientist | 2 % |
| 6 | EntraÃ®ner le modÃ¨le | Data Scientist | 10 % |
| 7 | Ã‰valuer le modÃ¨le | Data Scientist | 8 % |
| 8 | DÃ©ployer en production | **Data Engineer** / MLOps | 10 % |

> âš ï¸ **Attention** : "Les Ã©tapes 2, 3 et 4 (donnÃ©es) reprÃ©sentent **65 % du travail** d'un projet ML. C'est pourquoi le rÃ´le du Data Engineer est **fondamental** dans le ML â€” sans donnÃ©es propres, aucun modÃ¨le ne peut Ãªtre bon."

---

## 7. ğŸš¨ Les piÃ¨ges classiques du dÃ©butant

### 7.1 PiÃ¨ge nÂ°1 â€” Sauter l'exploration

```
âŒ MAUVAISE APPROCHE                    âœ… BONNE APPROCHE

"J'ai des donnÃ©es, je lance            "J'ai des donnÃ©es, je regarde
 un Random Forest tout de suite !"       Ã  quoi elles ressemblent,
                                         je comprends chaque colonne,
 â†’ RÃ©sultat : 45% d'accuracy            je nettoie, PUIS je modÃ©lise."
 â†’ 3 jours de perdu
                                         â†’ RÃ©sultat : 87% d'accuracy
                                         â†’ Temps total plus court
```

### 7.2 PiÃ¨ge nÂ°2 â€” Ignorer les valeurs manquantes

```python
# âŒ Ne JAMAIS faire Ã§a sans y rÃ©flÃ©chir :
df_nettoye = df.dropna()  # on perd potentiellement beaucoup de lignes !
print(f"Avant : {len(df)} lignes")
print(f"AprÃ¨s : {len(df_nettoye)} lignes")
print(f"Perdu : {len(df) - len(df_nettoye)} lignes ({(len(df) - len(df_nettoye))/len(df)*100:.1f}%)")
```

```
Avant : 1000 lignes
AprÃ¨s : 879 lignes
Perdu : 121 lignes (12.1%)
```

### 7.3 PiÃ¨ge nÂ°3 â€” Utiliser l'identifiant comme feature

```python
# âŒ Le modÃ¨le apprend que "client 42 part" â€” non-sens !
# X = df[["client_id", "anciennete_mois", "forfait_mensuel"]]

# âœ… Exclure l'identifiant
X = df[["anciennete_mois", "forfait_mensuel", "nb_appels_support"]]
```

### 7.4 PiÃ¨ge nÂ°4 â€” Ne pas vÃ©rifier le dÃ©sÃ©quilibre de la target

```python
# âŒ Si 99% des clients restent et on ne vÃ©rifie pas :
# Le modÃ¨le prÃ©dit TOUJOURS "reste" â†’ 99% d'accuracy, 0% d'utilitÃ©

# âœ… Toujours vÃ©rifier :
print(df["churn"].value_counts(normalize=True))
```

### 7.5 PiÃ¨ge nÂ°5 â€” Confondre corrÃ©lation et causalitÃ©

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            CORRÃ‰LATION â‰  CAUSALITÃ‰                            â•‘
â•‘                                                               â•‘
â•‘  Â« Les clients qui appellent souvent le support partent       â•‘
â•‘    plus souvent. Â»                                            â•‘
â•‘                                                               â•‘
â•‘  â†’ Cela NE VEUT PAS DIRE que les appels CAUSENT le churn.    â•‘
â•‘  â†’ C'est peut-Ãªtre l'inverse : les clients mÃ©contents         â•‘
â•‘    appellent ET partent (cause commune : mauvais service).    â•‘
â•‘                                                               â•‘
â•‘  Le ML dÃ©tecte des CORRÃ‰LATIONS, pas des CAUSES.              â•‘
â•‘  Les dÃ©cisions business doivent en tenir compte.              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 7.6 PiÃ¨ge nÂ°6 â€” NÃ©gliger les variables catÃ©gorielles

```python
# âŒ Oublier d'encoder les variables texte
# La plupart des algorithmes ML ne comprennent PAS le texte brut

# âœ… VÃ©rifier les types AVANT de modÃ©liser
print("Types de donnÃ©es :")
print(df.dtypes)
print()
print("Variables Ã  encoder :", df.select_dtypes(include="object").columns.tolist())
```

### 7.7 PiÃ¨ge nÂ°7 â€” Ne pas sÃ©parer les donnÃ©es correctement

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     âŒ ENTRAÃNER ET Ã‰VALUER SUR LES MÃŠMES DONNÃ‰ES            â•‘
â•‘                                                               â•‘
â•‘  C'est comme donner l'examen avec le corrigÃ© sous les yeux.  â•‘
â•‘  Le modÃ¨le aura 99 % de prÃ©cision... mais Ã©chouera sur       â•‘
â•‘  de nouvelles donnÃ©es qu'il n'a jamais vues.                  â•‘
â•‘                                                               â•‘
â•‘  âœ… Toujours sÃ©parer :                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â•‘
â•‘  â”‚  DonnÃ©es totales (100%)                  â”‚                 â•‘
â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚                 â•‘
â•‘  â”‚  â”‚  Train (70-80%)  â”‚  Test (20-30%)â”‚    â”‚                 â•‘
â•‘  â”‚  â”‚  Le modÃ¨le       â”‚  On Ã©value    â”‚    â”‚                 â•‘
â•‘  â”‚  â”‚  apprend ici     â”‚  ici          â”‚    â”‚                 â•‘
â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                 â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 7.8 RÃ©sumÃ© des piÃ¨ges

| PiÃ¨ge | ConsÃ©quence | Solution |
|-------|-------------|----------|
| Sauter l'exploration | Mauvais modÃ¨le, temps perdu | Toujours commencer par `df.info()` et `df.describe()` |
| Ignorer les NaN | Erreurs ou perte de donnÃ©es | Analyser le pattern de valeurs manquantes |
| Utiliser l'ID comme feature | Le modÃ¨le apprend du bruit | Exclure les identifiants |
| Ignorer le dÃ©sÃ©quilibre | MÃ©trique trompeuse | VÃ©rifier `value_counts(normalize=True)` |
| CorrÃ©lation = causalitÃ© | Mauvaises dÃ©cisions business | Toujours raisonner sur la causalitÃ© |
| Oublier l'encodage | Algorithme qui plante | VÃ©rifier les `dtypes` |
| Ã‰valuer sur les donnÃ©es d'entraÃ®nement | Surestimation des performances | Toujours sÃ©parer train/test |

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Explorer avant de modÃ©liser** : 65 % du travail ML, c'est comprendre et prÃ©parer les donnÃ©es.
2. `df.info()`, `df.describe()`, `df.head()` sont vos **trois premiÃ¨res commandes** sur tout nouveau dataset.
3. VÃ©rifiez **toujours** : les valeurs manquantes, le dÃ©sÃ©quilibre de la target, les types de variables.
4. Les visualisations (histogrammes, boxplots, heatmaps) rÃ©vÃ¨lent des patterns que les chiffres seuls ne montrent pas.
5. Le workflow ML comporte **8 Ã©tapes** â€” le modÃ¨le n'arrive qu'Ã  l'Ã©tape 6. Les 5 premiÃ¨res sont de la prÃ©paration.
6. Les **piÃ¨ges classiques** (dÃ©sÃ©quilibre, fuite de donnÃ©es, corrÃ©lation vs causalitÃ©) coÃ»tent plus cher que le choix de l'algorithme.
7. En tant que Data Engineer, votre rÃ´le couvre les Ã©tapes 2, 4 et 8 â€” les **fondations** sur lesquelles tout le reste repose.

---

## âœ… Checklist de validation

Avant de passer au chapitre suivant, vÃ©rifiez que vous pouvez :

- [ ] Charger un fichier CSV avec `pd.read_csv()` et afficher les 5 premiÃ¨res lignes
- [ ] Utiliser `df.info()` pour identifier les types de donnÃ©es et les valeurs manquantes
- [ ] Utiliser `df.describe()` pour obtenir les statistiques descriptives
- [ ] Calculer le taux de churn (ou toute target) avec `value_counts(normalize=True)`
- [ ] Identifier les variables numÃ©riques et catÃ©gorielles dans un dataset
- [ ] CrÃ©er au moins 3 types de graphiques : histogramme, boxplot, countplot
- [ ] Calculer des statistiques par groupe avec `groupby().agg()`
- [ ] Lister les 8 Ã©tapes du workflow ML dans l'ordre
- [ ] Nommer au moins 5 piÃ¨ges classiques du dÃ©butant ML
- [ ] RÃ©diger un rapport d'exploration synthÃ©tique Ã  partir de vos observations

---

**PrÃ©cÃ©dent** : [â† Chapitre 1 â€” Qu'est-ce que le ML ?](01-quest-ce-que-le-ml.md) | **Suivant** : [Chapitre 3 â€” PrÃ©paration des donnÃ©es â†’](03-preprocessing.md)
