# Chapitre 13 : Validation et GÃ©nÃ©ralisation

## ğŸ¯ Objectifs

- MaÃ®triser les stratÃ©gies de dÃ©coupage train/validation/test
- Comprendre et implÃ©menter la **cross-validation** (K-Fold, Stratified, Time Series)
- Savoir diagnostiquer l'**overfitting** et l'**underfitting** avec les courbes d'apprentissage
- Utiliser les courbes de validation pour trouver les bons hyperparamÃ¨tres
- MaÃ®triser **GridSearchCV** et **RandomizedSearchCV** pour le tuning

> **Phase 4 - Semaine 13**

---

## 1. ğŸ”€ Train/Test Split

### 1.1 Pourquoi sÃ©parer les donnÃ©es ?

Un modÃ¨le qui s'Ã©value sur les donnÃ©es qu'il a vues pendant l'entraÃ®nement est comme un Ã©tudiant qui connaÃ®t les rÃ©ponses Ã  l'avance : il aura 20/20 mais ne saura rien faire en situation rÃ©elle. C'est l'**overfitting**.

```
SANS split (MAUVAIS) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DonnÃ©es complÃ¨tes        â”‚
â”‚     Train dessus + Ã‰value dessus â”‚  â†’ Score artificiellement Ã©levÃ©
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AVEC split (BON) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Train (80%)        â”‚ Test (20%)â”‚
â”‚   Apprend ici        â”‚ Ã‰value iciâ”‚  â†’ Score honnÃªte
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Proportions classiques

| Split | Train | Test | Usage |
|-------|-------|------|-------|
| 80/20 | 80% | 20% | Le plus courant |
| 70/30 | 70% | 30% | Petits datasets (< 5000) |
| 90/10 | 90% | 10% | Grands datasets (> 100 000) |

### 1.3 ImplÃ©mentation

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer

# --- Charger les donnÃ©es ---
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target

print(f"Dataset : {X.shape[0]} Ã©chantillons, {X.shape[1]} features")
print(f"Distribution des classes : {np.bincount(y)}")

# --- Split basique ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 20% pour le test
    random_state=42       # ReproductibilitÃ© !
)

print(f"\nTrain : {X_train.shape[0]} Ã©chantillons")
print(f"Test  : {X_test.shape[0]} Ã©chantillons")
```

### 1.4 Stratification pour classes dÃ©sÃ©quilibrÃ©es

Sans stratification, le split alÃ©atoire peut crÃ©er un dÃ©sÃ©quilibre entre train et test :

```python
# --- SANS stratification (risquÃ©) ---
X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print(f"Train - Classe 0 : {(y_train_ns == 0).mean():.2%}")
print(f"Test  - Classe 0 : {(y_test_ns == 0).mean():.2%}")

# --- AVEC stratification (recommandÃ©) ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # â† ICI
)
print(f"\nAvec stratification :")
print(f"Train - Classe 0 : {(y_train == 0).mean():.2%}")
print(f"Test  - Classe 0 : {(y_test == 0).mean():.2%}")
# Les proportions sont identiques entre train et test
```

> ğŸ’¡ **Conseil** : "Utilisez **toujours** `stratify=y` pour la classification. Cela garantit que chaque split a la mÃªme proportion de classes que le dataset complet."

### 1.5 random_state pour la reproductibilitÃ©

```python
# Sans random_state â†’ rÃ©sultats diffÃ©rents Ã  chaque exÃ©cution
X_train_1, _, _, _ = train_test_split(X, y, test_size=0.2)
X_train_2, _, _, _ = train_test_split(X, y, test_size=0.2)
print(f"MÃªmes donnÃ©es ? {(X_train_1.index == X_train_2.index).all()}")  # False

# Avec random_state â†’ rÃ©sultats identiques
X_train_3, _, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)
X_train_4, _, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"MÃªmes donnÃ©es ? {(X_train_3.index == X_train_4.index).all()}")  # True
```

> âš ï¸ **Attention** : "Fixez **toujours** `random_state` pour que vos rÃ©sultats soient reproductibles. Cela ne biaise pas les donnÃ©es, Ã§a fixe juste le tirage alÃ©atoire."

---

## 2. ğŸ“Š Validation Set (Train / Validation / Test)

### 2.1 Pourquoi 3 ensembles ?

Avec seulement train/test, si vous tuner vos hyperparamÃ¨tres en vous basant sur le score test, vous **overfittez sur le test set** ! C'est comme si vous ajustiez vos rÃ©ponses aprÃ¨s avoir vu la correction.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Train (60%)      â”‚ Val (20%)â”‚ Test (20%)â”‚
â”‚                    â”‚          â”‚           â”‚
â”‚ EntraÃ®ner le       â”‚ Tuner    â”‚ Ã‰valuationâ”‚
â”‚ modÃ¨le             â”‚ hyper-   â”‚ FINALE    â”‚
â”‚                    â”‚ paramÃ¨tresâ”‚ (1 seule â”‚
â”‚                    â”‚          â”‚  fois !)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Workflow complet

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

# Ã‰tape 1 : SÃ©parer le test set (on n'y touche PLUS)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Ã‰tape 2 : SÃ©parer train et validation
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)
# 0.25 * 0.80 = 0.20 â†’ on a bien 60/20/20

print(f"Train : {X_train.shape[0]} | Val : {X_val.shape[0]} | Test : {X_test.shape[0]}")

# Ã‰tape 3 : Tester plusieurs hyperparamÃ¨tres sur le validation set
for n_est in [50, 100, 200]:
    for depth in [3, 5, 10, None]:
        rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, random_state=42)
        rf.fit(X_train, y_train)
        score_val = f1_score(y_val, rf.predict(X_val))
        print(f"n_estimators={n_est}, max_depth={depth} â†’ F1 val = {score_val:.4f}")

# Ã‰tape 4 : Meilleur modÃ¨le â†’ Ã©valuer UNE SEULE FOIS sur le test set
best_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
best_model.fit(X_train, y_train)
score_test = f1_score(y_test, best_model.predict(X_test))
print(f"\n=== Score FINAL sur test set : F1 = {score_test:.4f} ===")
```

> âš ï¸ **Attention** : "Le test set ne doit Ãªtre utilisÃ© qu'UNE SEULE FOIS, Ã  la toute fin. Si vous l'utilisez plusieurs fois pour ajuster votre modÃ¨le, votre estimation de performance est biaisÃ©e."

---

## 3. ğŸ”„ Cross-Validation

### 3.1 K-Fold expliquÃ© visuellement

Le problÃ¨me du validation set : on perd 20% de donnÃ©es pour la validation. Avec peu de donnÃ©es, c'est dommage.

La **cross-validation** rÃ©sout ce problÃ¨me : chaque Ã©chantillon sert **Ã  la fois** Ã  l'entraÃ®nement et Ã  la validation.

```
K-Fold avec K=5 :

Fold 1: [VAL][TRAIN][TRAIN][TRAIN][TRAIN] â†’ Score 1
Fold 2: [TRAIN][VAL][TRAIN][TRAIN][TRAIN] â†’ Score 2
Fold 3: [TRAIN][TRAIN][VAL][TRAIN][TRAIN] â†’ Score 3
Fold 4: [TRAIN][TRAIN][TRAIN][VAL][TRAIN] â†’ Score 4
Fold 5: [TRAIN][TRAIN][TRAIN][TRAIN][VAL] â†’ Score 5

Score final = moyenne(Score 1, Score 2, ..., Score 5) Â± Ã©cart-type
```

Chaque donnÃ©e est utilisÃ©e **exactement 1 fois** pour la validation et **K-1 fois** pour l'entraÃ®nement.

### 3.2 Code avec cross_val_score

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
import numpy as np

model = RandomForestClassifier(n_estimators=100, random_state=42)

# --- Cross-validation basique (5-Fold) ---
scores = cross_val_score(
    model, X, y,
    cv=5,                  # Nombre de folds
    scoring='f1',          # MÃ©trique
    n_jobs=-1              # ParallÃ©liser
)

print(f"Scores par fold : {scores}")
print(f"F1 moyen : {scores.mean():.4f} (Â± {scores.std():.4f})")
```

### 3.3 Stratified K-Fold pour classes dÃ©sÃ©quilibrÃ©es

Le K-Fold classique ne garantit pas que chaque fold ait la mÃªme proportion de classes. Le **Stratified K-Fold** le fait.

```python
from sklearn.model_selection import StratifiedKFold

# --- Stratified K-Fold (recommandÃ© pour la classification) ---
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores_strat = cross_val_score(
    model, X, y,
    cv=skf,                # Utiliser StratifiedKFold
    scoring='f1',
    n_jobs=-1
)

print(f"Stratified K-Fold - F1 : {scores_strat.mean():.4f} (Â± {scores_strat.std():.4f})")

# VÃ©rifier les proportions dans chaque fold
for i, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    prop = y[val_idx].mean()
    print(f"  Fold {i+1} - Proportion classe 1 : {prop:.2%}")
```

> ğŸ’¡ **Conseil** : "Pour la classification, utilisez **toujours** `StratifiedKFold` ou passez `cv=StratifiedKFold(5)` Ã  `cross_val_score`. C'est le comportement par dÃ©faut de sklearn quand vous passez un entier, mais c'est mieux d'Ãªtre explicite."

### 3.4 Leave-One-Out (LOO)

Le LOO est un K-Fold oÃ¹ K = N (nombre d'Ã©chantillons). Chaque fold utilise **un seul Ã©chantillon** comme validation.

```python
from sklearn.model_selection import LeaveOneOut

loo = LeaveOneOut()

# Attention : trÃ¨s lent sur de gros datasets !
# Utilisez seulement avec < 200 Ã©chantillons
# scores_loo = cross_val_score(model, X, y, cv=loo, scoring='f1')
print(f"Nombre de folds avec LOO : {loo.get_n_splits(X)}")
```

| MÃ©thode | K | Variance | Biais | Temps |
|---------|---|----------|-------|-------|
| 5-Fold | 5 | ModÃ©rÃ©e | ModÃ©rÃ© | Rapide |
| 10-Fold | 10 | Faible | Faible | ModÃ©rÃ© |
| LOO | N | TrÃ¨s faible | TrÃ¨s faible | TrÃ¨s lent |

### 3.5 Repeated K-Fold

Pour rÃ©duire la variance, on peut rÃ©pÃ©ter le K-Fold avec des shuffles diffÃ©rents :

```python
from sklearn.model_selection import RepeatedStratifiedKFold

rskf = RepeatedStratifiedKFold(
    n_splits=5,      # K = 5
    n_repeats=10,    # RÃ©pÃ©ter 10 fois
    random_state=42
)

scores_repeated = cross_val_score(model, X, y, cv=rskf, scoring='f1', n_jobs=-1)
print(f"Repeated 5-Fold (x10) - F1 : {scores_repeated.mean():.4f} (Â± {scores_repeated.std():.4f})")
print(f"Nombre total de fits : {len(scores_repeated)}")  # 50
```

---

## 4. â° Time Series Split

### 4.1 Pourquoi K-Fold ne marche pas pour les sÃ©ries temporelles

En sÃ©ries temporelles, les donnÃ©es futures dÃ©pendent des donnÃ©es passÃ©es. Si le fold de validation contient des donnÃ©es **passÃ©es** et le train des donnÃ©es **futures**, on a une **fuite de donnÃ©es** (data leakage).

```
K-Fold CLASSIQUE (MAUVAIS pour sÃ©ries temporelles) :
Fold 1: [VAL_jan][TRAIN_fev][TRAIN_mar][TRAIN_avr][TRAIN_mai]
         â†‘ Le modÃ¨le voit le futur avant le passÃ© ! FUITE !

Time Series Split (BON) :
Fold 1: [TRAIN_jan][VAL_fev]
Fold 2: [TRAIN_jan][TRAIN_fev][VAL_mar]
Fold 3: [TRAIN_jan][TRAIN_fev][TRAIN_mar][VAL_avr]
Fold 4: [TRAIN_jan][TRAIN_fev][TRAIN_mar][TRAIN_avr][VAL_mai]
         â†‘ Le modÃ¨le voit TOUJOURS le passÃ© avant le futur âœ“
```

### 4.2 TimeSeriesSplit de sklearn

```python
from sklearn.model_selection import TimeSeriesSplit
import matplotlib.pyplot as plt
import numpy as np

# Simuler des donnÃ©es temporelles
n_samples = 100
X_ts = np.arange(n_samples).reshape(-1, 1)
y_ts = np.sin(X_ts.ravel() / 10) + np.random.normal(0, 0.1, n_samples)

# --- TimeSeriesSplit ---
tscv = TimeSeriesSplit(n_splits=5)

# Visualisation des folds
fig, ax = plt.subplots(figsize=(12, 5))
for i, (train_idx, val_idx) in enumerate(tscv.split(X_ts)):
    ax.scatter(train_idx, [i] * len(train_idx), c='blue', s=10, label='Train' if i == 0 else '')
    ax.scatter(val_idx, [i] * len(val_idx), c='red', s=10, label='Validation' if i == 0 else '')

ax.set_xlabel('Index temporel')
ax.set_ylabel('Fold')
ax.set_yticks(range(5))
ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])
ax.set_title('TimeSeriesSplit - Visualisation des folds')
ax.legend(loc='upper left')
plt.tight_layout()
plt.show()

# Utilisation avec cross_val_score
from sklearn.linear_model import LinearRegression

scores_ts = cross_val_score(
    LinearRegression(), X_ts, y_ts,
    cv=tscv,
    scoring='neg_mean_squared_error'
)
print(f"MSE par fold : {-scores_ts}")
print(f"MSE moyen : {-scores_ts.mean():.4f}")
```

> âš ï¸ **Attention** : "Pour les sÃ©ries temporelles, n'utilisez **JAMAIS** un K-Fold classique. Utilisez toujours `TimeSeriesSplit` pour respecter l'ordre chronologique des donnÃ©es."

---

## 5. ğŸ“‰ Courbes d'apprentissage

### 5.1 Diagnostic : underfitting vs overfitting

Les courbes d'apprentissage tracent le score en fonction du **nombre d'Ã©chantillons d'entraÃ®nement**. Elles permettent de diagnostiquer les problÃ¨mes du modÃ¨le.

```
CAS 1 : BON MODÃˆLE                     CAS 2 : OVERFITTING
Score                                   Score
1.0 â”‚â”€â”€â”€â”€â”€ Train                        1.0 â”‚â”€â”€â”€â”€â”€ Train
    â”‚                                       â”‚
    â”‚                                       â”‚
    â”‚  â”€â”€â”€â”€â”€ Validation                     â”‚     Grand Ã©cart !
    â”‚                                       â”‚
    â”‚  Les deux convergent                  â”‚  â”€â”€â”€â”€â”€ Validation
0.5 â”‚                                   0.5 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Taille            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Taille


CAS 3 : UNDERFITTING                   CAS 4 : BESOIN DE PLUS DE DONNÃ‰ES
Score                                   Score
1.0 â”‚                                   1.0 â”‚â”€â”€â”€â”€â”€ Train
    â”‚                                       â”‚
    â”‚                                       â”‚
    â”‚                                       â”‚  Converge mais pas encore
    â”‚  â”€â”€â”€â”€â”€ Train                          â”‚
    â”‚  â”€â”€â”€â”€â”€ Validation                     â”‚  â”€â”€â”€â”€â”€ Validation
0.5 â”‚  Les deux sont bas !              0.5 â”‚  Encore en train de monter
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Taille            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Taille
```

| Diagnostic | SymptÃ´me | Solution |
|-----------|----------|----------|
| **Bon modÃ¨le** | Train et Val convergent Ã  un score Ã©levÃ© | Rien Ã  faire ! |
| **Overfitting** | Train Ã©levÃ©, Val bien plus bas | RÃ©gulariser, plus de donnÃ©es, simplifier |
| **Underfitting** | Train et Val tous deux bas | ModÃ¨le plus complexe, plus de features |
| **Besoin de donnÃ©es** | Val monte encore Ã  la fin | Collecter plus de donnÃ©es |

### 5.2 Code complet avec visualisation

```python
from sklearn.model_selection import learning_curve
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt

model = RandomForestClassifier(n_estimators=100, random_state=42)

# --- Calculer les courbes d'apprentissage ---
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y,
    train_sizes=np.linspace(0.1, 1.0, 10),  # De 10% Ã  100% des donnÃ©es
    cv=5,
    scoring='f1',
    n_jobs=-1,
    random_state=42
)

# Moyennes et Ã©carts-types
train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
val_mean = val_scores.mean(axis=1)
val_std = val_scores.std(axis=1)

# --- Visualiser ---
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, 'b-o', label='Score Train')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'r-o', label='Score Validation')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
plt.xlabel("Nombre d'Ã©chantillons d'entraÃ®nement")
plt.ylabel('F1-Score')
plt.title("Courbe d'apprentissage â€” Random Forest")
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# --- InterprÃ©tation automatique ---
gap = train_mean[-1] - val_mean[-1]
print(f"Score train final : {train_mean[-1]:.4f}")
print(f"Score val final   : {val_mean[-1]:.4f}")
print(f"Ã‰cart (gap)       : {gap:.4f}")

if gap > 0.1:
    print("â†’ OVERFITTING : le modÃ¨le mÃ©morise les donnÃ©es d'entraÃ®nement.")
elif val_mean[-1] < 0.7:
    print("â†’ UNDERFITTING : le modÃ¨le est trop simple.")
else:
    print("â†’ BON MODÃˆLE : les scores convergent Ã  un niveau Ã©levÃ©.")
```

---

## 6. ğŸ“ˆ Courbes de validation

### 6.1 Trouver le bon hyperparamÃ¨tre

La courbe de validation trace le score en fonction de **la valeur d'un hyperparamÃ¨tre**. Elle permet de trouver la valeur optimale.

```python
from sklearn.model_selection import validation_curve
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt

# --- Courbe de validation pour max_depth ---
param_range = [1, 2, 3, 5, 7, 10, 15, 20, None]
param_range_plot = [1, 2, 3, 5, 7, 10, 15, 20, 25]  # Pour le plot

train_scores, val_scores = validation_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X, y,
    param_name='max_depth',
    param_range=param_range,
    cv=5,
    scoring='f1',
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
val_mean = val_scores.mean(axis=1)

plt.figure(figsize=(10, 6))
plt.plot(param_range_plot, train_mean, 'b-o', label='Score Train')
plt.plot(param_range_plot, val_mean, 'r-o', label='Score Validation')
plt.xlabel('max_depth')
plt.ylabel('F1-Score')
plt.title('Courbe de validation â€” max_depth du Random Forest')
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Meilleur max_depth
best_idx = np.argmax(val_mean)
print(f"Meilleur max_depth : {param_range[best_idx]}")
print(f"F1 validation      : {val_mean[best_idx]:.4f}")
```

```
Score
1.0 â”‚â”€â”€â”€â”€â”€â”€â”€â”€ Train
    â”‚     â•±â•²
    â”‚    â•±  â•²â”€â”€â”€â”€â”€ Validation
    â”‚   â•±
    â”‚  â•±
    â”‚ â•±        â† Zone optimale
0.5 â”‚â•±            (juste avant que Val baisse)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ max_depth
     1  2  3  5  7  10  15  20
     â† Underfitting  Overfitting â†’
```

---

## 7. âš–ï¸ Bias-Variance Tradeoff revisitÃ©

### 7.1 Lien avec underfitting/overfitting

```
Erreur totale = BiaisÂ² + Variance + Bruit irrÃ©ductible

Erreur
  â”‚
  â”‚  â•² BiaisÂ²                    â•± Variance
  â”‚   â•²                         â•±
  â”‚    â•²                      â•±
  â”‚     â•²        â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±
  â”‚      â•²      â•± Erreur totale
  â”‚       â•²   â•±
  â”‚        â•²â•±  â† Sweet spot
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ComplexitÃ© du modÃ¨le
     Simple                    Complexe
     (Underfitting)            (Overfitting)
     Biais Ã©levÃ©               Variance Ã©levÃ©e
```

| | Biais Ã©levÃ© (Underfitting) | Variance Ã©levÃ©e (Overfitting) |
|---|---|---|
| **SymptÃ´me** | Score train ET val bas | Score train Ã©levÃ©, val bas |
| **Le modÃ¨le** | Est trop simple | Est trop complexe |
| **Solution** | Plus de features, modÃ¨le plus complexe | RÃ©gularisation, plus de donnÃ©es, simplifier |
| **Courbe d'apprentissage** | Les deux courbes convergent bas | Grand Ã©cart entre les courbes |

### 7.2 Comment diagnostiquer

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# --- ModÃ¨le trop simple (underfitting) ---
tree_simple = DecisionTreeClassifier(max_depth=1, random_state=42)
scores_simple = cross_val_score(tree_simple, X, y, cv=5, scoring='f1')
tree_simple.fit(X_train, y_train)
train_score_simple = f1_score(y_train, tree_simple.predict(X_train))

# --- ModÃ¨le trop complexe (overfitting) ---
tree_complex = DecisionTreeClassifier(max_depth=None, random_state=42)
scores_complex = cross_val_score(tree_complex, X, y, cv=5, scoring='f1')
tree_complex.fit(X_train, y_train)
train_score_complex = f1_score(y_train, tree_complex.predict(X_train))

# --- ModÃ¨le Ã©quilibrÃ© ---
tree_balanced = DecisionTreeClassifier(max_depth=5, random_state=42)
scores_balanced = cross_val_score(tree_balanced, X, y, cv=5, scoring='f1')
tree_balanced.fit(X_train, y_train)
train_score_balanced = f1_score(y_train, tree_balanced.predict(X_train))

print("=== Diagnostic ===")
print(f"Simple  (depth=1)    : Train={train_score_simple:.3f}, Val={scores_simple.mean():.3f} â†’ Underfitting")
print(f"Complexe (depth=None): Train={train_score_complex:.3f}, Val={scores_complex.mean():.3f} â†’ Overfitting")
print(f"Ã‰quilibrÃ© (depth=5)  : Train={train_score_balanced:.3f}, Val={scores_balanced.mean():.3f} â†’ Bon compromis")
```

---

## 8. ğŸ”§ GridSearchCV et RandomizedSearchCV

### 8.1 GridSearchCV : la recherche exhaustive

GridSearchCV teste **toutes les combinaisons** d'hyperparamÃ¨tres et utilise la cross-validation pour Ã©valuer chacune.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

# --- DÃ©finir la grille ---
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Nombre total de combinaisons
n_combinaisons = 3 * 4 * 3 * 3  # = 108
print(f"Nombre de combinaisons : {n_combinaisons}")
print(f"Avec 5-Fold CV : {n_combinaisons * 5} = {n_combinaisons * 5} fits !")

# --- GridSearchCV ---
grid = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,                    # 5-Fold CV
    scoring='f1',            # MÃ©trique Ã  optimiser
    n_jobs=-1,               # ParallÃ©liser
    verbose=1,
    return_train_score=True  # Pour diagnostiquer l'overfitting
)
grid.fit(X_train, y_train)

# --- RÃ©sultats ---
print(f"\nMeilleurs paramÃ¨tres : {grid.best_params_}")
print(f"Meilleur F1 (CV)     : {grid.best_score_:.4f}")

# Ã‰valuer sur le test set
y_pred_best = grid.best_estimator_.predict(X_test)
print(f"F1 test              : {f1_score(y_test, y_pred_best):.4f}")

# Analyser les rÃ©sultats
import pandas as pd
results = pd.DataFrame(grid.cv_results_)
print("\n=== Top 5 combinaisons ===")
cols = ['rank_test_score', 'mean_test_score', 'std_test_score', 'params']
print(results.nsmallest(5, 'rank_test_score')[cols].to_string(index=False))
```

### 8.2 RandomizedSearchCV : la recherche intelligente

Quand l'espace de recherche est trop grand, RandomizedSearchCV tire **alÃ©atoirement** un nombre fixe de combinaisons.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# --- Distributions d'hyperparamÃ¨tres ---
param_distributions = {
    'n_estimators': randint(50, 500),          # Entier entre 50 et 500
    'max_depth': [3, 5, 7, 10, 15, 20, None], # Liste de choix
    'min_samples_split': randint(2, 20),       # Entier entre 2 et 20
    'min_samples_leaf': randint(1, 10),        # Entier entre 1 et 10
    'max_features': uniform(0.1, 0.9),         # Float entre 0.1 et 1.0
}

# --- RandomizedSearchCV ---
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,               # Tester 50 combinaisons (au lieu de toutes)
    cv=5,
    scoring='f1',
    n_jobs=-1,
    random_state=42,
    verbose=1,
    return_train_score=True
)
random_search.fit(X_train, y_train)

print(f"\nMeilleurs paramÃ¨tres : {random_search.best_params_}")
print(f"Meilleur F1 (CV)     : {random_search.best_score_:.4f}")

y_pred_random = random_search.best_estimator_.predict(X_test)
print(f"F1 test              : {f1_score(y_test, y_pred_random):.4f}")
```

### 8.3 Quand utiliser quoi ?

| CritÃ¨re | GridSearchCV | RandomizedSearchCV |
|---------|-------------|-------------------|
| **Espace de recherche** | Petit (< 100 combinaisons) | Grand (> 1000 combinaisons) |
| **ExhaustivitÃ©** | Teste TOUT | Ã‰chantillonne alÃ©atoirement |
| **Temps** | Long si beaucoup de paramÃ¨tres | ContrÃ´lÃ© via `n_iter` |
| **Garantie** | Trouve le meilleur de la grille | Peut rater le meilleur |
| **Distributions** | Listes de valeurs | Distributions continues |
| **Premier choix** | Tuning fin (2-3 paramÃ¨tres) | Exploration large (> 4 paramÃ¨tres) |

> ğŸ’¡ **Conseil** : "StratÃ©gie recommandÃ©e : commencez par un `RandomizedSearchCV` avec `n_iter=100` pour explorer largement, puis affinez avec un `GridSearchCV` sur une grille rÃ©duite autour des meilleurs paramÃ¨tres trouvÃ©s."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Toujours** sÃ©parer train/test avant toute modÃ©lisation pour estimer honnÃªtement les performances
2. **Stratifier** le split pour conserver les proportions de classes (`stratify=y`)
3. **random_state** garantit la reproductibilitÃ© des rÃ©sultats
4. **3 ensembles** (train/val/test) : le test set ne sert qu'UNE seule fois Ã  la fin
5. **Cross-validation** utilise chaque donnÃ©e pour l'entraÃ®nement ET la validation
6. **StratifiedKFold** est indispensable pour les classes dÃ©sÃ©quilibrÃ©es
7. **TimeSeriesSplit** est obligatoire pour les donnÃ©es temporelles (pas de K-Fold !)
8. **Courbes d'apprentissage** : diagnostic overfitting (gap) vs underfitting (scores bas)
9. **GridSearchCV** pour explorer exhaustivement un petit espace de paramÃ¨tres
10. **RandomizedSearchCV** pour explorer efficacement un grand espace de paramÃ¨tres

---

## âœ… Checklist de validation

- [ ] Je sais faire un train/test split avec stratification et random_state
- [ ] Je comprends pourquoi il faut 3 ensembles (train/val/test)
- [ ] Je sais implÃ©menter un K-Fold et un Stratified K-Fold
- [ ] Je sais utiliser `cross_val_score` pour Ã©valuer un modÃ¨le
- [ ] Je comprends pourquoi K-Fold ne marche pas pour les sÃ©ries temporelles
- [ ] Je sais utiliser `TimeSeriesSplit`
- [ ] Je sais tracer et interprÃ©ter les courbes d'apprentissage (4 cas)
- [ ] Je sais tracer une courbe de validation pour trouver le bon hyperparamÃ¨tre
- [ ] Je comprends le bias-variance tradeoff et son lien avec under/overfitting
- [ ] Je sais utiliser GridSearchCV et RandomizedSearchCV
- [ ] Je connais la stratÃ©gie "RandomizedSearch large â†’ GridSearch fin"

---

**PrÃ©cÃ©dent** : [Chapitre 12 : MÃ©triques â€” Au-delÃ  de l'Accuracy](12-metriques-classification.md)

**Suivant** : [Chapitre 14 : InterprÃ©ter ses ModÃ¨les et Ã‰thique du ML](14-interpretabilite-ethique.md)
