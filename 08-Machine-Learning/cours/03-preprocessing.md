# Chapitre 3 : Preprocessing ‚Äì Pr√©parer ses Donn√©es

## üéØ Objectifs

- Nettoyer et pr√©parer des donn√©es pour le Machine Learning
- Ma√Ætriser les techniques d'encodage des variables cat√©gorielles
- Comprendre la diff√©rence entre normalisation et standardisation
- G√©rer les valeurs manquantes et les outliers efficacement
- Construire des pipelines de preprocessing reproductibles avec scikit-learn
- Savoir traiter les classes d√©s√©quilibr√©es

---

## 1. üß† Pourquoi le preprocessing est crucial

Le preprocessing est l'√©tape la plus importante (et la plus longue) d'un projet ML. La qualit√© de vos donn√©es d√©termine directement la qualit√© de votre mod√®le.

### Le principe "Garbage in, Garbage out"

```
Donn√©es sales     ‚Üí Mod√®le sophistiqu√© ‚Üí R√©sultats m√©diocres  ‚ùå
Donn√©es propres   ‚Üí Mod√®le simple      ‚Üí Bons r√©sultats       ‚úÖ
Donn√©es propres   ‚Üí Mod√®le sophistiqu√© ‚Üí Excellents r√©sultats ‚úÖ‚úÖ
```

> üí° **Conseil** : "Un bon preprocessing vaut souvent mieux qu'un mod√®le plus complexe. Investissez du temps dans la pr√©paration de vos donn√©es."

### Impact sur les performances

| Aspect du preprocessing | Impact potentiel sur le score |
|------------------------|-------------------------------|
| Gestion des valeurs manquantes | +5 √† +15% |
| Encodage correct des cat√©gorielles | +5 √† +20% |
| Normalisation/Standardisation | +10 √† +30% (pour SVM, KNN) |
| Gestion des outliers | +2 √† +10% |
| Feature engineering | +10 √† +50% |

> üí° **Conseil de pro** : "Ne sous-estimez jamais le preprocessing. Les data scientists exp√©riment√©s y consacrent 60 √† 80% de leur temps."

---

## 2. üîß Gestion des valeurs manquantes

### 2.1 D√©tecter les valeurs manquantes

```python
import pandas as pd
import numpy as np

# Charger les donn√©es
df = pd.read_csv("donnees.csv")

# --- D√©tection des valeurs manquantes ---

# Nombre de manquantes par colonne
print("=== Valeurs manquantes par colonne ===")
print(df.isnull().sum())

# Pourcentage de manquantes
print("\n=== Pourcentage de manquantes ===")
pct_manquantes = (df.isnull().sum() / len(df)) * 100
print(pct_manquantes.sort_values(ascending=False))

# R√©sum√© visuel
print(f"\nNombre total de valeurs manquantes : {df.isnull().sum().sum()}")
print(f"Pourcentage global : {(df.isnull().sum().sum() / df.size) * 100:.2f}%")
```

```python
# Visualisation avec missingno
import missingno as msno
import matplotlib.pyplot as plt

# Matrice de manquantes
msno.matrix(df, figsize=(12, 6))
plt.title("Matrice des valeurs manquantes")
plt.show()

# Heatmap de corr√©lation des manquantes
# (utile pour voir si les manquantes sont li√©es entre colonnes)
msno.heatmap(df, figsize=(10, 6))
plt.title("Corr√©lation des valeurs manquantes")
plt.show()
```

### 2.2 Strat√©gies de traitement

| Strat√©gie | Quand l'utiliser | Avantage | Inconv√©nient |
|-----------|-----------------|----------|-------------|
| **Suppression de lignes** | Peu de manquantes (<5%) | Simple | Perte de donn√©es |
| **Suppression de colonnes** | >50% manquantes | √âlimine la colonne probl√©matique | Perte d'information |
| **Imputation par la moyenne** | Num√©rique, peu de manquantes | Simple, rapide | R√©duit la variance |
| **Imputation par la m√©diane** | Num√©rique avec outliers | Robuste aux outliers | R√©duit la variance |
| **Imputation par le mode** | Cat√©goriel | Adapt√© aux cat√©gories | Peut biaiser |
| **Imputation KNN** | Relations entre features | Plus pr√©cis | Plus lent |
| **Imputation par constante** | Signification m√©tier du manquant | Explicite | Ajoute une "cat√©gorie" |

### 2.3 Impl√©mentation avec scikit-learn

```python
from sklearn.impute import SimpleImputer, KNNImputer

# --- Imputation par la moyenne (variables num√©riques) ---
imputer_mean = SimpleImputer(strategy='mean')
df_numerique_imputed = pd.DataFrame(
    imputer_mean.fit_transform(df[colonnes_numeriques]),
    columns=colonnes_numeriques
)

# --- Imputation par la m√©diane (si outliers) ---
imputer_median = SimpleImputer(strategy='median')
df_numerique_imputed = pd.DataFrame(
    imputer_median.fit_transform(df[colonnes_numeriques]),
    columns=colonnes_numeriques
)

# --- Imputation par le mode (variables cat√©gorielles) ---
imputer_mode = SimpleImputer(strategy='most_frequent')
df_categoriel_imputed = pd.DataFrame(
    imputer_mode.fit_transform(df[colonnes_categorielles]),
    columns=colonnes_categorielles
)

# --- Imputation par une constante ---
imputer_const = SimpleImputer(strategy='constant', fill_value='Inconnu')
df_categoriel_imputed = pd.DataFrame(
    imputer_const.fit_transform(df[colonnes_categorielles]),
    columns=colonnes_categorielles
)

# --- Imputation KNN (plus sophistiqu√©e) ---
imputer_knn = KNNImputer(n_neighbors=5)
df_knn_imputed = pd.DataFrame(
    imputer_knn.fit_transform(df[colonnes_numeriques]),
    columns=colonnes_numeriques
)
```

> ‚ö†Ô∏è **Attention** : "Ne **jamais** imputer AVANT le split train/test ! L'imputation doit √™tre `fit` sur le train set et `transform` sur le test set. Sinon, vous avez du **data leakage** ‚Äî le mod√®le 'voit' des informations du test set pendant l'entra√Ænement."

```python
from sklearn.model_selection import train_test_split

# 1. Splitter D'ABORD
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Fit sur le train SEULEMENT, transform sur train ET test
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)  # fit + transform
X_test_imputed = imputer.transform(X_test)          # transform SEULEMENT
```

---

## 3. üìä Gestion des valeurs aberrantes (outliers)

### 3.1 D√©tecter les outliers

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- M√©thode 1 : Boxplot (visuel) ---
fig, axes = plt.subplots(1, 4, figsize=(16, 4))
for i, col in enumerate(colonnes_numeriques[:4]):
    sns.boxplot(y=df[col], ax=axes[i])
    axes[i].set_title(f'{col}')
plt.suptitle("D√©tection des outliers par boxplot")
plt.tight_layout()
plt.show()

# --- M√©thode 2 : IQR (Interquartile Range) ---
def detecter_outliers_iqr(df, colonne):
    """D√©tecte les outliers avec la m√©thode IQR"""
    Q1 = df[colonne].quantile(0.25)
    Q3 = df[colonne].quantile(0.75)
    IQR = Q3 - Q1
    borne_inf = Q1 - 1.5 * IQR
    borne_sup = Q3 + 1.5 * IQR
    outliers = df[(df[colonne] < borne_inf) | (df[colonne] > borne_sup)]
    return outliers, borne_inf, borne_sup

for col in colonnes_numeriques:
    outliers, b_inf, b_sup = detecter_outliers_iqr(df, col)
    print(f"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.1f}%)")
    print(f"  Bornes: [{b_inf:.2f}, {b_sup:.2f}]")

# --- M√©thode 3 : Z-Score ---
from scipy import stats

def detecter_outliers_zscore(df, colonne, seuil=3):
    """D√©tecte les outliers avec le Z-Score"""
    z_scores = np.abs(stats.zscore(df[colonne].dropna()))
    outliers = df[colonne].dropna()[z_scores > seuil]
    return outliers

for col in colonnes_numeriques:
    outliers = detecter_outliers_zscore(df, col)
    print(f"{col}: {len(outliers)} outliers (Z-Score > 3)")
```

### 3.2 Strat√©gies de traitement

| Strat√©gie | Impl√©mentation | Quand l'utiliser |
|-----------|---------------|-----------------|
| **Suppression** | Supprimer les lignes | Peu d'outliers, clairement erron√©s |
| **Capping (winsorisation)** | Remplacer par la borne IQR | Valeurs extr√™mes mais pas impossibles |
| **Transformation log** | `np.log1p(x)` | Distribution tr√®s asym√©trique |
| **Ne rien faire** | Garder les outliers | Informations l√©gitimes (ex: fraude) |

```python
# --- Capping avec IQR ---
def capper_outliers(df, colonne):
    """Remplace les outliers par les bornes IQR"""
    Q1 = df[colonne].quantile(0.25)
    Q3 = df[colonne].quantile(0.75)
    IQR = Q3 - Q1
    borne_inf = Q1 - 1.5 * IQR
    borne_sup = Q3 + 1.5 * IQR
    df[colonne] = df[colonne].clip(lower=borne_inf, upper=borne_sup)
    return df

# Appliquer sur chaque colonne num√©rique
for col in colonnes_numeriques:
    df = capper_outliers(df, col)

# --- Transformation logarithmique ---
# Utile pour les variables avec distribution tr√®s asym√©trique (ex: revenus, prix)
df['prix_log'] = np.log1p(df['prix'])  # log(1+x) pour g√©rer les z√©ros
```

> üí° **Conseil de pro** : "Avant de supprimer un outlier, demandez-vous : est-ce une erreur de saisie ou une observation l√©gitime ? Un achat de 50 000‚Ç¨ peut √™tre un outlier statistique mais un client VIP r√©el."

---

## 4. üè∑Ô∏è Encodage des variables cat√©gorielles

Les algorithmes de ML ne comprennent que les **nombres**. Il faut donc convertir les variables cat√©gorielles en repr√©sentations num√©riques.

### 4.1 Types de variables cat√©gorielles

| Type | Description | Exemple | Encodage recommand√© |
|------|------------|---------|---------------------|
| **Nominale** | Pas d'ordre entre les cat√©gories | Couleur (rouge, bleu, vert) | One-Hot Encoding |
| **Ordinale** | Ordre significatif | Taille (S, M, L, XL) | Label/Ordinal Encoding |
| **Binaire** | Deux cat√©gories | Sexe (H, F) | Label Encoding (0/1) |

### 4.2 Label Encoding (variables ordinales)

```python
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

# --- LabelEncoder : pour la variable cible ---
le = LabelEncoder()
df['target_encoded'] = le.fit_transform(df['target'])
# Ex: ['chat', 'chien', 'oiseau'] ‚Üí [0, 1, 2]

# Inverser l'encodage
labels_originaux = le.inverse_transform([0, 1, 2])

# --- OrdinalEncoder : pour les features ordinales ---
# Sp√©cifier l'ORDRE des cat√©gories
categories_ordre = [['S', 'M', 'L', 'XL', 'XXL']]
oe = OrdinalEncoder(categories=categories_ordre)
df['taille_encoded'] = oe.fit_transform(df[['taille']])
# S‚Üí0, M‚Üí1, L‚Üí2, XL‚Üí3, XXL‚Üí4
```

> ‚ö†Ô∏è **Attention** : "N'utilisez **jamais** le Label Encoding pour des variables nominales (sans ordre). Le mod√®le interpr√©terait un ordre artificiel entre les cat√©gories. Par exemple, si rouge=0, bleu=1, vert=2, le mod√®le croirait que vert > bleu > rouge."

### 4.3 One-Hot Encoding (variables nominales)

```python
from sklearn.preprocessing import OneHotEncoder

# --- Avec pandas (simple et rapide) ---
df_encoded = pd.get_dummies(df, columns=['couleur', 'ville'], drop_first=True)
# couleur_bleu, couleur_vert (rouge est la r√©f√©rence avec drop_first=True)

# --- Avec sklearn (recommand√© pour les pipelines) ---
ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
encoded = ohe.fit_transform(df[['couleur', 'ville']])
colonnes_ohe = ohe.get_feature_names_out(['couleur', 'ville'])
df_ohe = pd.DataFrame(encoded, columns=colonnes_ohe)
```

### 4.4 Tableau comparatif des encodages

| Crit√®re | Label Encoding | One-Hot Encoding |
|---------|---------------|-----------------|
| **Type de variable** | Ordinale | Nominale |
| **Nombre de colonnes** | 1 (m√™me colonne) | N-1 nouvelles colonnes |
| **Ordre implicite** | Oui (attention !) | Non |
| **Risque** | Faux ordre pour le nominale | Explosion dimensionnelle |
| **Compatible avec** | Arbres de d√©cision | Tous les algorithmes |
| **Nombre de cat√©gories** | Illimit√© | Limit√© (<50 id√©alement) |

> üí° **Conseil de pro** : "Attention au One-Hot Encoding avec trop de cat√©gories (>50). Vous cr√©ez autant de colonnes que de cat√©gories, ce qui peut mener √† la **curse of dimensionality**. Dans ce cas, consid√©rez le Target Encoding ou le Feature Hashing."

> üí° **Conseil** : "Utilisez `drop='first'` dans le One-Hot Encoding pour √©viter la multicolin√©arit√© (le pi√®ge de la variable factice). Si vous avez 3 couleurs, 2 colonnes suffisent ‚Äî la 3√®me est implicite."

---

## 5. üìê Normalisation et Standardisation

### 5.1 Pourquoi mettre √† l'√©chelle ?

Certains algorithmes sont sensibles √† l'**√©chelle** des features. Si une feature va de 0 √† 1 000 000 et une autre de 0 √† 1, la premi√®re dominera.

| Algorithme | Sensible √† l'√©chelle ? | Scaling n√©cessaire ? |
|-----------|----------------------|---------------------|
| R√©gression lin√©aire | Partiellement | Recommand√© |
| R√©gression logistique | Oui | Oui |
| SVM | **Tr√®s sensible** | **Obligatoire** |
| KNN | **Tr√®s sensible** | **Obligatoire** |
| Arbres de d√©cision | Non | Non |
| Random Forest | Non | Non |
| Gradient Boosting | Non | Non |
| R√©seaux de neurones | **Tr√®s sensible** | **Obligatoire** |

> üí° **Conseil** : "SVM et KNN sont **tr√®s sensibles** √† l'√©chelle des features. Toujours normaliser avant d'utiliser ces algorithmes. Les arbres de d√©cision, eux, s'en moquent."

### 5.2 StandardScaler (Standardisation / Z-Score)

Centre les donn√©es √† moyenne 0 et √©cart-type 1.

**Formule** : `z = (x - moyenne) / √©cart-type`

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit sur le train, transform sur train ET test
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# V√©rification
print(f"Moyenne apr√®s scaling : {X_train_scaled.mean(axis=0)}")   # ‚âà 0
print(f"√âcart-type apr√®s scaling : {X_train_scaled.std(axis=0)}") # ‚âà 1
```

### 5.3 MinMaxScaler (Normalisation Min-Max)

Ram√®ne les donn√©es dans l'intervalle [0, 1].

**Formule** : `x_norm = (x - min) / (max - min)`

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()  # Par d√©faut [0, 1]
# Ou : MinMaxScaler(feature_range=(0, 10)) pour [0, 10]

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# V√©rification
print(f"Min apr√®s scaling : {X_train_scaled.min(axis=0)}")  # 0
print(f"Max apr√®s scaling : {X_train_scaled.max(axis=0)}")  # 1
```

### 5.4 RobustScaler (Robuste aux outliers)

Utilise la m√©diane et l'IQR au lieu de la moyenne et l'√©cart-type.

**Formule** : `x_robust = (x - m√©diane) / IQR`

```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### 5.5 Tableau comparatif des scalers

| Scaler | Formule | R√©sultat | Sensible outliers | Quand l'utiliser |
|--------|---------|---------|-------------------|-----------------|
| **StandardScaler** | `(x-Œº)/œÉ` | Moyenne=0, Std=1 | Oui | Cas g√©n√©ral, distribution ~normale |
| **MinMaxScaler** | `(x-min)/(max-min)` | Valeurs dans [0,1] | **Tr√®s sensible** | Quand on veut des bornes fixes |
| **RobustScaler** | `(x-m√©diane)/IQR` | Centr√© sur m√©diane | **Robuste** | Donn√©es avec beaucoup d'outliers |

> üí° **Conseil de pro** : "En cas de doute, commencez par `StandardScaler`. Si vos donn√©es ont beaucoup d'outliers, passez √† `RobustScaler`. Utilisez `MinMaxScaler` uniquement si vous avez besoin de valeurs dans [0, 1]."

> ‚ö†Ô∏è **Attention** : "Comme pour l'imputation, le scaler doit √™tre `fit` sur le **train set uniquement** et `transform` sur le train ET le test set. Sinon = data leakage."

---

## 6. ‚úÇÔ∏è Train / Test / Validation Split

### 6.1 Pourquoi splitter ?

Le but du ML est la **g√©n√©ralisation** ‚Äî bien performer sur des donn√©es jamais vues. Le split permet de simuler cette situation.

```
Dataset complet (100%)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Training set (80%) ‚Üí Entra√Æner le mod√®le
    ‚îÇ
    ‚îî‚îÄ‚îÄ Test set (20%) ‚Üí √âvaluer la g√©n√©ralisation
```

### 6.2 Impl√©mentation

```python
from sklearn.model_selection import train_test_split

# Split classique 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 20% pour le test
    random_state=42,     # Reproductibilit√©
    shuffle=True         # M√©langer les donn√©es (par d√©faut)
)

print(f"Training set : {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X)*100:.0f}%)")
print(f"Test set : {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X)*100:.0f}%)")
```

### 6.3 Stratification (classes d√©s√©quilibr√©es)

```python
# SANS stratification ‚Üí les proportions de classes peuvent √™tre diff√©rentes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# Train: 70% classe 0, 30% classe 1 (al√©atoire)

# AVEC stratification ‚Üí les proportions sont conserv√©es
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,          # Conserver les proportions de classes
    random_state=42
)
# Train: 65% classe 0, 35% classe 1 (comme le dataset original)

# V√©rifier les proportions
print("Proportions dans le train set :")
print(pd.Series(y_train).value_counts(normalize=True))
print("\nProportions dans le test set :")
print(pd.Series(y_test).value_counts(normalize=True))
```

> ‚ö†Ô∏è **Attention** : "**TOUJOURS** splitter **AVANT** le preprocessing pour √©viter le data leakage. L'ordre correct est : split ‚Üí fit preprocessing sur train ‚Üí transform train et test ‚Üí entra√Æner le mod√®le."

### 6.4 Train / Validation / Test Split

Pour le tuning d'hyperparam√®tres, on ajoute un **validation set** :

```
Dataset complet (100%)
    ‚îÇ
    ‚îú‚îÄ‚îÄ Training set (60%) ‚Üí Entra√Æner le mod√®le
    ‚îÇ
    ‚îú‚îÄ‚îÄ Validation set (20%) ‚Üí Tuner les hyperparam√®tres
    ‚îÇ
    ‚îî‚îÄ‚îÄ Test set (20%) ‚Üí √âvaluation finale (UNE SEULE FOIS)
```

```python
# M√©thode : deux splits successifs
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val
)
# 0.25 * 0.8 = 0.2 ‚Üí 60% train, 20% val, 20% test

print(f"Train : {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)")
print(f"Validation : {len(X_val)} ({len(X_val)/len(X)*100:.0f}%)")
print(f"Test : {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)")
```

> üí° **Conseil de pro** : "Le **test set** ne doit √™tre utilis√© qu'**UNE SEULE FOIS** ‚Äî pour l'√©valuation finale. Si vous l'utilisez plusieurs fois pour ajuster votre mod√®le, vous faites du data leakage indirect."

---

## 7. üîó Pipelines scikit-learn

Les pipelines sont la fa√ßon **professionnelle** de construire un workflow de preprocessing. Ils garantissent la reproductibilit√© et √©vitent le data leakage.

### 7.1 Pipeline simple

```python
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# Pipeline avec noms explicites
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

# Ou version raccourcie (noms automatiques)
pipeline = make_pipeline(
    SimpleImputer(strategy='mean'),
    StandardScaler(),
    LogisticRegression()
)

# Utilisation : une seule ligne pour tout le workflow
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
score = pipeline.score(X_test, y_test)
print(f"Score : {score:.4f}")
```

### 7.2 ColumnTransformer (traitement diff√©renci√©)

En pratique, les colonnes num√©riques et cat√©gorielles n√©cessitent des traitements diff√©rents :

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Identifier les types de colonnes
colonnes_numeriques = ['age', 'revenu', 'score_credit']
colonnes_categorielles = ['ville', 'profession', 'statut_marital']

# --- Preprocessing pour les colonnes num√©riques ---
preprocessor_numerique = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# --- Preprocessing pour les colonnes cat√©gorielles ---
preprocessor_categoriel = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# --- Combiner les deux avec ColumnTransformer ---
preprocessor = ColumnTransformer([
    ('num', preprocessor_numerique, colonnes_numeriques),
    ('cat', preprocessor_categoriel, colonnes_categorielles)
])

# --- Pipeline complet : preprocessing + mod√®le ---
pipeline_complet = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Utilisation
pipeline_complet.fit(X_train, y_train)
predictions = pipeline_complet.predict(X_test)
score = pipeline_complet.score(X_test, y_test)
print(f"Score du pipeline complet : {score:.4f}")
```

### 7.3 Avantages des pipelines

| Avantage | Description |
|----------|-------------|
| **Pas de data leakage** | Le `fit` est automatiquement sur le train set |
| **Reproductibilit√©** | Tout le workflow en un objet |
| **Compatible GridSearch** | Tuner les hyperparam√®tres du preprocessing ET du mod√®le |
| **D√©ployable** | Sauvegarder un seul objet `pipeline.joblib` |
| **Lisible** | Le code est clair et structur√© |

> üí° **Conseil de pro** : "**Toujours** utiliser des pipelines en production. Cela √©vite le data leakage, rend le code reproductible et facilite le d√©ploiement. Un pipeline = un objet qui fait tout."

### 7.4 Pipeline avec GridSearchCV

```python
from sklearn.model_selection import GridSearchCV

# D√©finir les hyperparam√®tres √† tester
# Notez la syntaxe : '√©tape__param√®tre'
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean', 'median'],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [5, 10, None]
}

# GridSearch sur le pipeline complet
grid_search = GridSearchCV(
    pipeline_complet,
    param_grid,
    cv=5,                # 5-fold cross-validation
    scoring='f1',        # M√©trique d'optimisation
    n_jobs=-1,           # Parall√©liser
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Meilleurs param√®tres : {grid_search.best_params_}")
print(f"Meilleur score (F1) : {grid_search.best_score_:.4f}")

# √âvaluer sur le test set
score_test = grid_search.score(X_test, y_test)
print(f"Score sur le test set : {score_test:.4f}")
```

---

## 8. ‚öñÔ∏è Gestion des classes d√©s√©quilibr√©es

### 8.1 Le probl√®me

Quand une classe est beaucoup plus fr√©quente que l'autre, le mod√®le a tendance √† toujours pr√©dire la classe majoritaire.

```python
# Exemple : 95% classe 0, 5% classe 1
print(pd.Series(y_train).value_counts())
# 0    9500
# 1     500

# Un mod√®le qui pr√©dit TOUJOURS 0 a 95% d'accuracy ‚Üí mais il est INUTILE !
```

> ‚ö†Ô∏è **Attention** : "Avec des classes d√©s√©quilibr√©es, ne **JAMAIS** regarder l'accuracy seule. Un mod√®le 'stupide' qui pr√©dit toujours la classe majoritaire aura une accuracy √©lev√©e mais ne d√©tectera jamais la classe minoritaire."

### 8.2 Solutions

| Solution | Description | Quand l'utiliser |
|----------|-------------|-----------------|
| **class_weight='balanced'** | Donne plus de poids √† la classe minoritaire | Premier r√©flexe, simple |
| **SMOTE** | Cr√©e des √©chantillons synth√©tiques de la classe minoritaire | Peu de donn√©es minoritaires |
| **Undersampling** | R√©duit la classe majoritaire | Beaucoup de donn√©es |
| **Seuil ajust√©** | Modifier le seuil de d√©cision (pas 0.5) | Ajustement fin |
| **M√©triques adapt√©es** | F1, AUC-ROC au lieu de l'accuracy | Toujours |

```python
# --- Solution 1 : class_weight='balanced' ---
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# La plupart des classifieurs sklearn supportent class_weight
modele = LogisticRegression(class_weight='balanced', random_state=42)
modele.fit(X_train, y_train)

# Ou avec Random Forest
modele_rf = RandomForestClassifier(class_weight='balanced', random_state=42)

# --- Solution 2 : SMOTE (sur-√©chantillonnage synth√©tique) ---
# uv add imbalanced-learn
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"Avant SMOTE : {pd.Series(y_train).value_counts().to_dict()}")
print(f"Apr√®s SMOTE : {pd.Series(y_train_resampled).value_counts().to_dict()}")
# Avant : {0: 9500, 1: 500}
# Apr√®s : {0: 9500, 1: 9500}

# --- Solution 3 : Undersampling ---
from imblearn.under_sampling import RandomUnderSampler

undersampler = RandomUnderSampler(random_state=42)
X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)
```

> ‚ö†Ô∏è **Attention** : "SMOTE ne doit **jamais** √™tre appliqu√© sur le test set. On r√©√©quilibre uniquement le **train set**."

### 8.3 Impact sur les m√©triques

```python
from sklearn.metrics import classification_report, f1_score, roc_auc_score

# Comparer les m√©triques avec et sans class_weight
modele_sans = LogisticRegression(random_state=42)
modele_avec = LogisticRegression(class_weight='balanced', random_state=42)

modele_sans.fit(X_train, y_train)
modele_avec.fit(X_train, y_train)

print("=== SANS class_weight ===")
print(classification_report(y_test, modele_sans.predict(X_test)))

print("=== AVEC class_weight='balanced' ===")
print(classification_report(y_test, modele_avec.predict(X_test)))
```

> üí° **Conseil de pro** : "Avec des classes d√©s√©quilibr√©es, utilisez le **F1-Score** ou l'**AUC-ROC** comme m√©trique principale. L'accuracy est trompeuse et peut vous donner une fausse impression de performance."

---

## üéØ Points cl√©s √† retenir

1. **"Garbage in, Garbage out"** : la qualit√© des donn√©es d√©termine la qualit√© du mod√®le
2. **Valeurs manquantes** : d√©tecter d'abord, choisir la strat√©gie d'imputation ensuite (mean, median, mode, KNN)
3. **Outliers** : d√©tecter (boxplot, IQR, Z-Score), puis d√©cider (supprimer, capper, transformer)
4. **Encodage** : One-Hot pour le nominal, Ordinal pour l'ordinal, **jamais** de Label Encoding pour le nominal
5. **Scaling** : StandardScaler par d√©faut, RobustScaler si outliers, MinMaxScaler si besoin de bornes
6. **Split AVANT preprocessing** : c'est la r√®gle d'or pour √©viter le data leakage
7. **Pipelines sklearn** : utilisez-les toujours ‚Äî reproductibilit√©, pas de leakage, d√©ployable
8. **Classes d√©s√©quilibr√©es** : class_weight='balanced', SMOTE, et surtout les bonnes m√©triques (F1, AUC)
9. **Fit sur train, transform sur train et test** : ne jamais fit sur le test set
10. **ColumnTransformer** : traitement diff√©renci√© pour colonnes num√©riques et cat√©gorielles

---

## ‚úÖ Checklist de validation

- [ ] Je sais d√©tecter les valeurs manquantes et choisir une strat√©gie d'imputation
- [ ] Je sais d√©tecter et traiter les outliers (IQR, Z-Score, capping)
- [ ] Je connais la diff√©rence entre Label Encoding et One-Hot Encoding
- [ ] Je sais quand utiliser StandardScaler, MinMaxScaler et RobustScaler
- [ ] Je sais splitter mes donn√©es AVANT le preprocessing
- [ ] Je ma√Ætrise `train_test_split` avec stratification
- [ ] Je sais construire un pipeline sklearn avec ColumnTransformer
- [ ] Je comprends le data leakage et comment l'√©viter
- [ ] Je sais g√©rer les classes d√©s√©quilibr√©es (class_weight, SMOTE)
- [ ] Je sais combiner Pipeline + GridSearchCV pour le tuning

---

**Pr√©c√©dent** : [Chapitre 2 : Environnement et Outils](02-environnement-setup.md)

**Suivant** : [Chapitre 4 : R√©gression ‚Äì Pr√©dire des Valeurs Continues](04-regression.md)
