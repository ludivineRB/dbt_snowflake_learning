# Chapitre 7 : Clustering â€“ DÃ©couvrir des Structures CachÃ©es

## ğŸ¯ Objectifs

- Comprendre le principe de l'apprentissage non supervisÃ© et du clustering
- MaÃ®triser les algorithmes K-Means, DBSCAN et le clustering hiÃ©rarchique
- Savoir Ã©valuer la qualitÃ© d'un clustering avec les bonnes mÃ©triques
- Choisir le bon algorithme de clustering selon la nature des donnÃ©es
- Visualiser et interprÃ©ter les rÃ©sultats d'un clustering

---

## 1. ğŸ§  Introduction au clustering

### 1.1 Qu'est-ce que le clustering ?

Le clustering (ou partitionnement) est une technique d'**apprentissage non supervisÃ©** : il n'y a **pas de labels** (pas de variable cible). L'objectif est de dÃ©couvrir des **groupes naturels** (clusters) dans les donnÃ©es, de telle sorte que :

- Les Ã©lÃ©ments d'un mÃªme cluster soient **similaires** entre eux
- Les Ã©lÃ©ments de clusters diffÃ©rents soient **dissemblables**

C'est comme trier un tas de Lego par couleur sans qu'on vous ait dit quelles couleurs existent.

### 1.2 Cas d'usage en entreprise

| Domaine | Cas d'usage | Objectif |
|---|---|---|
| **Marketing** | Segmentation clients | Personnaliser les offres par profil |
| **E-commerce** | Segmentation produits | Recommandations, merchandising |
| **CybersÃ©curitÃ©** | DÃ©tection d'anomalies | Identifier les comportements suspects |
| **Biologie** | Groupes de gÃ¨nes | DÃ©couvrir des familles gÃ©nÃ©tiques |
| **Image** | Compression d'images | RÃ©duire le nombre de couleurs |
| **Texte** | Topic modeling | Regrouper des documents similaires |
| **Finance** | Profils de risque | CatÃ©goriser les clients par risque |

> ğŸ’¡ **Conseil** : "Le clustering est un outil d'exploration, pas de prÃ©diction. Utilisez-le pour comprendre vos donnÃ©es et gÃ©nÃ©rer des hypothÃ¨ses, que vous pourrez ensuite valider avec des mÃ©thodes supervisÃ©es."

### 1.3 La distance, notion fondamentale

Tout algorithme de clustering repose sur une notion de **distance** (ou similaritÃ©) entre les points.

| Distance | Formule | Utilisation |
|---|---|---|
| **Euclidienne** | âˆš(Î£(xi - yi)Â²) | Par dÃ©faut, donnÃ©es numÃ©riques continues |
| **Manhattan** | Î£|xi - yi| | DonnÃ©es sur grille, features indÃ©pendantes |
| **Cosinus** | 1 - cos(Î¸) | Texte (TF-IDF), donnÃ©es de haute dimension |
| **Minkowski** | (Î£|xi - yi|^p)^(1/p) | GÃ©nÃ©ralisation (p=1: Manhattan, p=2: Euclidienne) |

> âš ï¸ **Attention** : "La distance euclidienne est trÃ¨s sensible Ã  l'Ã©chelle des variables. Une feature en milliers (salaire) dominera une feature en unitÃ©s (Ã¢ge). Il faut TOUJOURS normaliser avant le clustering !"

---

## 2. ğŸ“Š K-Means

### 2.1 Algorithme pas Ã  pas

K-Means est l'algorithme de clustering le plus populaire. Son fonctionnement est simple et Ã©lÃ©gant :

1. **Initialisation** : Choisir K centroÃ¯des alÃ©atoires (ou avec K-Means++)
2. **Assignation** : Chaque point est assignÃ© au centroÃ¯de le plus proche
3. **Mise Ã  jour** : Recalculer chaque centroÃ¯de comme la moyenne de ses points
4. **RÃ©pÃ©ter** les Ã©tapes 2-3 jusqu'Ã  convergence (les assignations ne changent plus)

### 2.2 ImplÃ©mentation avec scikit-learn

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs

# GÃ©nÃ©rer des donnÃ©es d'exemple
X, y_true = make_blobs(
    n_samples=500,
    n_features=2,
    centers=4,          # 4 clusters rÃ©els
    cluster_std=1.0,
    random_state=42
)

# Ã‰TAPE CRUCIALE : Normaliser les donnÃ©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Appliquer K-Means
kmeans = KMeans(
    n_clusters=4,        # nombre de clusters
    init='k-means++',    # initialisation intelligente
    n_init=10,           # nombre d'initialisations (prendre la meilleure)
    max_iter=300,        # nombre max d'itÃ©rations
    random_state=42
)

# EntraÃ®ner et prÃ©dire
clusters = kmeans.fit_predict(X_scaled)

# RÃ©sultats
print(f"CentroÃ¯des : \n{kmeans.cluster_centers_}")
print(f"Inertie (somme des distances) : {kmeans.inertia_:.2f}")
print(f"Nombre d'itÃ©rations : {kmeans.n_iter_}")

# Visualiser les clusters
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.scatter(
    kmeans.cluster_centers_[:, 0],
    kmeans.cluster_centers_[:, 1],
    c='red', marker='X', s=200, label='CentroÃ¯des'
)
plt.colorbar(scatter, label='Cluster')
plt.xlabel('Feature 1 (normalisÃ©e)')
plt.ylabel('Feature 2 (normalisÃ©e)')
plt.title('RÃ©sultat du K-Means (K=4)')
plt.legend()
plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil** : "Toujours normaliser les donnÃ©es avant K-Means. Sans normalisation, les features avec de grandes valeurs dominent la distance et le clustering est biaisÃ©."

### 2.3 La mÃ©thode du coude (Elbow Method) pour choisir K

Le choix de K est la question centrale de K-Means. La mÃ©thode du coude trace l'**inertie** (somme des distances au centroÃ¯de) en fonction de K.

```python
# MÃ©thode du coude
inertias = []
K_range = range(2, 11)

for k in K_range:
    km = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
    km.fit(X_scaled)
    inertias.append(km.inertia_)

# Tracer la courbe du coude
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertias, 'bo-', linewidth=2)
plt.xlabel('Nombre de clusters (K)')
plt.ylabel('Inertie')
plt.title('MÃ©thode du Coude â€“ Choix de K')
plt.xticks(K_range)
plt.grid(True, alpha=0.3)

# Annoter le coude
plt.annotate('Coude probable',
    xy=(4, inertias[2]),
    xytext=(6, inertias[0]),
    arrowprops=dict(arrowstyle='->', color='red'),
    fontsize=12, color='red'
)
plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil de pro** : "La mÃ©thode du coude est parfois ambiguÃ« (pas de coude net). ComplÃ©tez-la toujours avec le silhouette score pour confirmer votre choix de K."

### 2.4 K-Means++ â€“ Une meilleure initialisation

K-Means classique initialise les centroÃ¯des alÃ©atoirement, ce qui peut donner de mauvais rÃ©sultats. K-Means++ utilise une initialisation intelligente :

1. Choisir le premier centroÃ¯de alÃ©atoirement
2. Pour chaque centroÃ¯de suivant : choisir un point avec une probabilitÃ© proportionnelle Ã  sa distance au centroÃ¯de le plus proche
3. Cela garantit que les centroÃ¯des initiaux sont **bien espacÃ©s**

> ğŸ’¡ **Conseil** : "K-Means++ est activÃ© par dÃ©faut dans scikit-learn (`init='k-means++'`). Ne changez jamais ce paramÃ¨tre. Il converge plus vite et donne de meilleurs rÃ©sultats."

### 2.5 Limites de K-Means

| Limitation | Explication |
|---|---|
| **Clusters sphÃ©riques** | Assume que les clusters sont ronds et de taille similaire |
| **K doit Ãªtre fixÃ©** | Il faut choisir le nombre de clusters a priori |
| **Sensible aux outliers** | Les outliers dÃ©placent les centroÃ¯des |
| **Sensible Ã  l'initialisation** | Peut converger vers un optimum local |
| **Pas adaptÃ© aux densitÃ©s variables** | Clusters de densitÃ©s diffÃ©rentes mal gÃ©rÃ©s |

> âš ï¸ **Attention** : "K-Means assume des clusters sphÃ©riques de taille similaire. Si vos donnÃ©es ont des clusters en forme de croissant, allongÃ©s ou de densitÃ©s trÃ¨s diffÃ©rentes, K-Means Ã©chouera. Utilisez DBSCAN dans ce cas."

---

## 3. ğŸ” DBSCAN

### 3.1 Principe : clustering par densitÃ©

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) regroupe les points qui sont **densÃ©ment connectÃ©s** et identifie les points isolÃ©s comme du **bruit** (outliers).

Deux paramÃ¨tres clÃ©s :

- **eps** (epsilon) : rayon de voisinage
- **min_samples** : nombre minimum de points dans le voisinage pour former un cluster

Trois types de points :

1. **Core points** : au moins `min_samples` points dans un rayon `eps`
2. **Border points** : dans le voisinage d'un core point mais pas core eux-mÃªmes
3. **Noise points** : ni core, ni border â†’ outliers (label = -1)

### 3.2 ImplÃ©mentation

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# DonnÃ©es en forme de croissants (non sphÃ©riques)
X_moons, _ = make_moons(n_samples=500, noise=0.1, random_state=42)
X_moons_scaled = StandardScaler().fit_transform(X_moons)

# DBSCAN
dbscan = DBSCAN(
    eps=0.3,          # rayon de voisinage
    min_samples=5,    # minimum de points pour un cluster
    metric='euclidean'
)

clusters_db = dbscan.fit_predict(X_moons_scaled)

# RÃ©sultats
n_clusters = len(set(clusters_db)) - (1 if -1 in clusters_db else 0)
n_noise = list(clusters_db).count(-1)

print(f"Nombre de clusters trouvÃ©s : {n_clusters}")
print(f"Points de bruit (outliers) : {n_noise}")

# Visualisation
plt.figure(figsize=(10, 6))
plt.scatter(
    X_moons_scaled[:, 0], X_moons_scaled[:, 1],
    c=clusters_db, cmap='viridis', alpha=0.7
)
# Mettre en Ã©vidence les outliers
mask_bruit = clusters_db == -1
plt.scatter(
    X_moons_scaled[mask_bruit, 0],
    X_moons_scaled[mask_bruit, 1],
    c='red', marker='x', s=100, label='Bruit (outliers)'
)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title(f'DBSCAN : {n_clusters} clusters, {n_noise} outliers')
plt.legend()
plt.tight_layout()
plt.show()
```

### 3.3 Comment choisir eps ?

Le **knee plot des k-distances** est la mÃ©thode standard pour trouver eps :

```python
from sklearn.neighbors import NearestNeighbors

# Calculer la distance au k-iÃ¨me voisin le plus proche
k = 5  # mÃªme valeur que min_samples
nn = NearestNeighbors(n_neighbors=k)
nn.fit(X_moons_scaled)
distances, _ = nn.kneighbors(X_moons_scaled)

# Trier les distances au k-iÃ¨me voisin
k_distances = np.sort(distances[:, k-1])

# Tracer le graphique
plt.figure(figsize=(8, 5))
plt.plot(k_distances, linewidth=2)
plt.xlabel('Points (triÃ©s)')
plt.ylabel(f'Distance au {k}-Ã¨me voisin')
plt.title(f'Knee Plot â€“ Choix de eps (k={k})')
plt.grid(True, alpha=0.3)

# Le coude indique la valeur de eps
plt.axhline(y=0.3, color='red', linestyle='--', label=f'eps â‰ˆ 0.3')
plt.legend()
plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil de pro** : "Utilisez le knee plot des k-distances pour trouver eps. Le coude dans la courbe correspond au seuil naturel entre les points denses (clusters) et les points isolÃ©s (bruit). Fixez min_samples = 2 * n_features comme point de dÃ©part."

### 3.4 Avantages et inconvÃ©nients de DBSCAN

| Avantages | InconvÃ©nients |
|---|---|
| DÃ©tecte les **formes arbitraires** | Sensible aux paramÃ¨tres eps et min_samples |
| **GÃ¨re le bruit** (outliers automatiques) | Pas adaptÃ© aux **densitÃ©s variables** |
| Pas besoin de spÃ©cifier K | **Difficile** en haute dimension |
| Robuste aux outliers | Pas de `predict()` pour de nouvelles donnÃ©es |
| DÃ©terministe (pas d'initialisation alÃ©atoire) | Peut Ã©chouer si les clusters ont des densitÃ©s trÃ¨s diffÃ©rentes |

> ğŸ’¡ **Conseil** : "DBSCAN est excellent pour la dÃ©tection d'anomalies : les points Ã©tiquetÃ©s comme Â« bruit Â» (-1) sont vos anomalies. C'est souvent plus intuitif que les algorithmes dÃ©diÃ©s Ã  la dÃ©tection d'anomalies."

---

## 4. ğŸŒ³ Clustering hiÃ©rarchique

### 4.1 Principe

Le clustering hiÃ©rarchique construit une **hiÃ©rarchie de clusters** reprÃ©sentÃ©e sous forme de **dendrogramme**. Deux approches :

- **AgglomÃ©ratif (bottom-up)** : chaque point commence seul â†’ on fusionne progressivement les clusters les plus proches
- **Divisif (top-down)** : tous les points commencent dans un seul cluster â†’ on divise progressivement

En pratique, l'approche **agglomÃ©rative** est la plus utilisÃ©e.

### 4.2 Types de linkage

Le linkage dÃ©finit comment on mesure la distance **entre deux clusters** :

| Linkage | Distance entre clusters | PropriÃ©tÃ©s |
|---|---|---|
| **Single** | Min des distances point-Ã -point | DÃ©tecte les formes allongÃ©es, sensible au bruit |
| **Complete** | Max des distances point-Ã -point | Clusters compacts, sensible aux outliers |
| **Average** | Moyenne des distances | Compromis entre single et complete |
| **Ward** | Minimise la variance intra-cluster | Clusters sphÃ©riques et de taille similaire (le plus utilisÃ©) |

### 4.3 ImplÃ©mentation et dendrogramme

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.datasets import make_blobs

# DonnÃ©es d'exemple
X_hier, _ = make_blobs(n_samples=150, centers=4, cluster_std=1.0, random_state=42)
X_hier_scaled = StandardScaler().fit_transform(X_hier)

# Calculer le linkage pour le dendrogramme
Z = linkage(X_hier_scaled, method='ward', metric='euclidean')

# Tracer le dendrogramme
plt.figure(figsize=(14, 7))
dendrogram(
    Z,
    truncate_mode='level',
    p=5,                     # afficher 5 niveaux
    leaf_rotation=90,
    leaf_font_size=8,
    color_threshold=7        # seuil de couleur pour les clusters
)
plt.xlabel('Ã‰chantillons')
plt.ylabel('Distance')
plt.title('Dendrogramme â€“ Clustering HiÃ©rarchique (Ward)')
plt.axhline(y=7, color='red', linestyle='--', label='Seuil de coupe (4 clusters)')
plt.legend()
plt.tight_layout()
plt.show()

# Appliquer le clustering agglomÃ©ratif
agg = AgglomerativeClustering(
    n_clusters=4,       # ou distance_threshold pour couper automatiquement
    linkage='ward'
)

clusters_hier = agg.fit_predict(X_hier_scaled)

# Visualiser
plt.figure(figsize=(8, 6))
plt.scatter(X_hier_scaled[:, 0], X_hier_scaled[:, 1], c=clusters_hier, cmap='viridis', alpha=0.7)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Clustering HiÃ©rarchique AgglomÃ©ratif (Ward, K=4)')
plt.colorbar(label='Cluster')
plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil** : "Le dendrogramme est un outil visuel puissant. Le bon nombre de clusters correspond souvent Ã  l'endroit oÃ¹ les branches du dendrogramme sont les plus longues avant de fusionner. C'est lÃ  qu'il faut Â« couper Â»."

### 4.4 Quand utiliser le clustering hiÃ©rarchique ?

| Situation | RecommandÃ© ? |
|---|---|
| Explorer la structure hiÃ©rarchique des donnÃ©es | Oui |
| Petit dataset (< 10 000 points) | Oui |
| Grand dataset (> 50 000 points) | Non (complexitÃ© O(nÂ³) avec Ward) |
| Besoin de visualiser la hiÃ©rarchie | Oui (dendrogramme) |
| Clusters de formes complexes | DÃ©pend du linkage |

---

## 5. ğŸ“Š MÃ‰TRIQUES DE CLUSTERING â€“ Ã‰valuer sans labels

L'Ã©valuation du clustering est un dÃ©fi particulier car il n'y a **pas de vÃ©ritÃ© terrain**. On utilise des mÃ©triques **intrinsÃ¨ques** qui mesurent la qualitÃ© de la structure trouvÃ©e.

### 5.1 Silhouette Score (-1 Ã  1)

Le silhouette score mesure Ã  quel point chaque point est bien assignÃ© Ã  son cluster. Pour chaque point i :

- **a(i)** = distance moyenne aux autres points du **mÃªme cluster** (cohÃ©sion)
- **b(i)** = distance moyenne aux points du **cluster le plus proche** (sÃ©paration)
- **s(i)** = (b(i) - a(i)) / max(a(i), b(i))

| Valeur | InterprÃ©tation |
|---|---|
| **s â‰ˆ 1** | Point bien assignÃ©, loin des autres clusters |
| **s â‰ˆ 0** | Point Ã  la frontiÃ¨re entre deux clusters |
| **s < 0** | Point probablement mal assignÃ© |

```python
from sklearn.metrics import silhouette_score, silhouette_samples

# Calculer le silhouette score global
sil_score = silhouette_score(X_scaled, clusters)
print(f"Silhouette Score global : {sil_score:.4f}")

# Silhouette score pour chaque K
sil_scores = []
K_range = range(2, 11)

for k in K_range:
    km = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
    labels = km.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    sil_scores.append(score)
    print(f"K={k} : Silhouette = {score:.4f}")

# Tracer
plt.figure(figsize=(8, 5))
plt.plot(K_range, sil_scores, 'go-', linewidth=2)
plt.xlabel('Nombre de clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score en fonction de K')
plt.xticks(K_range)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

#### Silhouette Plot â€“ Visualisation dÃ©taillÃ©e

```python
from sklearn.metrics import silhouette_samples

# Silhouette plot pour K=4
km = KMeans(n_clusters=4, init='k-means++', n_init=10, random_state=42)
labels = km.fit_predict(X_scaled)
sil_values = silhouette_samples(X_scaled, labels)

fig, ax = plt.subplots(figsize=(8, 6))
y_lower = 10

for i in range(4):
    # Valeurs de silhouette pour le cluster i
    cluster_sil = sil_values[labels == i]
    cluster_sil.sort()

    size_cluster = len(cluster_sil)
    y_upper = y_lower + size_cluster

    ax.fill_betweenx(
        np.arange(y_lower, y_upper),
        0, cluster_sil,
        alpha=0.7, label=f'Cluster {i}'
    )
    y_lower = y_upper + 10

# Ligne verticale pour le score moyen
sil_avg = silhouette_score(X_scaled, labels)
ax.axvline(x=sil_avg, color='red', linestyle='--', label=f'Moyenne ({sil_avg:.3f})')

ax.set_xlabel('Silhouette Score')
ax.set_ylabel('Ã‰chantillons (par cluster)')
ax.set_title('Silhouette Plot (K=4)')
ax.legend()
plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil de pro** : "Le silhouette score est la mÃ©trique la plus universelle pour le clustering. Visez un score > 0.5 pour un bon clustering. En dessous de 0.25, la structure est faible. Utilisez le silhouette plot pour identifier les clusters problÃ©matiques."

### 5.2 Inertie (K-Means)

L'inertie est la **somme des distances** de chaque point Ã  son centroÃ¯de le plus proche. C'est la mÃ©trique optimisÃ©e par K-Means.

- **Plus l'inertie est faible**, plus les clusters sont compacts
- L'inertie diminue **toujours** quand K augmente (elle atteint 0 quand K = n)
- On cherche le **coude** dans la courbe inertie vs K

```python
# DÃ©jÃ  calculÃ©e dans la section mÃ©thode du coude
print(f"Inertie (K=4) : {kmeans.inertia_:.2f}")
```

> âš ï¸ **Attention** : "L'inertie seule n'est pas suffisante pour choisir K car elle diminue toujours. Elle doit Ãªtre combinÃ©e avec le silhouette score ou d'autres mÃ©triques."

### 5.3 Calinski-Harabasz Index

Le Calinski-Harabasz Index (aussi appelÃ© Variance Ratio Criterion) mesure le **ratio entre la dispersion inter-clusters et la dispersion intra-clusters**. Plus il est Ã©levÃ©, mieux c'est.

```python
from sklearn.metrics import calinski_harabasz_score

ch_score = calinski_harabasz_score(X_scaled, clusters)
print(f"Calinski-Harabasz Index : {ch_score:.2f}")

# Pour chaque K
ch_scores = []
for k in K_range:
    km = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
    labels = km.fit_predict(X_scaled)
    score = calinski_harabasz_score(X_scaled, labels)
    ch_scores.append(score)
    print(f"K={k} : CH = {score:.2f}")
```

### 5.4 Davies-Bouldin Index

Le Davies-Bouldin Index mesure la **similaritÃ© entre clusters**. Plus il est **bas**, mieux c'est (les clusters sont bien sÃ©parÃ©s).

```python
from sklearn.metrics import davies_bouldin_score

db_score = davies_bouldin_score(X_scaled, clusters)
print(f"Davies-Bouldin Index : {db_score:.4f}")
```

### 5.5 Table comparative des mÃ©triques de clustering

| MÃ©trique | Range | Meilleur si | Avantages | Limites |
|---|---|---|---|---|
| **Silhouette Score** | -1 Ã  1 | Proche de 1 | Universelle, intuitive | O(nÂ²) pour grands datasets |
| **Inertie** | 0 Ã  +âˆ | Bas (coude) | Simple, native K-Means | Diminue toujours avec K |
| **Calinski-Harabasz** | 0 Ã  +âˆ | Haut | Rapide Ã  calculer | Favorise les clusters sphÃ©riques |
| **Davies-Bouldin** | 0 Ã  +âˆ | Bas | Simple d'interprÃ©tation | Favorise les clusters convexes |

> ğŸ’¡ **Conseil** : "Utilisez toujours plusieurs mÃ©triques en parallÃ¨le. Si le silhouette score, le Calinski-Harabasz et la mÃ©thode du coude convergent vers le mÃªme K, vous pouvez Ãªtre confiant dans votre choix."

---

## 6. ğŸ“ˆ Comment amÃ©liorer un clustering

### 6.1 Checklist d'amÃ©lioration

1. **Normaliser les donnÃ©es** : StandardScaler ou MinMaxScaler (indispensable !)
2. **Choisir le bon K** : mÃ©thode du coude + silhouette score
3. **SÃ©lectionner les bonnes features** : Ã©liminer les features non pertinentes
4. **Essayer plusieurs algorithmes** : K-Means, DBSCAN, HiÃ©rarchique
5. **RÃ©duire la dimension** : PCA ou t-SNE pour explorer visuellement
6. **Traiter les outliers** : les retirer ou utiliser DBSCAN
7. **InterprÃ©ter les clusters** : statistiques descriptives par cluster

### 6.2 Visualisation avec PCA pour vÃ©rification

```python
from sklearn.decomposition import PCA

# RÃ©duire Ã  2D pour visualisation
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"Variance expliquÃ©e : {pca.explained_variance_ratio_.sum():.2%}")

# Visualiser les clusters dans l'espace PCA
plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('Visualisation des clusters (PCA 2D)')
plt.colorbar(scatter, label='Cluster')
plt.tight_layout()
plt.show()
```

### 6.3 Profiling des clusters

```python
# CrÃ©er un DataFrame avec les clusters
df_clusters = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
df_clusters['cluster'] = clusters

# Statistiques par cluster
profil = df_clusters.groupby('cluster').agg(['mean', 'std', 'count'])
print("Profil des clusters :")
print(profil)

# Taille des clusters
print("\nTaille des clusters :")
print(df_clusters['cluster'].value_counts().sort_index())
```

> ğŸ’¡ **Conseil de pro** : "Un clustering n'a de valeur que si les clusters sont interprÃ©tables et actionnables. AprÃ¨s le clustering, faites TOUJOURS un profiling : quelles sont les caractÃ©ristiques de chaque groupe ? Donnez des noms parlants Ã  vos clusters (Â« Clients premium Â», Â« Acheteurs occasionnels Â», etc.)."

### 6.4 Comparaison des algorithmes de clustering

| CritÃ¨re | K-Means | DBSCAN | HiÃ©rarchique |
|---|---|---|---|
| **Forme des clusters** | SphÃ©rique | Arbitraire | DÃ©pend du linkage |
| **Nombre de clusters** | Ã€ fixer (K) | Automatique | Ã€ fixer ou seuil |
| **Gestion du bruit** | Non | Oui (outliers) | Non |
| **ScalabilitÃ©** | Excellente (O(nKT)) | Bonne (O(n log n)) | Faible (O(nÂ²) Ã  O(nÂ³)) |
| **Taille max** | Millions | Centaines de milliers | Dizaines de milliers |
| **ReproductibilitÃ©** | Non (init alÃ©atoire) | Oui (dÃ©terministe) | Oui |
| **InterprÃ©tabilitÃ©** | CentroÃ¯des | DensitÃ© | Dendrogramme |

> ğŸ’¡ **Conseil** : "Pour choisir votre algorithme : (1) Essayez K-Means en premier car c'est le plus rapide. (2) Si les clusters ne sont pas sphÃ©riques ou s'il y a des outliers, passez Ã  DBSCAN. (3) Si vous voulez explorer la hiÃ©rarchie des donnÃ©es, utilisez le clustering hiÃ©rarchique avec un dendrogramme."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. Le clustering est un apprentissage **non supervisÃ©** : pas de labels, on dÃ©couvre des groupes
2. **K-Means** est rapide et simple mais assume des clusters sphÃ©riques
3. **DBSCAN** dÃ©tecte les formes arbitraires et gÃ¨re les outliers automatiquement
4. Le **clustering hiÃ©rarchique** fournit un dendrogramme pour explorer la structure
5. **Toujours normaliser** les donnÃ©es avant le clustering (StandardScaler)
6. Le **silhouette score** est la mÃ©trique la plus fiable (viser > 0.5)
7. Combiner **plusieurs mÃ©triques** : silhouette, Calinski-Harabasz, Davies-Bouldin
8. La **mÃ©thode du coude** aide Ã  choisir K mais n'est pas toujours claire
9. Toujours **profiler les clusters** pour les rendre interprÃ©tables
10. **Visualiser avec PCA** 2D pour vÃ©rifier la qualitÃ© du clustering

## âœ… Checklist de validation

- [ ] Je comprends la diffÃ©rence entre apprentissage supervisÃ© et non supervisÃ©
- [ ] Je sais appliquer K-Means et choisir K (coude + silhouette)
- [ ] Je sais utiliser DBSCAN et trouver les bons paramÃ¨tres (eps, min_samples)
- [ ] Je sais lire un dendrogramme et choisir oÃ¹ couper
- [ ] Je maÃ®trise les 4 mÃ©triques de clustering (silhouette, inertie, CH, DB)
- [ ] Je sais normaliser les donnÃ©es avant le clustering
- [ ] Je sais profiler et interprÃ©ter les clusters obtenus
- [ ] Je sais choisir entre K-Means, DBSCAN et HiÃ©rarchique

---

[â¬…ï¸ Chapitre 6 : MÃ©thodes d'Ensemble](06-ensemble-methods.md) | [â¡ï¸ Chapitre 8 : Ã‰valuation et MÃ©triques](08-evaluation-metriques.md)
