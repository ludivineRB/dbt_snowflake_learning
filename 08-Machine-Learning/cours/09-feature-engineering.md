# Chapitre 9 : Feature Engineering ‚Äì L'Art de Cr√©er des Features

## üéØ Objectifs

- Comprendre pourquoi le feature engineering est l'√©tape la plus d√©cisive du ML
- Savoir cr√©er de nouvelles features √† partir de donn√©es num√©riques, temporelles, textuelles et cat√©gorielles
- Ma√Ætriser les m√©thodes de s√©lection de features (filter, wrapper, embedded)
- Comprendre la r√©duction de dimensionnalit√© avec PCA
- Construire des pipelines complets et reproductibles avec scikit-learn

---

## 1. üß† Pourquoi le feature engineering est d√©cisif

### 1.1 Le carburant des mod√®les ML

> "Features are the fuel of ML models. Better features = better models. Period."

Le feature engineering est le processus de **transformation des donn√©es brutes en features informatives** pour les algorithmes de Machine Learning. C'est l'√©tape qui a le **plus grand impact** sur la performance d'un mod√®le.

| Levier d'am√©lioration | Impact typique | Effort |
|---|---|---|
| Donn√©es propres | 10-30% | √âlev√© |
| **Feature engineering** | **10-30%** | **√âlev√©** |
| Choix de l'algorithme | 5-15% | Moyen |
| Tuning des hyperparam√®tres | 2-5% | Moyen |
| Ensemble methods | 1-3% | Faible |

> üí° **Conseil de pro** : "Le feature engineering est ce qui s√©pare un bon data scientist d'un data scientist moyen. C'est l√† que la connaissance du domaine m√©tier fait la diff√©rence. Un bon feature engineering avec une r√©gression logistique battra souvent un mauvais feature engineering avec XGBoost."

### 1.2 Principes fondamentaux

1. **Comprendre le domaine** : Parlez aux experts m√©tier. Quelles informations utilisent-ils pour prendre des d√©cisions ?
2. **Explorer les donn√©es** : Visualisez les distributions, les corr√©lations, les patterns temporels
3. **It√©rer** : Cr√©ez des features, testez leur impact, gardez les meilleures, supprimez les inutiles
4. **Mesurer** : Chaque feature doit am√©liorer la m√©trique cible (ou ne pas la d√©grader)

> üí° **Conseil** : "Avant de cr√©er des features, passez du temps √† COMPRENDRE les donn√©es. Chaque colonne, chaque distribution, chaque corr√©lation. Le feature engineering vient naturellement quand on comprend les donn√©es."

---

## 2. ‚öôÔ∏è Cr√©ation de features

### 2.1 Features num√©riques

#### Transformations math√©matiques

```python
import pandas as pd
import numpy as np

# Donn√©es d'exemple : transactions e-commerce
df = pd.DataFrame({
    'prix': [29.99, 149.50, 9.99, 499.00, 74.50],
    'quantite': [2, 1, 5, 1, 3],
    'poids_kg': [0.5, 2.1, 0.1, 5.0, 1.2],
    'surface_m2': [10, 50, 5, 200, 30],
    'anciennete_jours': [30, 365, 7, 730, 180]
})

# Features d√©riv√©es num√©riques
df['montant_total'] = df['prix'] * df['quantite']              # ratio / produit
df['prix_par_kg'] = df['prix'] / df['poids_kg']                # ratio
df['log_prix'] = np.log1p(df['prix'])                          # transformation log
df['prix_carre'] = df['prix'] ** 2                             # polynomiale
df['prix_racine'] = np.sqrt(df['prix'])                        # racine carr√©e
df['surface_log'] = np.log1p(df['surface_m2'])                 # log pour distributions skewed

# Binning (discr√©tisation)
df['categorie_prix'] = pd.cut(
    df['prix'],
    bins=[0, 20, 100, 500],
    labels=['pas_cher', 'moyen', 'cher']
)

# Quantile binning (m√™me nombre d'observations par bin)
df['quantile_prix'] = pd.qcut(df['prix'], q=3, labels=['bas', 'moyen', 'haut'])

print(df)
```

> üí° **Conseil** : "La transformation logarithmique est votre meilleur ami pour les distributions asym√©triques (prix, revenus, surfaces). Elle r√©duit l'impact des valeurs extr√™mes et rend souvent les relations plus lin√©aires."

#### Interactions entre features

```python
# Interactions polynomiales
from sklearn.preprocessing import PolynomialFeatures

X_num = df[['prix', 'quantite', 'poids_kg']].values

# Cr√©er des interactions d'ordre 2 (a, b, a¬≤, ab, b¬≤)
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)
X_poly = poly.fit_transform(X_num)

# Voir les noms des features cr√©√©es
feature_names = poly.get_feature_names_out(['prix', 'quantite', 'poids_kg'])
print(f"Features originales : {X_num.shape[1]}")
print(f"Features apr√®s polynomiales : {X_poly.shape[1]}")
print(f"Noms : {feature_names}")
```

> ‚ö†Ô∏è **Attention** : "Les features polynomiales peuvent exploser combinatoirement. Avec 10 features et degree=3, vous obtenez 286 features ! Utilisez `interaction_only=True` pour limiter aux interactions sans les puissances."

### 2.2 Features temporelles

Les donn√©es temporelles sont une mine d'or pour le feature engineering :

```python
# Donn√©es temporelles
df_time = pd.DataFrame({
    'date_achat': pd.to_datetime([
        '2024-01-15 14:30:00',
        '2024-03-22 09:15:00',
        '2024-07-04 22:45:00',
        '2024-12-25 11:00:00',
        '2024-06-15 16:30:00'
    ]),
    'montant': [50, 120, 30, 200, 75]
})

# Features temporelles de base
df_time['annee'] = df_time['date_achat'].dt.year
df_time['mois'] = df_time['date_achat'].dt.month
df_time['jour'] = df_time['date_achat'].dt.day
df_time['heure'] = df_time['date_achat'].dt.hour
df_time['jour_semaine'] = df_time['date_achat'].dt.dayofweek  # 0=lundi, 6=dimanche
df_time['jour_annee'] = df_time['date_achat'].dt.dayofyear
df_time['semaine'] = df_time['date_achat'].dt.isocalendar().week.astype(int)

# Features d√©riv√©es
df_time['est_weekend'] = df_time['jour_semaine'].isin([5, 6]).astype(int)
df_time['est_matin'] = (df_time['heure'] < 12).astype(int)
df_time['trimestre'] = df_time['date_achat'].dt.quarter

# Saisonnalit√© (encodage cyclique pour capturer la circularit√©)
df_time['mois_sin'] = np.sin(2 * np.pi * df_time['mois'] / 12)
df_time['mois_cos'] = np.cos(2 * np.pi * df_time['mois'] / 12)
df_time['heure_sin'] = np.sin(2 * np.pi * df_time['heure'] / 24)
df_time['heure_cos'] = np.cos(2 * np.pi * df_time['heure'] / 24)

# Jours f√©ri√©s (simplifi√©)
jours_feries = ['2024-01-01', '2024-07-14', '2024-12-25']
df_time['est_ferie'] = df_time['date_achat'].dt.date.astype(str).isin(jours_feries).astype(int)

print(df_time)
```

#### Lag features et rolling averages (s√©ries temporelles)

```python
# Lag features : valeur aux pas de temps pr√©c√©dents
df_ts = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=30, freq='D'),
    'ventes': np.random.randint(50, 200, 30)
})
df_ts = df_ts.set_index('date')

# Lag features
df_ts['ventes_j-1'] = df_ts['ventes'].shift(1)   # ventes d'hier
df_ts['ventes_j-7'] = df_ts['ventes'].shift(7)   # ventes il y a 7 jours

# Rolling averages (moyenne glissante)
df_ts['moyenne_7j'] = df_ts['ventes'].rolling(window=7).mean()
df_ts['moyenne_14j'] = df_ts['ventes'].rolling(window=14).mean()
df_ts['std_7j'] = df_ts['ventes'].rolling(window=7).std()

# Variation par rapport √† la moyenne
df_ts['ratio_vs_moy7j'] = df_ts['ventes'] / df_ts['moyenne_7j']

# Tendance (diff√©rence)
df_ts['diff_j-1'] = df_ts['ventes'].diff(1)

print(df_ts.head(15))
```

> üí° **Conseil de pro** : "Pour les s√©ries temporelles, les lag features et les rolling averages sont parmi les features les plus puissantes. Testez toujours les lags 1, 7, 14, 28 (quotidien) ou 1, 12, 24 (horaire). L'encodage cyclique (sin/cos) est crucial pour les heures et les mois."

> ‚ö†Ô∏è **Attention** : "Les lag features cr√©ent des valeurs manquantes en d√©but de s√©rie (shift). N'oubliez pas de les supprimer ou de les imputer. Et surtout : JAMAIS utiliser des donn√©es futures dans les lags ‚Üí data leakage temporel !"

### 2.3 Features textuelles

```python
# Features de base √† partir de texte
df_text = pd.DataFrame({
    'description': [
        'Excellent produit, livraison rapide !',
        'Nul. Produit cass√© √† la r√©ception.',
        'Correct pour le prix. RAS.',
        'INCROYABLE !!! Le meilleur achat de ma vie !!!',
        'Bof, pas terrible mais pas catastrophique non plus.'
    ]
})

# Features simples
df_text['nb_mots'] = df_text['description'].str.split().str.len()
df_text['nb_caracteres'] = df_text['description'].str.len()
df_text['nb_exclamation'] = df_text['description'].str.count('!')
df_text['nb_majuscules'] = df_text['description'].str.count('[A-Z]')
df_text['ratio_majuscules'] = df_text['nb_majuscules'] / df_text['nb_caracteres']
df_text['nb_points'] = df_text['description'].str.count('\\.')
df_text['longueur_moy_mot'] = (
    df_text['nb_caracteres'] / df_text['nb_mots']
)

print(df_text)
```

#### TF-IDF pour le texte (introduction vers le NLP)

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF : Term Frequency - Inverse Document Frequency
tfidf = TfidfVectorizer(
    max_features=100,      # garder les 100 mots les plus importants
    stop_words=None,       # ou liste de stop words fran√ßais
    ngram_range=(1, 2),    # unigrammes et bigrammes
    min_df=1,              # mot doit appara√Ætre au moins 1 fois
    max_df=0.95            # pas de mots dans 95%+ des documents
)

tfidf_matrix = tfidf.fit_transform(df_text['description'])
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=tfidf.get_feature_names_out()
)

print(f"Shape TF-IDF : {tfidf_df.shape}")
print(tfidf_df.head())
```

> üí° **Conseil** : "Les features textuelles simples (longueur, nombre de mots, ponctuation) sont souvent tr√®s utiles en compl√©ment du TF-IDF. Un avis avec beaucoup de points d'exclamation et de majuscules a souvent un sentiment fort."

### 2.4 Features cat√©gorielles avanc√©es

Au-del√† du One-Hot Encoding classique, il existe des encodages plus sophistiqu√©s :

#### Target Encoding

```python
# Target encoding : remplacer la cat√©gorie par la moyenne de la cible
df_cat = pd.DataFrame({
    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris', 'Marseille', 'Lyon'],
    'prix': [500, 300, 450, 280, 320, 480, 260, 310]
})

# Target encoding avec r√©gularisation (smoothing)
global_mean = df_cat['prix'].mean()
smoothing = 10  # param√®tre de lissage

target_encoding = df_cat.groupby('ville')['prix'].agg(['mean', 'count'])
target_encoding['target_encode'] = (
    (target_encoding['count'] * target_encoding['mean'] + smoothing * global_mean) /
    (target_encoding['count'] + smoothing)
)

df_cat['ville_encoded'] = df_cat['ville'].map(target_encoding['target_encode'])
print(df_cat)
```

> ‚ö†Ô∏è **Attention** : "Le target encoding peut facilement cr√©er un data leakage ! La cible (target) est utilis√©e pour encoder les features. Utilisez TOUJOURS le target encoding dans un pipeline avec cross-validation pour √©viter ce probl√®me."

#### Frequency Encoding

```python
# Frequency encoding : remplacer par la fr√©quence d'apparition
freq = df_cat['ville'].value_counts(normalize=True)
df_cat['ville_freq'] = df_cat['ville'].map(freq)
print(df_cat[['ville', 'ville_freq']])
```

> üí° **Conseil de pro** : "Le frequency encoding est un bon compromis entre simplicit√© et performance. Il capture l'information de raret√© sans data leakage. Utilisez-le pour les features cat√©gorielles √† haute cardinalit√© (>20 cat√©gories)."

#### Interactions entre features cat√©gorielles

```python
# Combiner deux cat√©gories
df_inter = pd.DataFrame({
    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille'],
    'type_bien': ['Appartement', 'Maison', 'Maison', 'Appartement']
})

# Interaction : combiner les cat√©gories
df_inter['ville_type'] = df_inter['ville'] + '_' + df_inter['type_bien']
print(df_inter)
```

---

## 3. üîç S√©lection de features

Trop de features = bruit, overfitting, lenteur. La s√©lection de features √©limine les features inutiles.

### 3.1 Filter methods (avant le mod√®le)

Les filter methods √©valuent les features **ind√©pendamment du mod√®le**, en utilisant des statistiques.

```python
from sklearn.feature_selection import (
    mutual_info_classif,
    chi2,
    f_classif,
    SelectKBest
)
from sklearn.datasets import make_classification

# Donn√©es d'exemple
X, y = make_classification(
    n_samples=1000, n_features=20,
    n_informative=10, n_redundant=5, n_useless=5,
    random_state=42
)

feature_names = [f'feature_{i}' for i in range(20)]

# Mutual Information (fonctionne pour toute relation, pas juste lin√©aire)
mi_scores = mutual_info_classif(X, y, random_state=42)
mi_df = pd.DataFrame({
    'feature': feature_names,
    'mutual_info': mi_scores
}).sort_values('mutual_info', ascending=False)

print("Top 10 features (Mutual Information) :")
print(mi_df.head(10))

# ANOVA F-test (lin√©aire)
f_scores, p_values = f_classif(X, y)
f_df = pd.DataFrame({
    'feature': feature_names,
    'f_score': f_scores,
    'p_value': p_values
}).sort_values('f_score', ascending=False)

print("\nTop 10 features (ANOVA F-test) :")
print(f_df.head(10))

# S√©lectionner les K meilleures features
selector = SelectKBest(score_func=mutual_info_classif, k=10)
X_selected = selector.fit_transform(X, y)

# Quelles features ont √©t√© s√©lectionn√©es ?
selected_mask = selector.get_support()
selected_features = [f for f, s in zip(feature_names, selected_mask) if s]
print(f"\nFeatures s√©lectionn√©es : {selected_features}")
```

#### Matrice de corr√©lation

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Matrice de corr√©lation
df_corr = pd.DataFrame(X, columns=feature_names)
correlation_matrix = df_corr.corr()

# Visualisation
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, vmin=-1, vmax=1)
plt.title('Matrice de Corr√©lation des Features')
plt.tight_layout()
plt.show()

# Identifier les features tr√®s corr√©l√©es entre elles (> 0.8)
upper_tri = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)
high_corr = [(col, row, correlation_matrix.loc[row, col])
             for col in upper_tri.columns
             for row in upper_tri.index
             if abs(upper_tri.loc[row, col]) > 0.8]

print(f"\nPaires tr√®s corr√©l√©es (|r| > 0.8) :")
for col, row, corr in high_corr:
    print(f"  {col} <-> {row} : {corr:.3f}")
```

> üí° **Conseil** : "Supprimez une des deux features dans chaque paire tr√®s corr√©l√©e (|r| > 0.8). Elles apportent la m√™me information mais ajoutent du bruit et ralentissent l'entra√Ænement."

### 3.2 Wrapper methods (avec le mod√®le)

Les wrapper methods utilisent le **mod√®le lui-m√™me** pour √©valuer les features.

#### RFE (Recursive Feature Elimination)

```python
from sklearn.feature_selection import RFE, RFECV
from sklearn.ensemble import RandomForestClassifier

# RFE : √©limination r√©cursive
rfe = RFE(
    estimator=RandomForestClassifier(n_estimators=100, random_state=42),
    n_features_to_select=10,  # garder 10 features
    step=1                     # √©liminer 1 feature √† chaque √©tape
)

rfe.fit(X, y)

# Features s√©lectionn√©es
rfe_features = [f for f, s in zip(feature_names, rfe.support_) if s]
print(f"Features s√©lectionn√©es (RFE) : {rfe_features}")
print(f"Ranking : {dict(zip(feature_names, rfe.ranking_))}")

# RFECV : RFE avec cross-validation (trouve automatiquement le nombre optimal)
rfecv = RFECV(
    estimator=RandomForestClassifier(n_estimators=100, random_state=42),
    step=1,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1
)

rfecv.fit(X, y)
print(f"\nNombre optimal de features : {rfecv.n_features_}")
print(f"Score avec les features optimales : {rfecv.cv_results_['mean_test_score'].max():.4f}")
```

> üí° **Conseil de pro** : "RFECV est la m√©thode la plus fiable pour la s√©lection de features car elle utilise la cross-validation. C'est lent mais robuste. Pour aller plus vite, utilisez `step=2` ou `step=0.1` (10% des features √©limin√©es √† chaque √©tape)."

### 3.3 Embedded methods (pendant le mod√®le)

Les embedded methods int√®grent la s√©lection de features **dans l'entra√Ænement du mod√®le**.

```python
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestClassifier

# Lasso (L1) : met automatiquement certains coefficients √† 0
lasso = LassoCV(cv=5, random_state=42)
# lasso.fit(X, y)  # pour la r√©gression

# Feature importance du Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X, y)

# S√©lection par importance (seuil = moyenne)
from sklearn.feature_selection import SelectFromModel

selector_rf = SelectFromModel(rf, threshold='mean')
X_selected_rf = selector_rf.fit_transform(X, y)

selected_rf = [f for f, s in zip(feature_names, selector_rf.get_support()) if s]
print(f"Features s√©lectionn√©es (RF importance > moyenne) : {selected_rf}")
print(f"Nombre : {len(selected_rf)} / {len(feature_names)}")
```

### 3.4 Comparaison des m√©thodes de s√©lection

| M√©thode | Type | Avantages | Inconv√©nients | Quand utiliser |
|---|---|---|---|---|
| **Corr√©lation** | Filter | Rapide, simple | Lin√©aire uniquement | Exploration rapide |
| **Mutual Info** | Filter | D√©tecte les relations non lin√©aires | Peut √™tre bruit√©e | Toujours en compl√©ment |
| **Chi2** | Filter | Adapt√© aux features cat√©gorielles | Features positives uniquement | Texte (TF-IDF) |
| **RFE** | Wrapper | Fiable, utilise le mod√®le | Lent | S√©lection finale |
| **RFECV** | Wrapper | Le plus fiable, trouve le nombre optimal | Tr√®s lent | Quand la performance prime |
| **Lasso (L1)** | Embedded | Int√©gr√© √† l'entra√Ænement | Lin√©aire | R√©gression, interpr√©tabilit√© |
| **Feature importance RF** | Embedded | Rapide, non lin√©aire | Biais√© (haute cardinalit√©) | Exploration rapide |

> üí° **Conseil** : "Utilisez une approche en entonnoir : (1) Filter method pour une premi√®re √©limination rapide (supprimer les features corr√©l√©es, p-value > 0.05). (2) Embedded method pour affiner (RF importance). (3) RFECV pour la s√©lection finale."

---

## 4. üìä PCA (Principal Component Analysis)

### 4.1 Concept

La PCA (Analyse en Composantes Principales) est une technique de **r√©duction de dimensionnalit√©**. Elle transforme les features originales en un nouveau jeu de **composantes principales** qui :

1. Sont **orthogonales** (non corr√©l√©es) entre elles
2. Maximisent la **variance expliqu√©e**
3. Sont ordonn√©es par variance d√©croissante (PC1 capture le plus de variance)

### 4.2 Impl√©mentation

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# IMPORTANT : toujours normaliser avant PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA : garder 95% de la variance
pca = PCA(n_components=0.95)  # garder 95% de la variance
X_pca = pca.fit_transform(X_scaled)

print(f"Features originales : {X.shape[1]}")
print(f"Composantes PCA : {X_pca.shape[1]}")
print(f"Variance expliqu√©e : {pca.explained_variance_ratio_.sum():.2%}")
print(f"Variance par composante : {pca.explained_variance_ratio_}")

# Visualisation : Scree plot (variance expliqu√©e cumul√©e)
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),
        pca.explained_variance_ratio_, alpha=0.7)
plt.xlabel('Composante principale')
plt.ylabel('Variance expliqu√©e')
plt.title('Variance par composante')

plt.subplot(1, 2, 2)
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         np.cumsum(pca.explained_variance_ratio_), 'bo-', linewidth=2)
plt.axhline(y=0.95, color='red', linestyle='--', label='95% de variance')
plt.xlabel('Nombre de composantes')
plt.ylabel('Variance expliqu√©e cumul√©e')
plt.title('Variance cumul√©e')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 4.3 Visualisation 2D avec PCA

```python
# PCA pour visualisation (2D)
pca_2d = PCA(n_components=2)
X_2d = pca_2d.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='viridis', alpha=0.5)
plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)')
plt.title('Projection PCA 2D')
plt.colorbar(scatter, label='Classe')
plt.tight_layout()
plt.show()
```

### 4.4 Quand utiliser PCA ?

| Situation | PCA recommand√© ? | Pourquoi |
|---|---|---|
| Beaucoup de features (100+) | Oui | R√©duction du bruit et de la dimensionnalit√© |
| Visualisation en 2D/3D | Oui | Projection exploratoire |
| Features tr√®s corr√©l√©es | Oui | PCA les d√©corr√®le |
| Besoin d'interpr√©tabilit√© | Non | Les composantes n'ont pas de sens m√©tier |
| Peu de features (<20) | Non | Pas n√©cessaire |
| Features non lin√©aires | Non (ou Kernel PCA) | PCA est lin√©aire |

> üí° **Conseil** : "PCA perd l'interpr√©tabilit√©. Les composantes principales n'ont pas de signification m√©tier (¬´ PC1 ¬ª ne veut rien dire pour un m√©tier). Utilisez PCA surtout pour la visualisation ou quand vous avez 100+ features et que l'interpr√©tabilit√© n'est pas critique."

> ‚ö†Ô∏è **Attention** : "TOUJOURS normaliser (StandardScaler) avant PCA. Sans normalisation, les features avec de grandes valeurs dominent les composantes principales."

---

## 5. üîß Pipelines complets scikit-learn

### 5.1 Pourquoi les pipelines ?

Un pipeline regroupe toutes les √©tapes de preprocessing et de mod√©lisation en **un seul objet**. Avantages :

1. **Pas de data leakage** : le fit se fait uniquement sur le train set
2. **Code reproductible** : une seule ligne pour tout le processus
3. **D√©ploiement facile** : sauvegarder le pipeline = sauvegarder tout le workflow
4. **Compatible avec GridSearchCV** : tuner le preprocessing ET le mod√®le ensemble

### 5.2 Pipeline simple

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Pipeline : normalisation ‚Üí mod√®le
pipeline_simple = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Entra√Æner et √©valuer en une seule ligne
scores = cross_val_score(pipeline_simple, X, y, cv=5, scoring='roc_auc')
print(f"AUC-ROC : {scores.mean():.4f} (+/- {scores.std():.4f})")
```

### 5.3 Pipeline complet avec ColumnTransformer

Le ColumnTransformer permet d'appliquer des transformations **diff√©rentes** selon le type de colonne :

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV

# Simuler un dataset r√©aliste
import pandas as pd
import numpy as np

np.random.seed(42)
n = 1000

df = pd.DataFrame({
    'age': np.random.randint(18, 70, n),
    'revenu': np.random.normal(45000, 15000, n),
    'nb_achats': np.random.randint(0, 50, n),
    'anciennete_mois': np.random.randint(1, 120, n),
    'ville': np.random.choice(['Paris', 'Lyon', 'Marseille', 'Toulouse', 'Nantes'], n),
    'type_contrat': np.random.choice(['CDI', 'CDD', 'Freelance'], n),
    'canal_acquisition': np.random.choice(['Web', 'Magasin', 'Telephone'], n),
})
df['churn'] = (
    (df['nb_achats'] < 10).astype(int) * 0.3 +
    (df['anciennete_mois'] < 12).astype(int) * 0.4 +
    np.random.random(n) * 0.3
) > 0.5
df['churn'] = df['churn'].astype(int)

# Introduire des valeurs manquantes
df.loc[np.random.choice(n, 50), 'revenu'] = np.nan
df.loc[np.random.choice(n, 30), 'age'] = np.nan

X = df.drop('churn', axis=1)
y = df['churn']

# Identifier les colonnes par type
colonnes_num = ['age', 'revenu', 'nb_achats', 'anciennete_mois']
colonnes_cat = ['ville', 'type_contrat', 'canal_acquisition']

# D√©finir les transformations par type de colonne
preprocessing_num = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),  # valeurs manquantes ‚Üí m√©diane
    ('scaler', StandardScaler())                     # normalisation
])

preprocessing_cat = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),  # valeurs manquantes ‚Üí mode
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# ColumnTransformer : appliquer les bonnes transformations aux bonnes colonnes
preprocessor = ColumnTransformer(
    transformers=[
        ('num', preprocessing_num, colonnes_num),
        ('cat', preprocessing_cat, colonnes_cat)
    ]
)

# Pipeline complet : preprocessing ‚Üí mod√®le
pipeline_complet = Pipeline([
    ('preprocessing', preprocessor),
    ('model', RandomForestClassifier(n_estimators=200, random_state=42))
])

# √âvaluation avec cross-validation
scores = cross_val_score(pipeline_complet, X, y, cv=5, scoring='roc_auc', n_jobs=-1)
print(f"AUC-ROC (5-Fold CV) : {scores.mean():.4f} (+/- {scores.std():.4f})")
```

### 5.4 Tuning du pipeline complet avec GridSearchCV

```python
# Grille d'hyperparam√®tres (noter la syntaxe avec __ pour acc√©der aux √©tapes)
param_grid = {
    'preprocessing__num__imputer__strategy': ['mean', 'median'],
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [5, 10, 15, None],
    'model__min_samples_leaf': [1, 2, 5]
}

# GridSearchCV sur le pipeline complet
grid = GridSearchCV(
    pipeline_complet,
    param_grid=param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid.fit(X, y)

print(f"Meilleurs param√®tres : {grid.best_params_}")
print(f"Meilleur AUC-ROC : {grid.best_score_:.4f}")
```

> üí° **Conseil de pro** : "Un pipeline bien construit = code reproductible + pas de data leakage + d√©ploiement facile. TOUJOURS mettre le preprocessing dans le pipeline, JAMAIS le faire s√©par√©ment avant le split. Sinon vous avez un data leakage (le scaler voit les donn√©es de test)."

### 5.5 Sauvegarder le pipeline complet

```python
import joblib

# Entra√Æner le meilleur pipeline
best_pipeline = grid.best_estimator_

# Sauvegarder TOUT le pipeline (preprocessing + mod√®le)
joblib.dump(best_pipeline, 'pipeline_churn_v1.joblib')

# Charger et utiliser
pipeline_charge = joblib.load('pipeline_churn_v1.joblib')

# Pr√©diction sur de nouvelles donn√©es brutes (le pipeline g√®re tout)
nouveau_client = pd.DataFrame({
    'age': [35],
    'revenu': [52000],
    'nb_achats': [3],
    'anciennete_mois': [6],
    'ville': ['Paris'],
    'type_contrat': ['CDI'],
    'canal_acquisition': ['Web']
})

prediction = pipeline_charge.predict(nouveau_client)
proba = pipeline_charge.predict_proba(nouveau_client)[:, 1]
print(f"Pr√©diction : {'Churn' if prediction[0] else 'Pas de churn'}")
print(f"Probabilit√© de churn : {proba[0]:.2%}")
```

> üí° **Conseil** : "Quand vous sauvegardez un mod√®le, sauvegardez le PIPELINE COMPLET (preprocessing + mod√®le). Ainsi, en production, vous passez les donn√©es brutes directement et le pipeline fait tout le travail."

---

## üéØ Points cl√©s √† retenir

1. Le **feature engineering** est l'√©tape qui a le plus grand impact sur la performance (10-30%)
2. Les **features num√©riques** : log, ratio, polynomiales, binning
3. Les **features temporelles** : jour, heure, weekend, lag, rolling average, encodage cyclique
4. Les **features textuelles** : longueur, ponctuation, TF-IDF
5. Le **target encoding** est puissant mais attention au data leakage
6. **Filter methods** pour une s√©lection rapide, **RFECV** pour la s√©lection finale
7. **PCA** pour la r√©duction de dimension (normaliser avant !)
8. **Pipelines sklearn** = pas de data leakage + code reproductible + d√©ploiement facile
9. **ColumnTransformer** pour traiter diff√©remment les colonnes num√©riques et cat√©gorielles
10. **Sauvegarder le pipeline complet**, jamais juste le mod√®le

## ‚úÖ Checklist de validation

- [ ] Je sais cr√©er des features num√©riques (log, ratio, polynomiales)
- [ ] Je sais extraire des features temporelles (lag, rolling, cyclique)
- [ ] Je connais les m√©thodes de s√©lection de features (filter, wrapper, embedded)
- [ ] Je sais utiliser PCA et interpr√©ter le scree plot
- [ ] Je ma√Ætrise Pipeline et ColumnTransformer
- [ ] Je sais int√©grer le tuning dans un pipeline avec GridSearchCV
- [ ] Je sais sauvegarder et charger un pipeline complet
- [ ] Je comprends pourquoi le preprocessing doit √™tre DANS le pipeline

---

[‚¨ÖÔ∏è Chapitre 8 : √âvaluation et M√©triques](08-evaluation-metriques.md) | [‚û°Ô∏è Chapitre 10 : MLOps](10-mlops-production.md)
