# Chapitre 11 : Boosting â€” Les Champions de Kaggle

## ğŸ¯ Objectifs

- Comprendre l'intuition du boosting : apprendre de ses erreurs
- MaÃ®triser la diffÃ©rence fondamentale entre Random Forest et Boosting
- Comprendre le Gradient Boosting pas Ã  pas
- Savoir utiliser XGBoost, LightGBM et CatBoost
- Comparer les trois frameworks de boosting
- DÃ©couvrir les SVM (Support Vector Machines) en aperÃ§u
- Savoir choisir le bon algorithme selon le problÃ¨me

**Phase 3 â€” Semaine 11**

---

## 1. ğŸ§  Intuition : Apprendre de ses erreurs

### 1.1 Le principe fondamental

Le boosting repose sur une idÃ©e simple et puissante : **chaque nouveau modÃ¨le se concentre sur les erreurs des modÃ¨les prÃ©cÃ©dents**. Au lieu d'entraÃ®ner des modÃ¨les indÃ©pendants (comme Random Forest), on les entraÃ®ne **sÃ©quentiellement**, chacun corrigeant les faiblesses du prÃ©cÃ©dent.

```
Boosting = Apprentissage sÃ©quentiel

  ModÃ¨le 1 : prÃ©dit sur tout le dataset
             â†’ fait des erreurs sur certains exemples

  ModÃ¨le 2 : se concentre sur les ERREURS du modÃ¨le 1
             â†’ corrige une partie des erreurs

  ModÃ¨le 3 : se concentre sur les ERREURS rÃ©siduelles (modÃ¨le 1 + 2)
             â†’ corrige encore plus

  ...

  ModÃ¨le N : corrige les derniÃ¨res erreurs subtiles

  PrÃ©diction finale = combinaison pondÃ©rÃ©e de TOUS les modÃ¨les
```

### 1.2 Analogie : un Ã©tudiant qui rÃ©vise ses erreurs

```
Examen blanc 1 :
  âœ… AlgÃ¨bre â†’ 18/20
  âœ… GÃ©omÃ©trie â†’ 16/20
  âŒ ProbabilitÃ©s â†’ 8/20
  âŒ Statistiques â†’ 6/20

L'Ã©tudiant intelligent ne rÃ©vise pas TOUT :
  â†’ Il se concentre sur ProbabilitÃ©s et Statistiques

Examen blanc 2 (aprÃ¨s rÃ©vision ciblÃ©e) :
  âœ… AlgÃ¨bre â†’ 17/20 (stable)
  âœ… GÃ©omÃ©trie â†’ 15/20 (stable)
  âœ… ProbabilitÃ©s â†’ 14/20 (amÃ©liorÃ© !)
  âŒ Statistiques â†’ 10/20 (amÃ©liorÃ©, mais encore faible)

Il continue Ã  cibler les faiblesses restantes :
  â†’ RÃ©vise intensÃ©ment les Statistiques

Examen blanc 3 :
  âœ… AlgÃ¨bre â†’ 17/20
  âœ… GÃ©omÃ©trie â†’ 16/20
  âœ… ProbabilitÃ©s â†’ 15/20
  âœ… Statistiques â†’ 14/20  (enfin bon !)

C'est EXACTEMENT ce que fait le Boosting :
  â†’ Chaque itÃ©ration cible les exemples les plus difficiles
```

> ğŸ’¡ **Conseil** : "Le boosting est comme un Ã©tudiant mÃ©thodique : il identifie ses faiblesses et travaille dessus en prioritÃ©. C'est pourquoi il est si efficace pour atteindre de hautes performances."

---

## 2. ğŸ“Š DiffÃ©rence fondamentale avec Random Forest

### 2.1 ParallÃ¨le (RF) vs sÃ©quentiel (Boosting)

```
Random Forest (Bagging) :                Boosting :

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Arbre 1â”‚  â”‚ Arbre 2â”‚  â”‚ Arbre 3â”‚     â”‚ Arbre 1â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚           â”‚           â”‚               â”‚ erreurs
      â”‚           â”‚           â”‚               â–¼
      â”‚           â”‚           â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚           â”‚           â”‚           â”‚ Arbre 2â”‚
      â”‚           â”‚           â”‚           â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚           â”‚           â”‚               â”‚ erreurs
      â–¼           â–¼           â–¼               â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   VOTE MAJORITAIRE / MOYENNE â”‚       â”‚ Arbre 3â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                              â”‚
  Arbres indÃ©pendants, en parallÃ¨le           â–¼
  â†’ RÃ©duit la VARIANCE                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ SOMME PONDÃ‰RÃ‰E   â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                          Arbres sÃ©quentiels
                                          â†’ RÃ©duit le BIAIS
```

### 2.2 Variance vs biais

| Concept | Random Forest (Bagging) | Boosting |
|---------|------------------------|----------|
| **StratÃ©gie** | ModÃ¨les indÃ©pendants en parallÃ¨le | ModÃ¨les sÃ©quentiels, chacun corrige le prÃ©cÃ©dent |
| **RÃ©duit principalement** | La **variance** (instabilitÃ©) | Le **biais** (erreur systÃ©matique) |
| **ModÃ¨les de base** | Arbres profonds (haute variance) | Arbres peu profonds (haut biais) |
| **Risque d'overfitting** | Faible (les erreurs se moyennent) | ModÃ©rÃ© Ã  Ã©levÃ© (apprend le bruit) |
| **DonnÃ©es bruitÃ©es** | Robuste (rÃ©siste au bruit) | Sensible (peut apprendre le bruit) |
| **Performance typique** | Bonne | Excellente (si bien rÃ©glÃ©) |
| **FacilitÃ© de tuning** | Peu de tuning nÃ©cessaire | Tuning important pour Ã©viter l'overfitting |
| **ParallÃ©lisable** | Oui (entraÃ®nement) | Non (sÃ©quentiel par nature) |

### 2.3 Tableau comparatif dÃ©taillÃ©

| CritÃ¨re | Random Forest | Gradient Boosting | XGBoost | LightGBM |
|---------|--------------|-------------------|---------|----------|
| **Principe** | Bagging | Boosting sÃ©quentiel | Boosting optimisÃ© | Boosting ultra-rapide |
| **Arbres** | Profonds, indÃ©pendants | Peu profonds, sÃ©quentiels | Peu profonds, sÃ©quentiels | Peu profonds, sÃ©quentiels |
| **Vitesse entraÃ®nement** | Rapide (parallÃ¨le) | Lent (sÃ©quentiel) | ModÃ©rÃ©e | Rapide |
| **Vitesse prÃ©diction** | ModÃ©rÃ©e | ModÃ©rÃ©e | Rapide | TrÃ¨s rapide |
| **Risque overfitting** | Faible | ModÃ©rÃ© | ModÃ©rÃ© (rÃ©gularisation) | ModÃ©rÃ© |
| **Gestion NaN** | Non | Non | Oui | Oui |
| **Performance** | Bonne | TrÃ¨s bonne | Excellente | Excellente |
| **FacilitÃ© d'usage** | TrÃ¨s facile | ModÃ©rÃ©e | ModÃ©rÃ©e | ModÃ©rÃ©e |

> âš ï¸ **Attention** : "Le boosting est plus puissant que le bagging, mais aussi plus fragile. Un Random Forest mal rÃ©glÃ© donnera quand mÃªme de bons rÃ©sultats. Un Gradient Boosting mal rÃ©glÃ© peut donner des rÃ©sultats catastrophiques (overfitting sÃ©vÃ¨re)."

---

## 3. âš™ï¸ Gradient Boosting expliquÃ© pas Ã  pas

### 3.1 L'algorithme en 4 Ã©tapes

Prenons un exemple de **rÃ©gression** pour illustrer le Gradient Boosting :

```
Dataset : prÃ©dire le prix d'un appartement

Surface (mÂ²) :  [30,  50,  70,  90,  120]
Prix rÃ©el (kâ‚¬) : [90, 150, 200, 250, 350]

â•â•â• Ã‰TAPE 1 : ModÃ¨le initial (la moyenne) â•â•â•

  PrÃ©diction initiale Fâ‚€ = moyenne(prix) = (90+150+200+250+350)/5 = 208 kâ‚¬

  PrÃ©dictions : [208, 208, 208, 208, 208]
  Prix rÃ©els  : [ 90, 150, 200, 250, 350]

â•â•â• Ã‰TAPE 2 : Calculer les rÃ©sidus â•â•â•

  RÃ©sidus = Prix rÃ©el - PrÃ©diction
  RÃ©sidus : [90-208, 150-208, 200-208, 250-208, 350-208]
          = [-118,    -58,      -8,     42,     142]

  Ces rÃ©sidus sont les ERREURS du modÃ¨le actuel.

â•â•â• Ã‰TAPE 3 : EntraÃ®ner un arbre sur les rÃ©sidus â•â•â•

  Arbre 1 apprend Ã  prÃ©dire les rÃ©sidus Ã  partir de la surface :
  Surface â†’ RÃ©sidu
  30 mÂ²   â†’ -118
  50 mÂ²   â†’ -58
  70 mÂ²   â†’ -8
  90 mÂ²   â†’ +42
  120 mÂ²  â†’ +142

â•â•â• Ã‰TAPE 4 : Mettre Ã  jour les prÃ©dictions â•â•â•

  Fâ‚ = Fâ‚€ + learning_rate Ã— Arbre1(X)

  Avec learning_rate = 0.1 :
  Nouvelles prÃ©dictions = 208 + 0.1 Ã— [-118, -58, -8, 42, 142]
                        = [196.2, 202.2, 207.2, 212.2, 222.2]

  Nouvelles erreurs : [90-196.2, 150-202.2, 200-207.2, 250-212.2, 350-222.2]
                     = [-106.2,   -52.2,     -7.2,      37.8,     127.8]

  â†’ Les erreurs sont plus petites ! On continue...

â•â•â• RÃ‰PÃ‰TER â•â•â•

  Ã‰tape 2 bis : rÃ©sidus de Fâ‚
  Ã‰tape 3 bis : arbre 2 sur les nouveaux rÃ©sidus
  Ã‰tape 4 bis : Fâ‚‚ = Fâ‚ + 0.1 Ã— Arbre2(X)

  ... AprÃ¨s 100 itÃ©rations, les rÃ©sidus â†’ ~0
```

### 3.2 Pourquoi le learning rate ?

```
Sans learning rate (= 1.0) :
  Chaque arbre corrige 100% de l'erreur d'un coup
  â†’ Risque Ã©levÃ© d'overfitting (trop agressif)

Avec learning rate petit (= 0.1) :
  Chaque arbre corrige seulement 10% de l'erreur
  â†’ Plus de pas nÃ©cessaires, mais meilleure gÃ©nÃ©ralisation
  â†’ Comme marcher prudemment vers le minimum

RÃ¨gle d'or :
  learning_rate â†“ = n_estimators â†‘ = meilleur rÃ©sultat (mais plus lent)
  learning_rate=0.01 + n_estimators=1000 > learning_rate=0.3 + n_estimators=100
```

### 3.3 Code from scratch simplifiÃ©

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor

def gradient_boosting_simple(X, y, n_estimators=100, learning_rate=0.1, max_depth=3):
    """Gradient Boosting from scratch (version simplifiÃ©e)"""

    # Ã‰tape 1 : prÃ©diction initiale = moyenne
    prediction = np.full(len(y), y.mean())
    arbres = []

    for i in range(n_estimators):
        # Ã‰tape 2 : calculer les rÃ©sidus
        residus = y - prediction

        # Ã‰tape 3 : entraÃ®ner un arbre sur les rÃ©sidus
        arbre = DecisionTreeRegressor(max_depth=max_depth)
        arbre.fit(X, residus)
        arbres.append(arbre)

        # Ã‰tape 4 : mettre Ã  jour les prÃ©dictions
        prediction += learning_rate * arbre.predict(X)

        # Afficher la progression
        if (i + 1) % 20 == 0:
            mse = np.mean((y - prediction) ** 2)
            print(f"  ItÃ©ration {i+1:>3} : MSE = {mse:.4f}")

    return arbres, y.mean()

# Exemple d'utilisation
from sklearn.datasets import make_regression
X, y = make_regression(n_samples=200, n_features=5, noise=20, random_state=42)

print("=== Gradient Boosting from scratch ===\n")
arbres, base = gradient_boosting_simple(X, y, n_estimators=100, learning_rate=0.1)

# PrÃ©diction
def predire(X, arbres, base, learning_rate=0.1):
    prediction = np.full(X.shape[0], base)
    for arbre in arbres:
        prediction += learning_rate * arbre.predict(X)
    return prediction

y_pred = predire(X, arbres, base)
print(f"\nRÂ² final : {1 - np.sum((y - y_pred)**2) / np.sum((y - y.mean())**2):.4f}")
```

> ğŸ’¡ **Conseil** : "Comprendre le Gradient Boosting from scratch permet de saisir intuitivement comment fonctionne XGBoost/LightGBM. L'idÃ©e clÃ© : chaque arbre apprend Ã  prÃ©dire les **rÃ©sidus** (erreurs) du modÃ¨le prÃ©cÃ©dent."

---

## 4. ğŸ† XGBoost

### 4.1 Pourquoi XGBoost est populaire

XGBoost (eXtreme Gradient Boosting) domine les compÃ©titions de ML depuis 2014 pour plusieurs raisons :

| Avantage | Description |
|----------|-------------|
| **RÃ©gularisation intÃ©grÃ©e** | L1 (Lasso) et L2 (Ridge) pour limiter l'overfitting |
| **Gestion des NaN** | Apprend automatiquement la direction optimale pour les valeurs manquantes |
| **ParallÃ©lisation des splits** | Bien que sÃ©quentiel au niveau des arbres, les splits sont parallÃ©lisÃ©s |
| **Pruning intelligent** | Ã‰lagage par "gain" plutÃ´t que par profondeur fixe |
| **Cache-aware** | OptimisÃ© pour l'utilisation du cache CPU |
| **Early stopping** | ArrÃªt automatique quand le score stagne |

### 4.2 Installation et API

```bash
# Installation
uv add xgboost
```

### 4.3 Code complet sur churn dataset

```python
import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, roc_auc_score,
    accuracy_score, roc_curve
)
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# GÃ©nÃ©rer un dataset churn
X, y = make_classification(
    n_samples=5000, n_features=15, n_informative=8,
    n_redundant=3, n_clusters_per_class=2,
    weights=[0.7, 0.3],  # 30% de churn
    random_state=42
)
feature_names = [f'feature_{i}' for i in range(X.shape[1])]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# CrÃ©er le modÃ¨le XGBoost
xgb_clf = xgb.XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,         # petit = meilleur, mais plus lent
    max_depth=6,                # profondeur des arbres
    min_child_weight=5,         # rÃ©gularisation (â‰ˆ min_samples_leaf)
    subsample=0.8,              # 80% des lignes par arbre
    colsample_bytree=0.8,      # 80% des features par arbre
    reg_alpha=0.1,              # rÃ©gularisation L1
    reg_lambda=1.0,             # rÃ©gularisation L2
    scale_pos_weight=2.3,       # ratio nÃ©gatifs/positifs pour classes dÃ©sÃ©quilibrÃ©es
    random_state=42,
    n_jobs=-1,
    eval_metric='logloss',
    early_stopping_rounds=50    # arrÃªter si pas d'amÃ©lioration pendant 50 rounds
)

# EntraÃ®ner avec early stopping
xgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=50
)

# RÃ©sultats
y_pred = xgb_clf.predict(X_test)
y_proba = xgb_clf.predict_proba(X_test)[:, 1]

print(f"\n=== XGBoost â€” RÃ©sultats ===")
print(f"Meilleur nombre d'itÃ©rations : {xgb_clf.best_iteration}")
print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba):.4f}")
print(f"\n{classification_report(y_test, y_pred)}")

# Feature importance
importance = pd.DataFrame({
    'Feature': feature_names,
    'Importance': xgb_clf.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'][:10][::-1],
         importance['Importance'][:10][::-1],
         color='steelblue')
plt.xlabel('Importance (gain)')
plt.title('Top 10 Features â€” XGBoost')
plt.tight_layout()
plt.show()
```

### 4.4 HyperparamÃ¨tres clÃ©s

| HyperparamÃ¨tre | Description | Plage recommandÃ©e | Impact |
|----------------|-------------|-------------------|--------|
| `learning_rate` (eta) | Contribution de chaque arbre | 0.01 - 0.1 | Plus petit = plus robuste, plus lent |
| `n_estimators` | Nombre d'arbres | 500-5000 (avec early stop) | Utiliser early stopping |
| `max_depth` | Profondeur des arbres | 3-10 | Plus profond = plus complexe |
| `min_child_weight` | Poids min par feuille | 1-10 | RÃ©gularisation |
| `subsample` | % lignes par arbre | 0.6-0.9 | RÃ©duction de variance |
| `colsample_bytree` | % features par arbre | 0.6-0.9 | DiversitÃ© des arbres |
| `reg_alpha` | RÃ©gularisation L1 | 0-1 | SparsitÃ© |
| `reg_lambda` | RÃ©gularisation L2 | 0-10 | Lissage |
| `gamma` | Min gain pour split | 0-5 | ComplexitÃ© |
| `scale_pos_weight` | Ratio classes | n_neg / n_pos | Classes dÃ©sÃ©quilibrÃ©es |

> ğŸ’¡ **Conseil** : "Recette XGBoost qui marche : learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, n_estimators=2000 avec early_stopping_rounds=50. Laissez l'early stopping trouver le bon nombre d'arbres."

---

## 5. âš¡ LightGBM

### 5.1 DiffÃ©rence avec XGBoost

La diffÃ©rence principale est dans la **stratÃ©gie de croissance des arbres** :

```
XGBoost : Level-wise (croissance par niveau)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Niveau 0 :       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Noeud â”‚
                    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â•±         â•²
  Niveau 1 : â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â† TOUS les noeuds du niveau 1
              â”‚ Noeud  â”‚  â”‚ Noeud  â”‚        sont dÃ©veloppÃ©s avant
              â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        de passer au niveau 2
             â•±    â•²      â•±    â•²
  Niveau 2 : ...  ...  ...  ...          â† puis tous ceux du niveau 2

  â†’ Arbres Ã©quilibrÃ©s, mais dÃ©veloppe des branches inutiles


LightGBM : Leaf-wise (croissance par feuille)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Ã‰tape 1 :        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Noeud â”‚
                    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â•±         â•²
  Ã‰tape 2 :  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  [feuille]     â† DÃ©veloppe SEULEMENT
              â”‚ Noeud  â”‚                   la feuille avec le
              â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                   plus grand gain
             â•±         â•²
  Ã‰tape 3 : [feuille]  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Noeud  â”‚      â† Puis la suivante
                        â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                       â•±         â•²
                     ...        ...

  â†’ Arbres dÃ©sÃ©quilibrÃ©s, mais plus efficaces (moins de splits inutiles)
  â†’ Plus rapide, mÃªme performance ou meilleure
```

| CritÃ¨re | XGBoost | LightGBM |
|---------|---------|----------|
| **Croissance** | Level-wise (par niveau) | Leaf-wise (par feuille) |
| **Vitesse** | ModÃ©rÃ©e | 2-10x plus rapide |
| **MÃ©moire** | ModÃ©rÃ©e | Plus efficace |
| **Grands datasets** | Correct | Excellent (>1M lignes) |
| **CatÃ©gorielles** | Encoding nÃ©cessaire | Gestion native |
| **Risque overfitting** | ModÃ©rÃ© | LÃ©gÃ¨rement plus Ã©levÃ© |
| **MaturitÃ©** | TrÃ¨s mature | Mature |

### 5.2 Code complet LightGBM

```python
import lightgbm as lgb
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score

# CrÃ©er le modÃ¨le LightGBM
lgb_clf = lgb.LGBMClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=-1,              # -1 = illimitÃ© (leaf-wise gÃ¨re la profondeur)
    num_leaves=31,             # paramÃ¨tre clÃ© de LightGBM (2^max_depth - 1)
    min_child_samples=20,      # min Ã©chantillons par feuille
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1,
    verbose=-1                 # silencieux
)

# EntraÃ®ner avec early stopping
lgb_clf.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    callbacks=[
        lgb.early_stopping(50, verbose=True),
        lgb.log_evaluation(50)
    ]
)

# RÃ©sultats
y_pred_lgb = lgb_clf.predict(X_test)
y_proba_lgb = lgb_clf.predict_proba(X_test)[:, 1]

print(f"\n=== LightGBM â€” RÃ©sultats ===")
print(f"Meilleur nombre d'itÃ©rations : {lgb_clf.best_iteration_}")
print(f"Accuracy  : {accuracy_score(y_test, y_pred_lgb):.4f}")
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba_lgb):.4f}")
```

### 5.3 Gestion native des catÃ©gorielles

L'un des grands avantages de LightGBM est la gestion **native** des variables catÃ©gorielles, sans besoin de One-Hot Encoding :

```python
import lightgbm as lgb
import pandas as pd

# Exemple avec des catÃ©gorielles
df = pd.DataFrame({
    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris'],
    'contrat': ['mensuel', 'annuel', 'mensuel', 'annuel', 'mensuel', 'annuel'],
    'montant': [50, 30, 60, 25, 45, 35],
    'churn': [1, 0, 1, 0, 1, 0]
})

# Convertir en category (LightGBM les gÃ¨re automatiquement)
df['ville'] = df['ville'].astype('category')
df['contrat'] = df['contrat'].astype('category')

X = df.drop('churn', axis=1)
y = df['churn']

# LightGBM gÃ¨re nativement les catÃ©gorielles
lgb_model = lgb.LGBMClassifier(n_estimators=100, verbose=-1)
lgb_model.fit(X, y, categorical_feature=['ville', 'contrat'])
# Pas besoin de One-Hot Encoding !
```

> ğŸ’¡ **Conseil** : "LightGBM est le meilleur choix quand vous avez un grand dataset (>100k lignes) ou beaucoup de variables catÃ©gorielles. Il est souvent 2-10x plus rapide que XGBoost avec des performances similaires ou meilleures."

---

## 6. ğŸ± CatBoost

### 6.1 SpÃ©cialisÃ© catÃ©gorielles

CatBoost (Categorical Boosting) a Ã©tÃ© dÃ©veloppÃ© par Yandex avec un focus sur les **variables catÃ©gorielles** et la rÃ©duction de l'overfitting.

| CaractÃ©ristique | Description |
|----------------|-------------|
| **Ordered Boosting** | Technique anti-overfitting unique qui Ã©vite le target leakage |
| **CatÃ©gorielles natives** | Meilleure gestion que XGBoost, comparable Ã  LightGBM |
| **Target Encoding** | Encoding intelligent intÃ©grÃ© (ordered target statistics) |
| **Symmetric Trees** | Arbres symÃ©triques par dÃ©faut = prÃ©diction trÃ¨s rapide |
| **GPU natif** | Excellente accÃ©lÃ©ration GPU |

### 6.2 Ordered Boosting

```
ProblÃ¨me classique du boosting :
  Le modÃ¨le Ã  l'itÃ©ration t est entraÃ®nÃ© sur des rÃ©sidus calculÃ©s
  avec les MÃŠMES donnÃ©es â†’ risque de target leakage â†’ overfitting

Ordered Boosting (CatBoost) :
  Pour chaque Ã©chantillon, les rÃ©sidus sont calculÃ©s uniquement
  avec les modÃ¨les entraÃ®nÃ©s sur les Ã©chantillons PRÃ‰CÃ‰DENTS
  (dans un ordre alÃ©atoire) â†’ pas de fuite d'information â†’ moins d'overfitting
```

### 6.3 Code rapide CatBoost

```python
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

# CrÃ©er le modÃ¨le CatBoost
cat_clf = CatBoostClassifier(
    iterations=500,               # = n_estimators
    learning_rate=0.05,
    depth=6,                      # = max_depth
    l2_leaf_reg=3,                # rÃ©gularisation L2
    random_seed=42,
    verbose=100,                  # afficher toutes les 100 itÃ©rations
    eval_metric='AUC',
    early_stopping_rounds=50
)

# EntraÃ®ner
cat_clf.fit(
    X_train, y_train,
    eval_set=(X_test, y_test),
    verbose=100
)

# RÃ©sultats
y_pred_cat = cat_clf.predict(X_test)
y_proba_cat = cat_clf.predict_proba(X_test)[:, 1]

print(f"\n=== CatBoost â€” RÃ©sultats ===")
print(f"Accuracy  : {accuracy_score(y_test, y_pred_cat):.4f}")
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba_cat):.4f}")
```

> ğŸ’¡ **Conseil** : "CatBoost est excellent quand vous avez beaucoup de variables catÃ©gorielles et peu de temps pour le preprocessing. Il gÃ¨re tout automatiquement et l'ordered boosting rÃ©duit naturellement l'overfitting."

---

## 7. ğŸ“‹ Comparaison XGBoost vs LightGBM vs CatBoost

### 7.1 Tableau comparatif

| CritÃ¨re | XGBoost | LightGBM | CatBoost |
|---------|---------|----------|----------|
| **DÃ©veloppeur** | Tianqi Chen (2014) | Microsoft (2017) | Yandex (2017) |
| **StratÃ©gie de croissance** | Level-wise | Leaf-wise | Symmetric trees |
| **Vitesse entraÃ®nement** | ModÃ©rÃ©e | Rapide | ModÃ©rÃ©e |
| **Vitesse prÃ©diction** | Rapide | TrÃ¨s rapide | TrÃ¨s rapide |
| **Gestion des NaN** | Oui (apprise) | Oui | Oui |
| **CatÃ©gorielles natives** | Non (encoding requis) | Oui (basique) | Oui (excellente) |
| **RÃ©gularisation** | L1 + L2 | L1 + L2 | L2 + Ordered Boosting |
| **GPU** | Oui | Oui | Oui (excellent) |
| **Datasets volumineux** | Correct | Excellent | Correct |
| **FacilitÃ© d'utilisation** | ModÃ©rÃ©e | ModÃ©rÃ©e | Facile (peu de tuning) |
| **CommunautÃ©** | TrÃ¨s large | Large | Croissante |
| **API sklearn** | Oui | Oui | Oui |

### 7.2 Quand utiliser lequel ?

| Situation | Recommandation | Justification |
|-----------|---------------|---------------|
| Premier essai / dÃ©faut | **XGBoost** | Le plus documentÃ©, communautÃ© la plus large |
| Grand dataset (>1M lignes) | **LightGBM** | 2-10x plus rapide |
| Beaucoup de catÃ©gorielles | **CatBoost** | Gestion native optimale |
| CompÃ©tition Kaggle | **LightGBM** ou **XGBoost** | Performances top, flexibilitÃ© de tuning |
| Production (infÃ©rence rapide) | **LightGBM** ou **CatBoost** | PrÃ©diction ultra-rapide |
| Peu de temps pour le tuning | **CatBoost** | Bons rÃ©sultats out-of-the-box |
| DonnÃ©es bruitÃ©es | **CatBoost** | Ordered boosting rÃ©duit l'overfitting |

### 7.3 Benchmark rapide

```python
import time
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score

# ParamÃ¨tres communs
params_communs = {
    'n_estimators': 300,
    'learning_rate': 0.05,
    'max_depth': 6,
    'random_state': 42,
}

modeles = {
    'XGBoost': xgb.XGBClassifier(
        **{k: v for k, v in params_communs.items()},
        n_jobs=-1, eval_metric='logloss', verbosity=0
    ),
    'LightGBM': lgb.LGBMClassifier(
        **{k: v for k, v in params_communs.items()},
        n_jobs=-1, verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=params_communs['n_estimators'],
        learning_rate=params_communs['learning_rate'],
        depth=params_communs['max_depth'],
        random_seed=params_communs['random_state'],
        verbose=0
    ),
}

print("=== Benchmark XGBoost vs LightGBM vs CatBoost ===\n")
print(f"{'ModÃ¨le':>12} {'AUC-ROC':>10} {'Temps fit (s)':>15} {'Temps predict (ms)':>20}")
print(f"{'â”€'*60}")

for nom, modele in modeles.items():
    # Temps de fit
    start = time.time()
    modele.fit(X_train, y_train)
    temps_fit = time.time() - start

    # Temps de predict
    start = time.time()
    for _ in range(100):
        y_proba = modele.predict_proba(X_test)[:, 1]
    temps_predict = (time.time() - start) / 100 * 1000

    # Score
    auc = roc_auc_score(y_test, y_proba)

    print(f"{nom:>12} {auc:>10.4f} {temps_fit:>15.2f} {temps_predict:>20.2f}")
```

---

## 8. ğŸ¯ Bonus : SVM (aperÃ§u)

### 8.1 Trouver la meilleure frontiÃ¨re

Les **Support Vector Machines** (SVM) cherchent l'hyperplan qui sÃ©pare les classes avec la **marge maximale** :

```
Mauvaise frontiÃ¨re :              Bonne frontiÃ¨re (SVM) :

  â— â—  â— / â–² â–²                    â—  â—  â— â”‚ â–²  â–²
  â—  â— /  â–²  â–² â–²                  â—  â— â—  â”‚  â–² â–² â–²
  â— â— / â–²  â–²  â–²                   â— â—  â—  â”‚ â–²  â–² â–²
                                       â†‘â†margeâ†’â†‘
  La frontiÃ¨re est proche             La frontiÃ¨re maximise
  de certains points                  la distance aux points
  â†’ instable, peu robuste            les plus proches (supports)
                                      â†’ stable, bonne gÃ©nÃ©ralisation
```

### 8.2 Concept de marge maximale

```
            Support vectors (les points les plus proches)
                    â†“           â†“
  â—  â—  â—    â—    â€–     â”‚     â€–    â–²    â–²  â–²  â–²
  â—  â— â—    â—     â€–     â”‚     â€–   â–²  â–²    â–²
  â—  â—  â—  â—      â€–     â”‚     â€–     â–²  â–²  â–²
                   â€–     â”‚     â€–
                   â† marge â†’  â† marge â†’
                    â†‘
              hyperplan de sÃ©paration

  L'objectif du SVM : maximiser la marge totale
  Seuls les "support vectors" (points sur la marge) influencent la frontiÃ¨re
```

### 8.3 Le Kernel Trick (intuition)

Quand les donnÃ©es ne sont pas linÃ©airement sÃ©parables, le **kernel trick** projette les donnÃ©es dans un espace de dimension supÃ©rieure oÃ¹ elles deviennent sÃ©parables :

```
En 1D : pas sÃ©parable                En 2D (aprÃ¨s kernel) : sÃ©parable !

  â–² â–² â— â— â— â–² â–²                          â–²           â–²
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x                      â”‚ â–²       â–² â”‚
                                          â”‚   â— â— â—   â”‚
  Impossible de sÃ©parer                   â”‚  â—  â—  â—  â”‚
  â— et â–² avec une droite                  â”‚   â— â— â—   â”‚
                                          â”‚ â–²       â–² â”‚
  Ï†(x) = xÂ² transforme :                 â–²           â–²
  â–² Ã  x=1,7 â†’ xÂ²=1,49                        â†‘ xÂ²
  â— Ã  x=3,4,5 â†’ xÂ²=9,16,25             On peut sÃ©parer
                                         avec un cercle !
```

```python
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# SVM avec kernel RBF (le plus courant)
svm = SVC(
    kernel='rbf',        # Radial Basis Function (gaussien)
    C=1.0,               # paramÃ¨tre de rÃ©gularisation
    gamma='scale',       # paramÃ¨tre du kernel RBF
    probability=True,    # activer predict_proba (plus lent)
    random_state=42
)

svm.fit(X_train, y_train)

y_pred_svm = svm.predict(X_test)
print(f"SVM Accuracy : {accuracy_score(y_test, y_pred_svm):.4f}")
print(classification_report(y_test, y_pred_svm))
```

### 8.4 Quand utiliser SVM

| Situation | SVM adaptÃ© ? | Justification |
|-----------|-------------|---------------|
| Peu de donnÃ©es (<10k) | Oui | SVM excelle en petit dataset |
| Haute dimension | Oui | GÃ¨re bien les features > Ã©chantillons |
| Grand dataset (>100k) | Non | Trop lent (O(nÂ²) Ã  O(nÂ³)) |
| Besoin d'interprÃ©tabilitÃ© | Non | ModÃ¨le "boÃ®te noire" |
| Classification d'images (classique) | Oui | Kernel RBF performant |
| Classification de texte | Oui | SVM linÃ©aire excellent |
| RÃ©gression | Possible | SVR existe, mais moins utilisÃ© |

> âš ï¸ **Attention** : "Le SVM est un excellent algorithme, mais il est dÃ©passÃ© par XGBoost/LightGBM sur la plupart des problÃ¨mes tabulaires. Il reste pertinent pour les petits datasets et les donnÃ©es textuelles avec peu de preprocessing."

---

## 9. ğŸ“‹ Guide de choix final : Quel algorithme pour quel problÃ¨me ?

### 9.1 Tableau de synthÃ¨se

| CritÃ¨re | RÃ©g. LinÃ©aire | RÃ©g. Logistique | KNN | Arbre | Random Forest | XGBoost/LightGBM | SVM |
|---------|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **RÃ©gression** | Oui | Non | Oui | Oui | Oui | Oui | Oui |
| **Classification** | Non | Oui | Oui | Oui | Oui | Oui | Oui |
| **InterprÃ©tabilitÃ©** | Excellente | Bonne | Faible | Excellente | Bonne | Moyenne | Faible |
| **Performance** | Faible-Moyenne | Moyenne | Moyenne | Moyenne | Bonne | Excellente | Bonne |
| **Scaling nÃ©cessaire** | Non* | Oui | Oui | Non | Non | Non | Oui |
| **GÃ¨re non-linÃ©aritÃ©** | Non | Non | Oui | Oui | Oui | Oui | Oui (kernel) |
| **DonnÃ©es > 100k** | Oui | Oui | Non | Oui | Oui | Oui | Non |
| **Peu de donnÃ©es** | Oui | Oui | Oui | RisquÃ© | Oui | RisquÃ© | Oui |
| **Sensible outliers** | TrÃ¨s | Peu | Oui | Peu | Peu | Peu | Oui |
| **FacilitÃ© tuning** | Facile | Facile | Moyen | Moyen | Facile | Complexe | Complexe |

*Scaling recommandÃ© pour comparer les coefficients

### 9.2 Arbre de dÃ©cision : quel algorithme choisir ?

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Mon problÃ¨me est...      â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â•±         â•²
               RÃ©gression  â•±               â•²  Classification
                         â•±                   â•²
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Besoin            â”‚    â”‚ Besoin            â”‚
            â”‚ d'interprÃ©tabilitÃ©â”‚    â”‚ d'interprÃ©tabilitÃ©â”‚
            â”‚ maximale ?        â”‚    â”‚ maximale ?        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â•±         â•²             â•±         â•²
          Oui â•±             â•² Non Oui â•±             â•² Non
            â•±                 â•²     â•±                 â•²
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ RÃ©g. LinÃ©aire  â”‚ â”‚ Combien  â”‚ â”‚ RÃ©g. Logistiqueâ”‚ â”‚ Combien  â”‚
  â”‚ (baseline)     â”‚ â”‚ de       â”‚ â”‚ (baseline)     â”‚ â”‚ de       â”‚
  â”‚                â”‚ â”‚ donnÃ©es ?â”‚ â”‚                â”‚ â”‚ donnÃ©es ?â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                      â•±       â•²                       â•±       â•²
                 <10k â•±         â•² >10k           <10k â•±         â•² >10k
                    â•±             â•²                 â•±             â•²
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Random Forestâ”‚ â”‚ LightGBM /   â”‚ â”‚ Random Forestâ”‚ â”‚ LightGBM /   â”‚
          â”‚ ou SVM       â”‚ â”‚ XGBoost      â”‚ â”‚ ou SVM       â”‚ â”‚ XGBoost      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 StratÃ©gie recommandÃ©e pour tout projet ML

```python
# STRATÃ‰GIE EN 4 Ã‰TAPES

# Ã‰tape 1 : BASELINE SIMPLE (5 minutes)
# â†’ RÃ©gression linÃ©aire ou logistique
from sklearn.linear_model import LogisticRegression
baseline = LogisticRegression(max_iter=1000)
baseline.fit(X_train, y_train)
print(f"Baseline : {baseline.score(X_test, y_test):.4f}")

# Ã‰tape 2 : RANDOM FOREST (10 minutes)
# â†’ Bon rÃ©sultat avec peu de tuning
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
print(f"Random Forest : {rf.score(X_test, y_test):.4f}")

# Ã‰tape 3 : BOOSTING (30 minutes)
# â†’ Performance maximale
import lightgbm as lgb
lgbm = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.05, verbose=-1)
lgbm.fit(X_train, y_train,
         eval_set=[(X_test, y_test)],
         callbacks=[lgb.early_stopping(50, verbose=False)])
print(f"LightGBM : {lgbm.score(X_test, y_test):.4f}")

# Ã‰tape 4 : TUNING (si nÃ©cessaire, 1-2 heures)
# â†’ Optimiser les hyperparamÃ¨tres du meilleur modÃ¨le
# â†’ Voir chapitres prÃ©cÃ©dents pour GridSearchCV / RandomizedSearchCV
```

> ğŸ’¡ **Conseil** : "Ne passez pas directement Ã  XGBoost. Commencez **toujours** par une baseline simple (rÃ©gression logistique), puis Random Forest, puis boosting. Si la baseline suffit, pas besoin de complexifier. La diffÃ©rence entre RF et XGBoost n'est souvent que de 1-3%."

> âš ï¸ **Attention** : "L'algorithme ne fait pas tout. Un bon feature engineering avec une rÃ©gression logistique battra souvent un XGBoost mal entraÃ®nÃ© sur des features brutes. Investissez dans la comprÃ©hension de vos donnÃ©es avant le choix de l'algorithme."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Le boosting apprend de ses erreurs** : chaque arbre corrige les erreurs des prÃ©cÃ©dents (apprentissage sÃ©quentiel)
2. **Random Forest rÃ©duit la variance** (modÃ¨les indÃ©pendants), le **boosting rÃ©duit le biais** (modÃ¨les sÃ©quentiels)
3. **Le Gradient Boosting** entraÃ®ne des arbres successifs sur les **rÃ©sidus** (erreurs) du modÃ¨le courant
4. **Le learning rate** contrÃ´le l'agressivitÃ© de chaque correction â€” plus petit = meilleur rÃ©sultat mais plus lent
5. **XGBoost** est la rÃ©fÃ©rence : rÃ©gularisation L1/L2, gestion des NaN, early stopping, parallÃ©lisation des splits
6. **LightGBM** est 2-10x plus rapide grÃ¢ce Ã  la croissance leaf-wise â€” idÃ©al pour les grands datasets
7. **CatBoost** excelle sur les donnÃ©es catÃ©gorielles et rÃ©duit l'overfitting avec l'ordered boosting
8. **L'early stopping** est indispensable pour le boosting : mettez n_estimators trÃ¨s haut et laissez l'algorithme s'arrÃªter seul
9. **Les SVM** restent pertinents pour les petits datasets et les donnÃ©es textuelles, mais sont dÃ©passÃ©s par le boosting sur les donnÃ©es tabulaires
10. **StratÃ©gie universelle** : baseline simple puis Random Forest puis LightGBM/XGBoost â€” ne jamais sauter les Ã©tapes

---

## âœ… Checklist de validation

- [ ] Je sais expliquer l'intuition du boosting (apprendre de ses erreurs, sÃ©quentiel)
- [ ] Je connais la diffÃ©rence fondamentale entre Random Forest (variance) et Boosting (biais)
- [ ] Je sais expliquer le Gradient Boosting pas Ã  pas (rÃ©sidus, learning rate, accumulation)
- [ ] Je sais implÃ©menter XGBoost avec early stopping
- [ ] Je connais les hyperparamÃ¨tres clÃ©s de XGBoost (learning_rate, max_depth, subsample, reg_alpha/lambda)
- [ ] Je sais quand utiliser LightGBM plutÃ´t que XGBoost (grands datasets, vitesse)
- [ ] Je comprends la diffÃ©rence level-wise vs leaf-wise
- [ ] Je sais que CatBoost excelle sur les catÃ©gorielles et rÃ©duit l'overfitting naturellement
- [ ] Je connais les bases des SVM (marge maximale, kernel trick)
- [ ] Je sais choisir le bon algorithme selon le contexte (taille des donnÃ©es, interprÃ©tabilitÃ©, performance)
- [ ] Je maÃ®trise la stratÃ©gie en 4 Ã©tapes : baseline â†’ RF â†’ Boosting â†’ Tuning

---

**PrÃ©cÃ©dent** : [Chapitre 10 : Arbres de DÃ©cision et ForÃªts AlÃ©atoires](10-arbres-forets.md)

**Suivant** : [Chapitre 12 : RÃ©duction de Dimension et Clustering AvancÃ©](12-dimension-clustering.md)
