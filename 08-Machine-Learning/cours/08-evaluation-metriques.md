# Chapitre 8 : √âvaluation et M√©triques ‚Äì L'Art de Mesurer la Performance

> **Ce chapitre est LE plus important de toute la formation.** Un mod√®le de Machine Learning n'existe que par sa capacit√© √† √™tre √©valu√© correctement. Un mod√®le mal √©valu√© est un mod√®le dangereux.

## üéØ Objectifs

- Ma√Ætriser toutes les m√©triques de Machine Learning (r√©gression et classification)
- Comprendre en profondeur l'overfitting et l'underfitting
- Ma√Ætriser toutes les formes de cross-validation
- Savoir choisir LA bonne m√©trique selon le contexte m√©tier
- Diagnostiquer les probl√®mes d'un mod√®le et savoir les corriger
- Construire une m√©thodologie compl√®te d'√©valuation et d'am√©lioration

---

## 1. üß† Pourquoi l'√©valuation est cruciale

### 1.1 Le pi√®ge du score d'entra√Ænement

Un mod√®le qui ¬´ marche ¬ª sur les donn√©es d'entra√Ænement peut √™tre **catastrophique** en production. C'est comme un √©tudiant qui apprend par c≈ìur les r√©ponses d'un examen pass√© : il obtient 100% sur cet examen, mais 30% sur un nouvel examen.

```
Entra√Ænement (√©tudier)  ‚Üí  accuracy_train = 99%   ‚Üê Ca ne veut RIEN dire
Test (examen)           ‚Üí  accuracy_test = 72%     ‚Üê La VRAIE performance
Production (vie r√©elle) ‚Üí  accuracy_prod = ???      ‚Üê Ce qui compte VRAIMENT
```

### 1.2 Les cons√©quences d'une mauvaise √©valuation

| Erreur d'√©valuation | Cons√©quence |
|---|---|
| √âvaluer sur le train set | Surestimation massive de la performance |
| Mauvaise m√©trique choisie | Mod√®le optimis√© pour le mauvais objectif |
| Pas de cross-validation | R√©sultats instables, non reproductibles |
| Data leakage | Performance artificielle, crash en production |
| Ignorer le d√©s√©quilibre des classes | Accuracy trompeuse (99% en pr√©disant toujours la classe majoritaire) |

> üí° **Conseil de pro** : "La m√©trique choisie doit refl√©ter le CO√õT M√âTIER de l'erreur. Une accuracy de 95% ne veut rien dire si les 5% d'erreurs co√ªtent des millions ou des vies. Toujours demander : ¬´ Quel est le co√ªt d'une erreur ? ¬ª"

---

## 2. üìä M√©triques de r√©gression

### 2.0 Pour les d√©butants : c'est quoi une m√©trique de r√©gression ?

En r√©gression, le mod√®le pr√©dit un **nombre** (un prix, une temp√©rature, un √¢ge...). La m√©trique mesure **√† quel point la pr√©diction est loin de la r√©alit√©**.

#### L'analogie du GPS

Imaginez un GPS qui estime le temps de trajet :

```
Trajet r√©el : 30 min     GPS dit : 28 min    ‚Üí Erreur = 2 min  ‚úÖ Pas grave
Trajet r√©el : 30 min     GPS dit : 55 min    ‚Üí Erreur = 25 min ‚ùå Probl√®me !
Trajet r√©el : 30 min     GPS dit : 30 min    ‚Üí Erreur = 0 min  üéØ Parfait !
```

Les m√©triques de r√©gression quantifient ces erreurs de diff√©rentes mani√®res.

#### Les 4 m√©triques essentielles expliqu√©es simplement

| M√©trique | En langage courant | Exemple avec des prix immobiliers |
|----------|-------------------|----------------------------------|
| **MAE** | "En moyenne, je me trompe de X euros" | MAE = 15 000‚Ç¨ ‚Üí les pr√©dictions sont √† ¬±15 000‚Ç¨ du vrai prix |
| **RMSE** | "Comme la MAE, mais les grosses erreurs comptent BEAUCOUP plus" | Une erreur de 100 000‚Ç¨ p√®se bien plus qu'une erreur de 10 000‚Ç¨ |
| **R¬≤** | "Quelle proportion du prix le mod√®le arrive-t-il √† expliquer ?" | R¬≤ = 0.85 ‚Üí le mod√®le explique 85% des variations de prix |
| **MAPE** | "En pourcentage, je me trompe de X%" | MAPE = 8% ‚Üí les pr√©dictions sont √† ¬±8% du vrai prix |

#### Comment savoir si c'est bon ?

```
         Bon mod√®le            Mauvais mod√®le
         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MAE  :   10 000‚Ç¨               80 000‚Ç¨
RMSE :   15 000‚Ç¨               120 000‚Ç¨
R¬≤   :   0.92                  0.35
MAPE :   5%                    40%
```

> üí° **Pour d√©buter** : Concentrez-vous sur le **R¬≤** (score de 0 √† 1, plus c'est proche de 1, mieux c'est) et la **MAE** (l'erreur moyenne en unit√© compr√©hensible). Les autres m√©triques viendront naturellement avec la pratique.

---

### 2.1 Vue d'ensemble technique

| M√©trique | Formule simplifi√©e | Interpr√©tation | Range | Bon si |
|---|---|---|---|---|
| **MSE** | Moyenne((y - ≈∑)¬≤) | Erreur quadratique moyenne | [0, +‚àû] | Proche de 0 |
| **RMSE** | ‚àöMSE | Erreur en unit√© de la cible | [0, +‚àû] | Proche de 0 |
| **MAE** | Moyenne(|y - ≈∑|) | Erreur absolue moyenne | [0, +‚àû] | Proche de 0 |
| **R¬≤** | 1 - SS_res/SS_tot | % de variance expliqu√©e | [-‚àû, 1] | Proche de 1 |
| **MAPE** | Moyenne(|y - ≈∑| / |y|) √ó 100 | Erreur relative en % | [0, +‚àû] | Proche de 0 |
| **RMSLE** | ‚àöMoyenne((log(1+y) - log(1+≈∑))¬≤) | Erreur log (p√©nalise sous-estimations) | [0, +‚àû] | Proche de 0 |

### 2.2 Impl√©mentation d√©taill√©e

```python
import numpy as np
import pandas as pd
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    mean_absolute_percentage_error
)

# Exemple : pr√©diction de prix immobilier
y_true = np.array([200000, 350000, 150000, 500000, 275000])
y_pred = np.array([210000, 330000, 160000, 480000, 290000])

# Calculer toutes les m√©triques
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)
mape = mean_absolute_percentage_error(y_true, y_pred)

print("=== M√©triques de R√©gression ===")
print(f"MSE  : {mse:,.0f}")
print(f"RMSE : {rmse:,.0f} ‚Ç¨")
print(f"MAE  : {mae:,.0f} ‚Ç¨")
print(f"R¬≤   : {r2:.4f}")
print(f"MAPE : {mape:.2%}")
```

### 2.3 Quand utiliser quelle m√©trique ?

| M√©trique | Quand l'utiliser | Sensibilit√© aux outliers |
|---|---|---|
| **RMSE** | Quand les grosses erreurs sont tr√®s co√ªteuses | Tr√®s sensible |
| **MAE** | Quand toutes les erreurs ont le m√™me co√ªt | Robuste |
| **R¬≤** | Pour communiquer avec des non-techniques | Mod√©r√©e |
| **MAPE** | Quand on veut une erreur relative (%) | Probl√®me si y ‚âà 0 |
| **RMSLE** | Prix, comptages (valeurs tr√®s variables) | Mod√©r√©e |

> üí° **Conseil** : "Pour un probl√®me de prix immobilier, utilisez le RMSE comme m√©trique principale (p√©nalise les grosses erreurs) et le MAPE pour communiquer (¬´ on se trompe de 8% en moyenne ¬ª). Le R¬≤ est utile pour comparer des mod√®les entre eux."

> ‚ö†Ô∏è **Attention** : "Le R¬≤ peut √™tre n√©gatif ! Cela signifie que votre mod√®le est PIRE que la moyenne. Un R¬≤ de 0 signifie que votre mod√®le pr√©dit toujours la moyenne. Seul un R¬≤ positif montre que le mod√®le apprend quelque chose."

### 2.4 Visualisation des erreurs de r√©gression

```python
import matplotlib.pyplot as plt

# Graphique : Pr√©dictions vs Valeurs r√©elles
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Predicted vs Actual
axes[0].scatter(y_true, y_pred, alpha=0.7)
axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()],
             'r--', linewidth=2, label='Pr√©diction parfaite')
axes[0].set_xlabel('Valeurs r√©elles')
axes[0].set_ylabel('Pr√©dictions')
axes[0].set_title('Pr√©dictions vs R√©alit√©')
axes[0].legend()

# 2. Distribution des r√©sidus
residus = y_true - y_pred
axes[1].hist(residus, bins=20, edgecolor='black', alpha=0.7)
axes[1].axvline(x=0, color='red', linestyle='--')
axes[1].set_xlabel('R√©sidus (erreur)')
axes[1].set_ylabel('Fr√©quence')
axes[1].set_title('Distribution des R√©sidus')

# 3. R√©sidus vs Pr√©dictions (v√©rifier l'homosc√©dasticit√©)
axes[2].scatter(y_pred, residus, alpha=0.7)
axes[2].axhline(y=0, color='red', linestyle='--')
axes[2].set_xlabel('Pr√©dictions')
axes[2].set_ylabel('R√©sidus')
axes[2].set_title('R√©sidus vs Pr√©dictions')

plt.tight_layout()
plt.show()
```

> üí° **Conseil de pro** : "Tracez TOUJOURS le graphique des r√©sidus. Les r√©sidus doivent √™tre centr√©s autour de 0, de distribution normale, et sans pattern. Un pattern en U ou en √©ventail indique un probl√®me dans le mod√®le."

---

## 3. üìä M√©triques de classification

### 3.0 Pour les d√©butants : c'est quoi une m√©trique de classification ?

En classification, le mod√®le pr√©dit une **cat√©gorie** (oui/non, spam/pas spam, malade/sain...). La m√©trique mesure **√† quel point le mod√®le se trompe dans ses cat√©gorisations**.

#### L'analogie du test m√©dical

Imaginez un test de d√©pistage pour une maladie. Le test peut donner **4 r√©sultats** :

```
                         Le test dit "Malade"       Le test dit "Sain"
                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Vraiment MALADE    ‚Üí     ‚úÖ Vrai Positif (TP)       ‚ùå Faux N√©gatif (FN)
                         "Bien d√©tect√© !"            "Rat√© ! Dangereux !"

Vraiment SAIN      ‚Üí     ‚ùå Faux Positif (FP)       ‚úÖ Vrai N√©gatif (TN)
                         "Fausse alarme"             "Bien identifi√©"
```

#### Les m√©triques expliqu√©es avec des mots simples

| M√©trique | Question qu'elle pose | Analogie m√©dicale |
|----------|----------------------|-------------------|
| **Accuracy** | "Quel % de r√©ponses sont correctes ?" | "Quel % de diagnostics sont bons ?" |
| **Precision** | "Quand je dis 'positif', ai-je raison ?" | "Quand le test dit 'malade', est-ce vrai ?" |
| **Recall** | "Est-ce que je d√©tecte TOUS les positifs ?" | "Est-ce qu'on d√©tecte TOUS les malades ?" |
| **F1-Score** | "Compromis entre precision et recall" | "Le test est-il √† la fois fiable ET exhaustif ?" |
| **AUC-ROC** | "Le mod√®le sait-il distinguer les classes ?" | "Le test distingue-t-il bien malades et sains ?" |

#### Le pi√®ge n¬∞1 des d√©butants : l'accuracy trompeuse

**Scenario** : Sur 1000 patients, 950 sont sains et 50 sont malades.

Un mod√®le qui pr√©dit **TOUJOURS "sain"** a **95% d'accuracy** ! Mais il ne d√©tecte **AUCUN** malade. C'est un mod√®le **parfaitement inutile**.

> üí° **R√®gle d'or** : Quand les classes sont d√©s√©quilibr√©es (beaucoup plus de "non" que de "oui"), **ne regardez jamais l'accuracy seule**. Utilisez le F1-Score et l'AUC-ROC.

#### Comment choisir entre Precision et Recall ?

Tout d√©pend du **co√ªt de l'erreur** :

| Situation | M√©trique prioritaire | Pourquoi |
|-----------|---------------------|----------|
| D√©tecter un cancer | **Recall** (ne rater personne) | Mieux vaut une fausse alarme que rater un malade |
| Filtrer les spams | **Precision** (ne pas se tromper) | Mieux vaut laisser passer un spam que bloquer un vrai email |
| D√©tecter le churn client | **F1-Score** (√©quilibre) | On veut d√©tecter les d√©parts sans harceler les clients fid√®les |
| D√©tecter une fraude bancaire | **Recall** (ne rien rater) | Mieux vaut bloquer une transaction l√©gitime que laisser passer une fraude |

---

### 3.1 Matrice de confusion ‚Äì La base de tout

La matrice de confusion est le point de d√©part de TOUTES les m√©triques de classification.

```
                    Pr√©dit Positif    Pr√©dit N√©gatif
R√©el Positif    |      TP            |      FN         |
R√©el N√©gatif    |      FP            |      TN         |
```

- **TP (True Positive)** : Correctement identifi√© comme positif
- **TN (True Negative)** : Correctement identifi√© comme n√©gatif
- **FP (False Positive)** : Fausse alarme (pr√©dit positif √† tort)
- **FN (False Negative)** : Rat√© (n'a pas d√©tect√© un positif)

```python
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    precision_recall_curve,
    average_precision_score
)
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

# Cr√©er un dataset d√©s√©quilibr√©
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_classes=2,
    weights=[0.9, 0.1],  # 90% classe 0, 10% classe 1
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entra√Æner un mod√®le
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred)
print("Matrice de confusion :")
print(cm)

# Visualisation de la matrice de confusion
import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pr√©dit N√©gatif', 'Pr√©dit Positif'],
            yticklabels=['R√©el N√©gatif', 'R√©el Positif'])
plt.xlabel('Pr√©diction')
plt.ylabel('R√©alit√©')
plt.title('Matrice de Confusion')
plt.tight_layout()
plt.show()
```

### 3.2 Les 5 m√©triques fondamentales

| M√©trique | Formule | Question √† laquelle elle r√©pond | Range |
|---|---|---|---|
| **Accuracy** | (TP + TN) / Total | Quelle proportion est correctement class√©e ? | [0, 1] |
| **Precision** | TP / (TP + FP) | Parmi les pr√©dits positifs, combien le sont vraiment ? | [0, 1] |
| **Recall** (Sensibilit√©) | TP / (TP + FN) | Parmi les vrais positifs, combien sont d√©tect√©s ? | [0, 1] |
| **F1-Score** | 2 √ó (P √ó R) / (P + R) | Compromis harmonique precision-recall | [0, 1] |
| **AUC-ROC** | Aire sous la courbe ROC | Capacit√© √† distinguer les classes | [0.5, 1] |

```python
# Rapport de classification complet
print("=== Rapport de Classification ===")
print(classification_report(y_test, y_pred, target_names=['N√©gatif', 'Positif']))

# M√©triques individuelles
print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision : {precision_score(y_test, y_pred):.4f}")
print(f"Recall    : {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score  : {f1_score(y_test, y_pred):.4f}")
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba):.4f}")
```

### 3.3 Le pi√®ge de l'Accuracy

> ‚ö†Ô∏è **Attention** : "L'accuracy est la m√©trique la plus TROMPEUSE qui existe ! Sur un dataset avec 99% de n√©gatifs, un mod√®le qui pr√©dit TOUJOURS n√©gatif a 99% d'accuracy mais ne d√©tecte AUCUN positif. C'est un mod√®le parfaitement inutile."

```python
# D√©monstration du pi√®ge de l'accuracy
# Mod√®le stupide : toujours pr√©dire la classe majoritaire
from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(strategy='most_frequent')
dummy.fit(X_train, y_train)
y_pred_dummy = dummy.predict(X_test)

print(f"Accuracy du mod√®le 'toujours n√©gatif' : {accuracy_score(y_test, y_pred_dummy):.4f}")
print(f"Precision : {precision_score(y_test, y_pred_dummy, zero_division=0):.4f}")
print(f"Recall    : {recall_score(y_test, y_pred_dummy):.4f}")
print(f"F1-Score  : {f1_score(y_test, y_pred_dummy):.4f}")
print("\n‚Üí 90% d'accuracy mais AUCUNE d√©tection des positifs !")
```

> üí° **Conseil de pro** : "Ne JAMAIS utiliser l'accuracy seule comme m√©trique, surtout avec des classes d√©s√©quilibr√©es. Utilisez toujours le classification_report complet et l'AUC-ROC."

### 3.4 Courbe ROC et AUC

#### Pour les d√©butants : c'est quoi une courbe ROC ?

Imaginez un **curseur** de sensibilit√© sur votre mod√®le :

```
Curseur √† gauche (prudent)              Curseur √† droite (sensible)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Peu de fausses alarmes                  D√©tecte tout
MAIS rate des positifs                  MAIS beaucoup de fausses alarmes
```

La courbe ROC trace **toutes les positions possibles du curseur**. L'**AUC** (l'aire sous la courbe) r√©sume la qualit√© globale :

| AUC | Interpr√©tation |
|-----|---------------|
| 0.50 | Le mod√®le tire au hasard (pile ou face) |
| 0.60-0.70 | M√©diocre |
| 0.70-0.80 | Acceptable |
| 0.80-0.90 | Bon mod√®le |
| 0.90-1.00 | Excellent mod√®le |

> üí° **Astuce de lecture** : Plus la courbe est proche du **coin sup√©rieur gauche** du graphique, meilleur est le mod√®le.

#### Explication technique

La courbe ROC (Receiver Operating Characteristic) trace le **True Positive Rate** (Recall) en fonction du **False Positive Rate** pour diff√©rents seuils de classification.

```python
# Courbe ROC
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Random Forest (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Al√©atoire (AUC = 0.5)')
plt.fill_between(fpr, tpr, alpha=0.1, color='blue')

plt.xlabel('Taux de Faux Positifs (FPR)')
plt.ylabel('Taux de Vrais Positifs (TPR / Recall)')
plt.title('Courbe ROC')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Trouver le seuil optimal (point le plus proche du coin sup√©rieur gauche)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
print(f"Seuil optimal : {optimal_threshold:.3f}")
print(f"TPR au seuil optimal : {tpr[optimal_idx]:.3f}")
print(f"FPR au seuil optimal : {fpr[optimal_idx]:.3f}")
```

### 3.5 Courbe Precision-Recall

#### Pour les d√©butants : comprendre le compromis Precision-Recall

C'est un **dilemme** permanent :

- **Augmenter la Precision** (moins de fausses alarmes) ‚Üí on baisse le Recall (on rate des vrais positifs)
- **Augmenter le Recall** (d√©tecter tout) ‚Üí on baisse la Precision (plus de fausses alarmes)

> **Analogie du filet de p√™che** : Un filet √† mailles serr√©es capture TOUS les poissons (Recall √©lev√©) mais aussi des d√©chets (Precision basse). Un filet √† grosses mailles ne capture que les gros poissons (Precision √©lev√©e) mais en laisse √©chapper beaucoup (Recall bas). Le F1-Score trouve le meilleur compromis entre les deux.

La courbe Precision-Recall visualise ce compromis pour tous les seuils possibles.

Pour les datasets tr√®s d√©s√©quilibr√©s, la courbe Precision-Recall est souvent **plus informative** que la courbe ROC.

```python
# Courbe Precision-Recall
precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(recall_curve, precision_curve, 'g-', linewidth=2,
         label=f'Random Forest (AP = {ap:.3f})')
plt.fill_between(recall_curve, precision_curve, alpha=0.1, color='green')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Courbe Precision-Recall')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

> üí° **Conseil de pro** : "Pour des classes tr√®s d√©s√©quilibr√©es (fraude, maladie rare), la courbe Precision-Recall et l'AP (Average Precision) sont PLUS fiables que la courbe ROC. L'AUC-ROC peut √™tre trompeusement √©lev√©e quand la classe n√©gative est tr√®s majoritaire."

### 3.6 Ajuster le seuil de classification

#### Pour les d√©butants : c'est quoi le seuil ?

Le mod√®le ne r√©pond pas directement "oui" ou "non". Il donne une **probabilit√©** : "ce patient a 73% de chances d'√™tre malade". Le **seuil** est la limite √† partir de laquelle on d√©cide "positif".

```
Seuil = 0.3 (prudent)              Seuil = 0.7 (strict)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"D√®s que c'est > 30%,              "Seulement si c'est > 70%,
 je dis POSITIF"                     je dis POSITIF"

‚Üí D√©tecte plus de positifs          ‚Üí D√©tecte moins de positifs
‚Üí Plus de fausses alarmes           ‚Üí Moins de fausses alarmes
‚Üí Meilleur Recall                   ‚Üí Meilleure Precision
```

> üí° **Pour d√©buter** : Gardez le seuil par d√©faut (0.5) pour commencer. Ajustez-le uniquement quand vous comprenez le co√ªt m√©tier de chaque type d'erreur.

Par d√©faut, sklearn utilise un seuil de 0.5. Mais ce n'est pas toujours optimal !

```python
# Analyser l'impact du seuil
seuils = np.arange(0.1, 0.9, 0.05)
resultats_seuil = []

for seuil in seuils:
    y_pred_seuil = (y_proba >= seuil).astype(int)
    resultats_seuil.append({
        'seuil': seuil,
        'precision': precision_score(y_test, y_pred_seuil, zero_division=0),
        'recall': recall_score(y_test, y_pred_seuil),
        'f1': f1_score(y_test, y_pred_seuil, zero_division=0),
        'accuracy': accuracy_score(y_test, y_pred_seuil)
    })

df_seuils = pd.DataFrame(resultats_seuil)

# Visualiser
plt.figure(figsize=(10, 6))
plt.plot(df_seuils['seuil'], df_seuils['precision'], 'b-', label='Precision', linewidth=2)
plt.plot(df_seuils['seuil'], df_seuils['recall'], 'r-', label='Recall', linewidth=2)
plt.plot(df_seuils['seuil'], df_seuils['f1'], 'g-', label='F1-Score', linewidth=2)
plt.axvline(x=0.5, color='gray', linestyle='--', label='Seuil par d√©faut')
plt.xlabel('Seuil de classification')
plt.ylabel('Score')
plt.title('Impact du seuil sur les m√©triques')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

> üí° **Conseil** : "Le seuil de 0.5 n'est pas sacr√© ! Si les faux n√©gatifs co√ªtent cher (diagnostic m√©dical), baissez le seuil (ex: 0.3) pour augmenter le recall. Si les faux positifs co√ªtent cher (spam filter), augmentez le seuil (ex: 0.7) pour augmenter la precision."

---

## 4. üìä Choisir LA bonne m√©trique

### 4.1 Selon le type de probl√®me

| Probl√®me | M√©trique principale | Pourquoi | M√©trique secondaire |
|---|---|---|---|
| **Prix immobilier** | RMSE | M√™me unit√© que la cible, p√©nalise les grosses erreurs | R¬≤, MAPE |
| **D√©tection de spam** | Precision | FP co√ªteux (email l√©gitime en spam) | F1, Recall |
| **Diagnostic m√©dical** | Recall | FN co√ªteux (rater une maladie = mortel) | F1, Specificity |
| **Fraude bancaire** | F1 ou AUC-PR | Classes tr√®s d√©s√©quilibr√©es | Precision, Recall |
| **Pr√©diction de churn** | AUC-PR | Classes d√©s√©quilibr√©es + besoin de ranking | F1 |
| **Scoring cr√©dit** | AUC-ROC | Besoin de bien discriminer les profils | KS statistic |
| **Demande de stock** | MAE | Erreurs sym√©triques, pas de grosses p√©nalit√©s | MAPE |
| **Pr√©vision m√©t√©o** | Accuracy | Classes relativement √©quilibr√©es | F1 par classe |
| **Recommandation** | AUC-ROC | Capacit√© √† classer les items pertinents | Precision@K |
| **V√©hicule autonome** | Recall | FN = ne pas d√©tecter un pi√©ton = mortel | Latence |

### 4.2 Selon le contexte m√©tier : co√ªt asym√©trique des erreurs

La question cl√© √† poser au m√©tier est : **¬´ Qu'est-ce qui co√ªte le plus cher, un faux positif ou un faux n√©gatif ? ¬ª**

| Sc√©nario | FP (fausse alarme) | FN (rat√©) | M√©trique privil√©gi√©e |
|---|---|---|---|
| **Cancer** | Examen inutile (~500‚Ç¨) | Mort possible (~‚àû) | **Recall** |
| **Spam** | Email l√©gitime perdu | Spam dans inbox | **Precision** |
| **Fraude CB** | Carte bloqu√©e √† tort | Fraude non d√©tect√©e (~5000‚Ç¨) | **Recall** (seuil bas) |
| **Embauche** | Candidat refus√© √† tort | Mauvais recrutement (~50k‚Ç¨) | **Precision** |
| **Assurance** | Prime sous-estim√©e | Prime surestim√©e | **MAE sym√©trique** |

> üí° **Conseil de pro** : "Demandez TOUJOURS au m√©tier : ¬´ Qu'est-ce qui co√ªte le plus cher, un faux positif ou un faux n√©gatif ? ¬ª. La r√©ponse d√©termine votre m√©trique. Si le m√©tier ne sait pas, utilisez le F1-Score comme compromis."

> üß† **Pour aller plus loin** : "Dans les cas avanc√©s, vous pouvez d√©finir une **matrice de co√ªts** personnalis√©e et optimiser directement le co√ªt total. Par exemple : co√ªt_total = nb_FP √ó co√ªt_FP + nb_FN √ó co√ªt_FN. Minimiser cette fonction est l'objectif ultime."

---

## 5. ‚öôÔ∏è Overfitting vs Underfitting

### 5.0 Pour les d√©butants : l'analogie de l'√©tudiant

Imaginez 3 √©tudiants qui pr√©parent un examen de maths :

**L'√©tudiant qui apprend par c≈ìur (Overfitting)** ü§ñ
- Il m√©morise toutes les r√©ponses des exercices du livre
- Il a 100% sur les exercices d√©j√† faits... mais 30% √† l'examen (questions nouvelles)
- Il n'a pas **compris** les r√®gles, il a juste **m√©moris√©** les exemples

**L'√©tudiant qui ne travaille pas assez (Underfitting)** üò¥
- Il survole le cours sans approfondir
- Il a 50% partout (exercices ET examen)
- Son "mod√®le mental" est trop **simpliste** pour r√©soudre les probl√®mes

**L'√©tudiant qui comprend (Bon mod√®le)** üéØ
- Il comprend les concepts et sait les appliquer
- Il a 90% sur les exercices ET 85% √† l'examen
- Il **g√©n√©ralise** bien car il a compris les r√®gles sous-jacentes

#### Comment le d√©tecter ?

```
Score train = 99%, Score test = 50%  ‚Üí  üî¥ OVERFITTING (a appris par c≈ìur)
Score train = 55%, Score test = 50%  ‚Üí  üü° UNDERFITTING (mod√®le trop simple)
Score train = 92%, Score test = 88%  ‚Üí  üü¢ BON MOD√àLE (g√©n√©ralise bien)
```

> üí° **R√®gle simple** : Si le score train est **beaucoup plus √©lev√©** que le score test, c'est de l'overfitting. Si les **deux sont bas**, c'est de l'underfitting.

---

### 5.1 Diagnostic

| Sympt√¥me | Diagnostic | Analogie |
|---|---|---|
| Train score √©lev√© + Test score faible | **Overfitting** (sur-apprentissage) | Apprendre par c≈ìur un examen |
| Train ET Test scores faibles | **Underfitting** (sous-apprentissage) | Ne pas assez √©tudier |
| Train ‚âà Test ‚âà √©lev√©s | **Bon mod√®le** | Bien comprendre le cours |

```python
from sklearn.model_selection import learning_curve
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

def tracer_learning_curves(model, X, y, title="Learning Curves", cv=5):
    """Trace les learning curves pour diagnostiquer overfitting/underfitting."""

    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y,
        cv=cv,
        n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='roc_auc',
        random_state=42
    )

    # Moyennes et √©carts-types
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    test_mean = test_scores.mean(axis=1)
    test_std = test_scores.std(axis=1)

    plt.figure(figsize=(10, 6))

    # Score d'entra√Ænement
    plt.plot(train_sizes, train_mean, 'b-', linewidth=2, label='Score train')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                     alpha=0.1, color='blue')

    # Score de validation
    plt.plot(train_sizes, test_mean, 'r-', linewidth=2, label='Score validation')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std,
                     alpha=0.1, color='red')

    plt.xlabel('Taille du jeu d\'entra√Ænement')
    plt.ylabel('Score AUC-ROC')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    plt.ylim(0.5, 1.05)
    plt.tight_layout()
    plt.show()

    # Diagnostic automatique
    gap = train_mean[-1] - test_mean[-1]
    if gap > 0.1:
        print(f"‚ö†Ô∏è OVERFITTING d√©tect√© (gap = {gap:.3f})")
        print("   ‚Üí Simplifier le mod√®le, plus de donn√©es, ou r√©gularisation")
    elif test_mean[-1] < 0.7:
        print(f"‚ö†Ô∏è UNDERFITTING d√©tect√© (test score = {test_mean[-1]:.3f})")
        print("   ‚Üí Mod√®le plus complexe, feature engineering, moins de r√©gularisation")
    else:
        print(f"‚úÖ Bon √©quilibre (gap = {gap:.3f}, test score = {test_mean[-1]:.3f})")

# Exemple d'utilisation
rf = RandomForestClassifier(n_estimators=100, random_state=42)
tracer_learning_curves(rf, X_train, y_train, title="Learning Curves - Random Forest")
```

> üí° **Conseil** : "Tracez TOUJOURS les learning curves. C'est le meilleur outil de diagnostic. En 30 secondes, vous savez si votre probl√®me est l'overfitting, l'underfitting, ou le manque de donn√©es."

### 5.2 Solutions √† l'overfitting

L'overfitting se produit quand le mod√®le apprend le **bruit** des donn√©es d'entra√Ænement en plus du signal.

| Solution | Description | Quand l'utiliser |
|---|---|---|
| **Plus de donn√©es** | Augmenter le dataset | Si possible (le plus efficace) |
| **R√©gularisation L1/L2** | P√©naliser les poids trop √©lev√©s | R√©gression, SVM, r√©seaux de neurones |
| **Simplifier le mod√®le** | R√©duire max_depth, min_samples_leaf | Arbres, Random Forest |
| **S√©lection de features** | √âliminer les features bruit√©es | Toujours pertinent |
| **Early stopping** | Arr√™ter l'entra√Ænement avant convergence | Gradient Boosting, XGBoost |
| **Cross-validation** | √âvaluation robuste sur plusieurs folds | Toujours |
| **Dropout** | D√©sactiver al√©atoirement des neurones | R√©seaux de neurones |
| **Data augmentation** | G√©n√©rer des variations des donn√©es | Images, texte |
| **Ensemble methods** | Combiner plusieurs mod√®les | Toujours b√©n√©fique |

```python
# Exemple : r√©duire l'overfitting d'un Random Forest
from sklearn.ensemble import RandomForestClassifier

# Mod√®le qui overfit (trop complexe)
rf_overfit = RandomForestClassifier(
    n_estimators=500,
    max_depth=None,        # profondeur illimit√©e ‚Üí apprend le bruit
    min_samples_leaf=1,    # feuilles avec 1 seul √©chantillon
    random_state=42
)
rf_overfit.fit(X_train, y_train)
print(f"Overfitting - Train: {rf_overfit.score(X_train, y_train):.4f}, "
      f"Test: {rf_overfit.score(X_test, y_test):.4f}")

# Mod√®le r√©gularis√©
rf_regularise = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,          # limiter la profondeur
    min_samples_leaf=5,    # au moins 5 √©chantillons par feuille
    min_samples_split=10,  # au moins 10 pour splitter
    max_features='sqrt',   # sous-ensemble de features
    random_state=42
)
rf_regularise.fit(X_train, y_train)
print(f"R√©gularis√©  - Train: {rf_regularise.score(X_train, y_train):.4f}, "
      f"Test: {rf_regularise.score(X_test, y_test):.4f}")
```

### 5.3 Solutions √† l'underfitting

L'underfitting se produit quand le mod√®le est **trop simple** pour capturer les patterns des donn√©es.

| Solution | Description | Quand l'utiliser |
|---|---|---|
| **Mod√®le plus complexe** | Passer de lin√©aire √† Random Forest/XGBoost | Relation non lin√©aire |
| **Feature engineering** | Cr√©er de nouvelles features informatives | Toujours (le plus efficace) |
| **Moins de r√©gularisation** | R√©duire alpha, C, augmenter max_depth | Si trop r√©gularis√© |
| **Plus de features** | Ajouter des variables explicatives | Si donn√©es disponibles |
| **Polynomiales** | Ajouter des interactions et termes polynomiaux | Relations non lin√©aires |
| **R√©duire la simplification** | Augmenter max_depth, r√©duire min_samples_leaf | Arbres trop simples |

> üí° **Conseil de pro** : "L'underfitting est souvent le signe que vos FEATURES sont insuffisantes, pas que votre mod√®le est trop simple. Investissez dans le feature engineering avant de complexifier le mod√®le."

---

## 6. üîÑ Cross-Validation

### 6.0 Pour les d√©butants : pourquoi ne pas simplement couper en train/test ?

#### Le probl√®me du split unique

Quand vous coupez vos donn√©es en train (80%) et test (20%), le r√©sultat **d√©pend du hasard du d√©coupage**. Si par malchance, tous les cas faciles sont dans le test, le score sera artificiellement √©lev√©.

> **Analogie de l'examen** : √âvaluer un √©tudiant sur UN SEUL examen n'est pas fiable. Il a peut-√™tre eu de la chance ce jour-l√†. Mieux vaut lui faire passer **5 examens diff√©rents** et faire la moyenne.

#### La cross-validation en images

```
Donn√©es : [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†|‚ñ°‚ñ°‚ñ°‚ñ°‚ñ°|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†|‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†]

Fold 1 : [TEST |Train |Train |Train |Train ]  ‚Üí Score = 0.85
Fold 2 : [Train|TEST  |Train |Train |Train ]  ‚Üí Score = 0.82
Fold 3 : [Train|Train |TEST  |Train |Train ]  ‚Üí Score = 0.87
Fold 4 : [Train|Train |Train |TEST  |Train ]  ‚Üí Score = 0.84
Fold 5 : [Train|Train |Train |Train |TEST  ]  ‚Üí Score = 0.83

                                     Moyenne = 0.84 (+/- 0.02)
```

Chaque partie sert de test **une et une seule fois**. Le r√©sultat est une **moyenne et un √©cart-type**, ce qui est beaucoup plus fiable qu'un seul score.

> üí° **Pour d√©buter** : Utilisez toujours `cross_val_score` avec `cv=5` pour avoir un r√©sultat fiable. Un seul split train/test peut √™tre trompeur.

La cross-validation est la m√©thode standard pour √©valuer un mod√®le de mani√®re **robuste et fiable**, en utilisant au mieux les donn√©es disponibles.

### 6.1 K-Fold Cross-Validation

Le dataset est divis√© en **K parties** (folds). √Ä tour de r√¥le, chaque fold sert de validation pendant que les K-1 autres servent d'entra√Ænement.

```python
from sklearn.model_selection import cross_val_score, cross_validate

# Cross-validation simple (5-fold)
scores = cross_val_score(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train,
    cv=5,               # 5 folds
    scoring='roc_auc',  # m√©trique
    n_jobs=-1
)

print(f"Scores par fold : {scores}")
print(f"AUC-ROC moyenne : {scores.mean():.4f} (+/- {scores.std():.4f})")

# Cross-validate avec plusieurs m√©triques
results = cross_validate(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train,
    cv=5,
    scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],
    return_train_score=True,
    n_jobs=-1
)

print("\n=== R√©sultats d√©taill√©s (5-Fold CV) ===")
for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:
    train_scores = results[f'train_{metric}']
    test_scores = results[f'test_{metric}']
    gap = train_scores.mean() - test_scores.mean()
    print(f"{metric:>12} : Train={train_scores.mean():.4f}, "
          f"Val={test_scores.mean():.4f} (+/- {test_scores.std():.4f}), "
          f"Gap={gap:.4f}")
```

> üí° **Conseil** : "K=5 est le standard. K=10 pour des datasets plus grands. K=3 si l'entra√Ænement est tr√®s lent. Le compromis est : plus de folds = estimation plus fiable mais plus lente."

### 6.2 Stratified K-Fold

Pour les **classes d√©s√©quilibr√©es**, le Stratified K-Fold garantit que chaque fold a la m√™me proportion de classes que le dataset complet.

```python
from sklearn.model_selection import StratifiedKFold

# Stratified K-Fold (pr√©serve les proportions des classes)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores_strat = cross_val_score(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1
)

print(f"Stratified K-Fold AUC : {scores_strat.mean():.4f} (+/- {scores_strat.std():.4f})")
```

> üí° **Conseil de pro** : "Utilisez TOUJOURS StratifiedKFold pour la classification, surtout avec des classes d√©s√©quilibr√©es. C'est le comportement par d√©faut de cross_val_score pour la classification, mais soyez explicite pour plus de clart√©."

### 6.3 Leave-One-Out (LOO)

Chaque observation sert de validation une fois, les N-1 autres servent d'entra√Ænement. Utile pour les tr√®s petits datasets.

```python
from sklearn.model_selection import LeaveOneOut

# LOO - attention : tr√®s lent pour les grands datasets
loo = LeaveOneOut()

# Uniquement pour les petits datasets (< 500 observations)
# scores_loo = cross_val_score(model, X_small, y_small, cv=loo, scoring='accuracy')
```

> ‚ö†Ô∏è **Attention** : "LOO est N fois plus lent qu'un train/test simple. Ne l'utilisez que pour des datasets de moins de 500 observations. Au-del√†, K-Fold avec K=10 est suffisant."

### 6.4 Time Series Split

Pour les donn√©es temporelles, il est **interdit** de m√©langer pass√© et futur. Le Time Series Split respecte l'ordre chronologique.

```python
from sklearn.model_selection import TimeSeriesSplit

# Time Series Split (respecte l'ordre chronologique)
tscv = TimeSeriesSplit(n_splits=5)

# Visualiser les splits
for i, (train_idx, test_idx) in enumerate(tscv.split(X_train)):
    print(f"Fold {i+1} : Train[{train_idx[0]}:{train_idx[-1]}], "
          f"Test[{test_idx[0]}:{test_idx[-1]}]")

# Utilisation
scores_ts = cross_val_score(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train,
    cv=tscv,
    scoring='roc_auc',
    n_jobs=-1
)

print(f"\nTime Series CV AUC : {scores_ts.mean():.4f} (+/- {scores_ts.std():.4f})")
```

> ‚ö†Ô∏è **Attention** : "JAMAIS de K-Fold standard sur des s√©ries temporelles ! Cela cr√©e un data leakage temporel : le mod√®le voit le futur pendant l'entra√Ænement. Les r√©sultats seront trompeusement bons mais le mod√®le √©chouera en production."

### 6.5 R√©sum√© des m√©thodes de cross-validation

| M√©thode | Quand l'utiliser | Nombre d'entra√Ænements | Stabilit√© |
|---|---|---|---|
| **K-Fold** | Cas g√©n√©ral | K (5-10) | Bonne |
| **Stratified K-Fold** | Classes d√©s√©quilibr√©es | K (5-10) | Tr√®s bonne |
| **Leave-One-Out** | Tr√®s petit dataset (<500) | N | Excellente mais lente |
| **Time Series Split** | Donn√©es temporelles | K (5-10) | Bonne (respecte le temps) |
| **Repeated K-Fold** | Haute fiabilit√© requise | K √ó R | Excellente |
| **Group K-Fold** | Groupes √† ne pas s√©parer | K | Bonne (pas de leakage) |

---

## 7. ‚öôÔ∏è Hyperparameter Tuning

### 7.1 GridSearchCV

Recherche **exhaustive** de toutes les combinaisons d'hyperparam√®tres.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_leaf': [1, 2, 5]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1,
    return_train_score=True
)

grid_search.fit(X_train, y_train)

print(f"Meilleurs param√®tres : {grid_search.best_params_}")
print(f"Meilleur AUC (CV) : {grid_search.best_score_:.4f}")

# V√©rifier l'overfitting
best_idx = grid_search.best_index_
train_score = grid_search.cv_results_['mean_train_score'][best_idx]
test_score = grid_search.cv_results_['mean_test_score'][best_idx]
print(f"Train score : {train_score:.4f}")
print(f"Val score   : {test_score:.4f}")
print(f"Gap         : {train_score - test_score:.4f}")
```

### 7.2 RandomizedSearchCV

Recherche **al√©atoire** dans des distributions d'hyperparam√®tres. Plus efficace pour de grands espaces de recherche.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(3, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': uniform(0.1, 0.9)
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,       # 50 combinaisons al√©atoires
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train, y_train)
print(f"Meilleurs param√®tres : {random_search.best_params_}")
print(f"Meilleur AUC (CV) : {random_search.best_score_:.4f}")
```

### 7.3 Optimisation bay√©sienne (introduction)

L'optimisation bay√©sienne utilise un **mod√®le probabiliste** (Gaussian Process) pour guider la recherche vers les r√©gions prometteuses de l'espace des hyperparam√®tres.

```python
# Installation : uv add scikit-optimize
# from skopt import BayesSearchCV
# from skopt.space import Integer, Real

# Exemple conceptuel (n√©cessite scikit-optimize)
# bayes_search = BayesSearchCV(
#     RandomForestClassifier(random_state=42),
#     search_spaces={
#         'n_estimators': Integer(50, 500),
#         'max_depth': Integer(3, 30),
#         'min_samples_leaf': Integer(1, 10),
#         'max_features': Real(0.1, 0.9)
#     },
#     n_iter=30,
#     cv=5,
#     scoring='roc_auc',
#     n_jobs=-1,
#     random_state=42
# )
```

> üí° **Conseil de pro** : "Le tuning des hyperparam√®tres donne g√©n√©ralement 2-5% d'am√©lioration. Le feature engineering en donne 10-30%. Ne passez pas 3 heures √† tuner si vos features sont m√©diocres. L'ordre d'investissement : donn√©es propres > features > algorithme > tuning."

---

## 8. üìà M√©thodologie compl√®te d'am√©lioration

### La checklist ultime pour am√©liorer un mod√®le

Cette checklist doit √™tre suivie **dans l'ordre**. La plupart des data scientists sautent directement aux √©tapes 7-10 alors que le probl√®me se trouve aux √©tapes 1-4.

```
üìã CHECKLIST D'AM√âLIORATION D'UN MOD√àLE ML

√âtape 1 : ‚úÖ V√©rifier les donn√©es
   ‚ñ° Valeurs manquantes identifi√©es et trait√©es
   ‚ñ° Outliers analys√©s (garder, transformer ou supprimer)
   ‚ñ° Distribution des features examin√©e
   ‚ñ° Pas de data leakage (feature qui "voit" la cible)
   ‚ñ° Classes √©quilibr√©es ou strat√©gie de gestion d√©finie

√âtape 2 : ‚úÖ √âtablir une baseline
   ‚ñ° DummyClassifier / DummyRegressor (mod√®le stupide)
   ‚ñ° R√©gression logistique / R√©gression lin√©aire (mod√®le simple)
   ‚ñ° M√©triques de la baseline not√©es ‚Üí tout mod√®le doit faire MIEUX

√âtape 3 : ‚úÖ Tester plusieurs algorithmes
   ‚ñ° Logistic Regression / Linear Regression
   ‚ñ° Random Forest
   ‚ñ° XGBoost / Gradient Boosting
   ‚ñ° Comparer les scores en cross-validation

√âtape 4 : ‚úÖ Choisir les bonnes m√©triques
   ‚ñ° M√©trique align√©e avec le co√ªt m√©tier
   ‚ñ° Pas JUSTE l'accuracy !
   ‚ñ° classification_report complet
   ‚ñ° Courbes ROC et Precision-Recall

√âtape 5 : ‚úÖ Cross-validation
   ‚ñ° K-Fold (K=5 minimum)
   ‚ñ° Stratified pour classes d√©s√©quilibr√©es
   ‚ñ° TimeSeriesSplit pour donn√©es temporelles
   ‚ñ° V√©rifier la stabilit√© (√©cart-type entre folds)

√âtape 6 : ‚úÖ Learning curves
   ‚ñ° Diagnostiquer overfitting vs underfitting
   ‚ñ° Gap train/test < 5% id√©alement
   ‚ñ° Assez de donn√©es ?

√âtape 7 : ‚úÖ Feature engineering
   ‚ñ° Nouvelles features (interactions, polynomiales, temporelles)
   ‚ñ° Connaissance du domaine exploit√©e
   ‚ñ° Impact mesur√© sur la m√©trique cible

√âtape 8 : ‚úÖ Feature selection
   ‚ñ° √âliminer les features bruit√©es
   ‚ñ° Permutation importance
   ‚ñ° Corr√©lation avec la cible (mutual information)

√âtape 9 : ‚úÖ Tuning des hyperparam√®tres
   ‚ñ° RandomizedSearchCV d'abord (exploration large)
   ‚ñ° GridSearchCV ensuite (affinage)
   ‚ñ° V√©rifier que le tuning n'overfitte pas

√âtape 10 : ‚úÖ Ensemble de mod√®les
   ‚ñ° Voting (combiner les pr√©dictions)
   ‚ñ° Stacking (m√©ta-mod√®le)
   ‚ñ° Blending
```

> üí° **Conseil de pro** : "Suivez cette checklist dans l'ORDRE. La plupart des gens sautent aux √©tapes 7-10 alors que le probl√®me est aux √©tapes 1-4. Un bon data scientist passe 80% de son temps sur les donn√©es (√©tapes 1-2) et 20% sur les mod√®les."

### Comparaison de l'impact de chaque √©tape

| √âtape | Am√©lioration typique | Effort | Priorit√© |
|---|---|---|---|
| Donn√©es propres (√©tape 1) | 10-30% | √âlev√© | Critique |
| Bonne m√©trique (√©tape 4) | N/A (change la perspective) | Faible | Critique |
| Feature engineering (√©tape 7) | 10-30% | √âlev√© | Tr√®s haute |
| Algorithme appropri√© (√©tape 3) | 5-15% | Moyen | Haute |
| Cross-validation (√©tape 5) | Stabilit√©, pas de score | Faible | Haute |
| Feature selection (√©tape 8) | 2-10% | Moyen | Moyenne |
| Tuning (√©tape 9) | 2-5% | Moyen | Moyenne |
| Ensembles (√©tape 10) | 1-3% | √âlev√© | Faible |

---

## 9. ‚úÖ Validation finale

### 9.1 Le split train / validation / test

```
Dataset complet
‚îú‚îÄ‚îÄ 60% Train       ‚Üí entra√Æner les mod√®les
‚îú‚îÄ‚îÄ 20% Validation  ‚Üí tuner, comparer, s√©lectionner
‚îî‚îÄ‚îÄ 20% Test        ‚Üí √©valuation FINALE (UNE SEULE FOIS)
```

```python
from sklearn.model_selection import train_test_split

# Split en 3 parties
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full
)
# 0.25 √ó 0.8 = 0.2 ‚Üí 60% train, 20% val, 20% test

print(f"Train      : {len(X_train)} ({len(X_train)/len(X):.0%})")
print(f"Validation : {len(X_val)} ({len(X_val)/len(X):.0%})")
print(f"Test       : {len(X_test)} ({len(X_test)/len(X):.0%})")
```

### 9.2 Le test set ne sert QU'UNE SEULE FOIS

> ‚ö†Ô∏è **Attention** : "Le test set est votre **estimation de la performance en production**. Si vous l'utilisez plusieurs fois pour prendre des d√©cisions (choisir un mod√®le, tuner des hyperparam√®tres), ce n'est plus un test set : c'est un validation set, et votre estimation de performance est biais√©e. R√©sultat : votre mod√®le performera MOINS BIEN en production que ce que le test set indique."

```python
# PROCESSUS CORRECT :
# 1. Entra√Æner et tuner sur train + validation (ou cross-validation sur train)
# 2. Choisir le meilleur mod√®le
# 3. √âvaluer UNE SEULE FOIS sur le test set
# 4. Rapporter ce score comme performance attendue en production

# Score final sur le test set
best_model = grid_search.best_estimator_
y_pred_final = best_model.predict(X_test)
y_proba_final = best_model.predict_proba(X_test)[:, 1]

print("=== √âVALUATION FINALE (Test Set) ===")
print(f"Accuracy  : {accuracy_score(y_test, y_pred_final):.4f}")
print(f"Precision : {precision_score(y_test, y_pred_final):.4f}")
print(f"Recall    : {recall_score(y_test, y_pred_final):.4f}")
print(f"F1-Score  : {f1_score(y_test, y_pred_final):.4f}")
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba_final):.4f}")
print("\n‚ö†Ô∏è Ce score est votre estimation de performance en production.")
print("   Il ne doit PAS √™tre utilis√© pour prendre d'autres d√©cisions.")
```

> üí° **Conseil de pro** : "Si votre score test est significativement inf√©rieur √† votre score de cross-validation, vous avez probablement une fuite de donn√©es (data leakage) ou vous avez sur-optimis√© sur le validation set. Investiguez."

---

## üéØ Points cl√©s √† retenir

1. **La m√©trique choisie** doit refl√©ter le co√ªt m√©tier de l'erreur
2. **L'accuracy est trompeuse** : ne l'utilisez jamais seule, surtout avec des classes d√©s√©quilibr√©es
3. **RMSE** pour la r√©gression (p√©nalise les grosses erreurs), **MAE** pour la robustesse
4. **Precision** quand les FP co√ªtent cher, **Recall** quand les FN co√ªtent cher, **F1** pour l'√©quilibre
5. **AUC-ROC** pour √©valuer la discrimination, **AUC-PR** pour les classes tr√®s d√©s√©quilibr√©es
6. **Learning curves** = outil de diagnostic #1 (overfitting vs underfitting)
7. **Cross-validation** TOUJOURS (K-Fold pour le standard, Stratified pour les classes, TimeSeries pour le temporel)
8. Le **test set** ne sert QU'UNE SEULE FOIS (sinon ce n'est plus un test)
9. **Feature engineering** > tuning (10-30% vs 2-5% d'am√©lioration)
10. Suivre la **checklist** dans l'ordre : donn√©es > baseline > m√©triques > mod√®les > tuning

## ‚úÖ Checklist de validation

- [ ] Je sais calculer et interpr√©ter MSE, RMSE, MAE, R¬≤, MAPE
- [ ] Je sais lire une matrice de confusion et calculer precision, recall, F1
- [ ] Je comprends pourquoi l'accuracy est trompeuse
- [ ] Je sais tracer et interpr√©ter les courbes ROC et Precision-Recall
- [ ] Je sais choisir la bonne m√©trique selon le contexte m√©tier
- [ ] Je sais diagnostiquer l'overfitting et l'underfitting avec les learning curves
- [ ] Je ma√Ætrise la cross-validation (K-Fold, Stratified, TimeSeriesSplit)
- [ ] Je sais utiliser GridSearchCV et RandomizedSearchCV
- [ ] Je comprends le protocole train/validation/test
- [ ] Je connais la checklist d'am√©lioration d'un mod√®le et son ordre

---

[‚¨ÖÔ∏è Chapitre 7 : Clustering](07-clustering.md) | [‚û°Ô∏è Chapitre 9 : Feature Engineering](09-feature-engineering.md)
