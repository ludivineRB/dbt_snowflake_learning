# Chapitre 4 : Fonctions, Erreurs et l'Art de s'Am√©liorer

## üéØ Objectifs

- Comprendre comment une machine "apprend" √† partir de ses erreurs
- Ma√Ætriser la r√©gression lin√©aire intuitivement puis math√©matiquement
- Savoir ce qu'est une fonction d'erreur (MSE, MAE, RMSE) et la visualiser
- Comprendre la d√©riv√©e comme une "direction d'am√©lioration"
- Impl√©menter la descente de gradient from scratch
- Coder une r√©gression lin√©aire from scratch puis avec scikit-learn
- D√©couvrir la r√©gularisation (Ridge et Lasso) intuitivement

**Livrable** : Notebook "Gradient Descent expliqu√© √† ma grand-m√®re"

---

## 1. üß† Comment une machine apprend-elle ?

### 1.1 Le probl√®me pos√©

Imaginez que vous devez pr√©dire le **prix d'un appartement** en fonction de sa **surface**. Vous avez des donn√©es historiques :

| Surface (m¬≤) | Prix (k‚Ç¨) |
|:------------:|:---------:|
| 30 | 150 |
| 50 | 220 |
| 70 | 310 |
| 90 | 380 |
| 110 | 470 |

Comment trouver une **r√®gle** qui permet de pr√©dire le prix pour une surface de 85 m¬≤ ?

### 1.2 L'approche du Machine Learning

```
√âtape 1 : Commencer avec une "devinette" (mod√®le initial)
         ‚Üí "Le prix = 100k‚Ç¨ pour tout le monde" (mauvais)

√âtape 2 : Mesurer l'erreur
         ‚Üí "Je me trompe de beaucoup !"

√âtape 3 : Ajuster pour r√©duire l'erreur
         ‚Üí "Si j'augmente un peu la pente..."

√âtape 4 : R√©p√©ter jusqu'√† satisfaction
         ‚Üí "Maintenant je me trompe tr√®s peu !"
```

> üí° **Conseil** : "L'apprentissage d'une machine se r√©sume en 3 mots : **deviner, mesurer, corriger**. C'est exactement comme un √©tudiant qui fait des exercices, v√©rifie ses r√©ponses et s'am√©liore."

### 1.3 Ce que "apprendre" signifie concr√®tement

```
Avant l'entra√Ænement :
  Mod√®le : prix = 0 √ó surface + 0
  ‚Üí Pr√©dit 0‚Ç¨ pour tout ‚Üí Erreur √©norme

Pendant l'entra√Ænement :
  Le mod√®le ajuste ses param√®tres petit √† petit...
  It√©ration 1   : prix = 0.5 √ó surface + 10  ‚Üí Erreur = 85000
  It√©ration 10  : prix = 2.0 √ó surface + 50  ‚Üí Erreur = 12000
  It√©ration 100 : prix = 3.2 √ó surface + 55  ‚Üí Erreur = 1500
  It√©ration 500 : prix = 3.5 √ó surface + 48  ‚Üí Erreur = 200

Apr√®s l'entra√Ænement :
  Mod√®le : prix ‚âà 3.5 √ó surface + 48
  ‚Üí Pr√©dit 346k‚Ç¨ pour 85m¬≤ ‚Üí Tr√®s proche de la r√©alit√© !
```

---

## 2. üîß Fonction = Machine √† transformer

### 2.1 L'analogie de la recette de cuisine

Une **fonction** prend une entr√©e et produit une sortie selon une r√®gle pr√©cise.

```
Fonction = Recette de cuisine

   Ingr√©dients (entr√©e)  ‚Üí  Recette (fonction)  ‚Üí  Plat (sortie)
   Farine, ≈ìufs, sucre   ‚Üí  M√©langer, cuire     ‚Üí  G√¢teau

En maths :
   x (entr√©e)            ‚Üí  f(x) (r√®gle)        ‚Üí  y (sortie)
   surface = 70 m¬≤       ‚Üí  prix = 3.5 √ó x + 50 ‚Üí  prix = 295 k‚Ç¨
```

### 2.2 Exemples de fonctions simples

```python
import numpy as np
import matplotlib.pyplot as plt

# Fonction lin√©aire : f(x) = 2x + 1
def f_lineaire(x):
    return 2 * x + 1

# Fonction quadratique : f(x) = x¬≤
def f_quadratique(x):
    return x ** 2

# Visualiser
x = np.linspace(-5, 5, 100)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(x, f_lineaire(x), 'b-', linewidth=2)
axes[0].set_title("Fonction lin√©aire : f(x) = 2x + 1")
axes[0].grid(True, alpha=0.3)
axes[0].axhline(y=0, color='k', linewidth=0.5)
axes[0].axvline(x=0, color='k', linewidth=0.5)

axes[1].plot(x, f_quadratique(x), 'r-', linewidth=2)
axes[1].set_title("Fonction quadratique : f(x) = x¬≤")
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=0, color='k', linewidth=0.5)
axes[1].axvline(x=0, color='k', linewidth=0.5)

plt.tight_layout()
plt.show()
```

### 2.3 Pourquoi c'est important en ML ?

En Machine Learning, le **mod√®le** est une fonction :

| √âl√©ment | En maths | En ML |
|---------|---------|-------|
| Entr√©e | x | Features (surface, nb pi√®ces...) |
| Fonction | f(x) | Mod√®le (r√©gression, KNN...) |
| Sortie | y | Pr√©diction (prix, classe...) |
| Param√®tres | a, b dans f(x) = ax + b | Poids appris par le mod√®le |

> L'objectif du ML est de **trouver la bonne fonction** (les bons param√®tres) qui transforme les features en pr√©dictions correctes.

### 2.4 Fonction lin√©aire vs non-lin√©aire

```
Lin√©aire : y = ax + b            Non-lin√©aire : y = ax¬≤ + bx + c

    y‚îÇ      /                       y‚îÇ       ‚ï±‚ï≤
     ‚îÇ    /                          ‚îÇ     ‚ï±    ‚ï≤
     ‚îÇ  /                            ‚îÇ   ‚ï±        ‚ï≤
     ‚îÇ/                              ‚îÇ ‚ï±            ‚ï≤
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x

‚Üí Une seule droite                  ‚Üí Courbes, polyn√¥mes, etc.
‚Üí Simple, interpr√©table             ‚Üí Plus flexible, risque d'overfitting
```

---

## 3. üìà R√©gression lin√©aire : trouver la meilleure droite

### 3.1 L'intuition visuelle

La r√©gression lin√©aire cherche la **droite** qui passe "au mieux" √† travers un nuage de points.

```
   Prix (k‚Ç¨)
    500 ‚îÇ                           ‚óè
        ‚îÇ                      ‚óè  /
    400 ‚îÇ                  ‚óè  / ‚Üê‚îÄ‚îÄ droite y = ax + b
        ‚îÇ              ‚óè /
    300 ‚îÇ          ‚óè /
        ‚îÇ       ‚óè /
    200 ‚îÇ     /‚óè
        ‚îÇ   /‚óè
    100 ‚îÇ  /
        ‚îÇ /
      0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Surface (m¬≤)
         0   20   40   60   80  100  120
```

### 3.2 L'√©quation y = ax + b

```
y = ax + b

O√π :
  y = la pr√©diction (prix)
  x = la feature (surface)
  a = la pente (combien y augmente quand x augmente de 1)
  b = l'ordonn√©e √† l'origine (y quand x = 0)
```

**Interpr√©tation concr√®te** :

```
prix = 3.5 √ó surface + 50

‚Üí a = 3.5 : chaque m¬≤ suppl√©mentaire co√ªte 3 500 ‚Ç¨
‚Üí b = 50  : un appartement de 0 m¬≤ co√ªterait 50 000 ‚Ç¨ (co√ªt fixe th√©orique)
```

### 3.3 Visualiser avec Python

```python
import numpy as np
import matplotlib.pyplot as plt

# Donn√©es
surfaces = np.array([30, 50, 70, 90, 110])
prix = np.array([150, 220, 310, 380, 470])

# Tracer le nuage de points
plt.figure(figsize=(10, 6))
plt.scatter(surfaces, prix, s=100, c='steelblue', edgecolors='black', zorder=5)

# Tracer plusieurs droites candidates
x_line = np.linspace(20, 120, 100)
plt.plot(x_line, 2 * x_line + 100, 'r--', alpha=0.5, label='y = 2x + 100 (trop plate)')
plt.plot(x_line, 5 * x_line - 20, 'g--', alpha=0.5, label='y = 5x - 20 (trop pentue)')
plt.plot(x_line, 3.5 * x_line + 50, 'b-', linewidth=2, label='y = 3.5x + 50 (bonne !)')

plt.xlabel("Surface (m¬≤)")
plt.ylabel("Prix (k‚Ç¨)")
plt.title("Quelle droite est la meilleure ?")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

> üí° **Conseil** : "La 'meilleure' droite est celle qui **minimise l'√©cart total** entre les pr√©dictions et les vraies valeurs. C'est exactement ce que fait la fonction d'erreur."

### 3.4 Le cas multi-variables

En r√©alit√©, le prix d√©pend de **plusieurs** features :

```
prix = a‚ÇÅ √ó surface + a‚ÇÇ √ó nb_pi√®ces + a‚ÇÉ √ó √©tage + b

Sous forme matricielle : y = X @ w + b
  X = matrice des features
  w = vecteur des poids (√† apprendre)
  b = biais (ordonn√©e √† l'origine)
```

```python
# R√©gression multi-variables : plusieurs features
import pandas as pd

df = pd.DataFrame({
    'surface': [30, 50, 70, 90, 110],
    'nb_pieces': [1, 2, 3, 4, 5],
    'etage': [0, 2, 5, 3, 8],
    'prix': [150, 220, 310, 380, 470]
})

X = df[['surface', 'nb_pieces', 'etage']].values  # Matrice (5, 3)
y = df['prix'].values                               # Vecteur (5,)

print(f"X shape : {X.shape}")
print(f"y shape : {y.shape}")
```

---

## 4. üìâ Erreur = Distance entre pr√©diction et r√©alit√©

### 4.1 Visualiser les erreurs

Pour savoir si une droite est "bonne", on mesure les **√©carts** (r√©sidus) entre les pr√©dictions et les vraies valeurs :

```
   Prix (k‚Ç¨)
    ‚îÇ                    ‚óè (vrai = 380)
    ‚îÇ                    ‚îÇ
    ‚îÇ                    ‚îÇ erreur = 30
    ‚îÇ                    ‚îÇ
    ‚îÇ               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (pr√©dit = 350)
    ‚îÇ          ‚óè (vrai = 310)
    ‚îÇ          ‚îÇ
    ‚îÇ          ‚îÇ erreur = -15
    ‚îÇ          ‚îÇ
    ‚îÇ     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (pr√©dit = 295)
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Surface
```

```python
# Pr√©dictions avec la droite y = 3.5x + 50
surfaces = np.array([30, 50, 70, 90, 110], dtype=float)
prix = np.array([150, 220, 310, 380, 470], dtype=float)
predictions = 3.5 * surfaces + 50

# Erreurs (r√©sidus)
erreurs = prix - predictions

print("Surface | Prix r√©el | Prix pr√©dit | Erreur")
print("-" * 50)
for s, p, pred, err in zip(surfaces, prix, predictions, erreurs):
    print(f"  {s:>3.0f} m¬≤ |   {p:>3.0f} k‚Ç¨  |   {pred:>5.0f} k‚Ç¨  |  {err:>+.0f} k‚Ç¨")

# Visualiser les erreurs
plt.figure(figsize=(10, 6))
plt.scatter(surfaces, prix, s=100, c='steelblue', edgecolors='black',
            zorder=5, label='Valeurs r√©elles')
plt.plot(surfaces, predictions, 'r-', linewidth=2, label='Pr√©dictions')

# Tracer les erreurs (segments verticaux)
for s, p, pred in zip(surfaces, prix, predictions):
    plt.plot([s, s], [p, pred], 'r--', alpha=0.7, linewidth=1.5)

plt.xlabel("Surface (m¬≤)")
plt.ylabel("Prix (k‚Ç¨)")
plt.title("Erreurs = √©carts entre pr√©dictions et r√©alit√©")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 4.2 MSE ‚Äî Mean Squared Error (Erreur Quadratique Moyenne)

La MSE est la m√©trique d'erreur la plus utilis√©e en r√©gression.

```
                    1     n
MSE = ‚îÄ‚îÄ‚îÄ √ó Œ£  (y·µ¢ - ≈∑·µ¢)¬≤
                    n    i=1

O√π :
  y·µ¢  = valeur r√©elle du point i
  ≈∑·µ¢  = valeur pr√©dite pour le point i
  n   = nombre de points
```

**Pourquoi mettre au carr√© ?**

| Raison | Explication |
|--------|-------------|
| **√âliminer les signes** | Sans carr√©, les erreurs positives et n√©gatives s'annulent |
| **P√©naliser les grosses erreurs** | Une erreur de 10 compte 4√ó plus qu'une erreur de 5 (100 vs 25) |
| **Propri√©t√© math√©matique** | La fonction est d√©rivable partout ‚Üí facile √† optimiser |

```python
def mse(y_true, y_pred):
    """Calcule la Mean Squared Error."""
    return np.mean((y_true - y_pred) ** 2)

# Calculer la MSE de notre droite
erreur_mse = mse(prix, predictions)
print(f"MSE = {erreur_mse:.2f}")
```

### 4.3 MAE ‚Äî Mean Absolute Error

```
                    1     n
MAE = ‚îÄ‚îÄ‚îÄ √ó Œ£  |y·µ¢ - ≈∑·µ¢|
                    n    i=1
```

La MAE est **plus robuste aux outliers** car elle ne met pas les erreurs au carr√©.

```python
def mae(y_true, y_pred):
    """Calcule la Mean Absolute Error."""
    return np.mean(np.abs(y_true - y_pred))

erreur_mae = mae(prix, predictions)
print(f"MAE = {erreur_mae:.2f} k‚Ç¨")
print(f"‚Üí En moyenne, nos pr√©dictions se trompent de ¬±{erreur_mae:.0f} k‚Ç¨")
```

### 4.4 RMSE ‚Äî Root Mean Squared Error

```
RMSE = ‚àöMSE
```

L'avantage du RMSE est d'√™tre dans la **m√™me unit√©** que la target (k‚Ç¨ ici), tout en p√©nalisant les grosses erreurs.

```python
def rmse(y_true, y_pred):
    """Calcule le Root Mean Squared Error."""
    return np.sqrt(mse(y_true, y_pred))

erreur_rmse = rmse(prix, predictions)
print(f"RMSE = {erreur_rmse:.2f} k‚Ç¨")
```

### 4.5 Comparaison des m√©triques

| M√©trique | Formule | Unit√© | Sensibilit√© aux outliers | Quand l'utiliser |
|----------|---------|-------|--------------------------|------------------|
| **MSE** | Œ£(y-≈∑)¬≤/n | unit√©¬≤ | Tr√®s haute | Optimisation (gradient) |
| **MAE** | Œ£\|y-≈∑\|/n | unit√© | Mod√©r√©e | Interpr√©tation, outliers |
| **RMSE** | ‚àöMSE | unit√© | Haute | Interpr√©tation + p√©nalise grosses erreurs |
| **R¬≤** | 1 - MSE/Var(y) | sans unit√© | ‚Äî | Comparer des mod√®les (0 √† 1) |

> üí° **Conseil** : "Utilisez le **RMSE** pour communiquer avec les m√©tiers ('on se trompe en moyenne de ¬±15k‚Ç¨'). Utilisez la **MSE** pour l'optimisation math√©matique (descente de gradient). Utilisez le **R¬≤** pour comparer des mod√®les entre eux."

### 4.6 Visualiser l'erreur en fonction des param√®tres

```python
# Tester diff√©rentes pentes (a) et voir l'erreur
pentes = np.linspace(1, 6, 100)
erreurs_mse = []

for a in pentes:
    pred = a * surfaces + 50  # On fixe b=50
    erreurs_mse.append(mse(prix, pred))

plt.figure(figsize=(10, 6))
plt.plot(pentes, erreurs_mse, 'b-', linewidth=2)
plt.xlabel("Pente (a)")
plt.ylabel("MSE")
plt.title("MSE en fonction de la pente ‚Äî On cherche le minimum !")
plt.axvline(x=pentes[np.argmin(erreurs_mse)], color='red', linestyle='--',
            label=f'Meilleure pente ‚âà {pentes[np.argmin(erreurs_mse)]:.2f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 4.7 Surface d'erreur en 3D (pente ET ordonn√©e √† l'origine)

```python
from mpl_toolkits.mplot3d import Axes3D

# Tester des combinaisons de (a, b)
a_vals = np.linspace(1, 6, 50)
b_vals = np.linspace(-50, 150, 50)
A, B = np.meshgrid(a_vals, b_vals)
Z = np.zeros_like(A)

for i in range(len(a_vals)):
    for j in range(len(b_vals)):
        pred = A[j, i] * surfaces + B[j, i]
        Z[j, i] = mse(prix, pred)

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(A, B, Z, cmap='coolwarm', alpha=0.8)
ax.set_xlabel("Pente (a)")
ax.set_ylabel("Ordonn√©e √† l'origine (b)")
ax.set_zlabel("MSE")
ax.set_title("Surface d'erreur ‚Äî La descente de gradient cherche le creux")
plt.show()
```

> üí° **Conseil** : "La surface d'erreur ressemble √† un **bol**. La descente de gradient, c'est une bille qu'on l√¢che sur le bord du bol et qui roule vers le fond (le minimum d'erreur)."

---

## 5. üèîÔ∏è D√©riv√©e = Direction pour r√©duire l'erreur

### 5.1 L'analogie de la montagne dans le brouillard

Imaginez que vous √™tes en montagne, dans un **brouillard √©pais**, et vous voulez descendre dans la **vall√©e** (le minimum d'erreur). Vous ne voyez pas le paysage, mais vous pouvez sentir la **pente sous vos pieds**.

```
        ‚ï±‚ï≤
       ‚ï±  ‚ï≤             Vous √™tes ici
      ‚ï±    ‚ï≤                 ‚Üì
     ‚ï±      ‚ï≤    ‚ï±‚ï≤    ‚óè ‚îÄ‚îÄ‚Üí La pente descend vers la droite
    ‚ï±        ‚ï≤  ‚ï±  ‚ï≤       ‚Üí Donc allez √† droite !
   ‚ï±          ‚ï≤‚ï±    ‚ï≤
  ‚ï±                  ‚ï≤
 ‚ï±        ‚òÖ           ‚ï≤    ‚òÖ = Vall√©e (minimum d'erreur)
‚ï±                      ‚ï≤       C'est l√† qu'on veut aller !
```

**La d√©riv√©e, c'est la pente sous vos pieds** :
- Pente n√©gative (descend vers la droite) ‚Üí allez √† droite
- Pente positive (descend vers la gauche) ‚Üí allez √† gauche
- Pente = 0 ‚Üí vous √™tes au minimum !

### 5.2 La d√©riv√©e visuellement

```
   f(x) = x¬≤

   f(x)
    ‚îÇ
  25‚îÇ                          ‚óè
    ‚îÇ                      ‚ï±
  16‚îÇ                  ‚óè ‚ï±
    ‚îÇ               ‚ï±  ‚ï±
   9‚îÇ           ‚óè ‚ï±  pente = 2x = 6  (x=3)
    ‚îÇ        ‚ï±
   4‚îÇ     ‚óè‚ï±  pente = 2x = 4  (x=2)
    ‚îÇ   ‚ï±
   1‚îÇ ‚óè‚ï±  pente = 2x = 2  (x=1)
    ‚îÇ‚ï±
   0‚óè  pente = 0 ‚Üí MINIMUM !
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
    0  1  2  3  4  5

   La pente (d√©riv√©e) indique si la fonction monte ou descend.
   Au minimum (x=0), la pente = 0.
```

### 5.3 Exemples de d√©riv√©es courantes

| Fonction f(x) | D√©riv√©e f'(x) | Interpr√©tation |
|--------------|---------------|----------------|
| f(x) = c (constante) | f'(x) = 0 | Plat, pas de pente |
| f(x) = ax | f'(x) = a | Pente constante |
| f(x) = x¬≤ | f'(x) = 2x | Pente qui augmente avec x |
| f(x) = x¬≥ | f'(x) = 3x¬≤ | Pente qui augmente encore plus vite |
| f(x) = ax¬≤ + bx + c | f'(x) = 2ax + b | Parabole : un seul minimum |

> üí° **Conseil** : "Pas besoin de retenir toutes les r√®gles de d√©rivation. L'important est de comprendre que la d√©riv√©e nous dit **dans quelle direction** modifier un param√®tre pour **r√©duire** l'erreur."

### 5.4 La d√©riv√©e appliqu√©e √† la MSE

Pour la r√©gression lin√©aire `≈∑ = a*x + b`, on veut trouver les valeurs de `a` et `b` qui minimisent la MSE.

```
MSE(a, b) = (1/n) √ó Œ£(y·µ¢ - (a√óx·µ¢ + b))¬≤

D√©riv√©e par rapport √† a (comment a affecte l'erreur) :
‚àÇMSE/‚àÇa = -(2/n) √ó Œ£ x·µ¢ √ó (y·µ¢ - (a√óx·µ¢ + b))

D√©riv√©e par rapport √† b (comment b affecte l'erreur) :
‚àÇMSE/‚àÇb = -(2/n) √ó Œ£ (y·µ¢ - (a√óx·µ¢ + b))
```

> ‚ö†Ô∏è **Attention** : "On n'a pas besoin de r√©soudre ces √©quations √† la main ! La descente de gradient le fait automatiquement, it√©ration par it√©ration."

---

## 6. üöÄ La descente de gradient expliqu√©e pas √† pas

### 6.1 L'algorithme en fran√ßais

```
Algorithme : Descente de gradient
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Initialiser les param√®tres au hasard (a=0, b=0)
2. R√©p√©ter N fois :
   a. Calculer les pr√©dictions : ≈∑ = a√óx + b
   b. Calculer l'erreur (MSE)
   c. Calculer les gradients (d√©riv√©es) :
      - gradient_a = comment a doit changer
      - gradient_b = comment b doit changer
   d. Mettre √† jour les param√®tres :
      - a = a - learning_rate √ó gradient_a
      - b = b - learning_rate √ó gradient_b
3. Retourner a et b optimis√©s
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### 6.2 Le Learning Rate

Le **learning rate** (taux d'apprentissage) contr√¥le la **taille des pas** que l'on fait.

```
Learning rate trop GRAND :          Learning rate trop PETIT :

    ‚ï±‚ï≤                                  ‚ï±‚ï≤
   ‚ï±  ‚ï≤    ‚ï±‚ï≤                         ‚ï±  ‚ï≤
  ‚óè    ‚ï≤  ‚ï±  ‚óè  ‚Üê On oscille !      ‚ï±    ‚ï≤
 ‚ï±      ‚ï≤‚ï±    ‚ï≤                     ‚óè      ‚ï≤
‚ï±   ‚òÖ         ‚ï≤‚ï±                   ‚ï±‚óè       ‚ï≤
                                  ‚ï±  ‚óè       ‚ï≤
‚Üí Ne converge jamais !           ‚ï±   ‚óè‚óè‚óè‚òÖ     ‚ï≤
                                      ‚Üë
                              Trop lent (10000 it√©rations)

Learning rate BON :

    ‚ï±‚ï≤
   ‚ï±  ‚ï≤
  ‚óè    ‚ï≤
 ‚ï± ‚óè    ‚ï≤
‚ï±   ‚óè‚òÖ   ‚ï≤

‚Üí Converge en quelques it√©rations !
```

| Learning Rate | Effet | Risque |
|:------------:|-------|--------|
| Trop grand (> 0.1) | Grands pas, rapide | Oscillation, divergence |
| Bon (0.001 - 0.01) | Convergence stable | ‚Äî |
| Trop petit (< 0.00001) | Tr√®s petits pas | Extr√™mement lent |

### 6.3 Descente de gradient from scratch (~30 lignes)

```python
import numpy as np

def mse(y_true, y_pred):
    """Mean Squared Error."""
    return np.mean((y_true - y_pred) ** 2)

def descente_gradient(X, y, learning_rate=0.01, n_iterations=1000):
    """
    Descente de gradient pour r√©gression lin√©aire (y = a*x + b).

    Args:
        X: features (1D)
        y: target
        learning_rate: taux d'apprentissage
        n_iterations: nombre d'it√©rations

    Returns:
        a, b: param√®tres optimis√©s
        historique: liste des MSE √† chaque it√©ration
    """
    n = len(X)

    # 1. Initialisation √† z√©ro
    a = 0.0
    b = 0.0
    historique = []

    for i in range(n_iterations):
        # 2. Pr√©dictions avec les param√®tres actuels
        y_pred = a * X + b

        # 3. Calcul de l'erreur
        erreur = mse(y, y_pred)
        historique.append(erreur)

        # 4. Calcul des gradients (d√©riv√©es partielles)
        gradient_a = -(2 / n) * np.sum(X * (y - y_pred))
        gradient_b = -(2 / n) * np.sum(y - y_pred)

        # 5. Mise √† jour des param√®tres (on descend la pente)
        a = a - learning_rate * gradient_a
        b = b - learning_rate * gradient_b

        # Afficher la progression
        if i % 200 == 0:
            print(f"It√©ration {i:>4} | MSE = {erreur:>10.4f} | a = {a:.4f} | b = {b:.4f}")

    return a, b, historique
```

### 6.4 Tester la descente de gradient

```python
# Donn√©es
surfaces = np.array([30, 50, 70, 90, 110], dtype=float)
prix = np.array([150, 220, 310, 380, 470], dtype=float)

# Normaliser pour faciliter la convergence
X_mean, X_std = surfaces.mean(), surfaces.std()
y_mean, y_std = prix.mean(), prix.std()

X_norm = (surfaces - X_mean) / X_std
y_norm = (prix - y_mean) / y_std

# Lancer la descente de gradient
a, b, historique = descente_gradient(X_norm, y_norm, learning_rate=0.1, n_iterations=1000)

print(f"\nR√©sultat final (normalis√©) : y = {a:.4f} * x + {b:.4f}")

# Convertir les param√®tres dans l'√©chelle originale
a_original = a * (y_std / X_std)
b_original = y_mean + b * y_std - a_original * X_mean
print(f"R√©sultat final (original) : prix = {a_original:.2f} √ó surface + {b_original:.2f}")

# Visualiser la convergence
plt.figure(figsize=(10, 5))
plt.plot(historique, 'b-', linewidth=2)
plt.xlabel("It√©rations")
plt.ylabel("MSE")
plt.title("Convergence de la descente de gradient")
plt.grid(True, alpha=0.3)
plt.show()
```

### 6.5 Visualiser l'apprentissage √©tape par √©tape

```python
def descente_gradient_visual(X, y, learning_rate=0.1, n_iterations=1000, save_every=100):
    """Descente de gradient avec sauvegarde des √©tapes interm√©diaires."""
    n = len(X)
    a, b = 0.0, 0.0
    etapes = []

    for i in range(n_iterations):
        y_pred = a * X + b
        gradient_a = -(2 / n) * np.sum(X * (y - y_pred))
        gradient_b = -(2 / n) * np.sum(y - y_pred)
        a -= learning_rate * gradient_a
        b -= learning_rate * gradient_b

        if i % save_every == 0:
            etapes.append((i, a, b, mse(y, y_pred)))

    return a, b, etapes

# Lancer
a_final, b_final, etapes = descente_gradient_visual(X_norm, y_norm)

# Visualiser les droites successives
fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for idx, (iteration, a, b, err) in enumerate(etapes[:6]):
    ax = axes[idx]
    ax.scatter(X_norm, y_norm, c='steelblue', edgecolors='black', zorder=5)
    x_line = np.linspace(X_norm.min() - 0.5, X_norm.max() + 0.5, 100)
    ax.plot(x_line, a * x_line + b, 'r-', linewidth=2)
    ax.set_title(f"It√©ration {iteration} | MSE = {err:.4f}")
    ax.set_xlabel("Surface (normalis√©e)")
    ax.set_ylabel("Prix (normalis√©)")
    ax.grid(True, alpha=0.3)

plt.suptitle("La droite s'ajuste au fil des it√©rations", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

### 6.6 Impact du learning rate

```python
# Comparer diff√©rents learning rates
learning_rates = [0.001, 0.01, 0.1, 0.5]

plt.figure(figsize=(12, 6))

for lr in learning_rates:
    _, _, hist = descente_gradient(X_norm, y_norm, learning_rate=lr, n_iterations=500)
    plt.plot(hist, linewidth=2, label=f'lr = {lr}')

plt.xlabel("It√©rations")
plt.ylabel("MSE")
plt.title("Impact du learning rate sur la convergence")
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale('log')
plt.show()
```

> ‚ö†Ô∏è **Attention** : "Toujours **normaliser** les donn√©es avant d'appliquer la descente de gradient ! Sans normalisation, les gradients peuvent √™tre √©normes pour certaines features et minuscules pour d'autres, ce qui emp√™che la convergence."

---

## 7. üõ†Ô∏è R√©gression lin√©aire from scratch puis avec scikit-learn

### 7.1 R√©gression lin√©aire multi-variables from scratch

```python
class RegressionLineaireFromScratch:
    """R√©gression lin√©aire par descente de gradient (multi-variables)."""

    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.lr = learning_rate
        self.n_iter = n_iterations
        self.weights = None
        self.bias = None
        self.historique = []

    def fit(self, X, y):
        """Entra√Æner le mod√®le sur les donn√©es."""
        n_samples, n_features = X.shape

        # Initialisation des param√®tres √† z√©ro
        self.weights = np.zeros(n_features)
        self.bias = 0.0

        for i in range(self.n_iter):
            # Pr√©dictions : y_pred = X @ w + b
            y_pred = X @ self.weights + self.bias

            # Gradients (d√©riv√©es partielles de la MSE)
            dw = -(2 / n_samples) * (X.T @ (y - y_pred))
            db = -(2 / n_samples) * np.sum(y - y_pred)

            # Mise √† jour des param√®tres
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

            # Sauvegarder l'historique de l'erreur
            self.historique.append(np.mean((y - y_pred) ** 2))

        return self

    def predict(self, X):
        """Pr√©dire les valeurs pour de nouvelles donn√©es."""
        return X @ self.weights + self.bias

    def score(self, X, y):
        """Calculer le R¬≤ (coefficient de d√©termination)."""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)      # Somme des r√©sidus au carr√©
        ss_tot = np.sum((y - np.mean(y)) ** 2)   # Variance totale
        return 1 - (ss_res / ss_tot)
```

### 7.2 Tester notre r√©gression from scratch

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Charger les donn√©es
housing = fetch_california_housing()
X, y = housing.data, housing.target

# S√©parer train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Normaliser (CRUCIAL pour la descente de gradient)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entra√Æner notre mod√®le
model = RegressionLineaireFromScratch(learning_rate=0.01, n_iterations=1000)
model.fit(X_train_scaled, y_train)

# √âvaluer
r2_train = model.score(X_train_scaled, y_train)
r2_test = model.score(X_test_scaled, y_test)
print(f"R¬≤ train : {r2_train:.4f}")
print(f"R¬≤ test  : {r2_test:.4f}")

# Visualiser la convergence
plt.figure(figsize=(10, 5))
plt.plot(model.historique, 'b-', linewidth=1)
plt.xlabel("It√©rations")
plt.ylabel("MSE")
plt.title("Convergence de notre r√©gression from scratch")
plt.grid(True, alpha=0.3)
plt.show()
```

### 7.3 Avec scikit-learn (3 lignes !)

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Entra√Æner (scikit-learn utilise une solution analytique, pas la descente de gradient)
model_sklearn = LinearRegression()
model_sklearn.fit(X_train, y_train)  # Pas besoin de normaliser !

# Pr√©dire
y_pred = model_sklearn.predict(X_test)

# √âvaluer
print(f"R¬≤ : {r2_score(y_test, y_pred):.4f}")
print(f"RMSE : {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
print(f"\nCoefficients : {model_sklearn.coef_}")
print(f"Intercept : {model_sklearn.intercept_:.4f}")
```

### 7.4 Comparaison : from scratch vs scikit-learn

| Aspect | From scratch | scikit-learn |
|--------|-------------|--------------|
| **Lignes de code** | ~30 | ~3 |
| **Normalisation** | Obligatoire | Optionnelle |
| **M√©thode** | Descente de gradient (it√©rative) | Solution analytique (OLS) |
| **Vitesse** | Plus lent | Tr√®s rapide |
| **Hyperparam√®tres** | learning_rate, n_iterations | Aucun |
| **Valeur p√©dagogique** | Excellente | Faible |
| **En production** | Non | Oui |

> üí° **Conseil** : "Coder from scratch est **essentiel pour comprendre**. Mais en pratique, utilisez **toujours** scikit-learn. La biblioth√®que est optimis√©e, test√©e et maintenue par une communaut√© de d√©veloppeurs."

### 7.5 Interpr√©ter les coefficients

```python
import pandas as pd

# Quels facteurs influencent le plus le prix ?
feature_names = housing.feature_names
coefficients = model_sklearn.coef_

# Trier par importance (valeur absolue)
importance = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients,
    'Importance absolue': np.abs(coefficients)
}).sort_values('Importance absolue', ascending=False)

print(importance.to_string(index=False))

# Visualiser
plt.figure(figsize=(10, 6))
colors = ['#e74c3c' if c > 0 else '#3498db' for c in importance['Coefficient']]
plt.barh(importance['Feature'], importance['Coefficient'], color=colors)
plt.xlabel("Coefficient")
plt.title("Importance des features dans la r√©gression lin√©aire")
plt.axvline(x=0, color='black', linewidth=0.5)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

---

## 8. üõ°Ô∏è R√©gularisation intuitive : Ridge et Lasso

### 8.1 Le probl√®me de l'overfitting en r√©gression

Quand un mod√®le a **trop de libert√©** (trop de features, coefficients trop grands), il peut **m√©moriser le bruit** au lieu d'apprendre les vrais patterns.

```
Sans r√©gularisation (overfitting) :     Avec r√©gularisation :

   y‚îÇ    ‚óè                               y‚îÇ    ‚óè
    ‚îÇ   ‚ï±‚ï≤  ‚óè                             ‚îÇ   ‚ï±  ‚óè
    ‚îÇ  ‚ï±  ‚ï≤‚ï±‚ï≤   ‚óè ‚Üê‚îÄ‚îÄ La courbe           ‚îÇ  ‚ï±     ‚óè  ‚Üê‚îÄ‚îÄ La droite est
    ‚îÇ ‚ï±      ‚ï≤ ‚ï±      passe par             ‚îÇ ‚ï±    ‚óè       plus simple
    ‚îÇ‚ï±  ‚óè     ‚óè       tous les points       ‚îÇ‚ï±  ‚óè          et g√©n√©ralise mieux
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
    Coefficients √©normes                    Coefficients mod√©r√©s
```

### 8.2 L'id√©e de la r√©gularisation

La r√©gularisation ajoute une **p√©nalit√©** sur la taille des coefficients :

```
Sans r√©gularisation :
   Objectif = minimiser MSE

Avec r√©gularisation :
   Objectif = minimiser MSE + Œª √ó (taille des coefficients)

   Œª (lambda, appel√© alpha dans scikit-learn) :
   Œª = 0    ‚Üí pas de r√©gularisation (r√©gression classique)
   Œª petit  ‚Üí l√©g√®re r√©gularisation
   Œª grand  ‚Üí forte r√©gularisation (coefficients ‚Üí 0)
   Œª ‚Üí ‚àû   ‚Üí tous les coefficients ‚Üí 0 (mod√®le trivial)
```

> üí° **Conseil** : "La r√©gularisation, c'est comme dire au mod√®le : 'Trouve de bons coefficients, MAIS garde-les aussi petits que possible.' √áa l'emp√™che de devenir trop complexe."

### 8.3 Ridge (L2) vs Lasso (L1)

| Aspect | Ridge (L2) | Lasso (L1) |
|--------|-----------|-----------|
| **P√©nalit√©** | Œ£(w·µ¢¬≤) ‚Äî somme des carr√©s | Œ£\|w·µ¢\| ‚Äî somme des valeurs absolues |
| **Effet** | R√©duit tous les coefficients (aucun √† z√©ro) | Met certains coefficients **exactement √† 0** |
| **S√©lection de features** | Non (garde toutes les features) | Oui (supprime les features inutiles) |
| **Quand l'utiliser** | Toutes les features sont potentiellement utiles | Beaucoup de features, certaines inutiles |
| **Analogie** | "Baisse le volume de tous les instruments" | "Coupe certaines pistes audio" |

```
Ridge : w = [0.8, 0.3, 0.5, 0.1, 0.2]  ‚Üí Tous non-nuls, mais plus petits
Lasso : w = [1.2, 0.0, 0.7, 0.0, 0.0]  ‚Üí Certains exactement √† 0 !
```

### 8.4 En pratique avec scikit-learn

```python
from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.preprocessing import StandardScaler

# Normaliser (important pour la r√©gularisation)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Comparer les mod√®les
models = {
    'R√©gression lin√©aire': LinearRegression(),
    'Ridge (Œ±=0.1)': Ridge(alpha=0.1),
    'Ridge (Œ±=1.0)': Ridge(alpha=1.0),
    'Ridge (Œ±=10.0)': Ridge(alpha=10.0),
    'Lasso (Œ±=0.01)': Lasso(alpha=0.01),
    'Lasso (Œ±=0.1)': Lasso(alpha=0.1),
    'Lasso (Œ±=1.0)': Lasso(alpha=1.0),
}

print(f"{'Mod√®le':<25} {'R¬≤ Train':>10} {'R¬≤ Test':>10} {'Coefs ‚âà 0':>12}")
print("-" * 60)

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    r2_train = model.score(X_train_scaled, y_train)
    r2_test = model.score(X_test_scaled, y_test)
    n_zeros = np.sum(np.abs(model.coef_) < 0.001)
    print(f"{name:<25} {r2_train:>10.4f} {r2_test:>10.4f} {n_zeros:>12}")
```

### 8.5 Visualiser l'effet de la r√©gularisation

```python
# Impact de alpha sur les coefficients Ridge
alphas = np.logspace(-2, 4, 100)
coefs_ridge = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    coefs_ridge.append(ridge.coef_)

coefs_ridge = np.array(coefs_ridge)

plt.figure(figsize=(12, 6))
for i in range(coefs_ridge.shape[1]):
    plt.plot(alphas, coefs_ridge[:, i], linewidth=2, label=housing.feature_names[i])

plt.xscale('log')
plt.xlabel("Alpha (force de r√©gularisation)")
plt.ylabel("Valeur du coefficient")
plt.title("Ridge : les coefficients diminuent quand alpha augmente")
plt.legend(loc='best', fontsize=8)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='black', linewidth=0.5)
plt.show()
```

### 8.6 Choisir le bon alpha avec Cross-Validation

```python
from sklearn.linear_model import RidgeCV, LassoCV

# Ridge avec cross-validation automatique
ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0], cv=5)
ridge_cv.fit(X_train_scaled, y_train)
print(f"Meilleur alpha Ridge : {ridge_cv.alpha_}")
print(f"R¬≤ test : {ridge_cv.score(X_test_scaled, y_test):.4f}")

# Lasso avec cross-validation automatique
lasso_cv = LassoCV(alphas=[0.001, 0.01, 0.1, 1.0], cv=5)
lasso_cv.fit(X_train_scaled, y_train)
print(f"\nMeilleur alpha Lasso : {lasso_cv.alpha_}")
print(f"R¬≤ test : {lasso_cv.score(X_test_scaled, y_test):.4f}")
print(f"Features √©limin√©es : {np.sum(lasso_cv.coef_ == 0)} / {X.shape[1]}")
```

### 8.7 Elastic Net : le meilleur des deux mondes

```python
from sklearn.linear_model import ElasticNet, ElasticNetCV

# Elastic Net combine Ridge et Lasso
# l1_ratio = 0 ‚Üí Ridge pur, l1_ratio = 1 ‚Üí Lasso pur
elastic_cv = ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9], cv=5)
elastic_cv.fit(X_train_scaled, y_train)
print(f"Meilleur l1_ratio : {elastic_cv.l1_ratio_}")
print(f"Meilleur alpha : {elastic_cv.alpha_:.4f}")
print(f"R¬≤ test : {elastic_cv.score(X_test_scaled, y_test):.4f}")
```

### 8.8 Guide de choix

| Situation | M√©thode recommand√©e |
|-----------|-------------------|
| Peu de features, pas d'overfitting | **R√©gression lin√©aire** simple |
| Beaucoup de features, toutes utiles | **Ridge** |
| Beaucoup de features, certaines inutiles | **Lasso** |
| Pas s√ªr, veut un compromis | **Elastic Net** |
| Besoin de s√©lection automatique de features | **Lasso** ou **Elastic Net** |

> ‚ö†Ô∏è **Attention** : "La r√©gularisation n√©cessite des donn√©es **normalis√©es** (StandardScaler). Sinon, les features avec de grandes valeurs seront plus p√©nalis√©es que les autres, ce qui fausse le r√©sultat."

---

## üéØ Points cl√©s √† retenir

1. **L'apprentissage ML** = deviner, mesurer l'erreur, corriger ‚Äî en boucle
2. **Une fonction** transforme des entr√©es en sorties ; en ML, le mod√®le est une fonction √† optimiser
3. **La r√©gression lin√©aire** cherche la droite `y = ax + b` qui minimise l'erreur
4. **La MSE** (Mean Squared Error) est la m√©trique d'erreur standard : elle p√©nalise les grosses erreurs
5. **La MAE** est plus robuste aux outliers, le **RMSE** est interpr√©table dans l'unit√© de y
6. **La d√©riv√©e** indique la direction de descente ‚Äî comme la pente sous vos pieds dans le brouillard
7. **La descente de gradient** ajuste les param√®tres pas √† pas en suivant la pente n√©gative
8. **Le learning rate** contr√¥le la taille des pas : trop grand ‚Üí diverge, trop petit ‚Üí trop lent
9. **Toujours normaliser** les donn√©es avant la descente de gradient et la r√©gularisation
10. **Ridge (L2)** r√©duit tous les coefficients, **Lasso (L1)** met certains √† z√©ro (s√©lection de features)

---

## ‚úÖ Checklist de validation

- [ ] Je comprends l'analogie "deviner, mesurer, corriger" de l'apprentissage
- [ ] Je sais expliquer la r√©gression lin√©aire `y = ax + b` intuitivement
- [ ] Je sais calculer la MSE, la MAE et le RMSE √† la main et en Python
- [ ] Je comprends pourquoi la MSE met au carr√© les erreurs (3 raisons)
- [ ] Je sais expliquer la d√©riv√©e comme une "direction d'am√©lioration"
- [ ] Je comprends l'analogie de la montagne dans le brouillard
- [ ] Je sais impl√©menter la descente de gradient from scratch (~30 lignes)
- [ ] Je comprends l'impact du learning rate sur la convergence
- [ ] Je sais coder une r√©gression lin√©aire from scratch et avec scikit-learn
- [ ] Je sais interpr√©ter les coefficients d'une r√©gression lin√©aire
- [ ] Je comprends l'intuition derri√®re Ridge (L2) et Lasso (L1)
- [ ] Je sais quand utiliser la r√©gularisation et comment choisir alpha
- [ ] J'ai r√©alis√© le notebook "Gradient Descent expliqu√© √† ma grand-m√®re"

---

**Pr√©c√©dent** : [Chapitre 3 : Vecteurs, Matrices et KNN](03-vecteurs-matrices-knn.md)

**Suivant** : [Chapitre 5 : Probabilit√©s pour ne plus avoir Peur de l'Incertitude](05-probabilites-incertitude.md)
