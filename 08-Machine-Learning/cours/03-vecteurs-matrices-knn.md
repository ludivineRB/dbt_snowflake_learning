# Chapitre 3 : Vecteurs, Matrices et KNN ‚Äî Les Maths comme Outils

## üéØ Objectifs

- Comprendre ce qu'est un vecteur et pourquoi c'est la base du ML
- Savoir repr√©senter des donn√©es sous forme de vecteurs et de matrices
- Calculer des distances entre points (euclidienne, Manhattan)
- Faire le lien entre matrices et DataFrames pandas
- Impl√©menter l'algorithme KNN from scratch puis avec scikit-learn
- Visualiser des clusters et comprendre la notion de similarit√©

---

## 1. üß† Intuition : Un vecteur = une fiche d'identit√© num√©rique

### 1.1 L'id√©e fondamentale

En Machine Learning, **tout doit √™tre transform√© en nombres**. Un vecteur, c'est simplement une **liste ordonn√©e de nombres** qui d√©crit quelque chose.

> üí° **Conseil** : "Pensez √† un vecteur comme une **fiche d'identit√© num√©rique**. Chaque nombre capture une caract√©ristique mesurable d'un objet, d'une personne ou d'un √©v√©nement."

### 1.2 Exemple concret : d√©crire un client

Imaginons que vous travaillez dans une banque. Comment d√©crire un client avec des nombres ?

```
Client Alice :
  - √Çge : 35 ans
  - Salaire annuel : 45 000 ‚Ç¨
  - Anciennet√© : 3 ans
  - Nombre de produits : 2

‚Üí Vecteur Alice = [35, 45000, 3, 2]
```

```
Client Bob :
  - √Çge : 28 ans
  - Salaire annuel : 32 000 ‚Ç¨
  - Anciennet√© : 1 an
  - Nombre de produits : 1

‚Üí Vecteur Bob = [28, 32000, 1, 1]
```

Chaque client est maintenant un **point dans un espace √† 4 dimensions**. On peut les comparer, les regrouper, mesurer leur ressemblance.

### 1.3 En Python avec NumPy

```python
import numpy as np

# Chaque client est un vecteur
alice = np.array([35, 45000, 3, 2])
bob = np.array([28, 32000, 1, 1])
charlie = np.array([42, 60000, 10, 4])

print(f"Vecteur Alice : {alice}")
print(f"Dimension : {alice.shape[0]} caract√©ristiques")
print(f"Type : {type(alice)}")
```

### 1.4 Pourquoi des vecteurs et pas juste des listes Python ?

| Aspect | Liste Python | Vecteur NumPy |
|--------|-------------|---------------|
| **Op√©rations math√©matiques** | Impossible directement | Natives et rapides |
| **Performance** | Lente (boucles) | Rapide (op√©rations vectoris√©es) |
| **M√©moire** | Plus gourmande | Optimis√©e (types fixes) |
| **Utilisation ML** | Inadapt√©e | Standard du domaine |

```python
# Avec une liste Python (ne marche PAS comme attendu)
liste_a = [1, 2, 3]
liste_b = [4, 5, 6]
print(liste_a + liste_b)  # [1, 2, 3, 4, 5, 6] ‚Üí concat√©nation !

# Avec NumPy (op√©rations √©l√©ment par √©l√©ment)
vec_a = np.array([1, 2, 3])
vec_b = np.array([4, 5, 6])
print(vec_a + vec_b)  # [5, 7, 9] ‚Üí addition vectorielle !
print(vec_a * 2)      # [2, 4, 6] ‚Üí multiplication par un scalaire
print(vec_a * vec_b)  # [4, 10, 18] ‚Üí multiplication √©l√©ment par √©l√©ment
```

> ‚ö†Ô∏è **Attention** : "En Python pur, `+` concat√®ne les listes. Avec NumPy, `+` additionne √©l√©ment par √©l√©ment. C'est un pi√®ge classique pour les d√©butants."

---

## 2. üìä Visualisation : Points dans l'espace

### 2.1 Cas 2D : deux caract√©ristiques

Quand un vecteur n'a que **2 dimensions**, on peut le visualiser comme un point sur un graphique classique (x, y).

```python
import matplotlib.pyplot as plt
import numpy as np

# Des clients d√©crits par (√¢ge, salaire_en_milliers)
clients = np.array([
    [25, 30],  # Client 1
    [35, 45],  # Client 2
    [45, 60],  # Client 3
    [23, 28],  # Client 4
    [50, 80],  # Client 5
    [30, 35],  # Client 6
    [40, 55],  # Client 7
    [22, 25],  # Client 8
])

labels = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8']

plt.figure(figsize=(10, 7))
plt.scatter(clients[:, 0], clients[:, 1], s=100, c='steelblue', edgecolors='black')

for i, label in enumerate(labels):
    plt.annotate(label, (clients[i, 0] + 0.5, clients[i, 1] + 1))

plt.xlabel("√Çge")
plt.ylabel("Salaire (k‚Ç¨)")
plt.title("Chaque client = un point dans l'espace 2D")
plt.grid(True, alpha=0.3)
plt.show()
```

```
    Salaire (k‚Ç¨)
    80 ‚îÇ                              ‚óè C5
       ‚îÇ
    60 ‚îÇ                    ‚óè C3
    55 ‚îÇ                 ‚óè C7
    45 ‚îÇ          ‚óè C2
    35 ‚îÇ       ‚óè C6
    30 ‚îÇ    ‚óè C1
    28 ‚îÇ   ‚óè C4
    25 ‚îÇ  ‚óè C8
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √Çge
        22  25  30  35  40  45  50
```

### 2.2 Cas 3D : trois caract√©ristiques

Avec 3 dimensions, on peut encore visualiser (x, y, z) :

```python
from mpl_toolkits.mplot3d import Axes3D

# Clients d√©crits par (√¢ge, salaire_k, anciennet√©)
clients_3d = np.array([
    [25, 30, 1],
    [35, 45, 5],
    [45, 60, 12],
    [28, 32, 2],
    [50, 80, 20],
])

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(clients_3d[:, 0], clients_3d[:, 1], clients_3d[:, 2],
           s=100, c='steelblue', edgecolors='black')

ax.set_xlabel("√Çge")
ax.set_ylabel("Salaire (k‚Ç¨)")
ax.set_zlabel("Anciennet√© (ans)")
ax.set_title("Clients dans un espace 3D")
plt.show()
```

### 2.3 Au-del√† de 3 dimensions

En ML, on travaille souvent avec **des dizaines ou des centaines de dimensions**. On ne peut plus visualiser directement, mais les maths fonctionnent exactement pareil !

| Dimensions | Visualisable ? | Exemple |
|-----------|---------------|---------|
| 2 | Oui (graphique x, y) | √Çge + salaire |
| 3 | Oui (graphique 3D) | √Çge + salaire + anciennet√© |
| 4 √† 10 | Non, mais r√©duction possible (PCA, t-SNE) | Fiche client compl√®te |
| 100+ | Non, mais les algorithmes g√®rent | Pixels d'une image |

> üí° **Conseil** : "Ne vous inqui√©tez pas de ne pas pouvoir visualiser 50 dimensions. Les algorithmes de ML travaillent dans ces espaces de haute dimension sans aucun probl√®me. La distance euclidienne fonctionne en 2D, 3D ou 1000D exactement de la m√™me mani√®re."

---

## 3. üìè La distance entre deux points = ¬´ ressemblance ¬ª

### 3.1 L'intuition

La question centrale du ML est souvent : **est-ce que ces deux observations se ressemblent ?**

Pour y r√©pondre, on mesure la **distance** entre deux vecteurs. Plus la distance est **petite**, plus les observations sont **similaires**.

```
    Similaires                          Diff√©rents
    ‚óè ‚óè                                ‚óè              ‚óè
    (distance faible)                  (distance √©lev√©e)
```

### 3.2 Distance euclidienne

C'est la distance "en ligne droite" ‚Äî celle que vous connaissez d√©j√† !

**En 2D (th√©or√®me de Pythagore)** :

```
    B (x‚ÇÇ, y‚ÇÇ)
    ‚îÇ‚ï≤
    ‚îÇ  ‚ï≤  d = distance
    ‚îÇ    ‚ï≤
    ‚îÇ______‚ï≤
    A (x‚ÇÅ, y‚ÇÅ)

    d = ‚àö[(x‚ÇÇ - x‚ÇÅ)¬≤ + (y‚ÇÇ - y‚ÇÅ)¬≤]
```

**Exemple concret** :

```
Client A = (√¢ge: 30, salaire: 40k)
Client B = (√¢ge: 35, salaire: 50k)

d(A, B) = ‚àö[(35 - 30)¬≤ + (50 - 40)¬≤]
        = ‚àö[25 + 100]
        = ‚àö125
        ‚âà 11.18
```

**G√©n√©ralisation √† N dimensions** :

```
d(A, B) = ‚àö[(a‚ÇÅ - b‚ÇÅ)¬≤ + (a‚ÇÇ - b‚ÇÇ)¬≤ + ... + (a‚Çô - b‚Çô)¬≤]
```

La formule est identique, on additionne simplement plus de termes.

```python
import numpy as np

def distance_euclidienne(a, b):
    """Calcule la distance euclidienne entre deux vecteurs."""
    return np.sqrt(np.sum((a - b) ** 2))

# Clients
alice = np.array([35, 45000, 3])
bob = np.array([28, 32000, 1])
charlie = np.array([34, 44000, 4])

# Distances
d_alice_bob = distance_euclidienne(alice, bob)
d_alice_charlie = distance_euclidienne(alice, charlie)

print(f"Distance Alice-Bob     : {d_alice_bob:.2f}")
print(f"Distance Alice-Charlie : {d_alice_charlie:.2f}")
print(f"\n‚Üí Alice est plus proche de Charlie que de Bob")

# Avec NumPy directement (raccourci)
d = np.linalg.norm(alice - bob)
print(f"\nAvec np.linalg.norm : {d:.2f}")
```

### 3.3 Distance de Manhattan

Aussi appel√©e "distance L1" ou "distance du taxi" ‚Äî c'est la distance quand on ne peut se d√©placer que **en ligne droite** (comme dans les rues de Manhattan).

```
    B ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê
                 ‚îÇ
    Manhattan    ‚îÇ  (on suit les rues)
                 ‚îÇ
    A ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îò

    vs.

    B
     ‚ï≤
      ‚ï≤  Euclidienne (en diagonale)
       ‚ï≤
        A
```

**Formule** :

```
d_manhattan(A, B) = |a‚ÇÅ - b‚ÇÅ| + |a‚ÇÇ - b‚ÇÇ| + ... + |a‚Çô - b‚Çô|
```

On somme les **valeurs absolues** des diff√©rences, au lieu de les mettre au carr√©.

```python
def distance_manhattan(a, b):
    """Calcule la distance de Manhattan entre deux vecteurs."""
    return np.sum(np.abs(a - b))

# Comparaison
alice = np.array([35, 45, 3])
bob = np.array([28, 32, 1])

d_eucl = distance_euclidienne(alice, bob)
d_manh = distance_manhattan(alice, bob)

print(f"Distance euclidienne : {d_eucl:.2f}")
print(f"Distance de Manhattan : {d_manh:.2f}")
```

### 3.4 Comparaison des distances

| Distance | Formule | Quand l'utiliser | Sensibilit√© outliers |
|----------|---------|-----------------|---------------------|
| **Euclidienne** (L2) | ‚àö(Œ£(a·µ¢ - b·µ¢)¬≤) | Cas g√©n√©ral, donn√©es continues | Haute (carr√© amplifie) |
| **Manhattan** (L1) | Œ£\|a·µ¢ - b·µ¢\| | Donn√©es avec outliers, features ind√©pendantes | Plus robuste |
| **Minkowski** (Lp) | (Œ£\|a·µ¢ - b·µ¢\|·µñ)^(1/p) | G√©n√©ralisation (p=1 ‚Üí Manhattan, p=2 ‚Üí Euclidienne) | D√©pend de p |

> ‚ö†Ô∏è **Attention** : "Si vos features ont des √©chelles tr√®s diff√©rentes (√¢ge: 20-60, salaire: 20000-100000), la distance sera **domin√©e par le salaire**. Il faut **normaliser** les donn√©es avant de calculer des distances ! C'est une erreur tr√®s fr√©quente."

### 3.5 L'importance de la normalisation

```python
from sklearn.preprocessing import StandardScaler

# Donn√©es brutes (√©chelles tr√®s diff√©rentes)
clients = np.array([
    [35, 45000, 3],  # [√¢ge, salaire, anciennet√©]
    [28, 32000, 1],
    [34, 44000, 4],
])

# Sans normalisation : le salaire domine
d_brut = np.linalg.norm(clients[0] - clients[1])
print(f"Distance brute Alice-Bob : {d_brut:.2f}")  # ~13000 (domin√© par salaire)

# Avec normalisation (StandardScaler : moyenne=0, √©cart-type=1)
scaler = StandardScaler()
clients_norm = scaler.fit_transform(clients)
print(f"\nDonn√©es normalis√©es :\n{clients_norm}")

d_norm = np.linalg.norm(clients_norm[0] - clients_norm[1])
print(f"\nDistance normalis√©e Alice-Bob : {d_norm:.2f}")  # Toutes les features comptent
```

---

## 4. üóÇÔ∏è Matrices = Tableau de donn√©es

### 4.1 Qu'est-ce qu'une matrice ?

Une **matrice** est un tableau rectangulaire de nombres. Si un vecteur est une **ligne** (un seul client), une matrice c'est **toutes les lignes empil√©es** (tous les clients).

```
                     √¢ge   salaire   anciennet√©   produits
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   Alice (ligne 0) ‚îÇ  35 ‚îÇ  45000  ‚îÇ     3      ‚îÇ    2     ‚îÇ
   Bob   (ligne 1) ‚îÇ  28 ‚îÇ  32000  ‚îÇ     1      ‚îÇ    1     ‚îÇ
   Charlie (l. 2)  ‚îÇ  42 ‚îÇ  60000  ‚îÇ    10      ‚îÇ    4     ‚îÇ
   Diana (ligne 3) ‚îÇ  31 ‚îÇ  38000  ‚îÇ     2      ‚îÇ    2     ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ‚Üí Matrice de dimension (4, 4) = 4 lignes √ó 4 colonnes
   ‚Üí 4 √©chantillons (samples), 4 caract√©ristiques (features)
```

### 4.2 En NumPy

```python
import numpy as np

# Cr√©er une matrice
clients = np.array([
    [35, 45000, 3, 2],   # Alice
    [28, 32000, 1, 1],   # Bob
    [42, 60000, 10, 4],  # Charlie
    [31, 38000, 2, 2],   # Diana
])

print(f"Shape : {clients.shape}")         # (4, 4)
print(f"Nombre de clients : {clients.shape[0]}")    # 4
print(f"Nombre de features : {clients.shape[1]}")   # 4

# Acc√©der √† un client (une ligne)
print(f"\nAlice : {clients[0]}")          # [35, 45000, 3, 2]

# Acc√©der √† une feature (une colonne)
print(f"Tous les √¢ges : {clients[:, 0]}") # [35, 28, 42, 31]

# Acc√©der √† une valeur pr√©cise
print(f"Salaire de Bob : {clients[1, 1]}")  # 32000
```

### 4.3 Le lien avec pandas DataFrame

En pratique, on utilise souvent **pandas** pour manipuler les donn√©es, mais sous le capot, un DataFrame contient un tableau NumPy.

```python
import pandas as pd

# Cr√©er un DataFrame √† partir de la matrice
df = pd.DataFrame(
    clients,
    columns=['age', 'salaire', 'anciennete', 'nb_produits'],
    index=['Alice', 'Bob', 'Charlie', 'Diana']
)

print(df)
print(f"\nType des valeurs sous-jacentes : {type(df.values)}")

# Convertir DataFrame ‚Üí matrice NumPy
X = df.values  # ou df.to_numpy()
print(f"\nMatrice NumPy :\n{X}")
print(f"Shape : {X.shape}")
```

| Concept | NumPy (ndarray) | pandas (DataFrame) |
|---------|----------------|-------------------|
| **Structure** | Tableau num√©rique brut | Tableau avec noms de colonnes et index |
| **Utilisation** | Calculs math√©matiques rapides | Manipulation et exploration de donn√©es |
| **Acc√®s colonne** | `matrice[:, 0]` | `df['age']` ou `df.age` |
| **Acc√®s ligne** | `matrice[0]` | `df.iloc[0]` ou `df.loc['Alice']` |
| **Types** | Un seul type par matrice | Types mixtes par colonne |
| **Quand l'utiliser** | Calculs, algorithmes ML | Chargement, nettoyage, EDA |

### 4.4 La convention X et y en ML

En Machine Learning, on s√©pare toujours les **features** (X) de la **target** (y) :

```python
# X = matrice des features (ce qu'on observe)
# y = vecteur de la target (ce qu'on veut pr√©dire)

# Exemple : pr√©dire si un client va r√©silier (churn)
df = pd.DataFrame({
    'age': [35, 28, 42, 31, 55],
    'salaire': [45000, 32000, 60000, 38000, 75000],
    'anciennete': [3, 1, 10, 2, 15],
    'churn': [0, 1, 0, 1, 0]  # 0 = reste, 1 = part
})

# S√©parer features et target
X = df[['age', 'salaire', 'anciennete']].values  # Matrice (5, 3)
y = df['churn'].values                            # Vecteur (5,)

print(f"X shape : {X.shape}")  # (5, 3)
print(f"y shape : {y.shape}")  # (5,)
print(f"\nX :\n{X}")
print(f"\ny : {y}")
```

```
Convention ML :
                                        Target
    Features (X)                         (y)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 35  ‚îÇ  45000  ‚îÇ     3      ‚îÇ      ‚îÇ   0    ‚îÇ
‚îÇ 28  ‚îÇ  32000  ‚îÇ     1      ‚îÇ      ‚îÇ   1    ‚îÇ
‚îÇ 42  ‚îÇ  60000  ‚îÇ    10      ‚îÇ      ‚îÇ   0    ‚îÇ
‚îÇ 31  ‚îÇ  38000  ‚îÇ     2      ‚îÇ      ‚îÇ   1    ‚îÇ
‚îÇ 55  ‚îÇ  75000  ‚îÇ    15      ‚îÇ      ‚îÇ   0    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  n_samples √ó n_features              n_samples
```

> üí° **Conseil** : "En scikit-learn, **X** est toujours une matrice 2D (n_samples, n_features) et **y** est toujours un vecteur 1D (n_samples,). Respecter cette convention vous √©vitera beaucoup d'erreurs."

### 4.5 Op√©rations utiles sur les matrices

```python
# Op√©rations statistiques par colonne
print(f"Moyenne par feature : {np.mean(X, axis=0)}")
print(f"√âcart-type par feature : {np.std(X, axis=0)}")
print(f"Min par feature : {np.min(X, axis=0)}")
print(f"Max par feature : {np.max(X, axis=0)}")

# Transpos√©e : √©changer lignes et colonnes
print(f"\nX shape : {X.shape}")
print(f"X transpos√©e shape : {X.T.shape}")

# Produit matriciel (utile pour la r√©gression lin√©aire)
# y_pred = X @ w  (pr√©diction = features √ó poids)
w = np.array([0.1, 0.00005, 0.5])  # Poids fictifs
y_pred = X @ w  # Produit matriciel
print(f"\nPr√©dictions : {y_pred}")
```

---

## 5. ü§ñ Application : l'algorithme KNN (K-Nearest Neighbors)

### 5.1 L'intuition (sans formule)

L'id√©e de KNN est **incroyablement simple** :

> Pour pr√©dire la classe d'un nouveau point, on regarde les **K points les plus proches** (les "voisins") et on **vote √† la majorit√©**.

**Analogie** : vous d√©m√©nagez dans un nouveau quartier. Pour deviner si vous allez aimer le restaurant du coin, vous demandez l'avis de vos **5 voisins les plus proches**. Si 4 sur 5 disent "c'est bon", vous pr√©disez que c'est bon.

```
    Nouveau client (?)
         ‚îÇ
         ‚ñº
    On regarde les K=5 voisins les plus proches
         ‚îÇ
         ‚ñº
    3 voisins = "ne r√©silie pas" (‚óè)
    2 voisins = "r√©silie" (‚ñ≤)
         ‚îÇ
         ‚ñº
    Pr√©diction : "ne r√©silie pas" (majorit√© = ‚óè)
```

```
   Salaire
    ‚îÇ
    ‚îÇ  ‚ñ≤         ‚óè ‚óè
    ‚îÇ     ‚ñ≤    ‚óè
    ‚îÇ        ‚ùì‚Üê‚îÄ‚îÄ Nouveau point
    ‚îÇ     ‚óè      ‚óè
    ‚îÇ  ‚ñ≤     ‚óè
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √Çge

   ‚óè = Ne r√©silie pas    ‚ñ≤ = R√©silie

   Les 5 plus proches voisins de ‚ùì :
   ‚Üí 3 sont ‚óè et 2 sont ‚ñ≤
   ‚Üí Pr√©diction : ‚óè (ne r√©silie pas)
```

### 5.2 L'algorithme √©tape par √©tape

```
Algorithme KNN :
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Choisir K (nombre de voisins, ex: K=5)
2. Pour un nouveau point X_new :
   a. Calculer la distance entre X_new et TOUS les points d'entra√Ænement
   b. Trier les distances par ordre croissant
   c. S√©lectionner les K points les plus proches
   d. Compter les classes parmi ces K voisins
   e. Retourner la classe majoritaire
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### 5.3 KNN from scratch (~20 lignes)

```python
import numpy as np
from collections import Counter

class KNNFromScratch:
    def __init__(self, k=5):
        self.k = k

    def fit(self, X_train, y_train):
        """Stocker les donn√©es d'entra√Ænement (pas d'apprentissage r√©el)."""
        self.X_train = X_train
        self.y_train = y_train

    def predict(self, X_test):
        """Pr√©dire la classe pour chaque point de X_test."""
        return np.array([self._predict_one(x) for x in X_test])

    def _predict_one(self, x):
        """Pr√©dire la classe pour UN seul point."""
        # 1. Calculer les distances avec tous les points d'entra√Ænement
        distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))

        # 2. Trouver les indices des K plus proches
        k_indices = np.argsort(distances)[:self.k]

        # 3. R√©cup√©rer les classes de ces K voisins
        k_labels = self.y_train[k_indices]

        # 4. Voter : la classe la plus fr√©quente gagne
        most_common = Counter(k_labels).most_common(1)
        return most_common[0][0]
```

### 5.4 Tester notre KNN from scratch

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# G√©n√©rer un dataset simple
X, y = make_classification(
    n_samples=200,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    n_clusters_per_class=1,
    random_state=42
)

# S√©parer train / test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entra√Æner et pr√©dire avec notre KNN
knn = KNNFromScratch(k=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

# √âvaluer
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy de notre KNN : {accuracy:.2%}")
```

### 5.5 Visualiser les voisins

```python
import matplotlib.pyplot as plt

# Visualiser le r√©sultat
plt.figure(figsize=(12, 5))

# Subplot 1 : Donn√©es d'entra√Ænement
plt.subplot(1, 2, 1)
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],
            c='blue', label='Classe 0', alpha=0.6)
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],
            c='red', label='Classe 1', alpha=0.6)
plt.title("Donn√©es d'entra√Ænement")
plt.legend()
plt.grid(True, alpha=0.3)

# Subplot 2 : Pr√©dictions sur le test
plt.subplot(1, 2, 2)
correct = y_pred == y_test
plt.scatter(X_test[correct, 0], X_test[correct, 1],
            c='green', label='Correct', marker='o', s=60)
plt.scatter(X_test[~correct, 0], X_test[~correct, 1],
            c='red', label='Erreur', marker='x', s=100)
plt.title(f"Pr√©dictions (accuracy = {accuracy:.2%})")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### 5.6 Visualiser la fronti√®re de d√©cision

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(model, X, y, title="Fronti√®re de d√©cision KNN"):
    """Visualise la fronti√®re de d√©cision d'un mod√®le 2D."""
    h = 0.05  # Pas de la grille
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#AAAAFF', '#FFAAAA']))
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Classe 0', edgecolors='black')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Classe 1', edgecolors='black')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# Visualiser
knn = KNNFromScratch(k=5)
knn.fit(X_train, y_train)
plot_decision_boundary(knn, X_train, y_train, "KNN (K=5) ‚Äî Fronti√®re de d√©cision")
```

### 5.7 KNN avec scikit-learn

En pratique, on utilise la version optimis√©e de scikit-learn :

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# Cr√©er un pipeline : normalisation + KNN
pipeline = Pipeline([
    ('scaler', StandardScaler()),       # Normaliser les features
    ('knn', KNeighborsClassifier(n_neighbors=5))  # KNN avec K=5
])

# Entra√Æner
pipeline.fit(X_train, y_train)

# Pr√©dire
y_pred_sklearn = pipeline.predict(X_test)

# √âvaluer
print(f"Accuracy : {accuracy_score(y_test, y_pred_sklearn):.2%}")
print(f"\nRapport de classification :")
print(classification_report(y_test, y_pred_sklearn))
```

### 5.8 Comment choisir K ?

Le choix de **K** est crucial. Trop petit ‚Üí trop sensible au bruit. Trop grand ‚Üí trop liss√©.

```
K=1 : Chaque point est class√© selon      K=50 : Presque tout le dataset vote
      son voisin le PLUS proche                 ‚Üí fronti√®re trop liss√©e
      ‚Üí Fronti√®re tr√®s irr√©guli√®re              ‚Üí underfitting possible
      ‚Üí Overfitting possible

    ‚îÇ  ‚óè/‚ñ≤‚óè/‚ñ≤‚óè                              ‚îÇ  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚îÇ  ‚ñ≤/‚óè‚ñ≤/‚óè‚ñ≤  (zigzag)                    ‚îÇ  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
    ‚îÇ  ‚óè/‚ñ≤‚óè/‚ñ≤‚óè                              ‚îÇ  ‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤  (ligne droite)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

K=5 : Bon compromis ‚Äî fronti√®re
      lisse mais qui capture les
      patterns

    ‚îÇ  ‚óè‚óè‚óè‚óè‚óè
    ‚îÇ  ‚óè‚óè‚óè‚óè‚óè  (courbure raisonnable)
    ‚îÇ  ‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

```python
# Tester diff√©rentes valeurs de K
k_values = range(1, 31)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    acc = knn.score(X_test, y_test)
    accuracies.append(acc)

# Visualiser
plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracies, 'bo-', linewidth=2)
plt.xlabel("Nombre de voisins (K)")
plt.ylabel("Accuracy")
plt.title("Accuracy en fonction de K")
plt.axvline(x=k_values[np.argmax(accuracies)], color='red',
            linestyle='--', label=f'Meilleur K = {k_values[np.argmax(accuracies)]}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"Meilleur K = {k_values[np.argmax(accuracies)]} (accuracy = {max(accuracies):.2%})")
```

> üí° **Conseil** : "Utilisez toujours un **K impair** pour √©viter les √©galit√©s lors du vote. K=5 est un bon point de d√©part dans la plupart des cas."

### 5.9 Avantages et limites de KNN

| Avantage | Inconv√©nient |
|----------|-------------|
| Tr√®s simple √† comprendre | Lent sur de gros datasets (calcule toutes les distances) |
| Aucun apprentissage r√©el (lazy learner) | Sensible aux √©chelles (normalisation obligatoire) |
| Fonctionne bien en faible dimension | Mauvais en haute dimension (curse of dimensionality) |
| Pas d'hypoth√®se sur la forme des donn√©es | Sensible au bruit et aux outliers |
| Bon pour commencer / baseline | Le choix de K est critique |

> ‚ö†Ô∏è **Attention** : "KNN stocke **toutes** les donn√©es d'entra√Ænement en m√©moire et calcule les distances avec **chaque point** √† chaque pr√©diction. Sur un dataset de 1 million de lignes, √ßa peut √™tre tr√®s lent. Pour de gros volumes, pr√©f√©rez des algorithmes comme Random Forest ou les SVM."

---

## 6. üèãÔ∏è Exercices pratiques

### Exercice 1 : Calculer des distances

```python
# Calculez les distances euclidiennes et Manhattan entre ces clients :
clients = {
    'Alice':   np.array([30, 40000, 5]),
    'Bob':     np.array([25, 35000, 2]),
    'Charlie': np.array([45, 65000, 15]),
    'Diana':   np.array([32, 42000, 6]),
}

# TODO : Calculer la matrice de distances (toutes les paires)
# TODO : Qui est le plus proche d'Alice ?
# TODO : Qui est le plus √©loign√© de Bob ?
# TODO : Comparer les r√©sultats euclidien vs Manhattan
```

### Exercice 2 : KNN sur le dataset Iris

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Charger le dataset Iris
iris = load_iris()
X, y = iris.data, iris.target

# TODO : S√©parer train/test (80/20)
# TODO : Normaliser les donn√©es
# TODO : Entra√Æner un KNN avec K=5
# TODO : Calculer l'accuracy
# TODO : Tester K de 1 √† 20 et tracer la courbe d'accuracy
# TODO : Quel est le meilleur K ?
```

### Exercice 3 : Visualiser des clusters

```python
from sklearn.datasets import make_blobs

# G√©n√©rer des clusters
X, y = make_blobs(n_samples=300, centers=4, cluster_std=1.5, random_state=42)

# TODO : Visualiser les points color√©s par cluster
# TODO : Choisir un point au hasard et trouver ses 5 plus proches voisins
# TODO : Visualiser ce point et ses 5 voisins en surbrillance
# TODO : Utiliser KNN pour classifier ‚Äî quelle accuracy ?
```

### Exercice 4 : Impact de la normalisation

```python
# Donn√©es avec des √©chelles tr√®s diff√©rentes
X_raw = np.array([
    [25, 100000, 1],
    [30, 105000, 2],
    [60, 30000, 30],
    [65, 28000, 32],
    [35, 95000, 3],   # Point √† classer
])

y_raw = np.array([0, 0, 1, 1, -1])  # -1 = inconnu (√† pr√©dire)

# TODO : Calculer les distances SANS normalisation
# TODO : Trouver les 3 plus proches voisins (K=3) et pr√©dire
# TODO : Normaliser avec StandardScaler
# TODO : Recalculer les distances AVEC normalisation
# TODO : Trouver les 3 plus proches voisins et pr√©dire
# TODO : La pr√©diction change-t-elle ? Pourquoi ?
```

### Exercice 5 : Matrice de distances compl√®te

```python
from scipy.spatial.distance import cdist

# 10 points al√©atoires en 2D
np.random.seed(42)
points = np.random.randn(10, 2) * 5

# TODO : Calculer la matrice de distances euclidiennes (10x10)
#        Indice : utiliser scipy.spatial.distance.cdist
# TODO : Afficher cette matrice sous forme de heatmap avec seaborn
# TODO : Identifier les deux points les plus proches
# TODO : Identifier les deux points les plus √©loign√©s
```

---

## üéØ Points cl√©s √† retenir

1. **Un vecteur** est une liste ordonn√©e de nombres qui d√©crit un objet ‚Äî c'est la base de la repr√©sentation en ML
2. **Chaque observation** (client, image, transaction) est un point dans un espace √† N dimensions
3. **La distance euclidienne** mesure la "ressemblance" entre deux points en ligne droite : `‚àö(Œ£(a·µ¢ - b·µ¢)¬≤)`
4. **La distance de Manhattan** somme les diff√©rences absolues et est plus robuste aux outliers
5. **Une matrice** = un tableau de vecteurs empil√©s = un DataFrame pandas en version num√©rique
6. **Convention ML** : X (matrice de features) et y (vecteur target), toujours s√©par√©s
7. **KNN** pr√©dit en regardant les K voisins les plus proches et en votant √† la majorit√©
8. **La normalisation** est obligatoire avant de calculer des distances (sinon les grandes √©chelles dominent)
9. **Le choix de K** est un hyperparam√®tre crucial : K petit ‚Üí overfitting, K grand ‚Üí underfitting
10. **KNN est un lazy learner** : il stocke tout et calcule √† la pr√©diction, ce qui le rend lent sur de gros datasets

---

## ‚úÖ Checklist de validation

- [ ] Je sais expliquer ce qu'est un vecteur et pourquoi c'est utile en ML
- [ ] Je sais cr√©er et manipuler des vecteurs et matrices avec NumPy
- [ ] Je comprends la diff√©rence entre une liste Python et un ndarray NumPy
- [ ] Je sais calculer la distance euclidienne entre deux vecteurs
- [ ] Je sais calculer la distance de Manhattan entre deux vecteurs
- [ ] Je comprends le lien entre matrice NumPy et DataFrame pandas
- [ ] Je connais la convention X (features) et y (target)
- [ ] Je peux expliquer l'algorithme KNN sans formule (l'intuition)
- [ ] J'ai cod√© un KNN from scratch et je comprends chaque ligne
- [ ] Je sais utiliser `KNeighborsClassifier` de scikit-learn
- [ ] Je comprends pourquoi la normalisation est obligatoire pour KNN
- [ ] Je sais choisir une bonne valeur de K et tester plusieurs valeurs
- [ ] J'ai r√©alis√© les exercices de calcul de distances et de visualisation

---

**Pr√©c√©dent** : [Chapitre 2 : Environnement et Outils](02-environnement-setup.md)

**Suivant** : [Chapitre 4 : Fonctions, Erreurs et l'Art de s'Am√©liorer](04-fonctions-erreurs-gradient.md)
