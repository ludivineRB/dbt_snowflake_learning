# Cheatsheet Machine Learning

> RÃ©fÃ©rence rapide pour le Data Engineer / Data Scientist. Gardez cette page sous la main pendant vos projets ML.

---

## 1. ğŸ§­ Quel algorithme choisir ?

### Arbre de dÃ©cision rapide

```
DonnÃ©es labellisÃ©es ?
â”œâ”€â”€ OUI â†’ Apprentissage supervisÃ©
â”‚   â”œâ”€â”€ Target numÃ©rique â†’ RÃ‰GRESSION
â”‚   â”‚   â”œâ”€â”€ Relations linÃ©aires â†’ LinearRegression / Ridge / Lasso
â”‚   â”‚   â”œâ”€â”€ Relations complexes â†’ RandomForestRegressor / XGBRegressor
â”‚   â”‚   â””â”€â”€ Peu de donnÃ©es â†’ Ridge (rÃ©gularisation)
â”‚   â”‚
â”‚   â””â”€â”€ Target catÃ©gorielle â†’ CLASSIFICATION
â”‚       â”œâ”€â”€ 2 classes, interprÃ©table â†’ LogisticRegression
â”‚       â”œâ”€â”€ 2 classes, performance max â†’ XGBClassifier / LightGBM
â”‚       â”œâ”€â”€ Multi-classes â†’ RandomForestClassifier / XGBClassifier
â”‚       â””â”€â”€ Texte / images â†’ Deep Learning
â”‚
â””â”€â”€ NON â†’ Apprentissage non-supervisÃ©
    â”œâ”€â”€ Grouper des observations â†’ CLUSTERING
    â”‚   â”œâ”€â”€ Nombre de groupes connu â†’ KMeans
    â”‚   â”œâ”€â”€ Nombre inconnu â†’ DBSCAN
    â”‚   â””â”€â”€ HiÃ©rarchie souhaitÃ©e â†’ AgglomerativeClustering
    â”‚
    â””â”€â”€ RÃ©duire les dimensions â†’ PCA / t-SNE / UMAP
```

### Tableau comparatif des algorithmes

| Algorithme | Type | InterprÃ©table | Rapide | GÃ¨re le non-linÃ©aire | DonnÃ©es nÃ©cessaires |
|---|---|---|---|---|---|
| LinearRegression | RÃ©gression | +++  | +++ | - | Peu |
| Ridge / Lasso | RÃ©gression | +++ | +++ | - | Peu |
| DecisionTree | Les deux | +++ | ++ | ++ | Peu |
| RandomForest | Les deux | + | ++ | +++ | Moyen |
| XGBoost | Les deux | + | ++ | +++ | Moyen |
| LogisticRegression | Classification | +++ | +++ | - | Peu |
| SVM | Les deux | + | + | ++ | Moyen |
| KNN | Les deux | ++ | - | ++ | Moyen |
| KMeans | Clustering | ++ | +++ | - | Moyen |
| DBSCAN | Clustering | + | ++ | +++ | Moyen |

> ğŸ’¡ **Conseil de pro** : "En cas de doute, commencez par un Random Forest. Il marche bien dans 80% des cas, gÃ¨re les features numÃ©riques et catÃ©gorielles, et ne nÃ©cessite pas de normalisation."

---

## 2. ğŸ“Š MÃ©triques par type de problÃ¨me

### RÃ©gression

| MÃ©trique | Formule | Quand l'utiliser | Sensible aux outliers |
|---|---|---|---|
| **MAE** | mean(\|y - y_pred\|) | Erreur interprÃ©table en unitÃ©s | Non |
| **RMSE** | sqrt(mean((y - y_pred)^2)) | PÃ©naliser les grosses erreurs | Oui |
| **R2** | 1 - SS_res/SS_tot | Score global (0 Ã  1) | Oui |
| **MAPE** | mean(\|y - y_pred\|/\|y\|) * 100 | Erreur en pourcentage | Non |

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
```

### Classification

| MÃ©trique | Quand l'utiliser | Classes dÃ©sÃ©quilibrÃ©es ? |
|---|---|---|
| **Accuracy** | Classes Ã©quilibrÃ©es uniquement | NON |
| **Precision** | CoÃ»t Ã©levÃ© des faux positifs (spam) | OUI |
| **Recall** | CoÃ»t Ã©levÃ© des faux nÃ©gatifs (cancer) | OUI |
| **F1-Score** | Ã‰quilibre precision/recall | OUI |
| **AUC-ROC** | Comparaison globale de modÃ¨les | OUI |

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report
)

# Rapport complet en une ligne
print(classification_report(y_test, y_pred))

# MÃ©triques individuelles
f1 = f1_score(y_test, y_pred, average="weighted")
auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class="ovr")
```

> âš ï¸ **Attention** : "N'utilisez JAMAIS l'accuracy seule sur des classes dÃ©sÃ©quilibrÃ©es. Un modÃ¨le qui prÃ©dit toujours la classe majoritaire aura 95% d'accuracy sur un jeu 95/5, mais sera complÃ¨tement inutile."

### Clustering

| MÃ©trique | Avec labels | Sans labels | InterprÃ©tation |
|---|---|---|---|
| **Silhouette** | Non | Oui | -1 (mauvais) Ã  1 (bon) |
| **Inertie** | Non | Oui | Plus bas = mieux (elbow method) |
| **ARI** | Oui | Non | 0 (alÃ©atoire) Ã  1 (parfait) |
| **NMI** | Oui | Non | 0 Ã  1 |

```python
from sklearn.metrics import silhouette_score, adjusted_rand_score

sil = silhouette_score(X, labels_pred)
ari = adjusted_rand_score(labels_vrais, labels_pred)  # si labels disponibles
```

---

## 3. ğŸ”§ Commandes sklearn essentielles

### Preprocessing

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

# Normalisation (moyenne=0, Ã©cart-type=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)         # fit + transform sur train
X_test_scaled = scaler.transform(X_test)          # transform SEULEMENT sur test

# Encodage one-hot
encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
X_encoded = encoder.fit_transform(X_train[["ville", "type"]])
```

### Train/Test Split

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 80/20
    random_state=42,     # ReproductibilitÃ©
    stratify=y           # Garder les proportions de classes
)
```

### Cross-Validation

```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# Validation croisÃ©e rapide
scores = cross_val_score(model, X, y, cv=5, scoring="f1_weighted")
print(f"F1 moyen : {scores.mean():.4f} (+/- {scores.std():.4f})")

# Grid Search avec cross-validation
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [3, 5, 10, None],
}
grid = GridSearchCV(model, param_grid, cv=5, scoring="f1_weighted", n_jobs=-1)
grid.fit(X_train, y_train)
print(f"Meilleurs paramÃ¨tres : {grid.best_params_}")
```

---

## 4. ğŸ—ï¸ Template de Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# DÃ©finir les colonnes
colonnes_num = ["age", "revenu", "nb_achats"]
colonnes_cat = ["ville", "type_contrat"]

# Pipeline numÃ©rique : imputation + normalisation
pipeline_num = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

# Pipeline catÃ©goriel : imputation + encodage
pipeline_cat = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

# Combiner les deux
preprocessor = ColumnTransformer([
    ("num", pipeline_num, colonnes_num),
    ("cat", pipeline_cat, colonnes_cat)
])

# Pipeline complet : preprocessing + modÃ¨le
pipeline = Pipeline([
    ("preprocessing", preprocessor),
    ("model", RandomForestClassifier(n_estimators=100, random_state=42))
])

# EntraÃ®ner (le pipeline gÃ¨re tout)
pipeline.fit(X_train, y_train)

# PrÃ©dire (donnÃ©es brutes, pas besoin de prÃ©traiter)
y_pred = pipeline.predict(X_test)

# Sauvegarder le pipeline complet
import joblib
joblib.dump(pipeline, "model/pipeline_complet.joblib")
```

> ğŸ’¡ **Conseil de pro** : "Sauvegardez TOUJOURS le pipeline complet (preprocessing + modÃ¨le), jamais le modÃ¨le seul. En production, vous passez des donnÃ©es brutes et le pipeline fait tout le travail."

---

## 5. ğŸš€ Checklist avant mise en production

### QualitÃ© des donnÃ©es

- [ ] Pas de data leakage (le test set est vraiment isolÃ©)
- [ ] Valeurs manquantes traitÃ©es dans le pipeline
- [ ] Outliers identifiÃ©s et gÃ©rÃ©s
- [ ] Features cohÃ©rentes entre train et production

### ModÃ¨le

- [ ] MÃ©trique principale choisie ET justifiÃ©e
- [ ] Cross-validation effectuÃ©e (pas juste un train/test split)
- [ ] HyperparamÃ¨tres optimisÃ©s (GridSearch ou RandomSearch)
- [ ] Pas d'overfitting (Ã©cart train/test < 5%)
- [ ] Performance comparÃ©e Ã  une baseline simple

### DÃ©ploiement

- [ ] Pipeline complet sauvegardÃ© (preprocessing + modÃ¨le)
- [ ] API avec endpoint `/predict` et `/health`
- [ ] Validation des entrÃ©es (Pydantic)
- [ ] Tests unitaires + tests de performance du modÃ¨le
- [ ] Docker image construite et testÃ©e
- [ ] CI/CD configurÃ©

### Monitoring

- [ ] Logging des prÃ©dictions activÃ©
- [ ] DÃ©tection du data drift configurÃ©e
- [ ] Alertes en place (confiance basse, drift)
- [ ] ProcÃ©dure de rollback documentÃ©e

---

## 6. ğŸ› Erreurs courantes et solutions

| Erreur | SymptÃ´me | Solution |
|---|---|---|
| **Data leakage** | Accuracy 99% sur test, nulle en prod | SÃ©parer train/test AVANT tout preprocessing |
| **Overfitting** | Train=99%, Test=70% | RÃ©gularisation, plus de donnÃ©es, cross-validation |
| **Underfitting** | Train=60%, Test=58% | ModÃ¨le plus complexe, meilleur feature engineering |
| **Classes dÃ©sÃ©quilibrÃ©es** | Accuracy haute, recall bas | SMOTE, class_weight="balanced", F1 comme mÃ©trique |
| **Features non normalisÃ©es** | SVM/KNN marchent mal | StandardScaler dans le pipeline |
| **Categorical non encodÃ©** | Erreur sklearn | OneHotEncoder dans le pipeline |
| **fit sur le test set** | RÃ©sultats optimistes | fit_transform sur train, transform sur test |
| **random_state oubliÃ©** | RÃ©sultats non reproductibles | Toujours fixer random_state=42 |
| **Pipeline incomplet** | Bug en production | Sauvegarder le pipeline complet (pas le modÃ¨le seul) |
| **Pas de baseline** | Impossible d'Ã©valuer la valeur ajoutÃ©e | Comparer Ã  DummyClassifier/DummyRegressor |

> ğŸ’¡ **Conseil de pro** : "Si votre modÃ¨le a une accuracy suspectemement haute (>99%), cherchez du data leakage. C'est l'erreur la plus frÃ©quente et la plus dangereuse en ML."

---

## 7. ğŸ“ Commandes uv essentielles

```bash
# Initialiser un projet ML
uv init mon-projet-ml
cd mon-projet-ml

# Ajouter les dÃ©pendances ML courantes
uv add scikit-learn pandas numpy matplotlib seaborn
uv add xgboost lightgbm                          # Boosting
uv add mlflow                                     # Experiment tracking
uv add fastapi uvicorn                            # API
uv add joblib                                     # SÃ©rialisation

# Ajouter des dÃ©pendances de dÃ©veloppement
uv add --dev pytest ruff ipykernel jupyter

# ExÃ©cuter un script
uv run python src/train.py

# Lancer les tests
uv run pytest tests/ -v

# Lancer MLflow
uv run mlflow ui --port 5000

# Lancer l'API
uv run uvicorn app.main:app --reload --port 8000
```

---

[â¬…ï¸ Chapitre 10 : MLOps](10-mlops-production.md) | [ğŸ  Sommaire](../../README.md)
