# Cheatsheet Machine Learning

> Aide-mÃ©moire Ã  garder sous la main pendant tout le parcours

---

## ğŸ”§ Setup rapide

```bash
# Installation des dÃ©pendances
uv add numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm shap fastapi uvicorn joblib
```

```python
# Imports standards
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, classification_report,
                             confusion_matrix, mean_squared_error, r2_score)
```

---

## ğŸ“Š Exploration des donnÃ©es

```python
df = pd.read_csv("data.csv")

# Vue d'ensemble
df.shape                    # (lignes, colonnes)
df.info()                   # Types + valeurs manquantes
df.describe()               # Stats numÃ©riques
df.describe(include='object')  # Stats catÃ©gorielles
df.head()                   # PremiÃ¨res lignes
df.dtypes                   # Types de colonnes

# QualitÃ©
df.isnull().sum()           # Manquants par colonne
df.isnull().mean() * 100    # % manquants
df.duplicated().sum()       # Doublons
df['col'].value_counts()    # Distribution catÃ©gorielle
df['col'].nunique()         # Nombre de valeurs uniques

# CorrÃ©lations
df.corr()                   # Matrice de corrÃ©lation
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
```

---

## ğŸ”„ Preprocessing

### Split Train/Test

```python
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### Scaling

| Scaler | Formule | Quand |
|--------|---------|-------|
| `StandardScaler` | (x - Î¼) / Ïƒ | DÃ©faut, outliers modÃ©rÃ©s |
| `MinMaxScaler` | (x - min) / (max - min) | Borner entre 0 et 1 |
| `RobustScaler` | (x - mÃ©diane) / IQR | Beaucoup d'outliers |

### Encodage

| MÃ©thode | Quand | Code |
|---------|-------|------|
| One-Hot | < 5 catÃ©gories, modÃ¨les linÃ©aires | `OneHotEncoder(drop='first')` |
| Ordinal | Ordre naturel (low/med/high) | `OrdinalEncoder(categories=...)` |
| Label | Arbres de dÃ©cision | `LabelEncoder()` |

### Pipeline complet

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

num_features = ['age', 'salary', 'tenure']
cat_features = ['contract', 'payment']

num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_features),
    ('cat', cat_pipeline, cat_features)
])

# Pipeline complet avec modÃ¨le
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', LogisticRegression(max_iter=1000))
])

full_pipeline.fit(X_train, y_train)
y_pred = full_pipeline.predict(X_test)
```

---

## ğŸ¤– Algorithmes

### Classification

| Algorithme | sklearn | Normaliser ? | Forces |
|-----------|---------|-------------|--------|
| RÃ©gression Logistique | `LogisticRegression(max_iter=1000)` | Oui | Baseline, interprÃ©table |
| KNN | `KNeighborsClassifier(n_neighbors=5)` | **Oui** | Simple, non-paramÃ©trique |
| SVM | `SVC(kernel='rbf')` | **Oui** | Petits datasets |
| Arbre de DÃ©cision | `DecisionTreeClassifier(max_depth=5)` | Non | InterprÃ©table |
| Random Forest | `RandomForestClassifier(n_estimators=100)` | Non | Robuste, polyvalent |
| Gradient Boosting | `GradientBoostingClassifier()` | Non | Performant |
| XGBoost | `XGBClassifier(n_estimators=100)` | Non | Champion Kaggle |
| LightGBM | `LGBMClassifier(n_estimators=100)` | Non | Rapide, grands datasets |

### RÃ©gression

| Algorithme | sklearn | Normaliser ? |
|-----------|---------|-------------|
| RÃ©gression LinÃ©aire | `LinearRegression()` | Oui (pour Ridge/Lasso) |
| Ridge | `Ridge(alpha=1.0)` | Oui |
| Lasso | `Lasso(alpha=1.0)` | Oui |
| Random Forest | `RandomForestRegressor()` | Non |
| XGBoost | `XGBRegressor()` | Non |

### Pattern universel sklearn

```python
model = Algorithm(hyperparams)
model.fit(X_train, y_train)        # EntraÃ®ner
y_pred = model.predict(X_test)     # PrÃ©dire
score = model.score(X_test, y_test)  # Ã‰valuer
```

---

## ğŸ“ MÃ©triques

### Classification

| MÃ©trique | Formule | Quand l'utiliser | Code |
|----------|---------|-----------------|------|
| Accuracy | (TP+TN)/Total | Classes Ã©quilibrÃ©es | `accuracy_score(y, y_pred)` |
| Precision | TP/(TP+FP) | FP coÃ»teux (spam) | `precision_score(y, y_pred)` |
| Recall | TP/(TP+FN) | FN coÃ»teux (cancer) | `recall_score(y, y_pred)` |
| F1 | 2*P*R/(P+R) | DÃ©faut | `f1_score(y, y_pred)` |
| AUC-ROC | Aire sous ROC | Vue d'ensemble | `roc_auc_score(y, y_proba)` |

```python
# Rapport complet
print(classification_report(y_test, y_pred))

# Matrice de confusion
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Blues')
```

### RÃ©gression

| MÃ©trique | Code | IdÃ©al |
|----------|------|-------|
| MSE | `mean_squared_error(y, y_pred)` | â†’ 0 |
| RMSE | `mean_squared_error(y, y_pred, squared=False)` | â†’ 0 |
| MAE | `mean_absolute_error(y, y_pred)` | â†’ 0 |
| RÂ² | `r2_score(y, y_pred)` | â†’ 1 |

---

## ğŸ” Validation

```python
# Cross-validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='f1')
print(f"F1: {scores.mean():.4f} Â± {scores.std():.4f}")

# GridSearch
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid.fit(X_train, y_train)
print(grid.best_params_)
print(grid.best_score_)

# Courbe d'apprentissage
from sklearn.model_selection import learning_curve
train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='f1',
    train_sizes=np.linspace(0.1, 1.0, 10)
)
```

---

## ğŸ§  InterprÃ©tabilitÃ©

```python
# Feature Importance (arbres)
importances = model.feature_importances_
pd.Series(importances, index=feature_names).sort_values().plot.barh()

# Permutation Importance (tous modÃ¨les)
from sklearn.inspection import permutation_importance
result = permutation_importance(model, X_test, y_test, n_repeats=10)

# SHAP
import shap
explainer = shap.TreeExplainer(model)  # ou shap.Explainer(model)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)            # Global
shap.plots.waterfall(shap_values[0])               # Individuel
shap.plots.dependence(shap_values, "feature_name") # DÃ©pendance
```

---

## ğŸš€ Production

### SÃ©rialisation

```python
import joblib

# Sauvegarder
joblib.dump(full_pipeline, 'models/pipeline.joblib')

# Charger
pipeline = joblib.load('models/pipeline.joblib')
prediction = pipeline.predict(new_data)
```

### API FastAPI

```python
from fastapi import FastAPI
from pydantic import BaseModel
import joblib

app = FastAPI()
pipeline = joblib.load("models/pipeline.joblib")

class ClientData(BaseModel):
    tenure: int
    monthly_charges: float
    contract: str
    # ...

@app.post("/predict")
def predict(client: ClientData):
    df = pd.DataFrame([client.model_dump()])
    proba = pipeline.predict_proba(df)[0][1]
    return {"churn_probability": round(proba, 4),
            "prediction": "churn" if proba > 0.5 else "no_churn"}

@app.get("/health")
def health():
    return {"status": "ok"}
```

### Docker

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY pyproject.toml .
RUN pip install .
COPY src/ src/
COPY models/ models/
EXPOSE 8000
CMD ["uvicorn", "src.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## âš ï¸ Erreurs courantes

| Erreur | ProblÃ¨me | Solution |
|--------|----------|----------|
| Accuracy de 99% | Data leakage probable | VÃ©rifier le pipeline, les features |
| KNN mauvais score | Pas de normalisation | `StandardScaler` avant KNN |
| fit_transform sur test | Leakage | `fit_transform(train)` puis `transform(test)` |
| ModÃ¨le instable | Overfitting | RÃ©duire complexitÃ©, plus de donnÃ©es, rÃ©gularisation |
| Score train >> test | Overfitting | `max_depth`, `min_samples`, cross-validation |
| Score train â‰ˆ test â‰ˆ bas | Underfitting | ModÃ¨le trop simple, plus de features |
| MÃ©moire saturÃ©e | One-Hot sur haute cardinalitÃ© | Target encoding ou feature hashing |

---

## ğŸ—ºï¸ Guide de choix rapide

```
Quel type de problÃ¨me ?
â”œâ”€â”€ Classification (catÃ©gorie)
â”‚   â”œâ”€â”€ Baseline â†’ Logistic Regression
â”‚   â”œâ”€â”€ Petit dataset â†’ SVM
â”‚   â”œâ”€â”€ InterprÃ©tabilitÃ© requise â†’ Decision Tree
â”‚   â””â”€â”€ Performance max â†’ XGBoost / LightGBM
â”‚
â”œâ”€â”€ RÃ©gression (nombre)
â”‚   â”œâ”€â”€ Baseline â†’ Linear Regression
â”‚   â”œâ”€â”€ RÃ©gularisation â†’ Ridge / Lasso
â”‚   â””â”€â”€ Performance max â†’ XGBoost / LightGBM
â”‚
â””â”€â”€ Clustering (groupes)
    â”œâ”€â”€ Nombre de clusters connu â†’ KMeans
    â”œâ”€â”€ Forme arbitraire â†’ DBSCAN
    â””â”€â”€ HiÃ©rarchie â†’ Agglomerative Clustering
```
