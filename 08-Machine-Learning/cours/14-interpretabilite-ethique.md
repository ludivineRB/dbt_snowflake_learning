# Chapitre 14 : Interpr√©ter ses Mod√®les et √âthique du ML

## üéØ Objectifs

- Comprendre pourquoi l'**interpr√©tabilit√©** est cruciale (confiance, l√©gal, debug)
- Ma√Ætriser les m√©thodes d'interpr√©tation : feature importance, permutation, SHAP, PDP
- Savoir expliquer une pr√©diction individuelle √† un non-technique
- Conna√Ætre les enjeux **√©thiques** du ML : biais, fairness, RGPD
- Appliquer une checklist √©thique √† tout projet ML

> **Phase 5 - Semaine 14**

---

## 1. üß† Pourquoi l'interpr√©tabilit√© est cruciale

### 1.1 Trois raisons fondamentales

**Raison 1 : La confiance m√©tier**

Le manager ou le directeur financier ne d√©ploiera jamais un mod√®le qu'il ne comprend pas. "Le mod√®le a dit non" n'est pas une explication acceptable.

**Raison 2 : Les obligations l√©gales**

Le **RGPD** (R√®glement G√©n√©ral sur la Protection des Donn√©es) impose un **droit √† l'explication** : toute personne affect√©e par une d√©cision automatis√©e a le droit de comprendre pourquoi.

```
Article 22 du RGPD :
"La personne concern√©e a le droit de ne pas faire l'objet d'une
d√©cision fond√©e exclusivement sur un traitement automatis√© [...]
produisant des effets juridiques la concernant."

‚Üí Si un mod√®le refuse un cr√©dit, le client peut exiger une explication.
```

**Raison 3 : D√©bugger le mod√®le**

Une feature importance inattendue r√©v√®le souvent un **bug** ou une **fuite de donn√©es** :

```
Exemple r√©el :
  Un mod√®le de pr√©diction de pneumonie trouvait que
  l'asthme R√âDUISAIT le risque de d√©c√®s par pneumonie.

  Pourquoi ? Les patients asthmatiques √©taient envoy√©s
  directement en soins intensifs ‚Üí meilleure prise en
  charge ‚Üí moins de d√©c√®s.

  Le mod√®le avait appris un biais de s√©lection,
  pas une relation causale !
```

> ‚ö†Ô∏è **Attention** : "Un mod√®le performant avec des feature importances illogiques est potentiellement **dangereux**. Toujours v√©rifier que le mod√®le apprend les bonnes corr√©lations."

---

## 2. üìä Mod√®les interpr√©tables vs bo√Ætes noires

### 2.1 Le spectre de l'interpr√©tabilit√©

| Mod√®le | Interpr√©tabilit√© | Performance typique | Quand l'utiliser |
|--------|-----------------|---------------------|------------------|
| R√©gression lin√©aire | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Mod√©r√©e | Baseline, domaines r√©gul√©s |
| Arbre de d√©cision | ‚≠ê‚≠ê‚≠ê‚≠ê | Mod√©r√©e | Quand l'explication est prioritaire |
| R√©gression logistique | ‚≠ê‚≠ê‚≠ê‚≠ê | Mod√©r√©e | Classification binaire simple |
| Random Forest | ‚≠ê‚≠ê‚≠ê | Bonne | Bon compromis g√©n√©ral |
| Gradient Boosting | ‚≠ê‚≠ê | Tr√®s bonne | Comp√©titions, performance max |
| R√©seau de neurones | ‚≠ê | Variable | Images, texte, s√©quences |

```
Interpr√©tabilit√© ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Performance
  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê                                    ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

  R√©gression    Arbre     Random    XGBoost    Deep
  lin√©aire               Forest               Learning

  "Je comprends  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  "Je ne comprends
   exactement                             rien mais √ßa
   pourquoi"                              marche bien"
```

### 2.2 Le trade-off performance vs interpr√©tabilit√©

> üí° **Conseil** : "En pratique, la diff√©rence de performance entre un mod√®le interpr√©table et une bo√Æte noire est souvent **faible** (1-2%). Si l'interpr√©tabilit√© est importante pour votre cas d'usage, ne sacrifiez pas la compr√©hension pour un gain marginal de performance."

---

## 3. üîç Feature Importance globale

### 3.1 Coefficients de la r√©gression lin√©aire

Les coefficients d'une r√©gression lin√©aire sont **directement interpr√©tables** :

```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Pr√©parer ---
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

# --- R√©gression logistique ---
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train_s, y_train)

# Coefficients (features standardis√©es ‚Üí comparables)
coefs = pd.Series(log_reg.coef_[0], index=cancer.feature_names)
coefs_sorted = coefs.abs().sort_values(ascending=False)

print("=== Top 10 features les plus influentes ===")
for feat in coefs_sorted.head(10).index:
    direction = "‚Üë b√©nin" if coefs[feat] > 0 else "‚Üì malin"
    print(f"  {feat:30s} : coef = {coefs[feat]:+.4f} ({direction})")
```

> üí° **Conseil** : "Pour que les coefficients soient comparables entre eux, les features doivent √™tre **standardis√©es** (StandardScaler). Un coefficient de 2.0 sur une feature en m√®tres n'est pas comparable √† un coefficient de 0.001 sur une feature en millim√®tres."

### 3.2 Feature importance des arbres (MDI)

Les arbres calculent l'importance d'une feature comme la **diminution totale d'impuret√©** (Mean Decrease in Impurity) qu'elle apporte.

```python
from sklearn.ensemble import RandomForestClassifier

# --- Random Forest ---
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# Feature importance (MDI)
importances = pd.Series(rf.feature_importances_, index=cancer.feature_names)
importances_sorted = importances.sort_values(ascending=True)

# --- Visualisation ---
plt.figure(figsize=(10, 8))
importances_sorted.tail(15).plot(kind='barh', color='steelblue')
plt.xlabel('Importance (MDI)')
plt.title('Feature Importance ‚Äî Random Forest (Top 15)')
plt.tight_layout()
plt.show()
```

> ‚ö†Ô∏è **Attention** : "La feature importance MDI des arbres est **biais√©e** en faveur des features avec beaucoup de valeurs uniques (features continues vs cat√©gorielles). Pr√©f√©rez la **permutation importance** pour une √©valuation plus fiable."

### 3.3 Permutation importance (model-agnostic)

La permutation importance mesure la **baisse de performance** quand on m√©lange al√©atoirement les valeurs d'une feature. Si le score baisse beaucoup, la feature est importante.

```python
from sklearn.inspection import permutation_importance

# --- Permutation importance ---
result = permutation_importance(
    rf, X_test, y_test,
    n_repeats=30,          # R√©p√©ter 30 fois pour la robustesse
    random_state=42,
    scoring='f1'
)

# R√©sultats
perm_importance = pd.DataFrame({
    'importance_mean': result.importances_mean,
    'importance_std': result.importances_std,
}, index=cancer.feature_names).sort_values('importance_mean', ascending=True)

# --- Visualisation ---
fig, ax = plt.subplots(figsize=(10, 8))
perm_importance.tail(15)['importance_mean'].plot(
    kind='barh',
    xerr=perm_importance.tail(15)['importance_std'],
    color='coral',
    ax=ax
)
ax.set_xlabel('Baisse de F1 quand la feature est permut√©e')
ax.set_title('Permutation Importance (Top 15)')
plt.tight_layout()
plt.show()
```

---

## 4. üéØ SHAP expliqu√© simplement

### 4.1 Valeurs de Shapley ‚Äî L'analogie avec les joueurs d'une √©quipe

Imaginez une √©quipe de football qui gagne un match. Comment attribuer le m√©rite √† chaque joueur ?

```
√âquipe : Joueur A, Joueur B, Joueur C
Score final : 3 buts

La valeur de Shapley de chaque joueur =
  sa contribution MOYENNE au score,
  en consid√©rant TOUTES les coalitions possibles :

  - A seul : marque 1 but
  - B seul : marque 0 but
  - C seul : marque 1 but
  - A + B   : marquent 2 buts
  - A + C   : marquent 2 buts
  - B + C   : marquent 1 but
  - A + B + C : marquent 3 buts

  ‚Üí Shapley(A) = contribution moyenne de A ‚âà 1.17
  ‚Üí Shapley(B) = contribution moyenne de B ‚âà 0.50
  ‚Üí Shapley(C) = contribution moyenne de C ‚âà 1.33
  ‚Üí Total = 3.0 ‚úì (les Shapley values s'additionnent !)
```

En ML, c'est pareil : la valeur SHAP d'une feature mesure sa **contribution √† la pr√©diction** pour UNE observation donn√©e.

### 4.2 Installation et usage

```python
# Installation
# pip install shap

import shap
from sklearn.ensemble import RandomForestClassifier

# --- Entra√Æner le mod√®le ---
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# --- Calculer les SHAP values ---
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)

# shap_values est une liste de 2 arrays (une par classe)
# shap_values[1] = contributions vers la classe 1 (b√©nin)
print(f"Shape des SHAP values : {shap_values[1].shape}")
# (n_√©chantillons, n_features)
```

### 4.3 Visualisations SHAP

#### Summary Plot ‚Äî Vue globale

```python
# Summary plot : importance ET direction de chaque feature
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values[1], X_test, feature_names=cancer.feature_names)
```

```
Interpr√©tation du Summary Plot :
  - Chaque point = une observation
  - Axe X = valeur SHAP (contribution √† la pr√©diction)
  - Couleur = valeur de la feature (rouge = √©lev√©e, bleu = basse)

  worst concave points  ‚óè‚óè‚óè‚óè‚óè‚óè‚óè|‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
  worst perimeter       ‚óè‚óè‚óè‚óè‚óè|‚óè‚óè‚óè‚óè‚óè‚óè
  mean concave points   ‚óè‚óè‚óè‚óè|‚óè‚óè‚óè‚óè‚óè
                        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                        Pousse  ‚îÇ  Pousse
                        vers    ‚îÇ  vers
                        malin   ‚îÇ  b√©nin
```

#### Waterfall Plot ‚Äî Expliquer UNE pr√©diction

```python
# Expliquer la pr√©diction pour le patient n¬∞0
idx = 0
print(f"Pr√©diction : {'B√©nin' if rf.predict(X_test.iloc[[idx]])[0] == 1 else 'Malin'}")
print(f"Probabilit√© b√©nin : {rf.predict_proba(X_test.iloc[[idx]])[0][1]:.2%}")

# Waterfall plot
shap.plots.waterfall(shap.Explanation(
    values=shap_values[1][idx],
    base_values=explainer.expected_value[1],
    data=X_test.iloc[idx].values,
    feature_names=cancer.feature_names.tolist()
))
```

#### Force Plot ‚Äî Visualisation compacte

```python
# Force plot pour une pr√©diction
shap.force_plot(
    explainer.expected_value[1],
    shap_values[1][idx],
    X_test.iloc[idx],
    feature_names=cancer.feature_names.tolist()
)
```

#### Dependence Plot ‚Äî Relation feature / SHAP value

```python
# Comment "worst perimeter" influence la pr√©diction
shap.dependence_plot(
    'worst perimeter',
    shap_values[1],
    X_test,
    feature_names=cancer.feature_names.tolist()
)
```

---

## 5. üìà Partial Dependence Plots (PDP)

### 5.1 Concept

Les PDP montrent l'effet **marginal** d'une feature sur la pr√©diction, en moyennant l'effet de toutes les autres features.

```
PDP pour "surface" dans un mod√®le de prix immobilier :

Prix pr√©dit
  500k ‚îÇ                          ‚ï±‚îÄ‚îÄ‚îÄ
       ‚îÇ                        ‚ï±
  400k ‚îÇ                      ‚ï±
       ‚îÇ                   ‚ï±
  300k ‚îÇ               ‚ï±‚îÄ‚îÄ
       ‚îÇ            ‚ï±
  200k ‚îÇ        ‚ï±‚îÄ‚îÄ
       ‚îÇ    ‚ï±‚îÄ‚îÄ
  100k ‚îÇ‚ï±‚îÄ‚îÄ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Surface (m¬≤)
       20   40   60   80  100  120  140
```

### 5.2 Code avec sklearn

```python
from sklearn.inspection import PartialDependenceDisplay
from sklearn.ensemble import GradientBoostingClassifier

# --- Entra√Æner un Gradient Boosting ---
gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)
gb.fit(X_train, y_train)

# --- PDP pour les 4 features les plus importantes ---
top_features = importances.sort_values(ascending=False).head(4).index.tolist()
top_indices = [list(cancer.feature_names).index(f) for f in top_features]

fig, ax = plt.subplots(figsize=(14, 8))
PartialDependenceDisplay.from_estimator(
    gb, X_train, features=top_indices,
    feature_names=cancer.feature_names,
    ax=ax
)
fig.suptitle('Partial Dependence Plots ‚Äî Top 4 features')
plt.tight_layout()
plt.show()
```

> üí° **Conseil** : "Les PDP montrent l'effet **moyen** d'une feature. Ils peuvent √™tre trompeurs si les features sont fortement corr√©l√©es. Utilisez les **ICE plots** (Individual Conditional Expectation) pour voir la variabilit√© entre les observations."

---

## 6. üî¨ LIME (aper√ßu)

### 6.1 Explication locale avec LIME

LIME (Local Interpretable Model-agnostic Explanations) explique UNE pr√©diction en cr√©ant un **mod√®le simple local** autour de l'observation.

```
Principe de LIME :

1. Prendre une observation √† expliquer
2. G√©n√©rer des perturbations autour de cette observation
3. Pr√©dire avec le mod√®le complexe sur ces perturbations
4. Entra√Æner un mod√®le simple (r√©gression lin√©aire)
   sur les perturbations et leurs pr√©dictions
5. Les coefficients du mod√®le simple = l'explication locale
```

```python
# Installation
# pip install lime

from lime.lime_tabular import LimeTabularExplainer
import numpy as np

# --- Cr√©er l'explainer ---
explainer_lime = LimeTabularExplainer(
    X_train.values,
    feature_names=cancer.feature_names,
    class_names=['Malin', 'B√©nin'],
    mode='classification'
)

# --- Expliquer une pr√©diction ---
idx = 0
explanation = explainer_lime.explain_instance(
    X_test.iloc[idx].values,
    rf.predict_proba,
    num_features=10
)

# Afficher
explanation.show_in_notebook()
# Ou en texte :
print("=== Explication LIME ===")
for feature, weight in explanation.as_list():
    print(f"  {feature:40s} : {weight:+.4f}")
```

### 6.2 LIME vs SHAP

| Crit√®re | LIME | SHAP |
|---------|------|------|
| **Type** | Local (1 pr√©diction) | Local + Global |
| **Th√©orie** | Approximation locale | Th√©orie des jeux (Shapley) |
| **Consistance** | Peut varier entre ex√©cutions | Garanti math√©matiquement |
| **Vitesse** | Rapide | Plus lent (exact) |
| **Quand utiliser** | Explication rapide | Explication rigoureuse |

---

## 7. üß™ TP : Expliquer une d√©cision de cr√©dit √† un client

### 7.1 Sc√©nario

Vous travaillez pour une banque. Un client demande un cr√©dit et le mod√®le le refuse. Le client demande **pourquoi**. Vous devez fournir une explication compr√©hensible.

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import shap

# --- Simuler un dataset de cr√©dit ---
np.random.seed(42)
n = 1000

data = pd.DataFrame({
    'revenu_annuel': np.random.normal(45000, 15000, n).clip(15000),
    'montant_credit': np.random.normal(15000, 8000, n).clip(1000),
    'duree_emploi_mois': np.random.exponential(48, n).astype(int).clip(0),
    'nb_credits_en_cours': np.random.poisson(1.5, n),
    'taux_endettement': np.random.uniform(0.05, 0.60, n),
    'age': np.random.normal(40, 12, n).clip(18, 75).astype(int),
    'historique_paiement': np.random.choice([0, 1, 2, 3], n, p=[0.6, 0.2, 0.1, 0.1]),
})

# Target : 1 = cr√©dit accord√©, 0 = refus√©
score = (
    0.3 * (data['revenu_annuel'] > 35000).astype(int) +
    0.2 * (data['taux_endettement'] < 0.35).astype(int) +
    0.2 * (data['duree_emploi_mois'] > 24).astype(int) +
    0.15 * (data['historique_paiement'] == 0).astype(int) +
    0.15 * (data['nb_credits_en_cours'] < 3).astype(int) +
    np.random.normal(0, 0.15, n)
)
data['credit_accorde'] = (score > 0.5).astype(int)

X = data.drop('credit_accorde', axis=1)
y = data['credit_accorde']

# --- Entra√Æner ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model_credit = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)
model_credit.fit(X_train, y_train)

# --- Client refus√© ---
client_refuse = X_test[model_credit.predict(X_test) == 0].iloc[0]
print("=== Profil du client refus√© ===")
for col in X.columns:
    print(f"  {col:25s} : {client_refuse[col]:.2f}")

proba = model_credit.predict_proba(client_refuse.values.reshape(1, -1))[0]
print(f"\nProbabilit√© d'accord : {proba[1]:.2%}")
print(f"D√©cision : {'Accord√©' if proba[1] > 0.5 else 'REFUS√â'}")

# --- Explication SHAP ---
explainer = shap.TreeExplainer(model_credit)
shap_values = explainer.shap_values(client_refuse.values.reshape(1, -1))

print("\n=== Explication pour le client ===")
contributions = pd.Series(shap_values[0], index=X.columns)
contributions_sorted = contributions.abs().sort_values(ascending=False)

for feat in contributions_sorted.index:
    val = client_refuse[feat]
    contrib = contributions[feat]
    direction = "FAVORABLE" if contrib > 0 else "D√âFAVORABLE"
    print(f"  {feat:25s} = {val:8.2f} ‚Üí {direction} (impact : {contrib:+.4f})")
```

**Explication au client** (en langage clair) :

```
"Monsieur, votre demande de cr√©dit a √©t√© refus√©e principalement
pour les raisons suivantes :

1. Votre taux d'endettement (45%) est sup√©rieur au seuil recommand√© (35%)
   ‚Üí C'est le facteur le plus impactant dans la d√©cision.

2. Vous avez 4 cr√©dits en cours, ce qui augmente le risque.

3. Votre anciennet√© dans votre emploi actuel (6 mois) est consid√©r√©e
   comme insuffisante pour ce montant de cr√©dit.

Pour am√©liorer vos chances :
- R√©duisez votre taux d'endettement en remboursant un cr√©dit existant
- Attendez d'avoir au moins 2 ans d'anciennet√© dans votre emploi"
```

---

## 8. ‚öñÔ∏è √âthique et Fairness

### 8.1 Biais dans les donn√©es ‚Äî Exemples c√©l√®bres

**Amazon (2018) :** Un outil de recrutement par IA entra√Æn√© sur 10 ans de donn√©es p√©nalisait les candidates **femmes**. Les donn√©es historiques refl√©taient le biais de l'industrie tech, majoritairement masculine.

**COMPAS (2016) :** Un algorithme de pr√©diction de r√©cidive criminelle utilis√© par la justice am√©ricaine donnait des scores de risque **plus √©lev√©s** aux personnes noires qu'aux personnes blanches, √† profil √©quivalent.

**Soins de sant√© (2019) :** Un algorithme utilis√© par des h√¥pitaux am√©ricains assignait des scores de risque plus bas aux patients noirs qu'aux patients blancs pour un m√™me niveau de maladie, car il utilisait les **co√ªts de sant√©** comme proxy de la maladie (les patients noirs avaient moins acc√®s aux soins, donc des co√ªts plus bas).

### 8.2 Types de biais

| Type de biais | Description | Exemple |
|---------------|-------------|---------|
| **Biais historique** | Les donn√©es refl√®tent des discriminations pass√©es | Donn√©es de recrutement sexistes |
| **Biais de repr√©sentation** | Certains groupes sont sous-repr√©sent√©s | Dataset de visages avec 90% de blancs |
| **Biais de mesure** | Les variables proxy capturent des in√©galit√©s | Code postal comme proxy de l'ethnie |
| **Biais d'agr√©gation** | Un mod√®le unique pour des populations diff√©rentes | M√™me seuil m√©dical pour hommes et femmes |
| **Biais de confirmation** | On cherche des r√©sultats qui confirment nos hypoth√®ses | S√©lection de features biais√©e |

### 8.3 Disparate Impact

Le **disparate impact** mesure si un mod√®le traite diff√©remment des groupes prot√©g√©s (genre, ethnie, age...).

```
Disparate Impact Ratio = P(favorable | groupe d√©favoris√©)
                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                         P(favorable | groupe favoris√©)

Si le ratio < 0.8 ‚Üí "R√®gle des 4/5" ‚Üí Discrimination potentielle
```

```python
# Exemple : v√©rifier le disparate impact par genre
def disparate_impact(y_pred, group):
    """Calcule le disparate impact ratio."""
    groups = np.unique(group)
    rates = {}
    for g in groups:
        mask = group == g
        rates[g] = y_pred[mask].mean()

    min_rate = min(rates.values())
    max_rate = max(rates.values())

    ratio = min_rate / max_rate if max_rate > 0 else 0
    return ratio, rates

# Simuler
np.random.seed(42)
y_pred_credit = np.random.choice([0, 1], 1000, p=[0.3, 0.7])
genre = np.random.choice(['H', 'F'], 1000)

# Introduire un biais
y_pred_credit[genre == 'F'] = np.random.choice([0, 1], (genre == 'F').sum(), p=[0.45, 0.55])

ratio, rates = disparate_impact(y_pred_credit, genre)
print(f"Taux d'acceptation par genre : {rates}")
print(f"Disparate Impact Ratio : {ratio:.3f}")
print(f"Discrimination potentielle : {'OUI' if ratio < 0.8 else 'NON'} (seuil = 0.80)")
```

### 8.4 Comment mitiger les biais

| √âtape | Action | D√©tails |
|-------|--------|---------|
| **Avant** | Auditer les donn√©es | V√©rifier la repr√©sentation, les proxy variables |
| **Avant** | Diversifier les donn√©es | Collecter des donn√©es plus repr√©sentatives |
| **Pendant** | Contraintes de fairness | Ajouter des contraintes d'√©quit√© dans la loss |
| **Pendant** | Reweighting | Repond√©rer les √©chantillons sous-repr√©sent√©s |
| **Apr√®s** | Ajuster les seuils | Seuils diff√©rents par groupe pour √©galiser les taux |
| **Apr√®s** | Monitorer | V√©rifier les m√©triques de fairness en production |

### 8.5 Fairness metrics

| M√©trique | D√©finition | Objectif |
|----------|-----------|----------|
| **Demographic Parity** | P(≈∑=1\|A=0) = P(≈∑=1\|A=1) | M√™me taux de positifs par groupe |
| **Equal Opportunity** | P(≈∑=1\|Y=1,A=0) = P(≈∑=1\|Y=1,A=1) | M√™me recall par groupe |
| **Equalized Odds** | TPR et FPR √©gaux entre groupes | M√™me TPR et FPR par groupe |
| **Predictive Parity** | P(Y=1\|≈∑=1,A=0) = P(Y=1\|≈∑=1,A=1) | M√™me precision par groupe |

> ‚ö†Ô∏è **Attention** : "Il est math√©matiquement **impossible** de satisfaire toutes les m√©triques de fairness simultan√©ment (th√©or√®me d'impossibilit√© de Chouldechova/Kleinberg). Vous devez **choisir** quelle notion d'√©quit√© est la plus pertinente pour votre cas d'usage."

### 8.6 Checklist √©thique pour tout projet ML

```
CHECKLIST √âTHIQUE ‚Äî √Ä v√©rifier pour CHAQUE projet ML

DONN√âES
‚ñ° Les donn√©es sont-elles repr√©sentatives de la population cible ?
‚ñ° Y a-t-il des groupes sous-repr√©sent√©s ?
‚ñ° Des variables proxy pourraient-elles capturer des attributs sensibles ?
‚ñ° Les labels sont-ils fiables et non biais√©s ?

MOD√âLISATION
‚ñ° Le mod√®le est-il suffisamment interpr√©table pour le cas d'usage ?
‚ñ° Les feature importances sont-elles coh√©rentes avec le domaine m√©tier ?
‚ñ° A-t-on test√© le mod√®le sur diff√©rents sous-groupes ?
‚ñ° Le disparate impact ratio est-il sup√©rieur √† 0.8 ?

D√âPLOIEMENT
‚ñ° Les personnes affect√©es peuvent-elles demander une explication ?
‚ñ° Existe-t-il un processus de recours humain ?
‚ñ° Le mod√®le est-il monitor√© pour d√©tecter les d√©rives ?
‚ñ° Qui est responsable en cas de d√©cision erron√©e ?

DOCUMENTATION
‚ñ° Les choix de mod√©lisation sont-ils document√©s ?
‚ñ° Les limitations connues sont-elles list√©es ?
‚ñ° Les biais potentiels sont-ils identifi√©s ?
```

---

## üéØ Points cl√©s √† retenir

1. **L'interpr√©tabilit√©** n'est pas un luxe : c'est une n√©cessit√© l√©gale (RGPD), technique (debug) et m√©tier (confiance)
2. Les **coefficients de r√©gression** sont l'outil d'interpr√©tation le plus simple (features standardis√©es !)
3. La **permutation importance** est model-agnostic et plus fiable que le MDI des arbres
4. **SHAP** est la m√©thode de r√©f√©rence : fond√©e th√©oriquement (Shapley values) et visuelle
5. Les **PDP** montrent l'effet marginal d'une feature sur la pr√©diction
6. **LIME** est rapide mais moins rigoureux que SHAP pour les explications locales
7. Les **biais dans les donn√©es** se retrouvent dans les mod√®les ‚Äî "garbage in, garbage out"
8. Le **disparate impact** (r√®gle des 4/5) est un indicateur simple de discrimination
9. Il est **impossible** de satisfaire toutes les m√©triques de fairness simultan√©ment
10. Une **checklist √©thique** devrait accompagner chaque projet ML en production

---

## ‚úÖ Checklist de validation

- [ ] Je comprends les 3 raisons de l'interpr√©tabilit√© (confiance, l√©gal, debug)
- [ ] Je sais interpr√©ter les coefficients d'une r√©gression lin√©aire
- [ ] Je sais calculer et visualiser la feature importance d'un Random Forest
- [ ] Je sais utiliser la permutation importance (model-agnostic)
- [ ] Je sais installer et utiliser SHAP (summary plot, waterfall, force plot)
- [ ] Je sais cr√©er et interpr√©ter un Partial Dependence Plot
- [ ] Je connais LIME et sais quand l'utiliser vs SHAP
- [ ] Je sais expliquer une d√©cision de cr√©dit en langage clair √† un client
- [ ] Je connais les exemples c√©l√®bres de biais en ML (Amazon, COMPAS)
- [ ] Je sais calculer le disparate impact ratio
- [ ] Je connais les principales fairness metrics et le th√©or√®me d'impossibilit√©
- [ ] Je sais appliquer la checklist √©thique √† un projet ML

---

**Pr√©c√©dent** : [Chapitre 13 : Validation et G√©n√©ralisation](13-validation-generalisation.md)

**Suivant** : [Chapitre 15 : Du Notebook √† l'API ‚Äî Mettre en Production](15-notebook-api.md)
