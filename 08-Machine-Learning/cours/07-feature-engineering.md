# Chapitre 7 : Feature Engineering â€” L'Art de PrÃ©parer les DonnÃ©es

## ğŸ¯ Objectifs

- Comprendre pourquoi les algorithmes ML ne peuvent pas traiter directement des donnÃ©es brutes
- MaÃ®triser les diffÃ©rentes techniques d'encodage des variables catÃ©gorielles
- Savoir quand et comment appliquer la normalisation/standardisation
- CrÃ©er de nouvelles features pertinentes Ã  partir des donnÃ©es existantes
- SÃ©lectionner les features les plus informatives pour le modÃ¨le
- Construire des pipelines scikit-learn robustes et reproductibles

> ğŸ’¡ **Conseil** : "Le feature engineering est ce qui sÃ©pare un bon data scientist d'un excellent data scientist. Un bon encodage + un bon scaling avec une rÃ©gression logistique battra souvent un mauvais preprocessing avec un modÃ¨le complexe."

---

## 1. ğŸ§  Pourquoi une Machine ne Comprend pas "Rouge" ou "Paris"

### 1.1 Le problÃ¨me fondamental

Les algorithmes de Machine Learning travaillent avec des **mathÃ©matiques** â€” des additions, des multiplications, des distances. Ils ne comprennent que les **nombres**.

```
Ce que VOUS voyez dans le dataset :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Couleur  â”‚ Ville   â”‚ Taille   â”‚ Prix    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Rouge    â”‚ Paris   â”‚ XL       â”‚ 49.99   â”‚
â”‚ Bleu     â”‚ Lyon    â”‚ M        â”‚ 29.99   â”‚
â”‚ Vert     â”‚ Paris   â”‚ S        â”‚ 19.99   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ce que l'ALGORITHME peut traiter :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ c_bleu â”‚ c_vert â”‚ v_lyon â”‚ taille â”‚ prix   â”‚  ...    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   0    â”‚   0    â”‚   0    â”‚   3    â”‚ 49.99  â”‚  ...    â”‚
â”‚   1    â”‚   0    â”‚   1    â”‚   2    â”‚ 29.99  â”‚  ...    â”‚
â”‚   0    â”‚   1    â”‚   0    â”‚   1    â”‚ 19.99  â”‚  ...    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Les trois transformations indispensables

| Transformation | Pourquoi | Exemple |
|---------------|----------|---------|
| **Encodage** | Transformer les catÃ©gories en nombres | "Rouge" â†’ [1, 0, 0] |
| **Scaling** | Mettre les nombres Ã  la mÃªme Ã©chelle | Salaire 50000 â†’ 0.5, Ã‚ge 35 â†’ 0.5 |
| **CrÃ©ation de features** | Extraire de l'information cachÃ©e | Date â†’ jour de semaine, mois, weekend |

> âš ï¸ **Attention** : "Si vous passez des donnÃ©es textuelles brutes Ã  un algorithme sklearn, vous obtiendrez une erreur. Le preprocessing n'est pas optionnel â€” c'est **obligatoire**."

---

## 2. ğŸ·ï¸ Encodage des Variables CatÃ©gorielles

### 2.1 One-Hot Encoding â€” Les Interrupteurs

#### Le principe visuel

Imaginez un panneau de contrÃ´le avec des **interrupteurs**. Pour chaque catÃ©gorie, un interrupteur est soit ON (1) soit OFF (0).

```
Couleur = "Rouge"  â†’  [Rouge=1, Bleu=0, Vert=0]
Couleur = "Bleu"   â†’  [Rouge=0, Bleu=1, Vert=0]
Couleur = "Vert"   â†’  [Rouge=0, Bleu=0, Vert=1]

Visuellement (les interrupteurs) :

Rouge  Bleu   Vert
  â—      â—‹      â—‹     â†’ "Rouge"
  â—‹      â—      â—‹     â†’ "Bleu"
  â—‹      â—‹      â—     â†’ "Vert"

(â— = ON = 1, â—‹ = OFF = 0)
```

#### Quand l'utiliser

| Situation | One-Hot Encoding ? |
|-----------|-------------------|
| Variable **nominale** (pas d'ordre) | âœ… Oui, c'est le choix par dÃ©faut |
| ModÃ¨les **linÃ©aires** (rÃ©gression, SVM) | âœ… Oui, indispensable |
| Peu de catÃ©gories (< 15-20) | âœ… Oui, pas de problÃ¨me |
| Beaucoup de catÃ©gories (> 50) | âš ï¸ Attention, explosion dimensionnelle |
| Arbres de dÃ©cision | ğŸ¤· Pas nÃ©cessaire (mais ne nuit pas) |

#### Le piÃ¨ge : haute cardinalitÃ©

```
Ville avec 500 catÃ©gories â†’ 500 nouvelles colonnes !
                         â†’ Le dataset devient Ã©norme
                         â†’ Le modÃ¨le est lent et peut overfitter
                         â†’ C'est la "curse of dimensionality"
```

#### ImplÃ©mentation

```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

# --- MÃ©thode 1 : pd.get_dummies (simple, rapide) ---
df = pd.DataFrame({
    'couleur': ['Rouge', 'Bleu', 'Vert', 'Rouge', 'Bleu'],
    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon'],
    'prix': [49.99, 29.99, 19.99, 39.99, 24.99]
})

# One-Hot Encoding avec pandas
df_encoded = pd.get_dummies(df, columns=['couleur', 'ville'], drop_first=True)
print("=== pd.get_dummies (drop_first=True) ===")
print(df_encoded)
# drop_first=True : supprime la 1Ã¨re catÃ©gorie (Ã©vite la multicolinÃ©aritÃ©)
# Si couleur_Bleu=0 ET couleur_Vert=0 â†’ c'est forcÃ©ment Rouge
```

```python
# --- MÃ©thode 2 : OneHotEncoder de sklearn (recommandÃ© pour les pipelines) ---
ohe = OneHotEncoder(
    sparse_output=False,        # Retourner un array dense (pas sparse)
    drop='first',               # Supprimer la 1Ã¨re catÃ©gorie
    handle_unknown='ignore'     # Ignorer les catÃ©gories inconnues en production
)

# Fit + Transform
encoded = ohe.fit_transform(df[['couleur', 'ville']])
colonnes = ohe.get_feature_names_out(['couleur', 'ville'])

df_ohe = pd.DataFrame(encoded, columns=colonnes)
print("\n=== OneHotEncoder sklearn ===")
print(df_ohe)
```

> ğŸ’¡ **Conseil** : "Utilisez `drop='first'` pour Ã©viter la **multicolinÃ©aritÃ©** (le piÃ¨ge de la variable factice). Si vous avez 3 couleurs, 2 colonnes suffisent â€” la 3Ã¨me est implicite. C'est important pour les modÃ¨les linÃ©aires."

> âš ï¸ **Attention** : "En production, votre modÃ¨le peut rencontrer des catÃ©gories jamais vues Ã  l'entraÃ®nement. Utilisez `handle_unknown='ignore'` dans `OneHotEncoder` pour Ã©viter les erreurs â€” la catÃ©gorie inconnue sera encodÃ©e comme un vecteur de zÃ©ros."

### 2.2 Label Encoding â€” Transformer en Nombres

#### Le principe

Chaque catÃ©gorie reÃ§oit un nombre entier : 0, 1, 2, 3...

```
Paris     â†’  0
Lyon      â†’  1
Marseille â†’  2
```

#### Le danger : crÃ©er un ordre artificiel

```
âš ï¸ Le modÃ¨le va interprÃ©ter :
   Marseille (2) > Lyon (1) > Paris (0)

   â†’ Il va calculer des distances :
     distance(Paris, Marseille) = 2
     distance(Paris, Lyon) = 1

   â†’ Il va faire des moyennes :
     moyenne(Paris, Marseille) = (0+2)/2 = 1 = Lyon ???

   â†’ C'est ABSURDE pour des variables nominales !
```

#### Quand l'utiliser

| Situation | Label Encoding ? |
|-----------|-----------------|
| Variable **ordinale** (avec un ordre naturel) | âœ… Oui |
| **Arbres de dÃ©cision** / Random Forest | âœ… Oui (ils gÃ¨rent bien) |
| Variable **nominale** | âŒ Non (sauf pour les arbres) |
| ModÃ¨les **linÃ©aires**, SVM, KNN | âŒ Non |
| Variable **cible** (target) | âœ… Oui, toujours |

#### ImplÃ©mentation

```python
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

# --- LabelEncoder : pour la variable cible ---
le = LabelEncoder()
y_encoded = le.fit_transform(['chat', 'chien', 'oiseau', 'chat', 'oiseau'])
print(f"EncodÃ© : {y_encoded}")           # [0, 1, 2, 0, 2]
print(f"Classes : {le.classes_}")         # ['chat', 'chien', 'oiseau']

# Inverser l'encodage
y_original = le.inverse_transform([0, 1, 2])
print(f"DÃ©codÃ© : {y_original}")           # ['chat', 'chien', 'oiseau']
```

```python
# --- OrdinalEncoder : pour les features ordinales ---
# IMPORTANT : spÃ©cifier l'ORDRE des catÃ©gories

df_ordinal = pd.DataFrame({
    'taille': ['M', 'S', 'XL', 'L', 'S', 'M'],
    'satisfaction': ['Neutre', 'MÃ©content', 'Satisfait', 'TrÃ¨s satisfait', 'MÃ©content', 'Satisfait']
})

# DÃ©finir l'ordre pour chaque colonne
oe = OrdinalEncoder(categories=[
    ['S', 'M', 'L', 'XL'],                                    # taille
    ['MÃ©content', 'Neutre', 'Satisfait', 'TrÃ¨s satisfait']    # satisfaction
])

df_ordinal[['taille_enc', 'satisfaction_enc']] = oe.fit_transform(
    df_ordinal[['taille', 'satisfaction']]
)
print(df_ordinal)
# Sâ†’0, Mâ†’1, Lâ†’2, XLâ†’3
# MÃ©contentâ†’0, Neutreâ†’1, Satisfaitâ†’2, TrÃ¨s satisfaitâ†’3
```

### 2.3 Target Encoding â€” Utiliser l'Information de la Cible

#### Le principe

Remplacer chaque catÃ©gorie par la **moyenne de la variable cible** pour cette catÃ©gorie.

```
Ville      | Churn moyen  | Target Encoding
-----------|-------------|----------------
Paris      | 0.25        | 0.25
Lyon       | 0.40        | 0.40
Marseille  | 0.15        | 0.15
```

#### Le risque : Data Leakage

```
âš ï¸  DANGER : le target encoding utilise la variable cible (y) !
    Si vous l'appliquez sur tout le dataset :
    â†’ La feature "voit" directement la target
    â†’ Le modÃ¨le "triche" en utilisant la rÃ©ponse
    â†’ Les performances en train sont artificiellement bonnes
    â†’ En production, les performances s'effondrent
```

#### ImplÃ©mentation sÃ©curisÃ©e

```python
from sklearn.model_selection import KFold
import numpy as np

def target_encoding_cv(df, colonne, target, n_splits=5):
    """
    Target encoding avec cross-validation pour Ã©viter le data leakage.
    Chaque fold utilise les AUTRES folds pour calculer la moyenne.
    """
    df = df.copy()
    df[f'{colonne}_target_enc'] = np.nan

    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    global_mean = df[target].mean()

    for train_idx, val_idx in kf.split(df):
        # Calculer la moyenne sur le fold d'entraÃ®nement
        means = df.iloc[train_idx].groupby(colonne)[target].mean()
        # Appliquer sur le fold de validation
        df.loc[df.index[val_idx], f'{colonne}_target_enc'] = (
            df.iloc[val_idx][colonne].map(means)
        )

    # Remplacer les NaN par la moyenne globale
    df[f'{colonne}_target_enc'].fillna(global_mean, inplace=True)
    return df

# Utilisation
df = target_encoding_cv(df, 'ville', 'churn')
print(df[['ville', 'churn', 'ville_target_enc']].head(10))
```

> âš ï¸ **Attention** : "Le target encoding est puissant mais dangereux. Utilisez TOUJOURS la version avec cross-validation pour Ã©viter le data leakage. Sans cette prÃ©caution, vos performances en validation seront trompeusement bonnes."

### 2.4 Ordinal Encoding â€” Pour les Variables Ordinales

Les variables ordinales ont un **ordre naturel**. L'encodage doit le respecter.

```python
# Exemples de variables ordinales
variables_ordinales = {
    'niveau_etudes': ['Sans diplÃ´me', 'Bac', 'Licence', 'Master', 'Doctorat'],
    'satisfaction': ['TrÃ¨s mÃ©content', 'MÃ©content', 'Neutre', 'Satisfait', 'TrÃ¨s satisfait'],
    'tranche_age': ['18-25', '26-35', '36-45', '46-55', '56+'],
    'priorite': ['Basse', 'Moyenne', 'Haute', 'Critique']
}

from sklearn.preprocessing import OrdinalEncoder

# Encoder en respectant l'ordre
oe = OrdinalEncoder(categories=[variables_ordinales['niveau_etudes']])
df['niveau_etudes_enc'] = oe.fit_transform(df[['niveau_etudes']])
# Sans diplÃ´meâ†’0, Bacâ†’1, Licenceâ†’2, Masterâ†’3, Doctoratâ†’4
```

### 2.5 Tableau rÃ©capitulatif des encodages

| Encodage | Type de variable | CrÃ©e un ordre ? | Nb colonnes | Risque principal | ModÃ¨les compatibles |
|----------|-----------------|----------------|-------------|-----------------|-------------------|
| **One-Hot** | Nominale | Non | N-1 | Haute cardinalitÃ© | Tous |
| **Label** | Target / Ordinale | Oui | 1 | Faux ordre | Arbres |
| **Ordinal** | Ordinale | Oui (contrÃ´lÃ©) | 1 | Aucun si bien fait | Tous |
| **Target** | Nominale haute cardinalitÃ© | Non | 1 | Data leakage | Tous |
| **Frequency** | Nominale | Non | 1 | CatÃ©gories de mÃªme frÃ©quence | Tous |

---

## 3. ğŸ“ Scaling / Normalisation â€” Mettre les Features Ã  la MÃªme Ã‰chelle

### 3.1 Le problÃ¨me concret

```
Imaginons deux features :

  Salaire :   10 000 ... 100 000  (range = 90 000)
  Ã‚ge :       20 ... 70           (range = 50)

Le modÃ¨le calcule des distances :
  distance = âˆš( (salaire1 - salaire2)Â² + (age1 - age2)Â² )

  Client A : salaire=50000, age=30
  Client B : salaire=51000, age=60

  distance = âˆš( (50000-51000)Â² + (30-60)Â² )
           = âˆš( 1000000 + 900 )
           = âˆš1000900
           â‰ˆ 1000.45

  â†’ Le salaire Ã‰CRASE totalement l'Ã¢ge !
  â†’ Le modÃ¨le "croit" que le salaire est 1000x plus important
  â†’ Mais c'est juste une question d'Ã‰CHELLE, pas d'importance
```

> ğŸ’¡ **Conseil** : "Le scaling n'est pas optionnel pour KNN, SVM et les rÃ©seaux de neurones. Sans scaling, ces modÃ¨les donnent des rÃ©sultats catastrophiques car ils sont basÃ©s sur des calculs de distance."

### 3.2 MinMaxScaler â€” Remettre entre 0 et 1

**Formule** : `x_norm = (x - min) / (max - min)`

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# DonnÃ©es d'exemple
X = np.array([[50000, 30], [80000, 45], [30000, 25], [100000, 60]])

scaler = MinMaxScaler()  # Par dÃ©faut : [0, 1]
X_scaled = scaler.fit_transform(X)

print("Avant scaling :")
print(X)
print("\nAprÃ¨s MinMaxScaler :")
print(X_scaled)
# Salaire : 30000â†’0.0, 100000â†’1.0
# Ã‚ge :     25â†’0.0, 60â†’1.0
```

### 3.3 StandardScaler â€” Centrer et RÃ©duire (moyenne=0, Ã©cart-type=1)

**Formule** : `z = (x - moyenne) / Ã©cart-type`

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("AprÃ¨s StandardScaler :")
print(X_scaled)
print(f"Moyenne : {X_scaled.mean(axis=0)}")  # â‰ˆ [0, 0]
print(f"Std :     {X_scaled.std(axis=0)}")   # â‰ˆ [1, 1]
```

### 3.4 RobustScaler â€” RÃ©sistant aux Outliers

**Formule** : `x_robust = (x - mÃ©diane) / IQR`

```python
from sklearn.preprocessing import RobustScaler

# DonnÃ©es avec un outlier extrÃªme
X_avec_outlier = np.array([[50000, 30], [80000, 45], [30000, 25], [1000000, 60]])

# Comparer les scalers
scaler_standard = StandardScaler()
scaler_robust = RobustScaler()

X_standard = scaler_standard.fit_transform(X_avec_outlier)
X_robust = scaler_robust.fit_transform(X_avec_outlier)

print("StandardScaler (sensible Ã  l'outlier 1M) :")
print(X_standard)
print("\nRobustScaler (rÃ©sistant Ã  l'outlier 1M) :")
print(X_robust)
```

### 3.5 Tableau comparatif des scalers

| Scaler | Formule | RÃ©sultat | Sensible aux outliers ? | Quand l'utiliser |
|--------|---------|---------|------------------------|-----------------|
| **MinMaxScaler** | `(x-min)/(max-min)` | Valeurs dans [0, 1] | **TrÃ¨s sensible** | Besoin de bornes fixes, pas d'outliers |
| **StandardScaler** | `(x-Î¼)/Ïƒ` | Moyenne=0, Std=1 | Sensible | Cas gÃ©nÃ©ral, distribution ~normale |
| **RobustScaler** | `(x-mÃ©diane)/IQR` | CentrÃ© sur mÃ©diane | **Robuste** | Beaucoup d'outliers |

### 3.6 Impact du scaling sur les modÃ¨les

| ModÃ¨le | Sensible Ã  l'Ã©chelle ? | Scaling nÃ©cessaire ? | Pourquoi |
|--------|----------------------|---------------------|---------|
| **KNN** | TrÃ¨s sensible | **Obligatoire** | Calcul de distances |
| **SVM** | TrÃ¨s sensible | **Obligatoire** | Calcul de distances et marges |
| **RÃ©gression linÃ©aire** | Partiellement | RecommandÃ© | InterprÃ©tation des coefficients |
| **RÃ©gression logistique** | Oui | Oui | Convergence du gradient |
| **RÃ©seaux de neurones** | TrÃ¨s sensible | **Obligatoire** | Convergence du gradient |
| **Arbres de dÃ©cision** | Non | Non | Splits sur des seuils |
| **Random Forest** | Non | Non | Ensemble d'arbres |
| **Gradient Boosting** | Non | Non | Ensemble d'arbres |

> ğŸ’¡ **Conseil** : "En cas de doute, appliquez `StandardScaler`. Si vos donnÃ©es ont beaucoup d'outliers, passez Ã  `RobustScaler`. Utilisez `MinMaxScaler` uniquement si vous avez besoin de valeurs strictement dans [0, 1] et que vos donnÃ©es n'ont pas d'outliers."

> âš ï¸ **Attention** : "Le scaler doit Ãªtre `fit` sur le **train set uniquement** et `transform` sur le train ET le test set. Si vous faites `fit_transform` sur tout le dataset avant le split â†’ **data leakage** !"

```python
from sklearn.model_selection import train_test_split

# âœ… BON : fit sur le train, transform sur le test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)   # fit + transform
X_test_scaled = scaler.transform(X_test)          # transform SEULEMENT

# âŒ MAUVAIS : fit_transform sur tout le dataset
# scaler.fit_transform(X)  # â† LEAKAGE !
```

---

## 4. âš™ï¸ CrÃ©ation de Features â€” Extraire l'Information CachÃ©e

### 4.1 Combinaisons de features existantes

```python
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'prix': [29.99, 149.50, 9.99, 499.00, 74.50],
    'quantite': [2, 1, 5, 1, 3],
    'surface_m2': [45, 120, 30, 200, 75],
    'nb_pieces': [2, 5, 1, 8, 3],
    'revenu_mensuel': [2500, 4500, 1800, 8000, 3200],
    'loyer': [800, 1200, 600, 2500, 950]
})

# --- Ratios ---
df['montant_total'] = df['prix'] * df['quantite']
df['prix_par_m2'] = df['prix'] / df['surface_m2']
df['surface_par_piece'] = df['surface_m2'] / df['nb_pieces']
df['taux_effort_logement'] = df['loyer'] / df['revenu_mensuel']

# --- Transformations mathÃ©matiques ---
df['log_prix'] = np.log1p(df['prix'])           # log(1+x) pour gÃ©rer les 0
df['sqrt_surface'] = np.sqrt(df['surface_m2'])   # racine carrÃ©e
df['prix_carre'] = df['prix'] ** 2               # polynomiale

print(df)
```

> ğŸ’¡ **Conseil** : "Les ratios sont souvent les features les plus puissantes. Le prix par m2 est plus informatif que le prix seul. Le taux d'effort logement (loyer/revenu) est plus informatif que le loyer seul. RÃ©flÃ©chissez en termes de ratios mÃ©tier."

### 4.2 Binning â€” DiscrÃ©tiser des Variables Continues

```python
# --- Binning par intervalles fixes ---
df['tranche_prix'] = pd.cut(
    df['prix'],
    bins=[0, 20, 100, 500],
    labels=['Pas cher', 'Moyen', 'Cher']
)

# --- Binning par quantiles (mÃªme nombre d'observations par bin) ---
df['quantile_revenu'] = pd.qcut(
    df['revenu_mensuel'],
    q=3,
    labels=['Bas', 'Moyen', 'Haut']
)

# --- Binning personnalisÃ© mÃ©tier ---
def categoriser_age(age):
    if age < 25:
        return 'Jeune'
    elif age < 45:
        return 'Adulte'
    elif age < 65:
        return 'Senior'
    else:
        return 'RetraitÃ©'

# df['categorie_age'] = df['age'].apply(categoriser_age)

print(df[['prix', 'tranche_prix', 'revenu_mensuel', 'quantile_revenu']])
```

### 4.3 Features Temporelles

```python
df_time = pd.DataFrame({
    'date_achat': pd.to_datetime([
        '2024-01-15 14:30:00',
        '2024-03-22 09:15:00',
        '2024-07-04 22:45:00',
        '2024-12-25 11:00:00',
        '2024-06-15 16:30:00'
    ])
})

# Features de base
df_time['annee'] = df_time['date_achat'].dt.year
df_time['mois'] = df_time['date_achat'].dt.month
df_time['jour'] = df_time['date_achat'].dt.day
df_time['heure'] = df_time['date_achat'].dt.hour
df_time['jour_semaine'] = df_time['date_achat'].dt.dayofweek  # 0=lundi

# Features dÃ©rivÃ©es
df_time['est_weekend'] = df_time['jour_semaine'].isin([5, 6]).astype(int)
df_time['est_matin'] = (df_time['heure'] < 12).astype(int)
df_time['trimestre'] = df_time['date_achat'].dt.quarter

# Encodage cyclique (capturer la circularitÃ© du temps)
# Janvier (1) et DÃ©cembre (12) sont proches, mais 1 et 12 sont loin en nombre
df_time['mois_sin'] = np.sin(2 * np.pi * df_time['mois'] / 12)
df_time['mois_cos'] = np.cos(2 * np.pi * df_time['mois'] / 12)
df_time['heure_sin'] = np.sin(2 * np.pi * df_time['heure'] / 24)
df_time['heure_cos'] = np.cos(2 * np.pi * df_time['heure'] / 24)

print(df_time)
```

> ğŸ’¡ **Conseil** : "L'encodage cyclique (sin/cos) est crucial pour les heures et les mois. Sans cela, le modÃ¨le croit que DÃ©cembre (12) et Janvier (1) sont trÃ¨s Ã©loignÃ©s alors qu'ils sont consÃ©cutifs. Avec sin/cos, les valeurs cycliques sont correctement reprÃ©sentÃ©es."

### 4.4 Features Textuelles

```python
df_text = pd.DataFrame({
    'commentaire': [
        'Excellent produit, livraison rapide !',
        'Nul. Produit cassÃ© Ã  la rÃ©ception.',
        'Correct pour le prix. RAS.',
        'INCROYABLE !!! Le meilleur achat de ma vie !!!',
        'Bof, pas terrible.'
    ]
})

# Features simples mais efficaces
df_text['nb_mots'] = df_text['commentaire'].str.split().str.len()
df_text['nb_caracteres'] = df_text['commentaire'].str.len()
df_text['nb_exclamation'] = df_text['commentaire'].str.count('!')
df_text['nb_points_inter'] = df_text['commentaire'].str.count('\\?')
df_text['nb_majuscules'] = df_text['commentaire'].str.count('[A-Z]')
df_text['ratio_majuscules'] = df_text['nb_majuscules'] / df_text['nb_caracteres']
df_text['longueur_moy_mot'] = df_text['nb_caracteres'] / df_text['nb_mots']

print(df_text)
```

> ğŸ’¡ **Conseil** : "Ces features textuelles simples sont souvent trÃ¨s prÃ©dictives. Un commentaire avec beaucoup de majuscules et de points d'exclamation exprime un sentiment fort (positif ou nÃ©gatif). Testez-les toujours en complÃ©ment du TF-IDF."

---

## 5. ğŸ” SÃ©lection de Features â€” Garder l'Essentiel

Trop de features = bruit, overfitting, lenteur. La sÃ©lection de features Ã©limine les features inutiles ou redondantes.

### 5.1 Variance Threshold â€” Supprimer les Features Constantes

```python
from sklearn.feature_selection import VarianceThreshold

# Supprimer les features avec une variance quasi nulle
selector = VarianceThreshold(threshold=0.01)  # variance < 0.01 â†’ supprimÃ©e
X_selected = selector.fit_transform(X)

# Quelles features ont Ã©tÃ© gardÃ©es ?
mask = selector.get_support()
features_gardees = [f for f, m in zip(feature_names, mask) if m]
features_supprimees = [f for f, m in zip(feature_names, mask) if not m]

print(f"Features gardÃ©es : {len(features_gardees)}")
print(f"Features supprimÃ©es (variance trop faible) : {features_supprimees}")
```

### 5.2 CorrÃ©lation avec la Target

```python
import pandas as pd
import numpy as np

# CorrÃ©lation de chaque feature avec la cible
correlations = df[colonnes_num].corrwith(df['target']).abs().sort_values(ascending=False)

print("=== CorrÃ©lation avec la target ===")
print(correlations)

# Garder les features avec |corrÃ©lation| > seuil
seuil = 0.05
features_pertinentes = correlations[correlations > seuil].index.tolist()
features_inutiles = correlations[correlations <= seuil].index.tolist()

print(f"\nFeatures pertinentes (|r| > {seuil}) : {features_pertinentes}")
print(f"Features Ã  supprimer (|r| â‰¤ {seuil}) : {features_inutiles}")
```

### 5.3 Feature Importance (aperÃ§u)

```python
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# EntraÃ®ner un Random Forest pour obtenir l'importance des features
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# Importance des features
importances = pd.Series(rf.feature_importances_, index=feature_names)
importances = importances.sort_values(ascending=True)

# Visualisation
plt.figure(figsize=(10, 8))
importances.plot(kind='barh', color='steelblue')
plt.title("Importance des features (Random Forest)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# Top 10 features
print("Top 10 features les plus importantes :")
print(importances.sort_values(ascending=False).head(10))
```

> ğŸ’¡ **Conseil** : "L'importance des features du Random Forest est un excellent point de dÃ©part pour comprendre quelles features comptent. Mais attention : elle est biaisÃ©e en faveur des features Ã  haute cardinalitÃ© et des features numÃ©riques. Utilisez-la comme un indicateur, pas comme une vÃ©ritÃ© absolue."

---

## 6. ğŸ”— Pipelines scikit-learn â€” Tout Assembler

### 6.1 Pourquoi les Pipelines sont Indispensables

```
SANS Pipeline :                    AVEC Pipeline :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Imputer     â”‚ â†’ fit_transform   â”‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    sur tout ?     â”‚  Pipeline               â”‚
â”‚ Scaler      â”‚ â†’ fit_transform   â”‚  â”Œâ”€ Imputer            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    sur tout ?     â”‚  â”œâ”€ Scaler             â”‚
â”‚ Encoder     â”‚ â†’ fit_transform   â”‚  â”œâ”€ Encoder            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    sur tout ?     â”‚  â””â”€ ModÃ¨le             â”‚
â”‚ ModÃ¨le      â”‚                   â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  .fit(X_train, y_train) â”‚
                                  â”‚  .predict(X_test)       â”‚
âŒ Risque de leakage              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âŒ Code fragile                    âœ… Pas de leakage
âŒ Non reproductible               âœ… Reproductible
âŒ Difficile Ã  dÃ©ployer            âœ… Un seul objet Ã  sauvegarder
```

### 6.2 ColumnTransformer â€” Traitement DiffÃ©renciÃ© par Type

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier

# Identifier les colonnes par type
colonnes_num = ['age', 'revenu', 'nb_achats', 'anciennete_mois']
colonnes_cat_nominales = ['ville', 'canal_acquisition']
colonnes_cat_ordinales = ['niveau_etudes']

# --- Transformations pour les colonnes numÃ©riques ---
pipeline_num = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# --- Transformations pour les colonnes catÃ©gorielles nominales ---
pipeline_cat_nom = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
])

# --- Transformations pour les colonnes catÃ©gorielles ordinales ---
pipeline_cat_ord = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OrdinalEncoder(categories=[
        ['Sans diplÃ´me', 'Bac', 'Licence', 'Master', 'Doctorat']
    ]))
])

# --- Combiner avec ColumnTransformer ---
preprocessor = ColumnTransformer(
    transformers=[
        ('num', pipeline_num, colonnes_num),
        ('cat_nom', pipeline_cat_nom, colonnes_cat_nominales),
        ('cat_ord', pipeline_cat_ord, colonnes_cat_ordinales)
    ],
    remainder='drop'  # Supprimer les colonnes non listÃ©es
)
```

### 6.3 Pipeline Complet : Preprocessing + ModÃ¨le

```python
from sklearn.model_selection import train_test_split, cross_val_score

# Pipeline complet
pipeline_complet = Pipeline([
    ('preprocessing', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42))
])

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# EntraÃ®ner
pipeline_complet.fit(X_train, y_train)

# Ã‰valuer
score_train = pipeline_complet.score(X_train, y_train)
score_test = pipeline_complet.score(X_test, y_test)
print(f"Score train : {score_train:.4f}")
print(f"Score test  : {score_test:.4f}")

# Cross-validation (plus fiable)
scores_cv = cross_val_score(pipeline_complet, X, y, cv=5, scoring='roc_auc', n_jobs=-1)
print(f"AUC-ROC (5-Fold CV) : {scores_cv.mean():.4f} (+/- {scores_cv.std():.4f})")
```

### 6.4 Tuning du Pipeline avec GridSearchCV

```python
from sklearn.model_selection import GridSearchCV

# Grille d'hyperparamÃ¨tres
# Syntaxe : 'Ã©tape__sous_Ã©tape__paramÃ¨tre'
param_grid = {
    'preprocessing__num__imputer__strategy': ['mean', 'median'],
    'preprocessing__num__scaler': [StandardScaler(), MinMaxScaler()],
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [5, 10, 15, None],
}

grid = GridSearchCV(
    pipeline_complet,
    param_grid=param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

grid.fit(X_train, y_train)

print(f"Meilleurs paramÃ¨tres : {grid.best_params_}")
print(f"Meilleur AUC-ROC (CV) : {grid.best_score_:.4f}")
print(f"Score sur test set : {grid.score(X_test, y_test):.4f}")
```

### 6.5 Sauvegarder et Charger le Pipeline

```python
import joblib

# Sauvegarder le meilleur pipeline (preprocessing + modÃ¨le)
best_pipeline = grid.best_estimator_
joblib.dump(best_pipeline, 'pipeline_churn_v1.joblib')
print("Pipeline sauvegardÃ© : pipeline_churn_v1.joblib")

# Charger en production
pipeline_prod = joblib.load('pipeline_churn_v1.joblib')

# PrÃ©dire sur des donnÃ©es brutes (le pipeline fait TOUT)
nouveau_client = pd.DataFrame({
    'age': [35],
    'revenu': [52000],
    'nb_achats': [3],
    'anciennete_mois': [6],
    'ville': ['Paris'],
    'canal_acquisition': ['Web'],
    'niveau_etudes': ['Master']
})

prediction = pipeline_prod.predict(nouveau_client)
proba = pipeline_prod.predict_proba(nouveau_client)[:, 1]
print(f"PrÃ©diction : {'Churn' if prediction[0] else 'Pas de churn'}")
print(f"ProbabilitÃ© de churn : {proba[0]:.2%}")
```

> ğŸ’¡ **Conseil** : "En production, vous ne sauvegardez JAMAIS le modÃ¨le seul. Vous sauvegardez le **pipeline complet** (imputation + encodage + scaling + modÃ¨le). Ainsi, les donnÃ©es brutes entrent directement et les prÃ©dictions sortent. Pas besoin de recoder le preprocessing."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Les algorithmes ML ne comprennent que les nombres** â€” tout texte, catÃ©gorie ou date doit Ãªtre transformÃ© en reprÃ©sentation numÃ©rique
2. **One-Hot Encoding pour les nominales** â€” crÃ©er des colonnes binaires (interrupteurs), attention Ã  la haute cardinalitÃ© (> 50 catÃ©gories)
3. **Label/Ordinal Encoding pour les ordinales** â€” respecter l'ordre naturel des catÃ©gories, ne JAMAIS l'utiliser sur du nominal pour les modÃ¨les linÃ©aires
4. **Target Encoding avec cross-validation** â€” puissant pour la haute cardinalitÃ© mais risque de data leakage sans prÃ©cautions
5. **Le scaling est obligatoire pour KNN, SVM et rÃ©seaux de neurones** â€” StandardScaler par dÃ©faut, RobustScaler si outliers, MinMaxScaler si besoin de bornes [0,1]
6. **CrÃ©er des ratios et combinaisons** â€” prix/m2, taux d'effort, features temporelles (weekend, mois cyclique) â€” c'est lÃ  que la connaissance mÃ©tier fait la diffÃ©rence
7. **SÃ©lectionner les features pertinentes** â€” variance threshold, corrÃ©lation, importance RF â€” trop de features = overfitting
8. **Les Pipelines sklearn sont indispensables** â€” pas de data leakage, code reproductible, dÃ©ploiement en un objet
9. **ColumnTransformer pour traiter diffÃ©remment chaque type** â€” numÃ©riques (impute + scale), catÃ©gorielles (impute + encode), ordinales (impute + ordinal encode)
10. **Sauvegarder le pipeline complet**, jamais le modÃ¨le seul â€” en production, les donnÃ©es brutes entrent et les prÃ©dictions sortent

---

## âœ… Checklist de validation

- [ ] Je comprends pourquoi les algorithmes ML ne peuvent pas traiter du texte ou des catÃ©gories directement
- [ ] Je sais utiliser One-Hot Encoding (pd.get_dummies et OneHotEncoder) avec `drop='first'`
- [ ] Je connais le danger du Label Encoding sur les variables nominales
- [ ] Je sais quand utiliser One-Hot, Label, Ordinal et Target Encoding
- [ ] Je comprends le problÃ¨me d'Ã©chelle entre features et ses consÃ©quences sur KNN/SVM
- [ ] Je sais choisir entre MinMaxScaler, StandardScaler et RobustScaler
- [ ] Je sais crÃ©er des features (ratios, binning, temporelles, textuelles)
- [ ] Je sais sÃ©lectionner les features pertinentes (variance, corrÃ©lation, importance)
- [ ] Je maÃ®trise Pipeline et ColumnTransformer de scikit-learn
- [ ] Je sais intÃ©grer le tuning (GridSearchCV) dans un pipeline
- [ ] Je sais sauvegarder et charger un pipeline complet avec joblib
- [ ] Je respecte la rÃ¨gle : fit sur le train, transform sur le test â€” TOUJOURS

---

**PrÃ©cÃ©dent** : [Chapitre 6 : Comprendre ses DonnÃ©es](06-comprendre-donnees.md)

**Suivant** : [Chapitre 8 : Data Leakage â€” Le Crime Parfait du ML](08-data-leakage.md)
