# Chapitre 5 : Classification ‚Äì Pr√©dire des Cat√©gories

## üéØ Objectifs

- Ma√Ætriser les algorithmes de classification fondamentaux (Logistique, KNN, SVM, Arbres)
- Comprendre en profondeur **toutes** les m√©triques de classification
- Savoir interpr√©ter une matrice de confusion et une courbe ROC
- Choisir la bonne m√©trique selon le contexte m√©tier
- Ajuster le seuil de d√©cision pour optimiser les performances
- Savoir choisir le bon algorithme pour le bon probl√®me

---

## 1. üìä R√©gression Logistique

### 1.1 Concept

Malgr√© son nom, la r√©gression logistique est un **classifieur** (pas un mod√®le de r√©gression). Elle pr√©dit la **probabilit√©** qu'un √©chantillon appartienne √† une classe.

**Fonction sigmo√Øde** : transforme n'importe quel nombre en probabilit√© [0, 1]

```
œÉ(z) = 1 / (1 + e^(-z))

O√π z = b‚ÇÄ + b‚ÇÅ*x‚ÇÅ + b‚ÇÇ*x‚ÇÇ + ... + b‚Çô*x‚Çô (comme une r√©gression lin√©aire)

Si œÉ(z) ‚â• 0.5 ‚Üí Classe 1 (positif)
Si œÉ(z) < 0.5 ‚Üí Classe 0 (n√©gatif)
```

```
  Probabilit√©
  1.0 ‚îÇ                    ___________
      ‚îÇ                ‚ï±
      ‚îÇ              ‚ï±
  0.5 ‚îÇ- - - - - -‚ï±- - - - - - - - - -  ‚Üê Seuil de d√©cision
      ‚îÇ         ‚ï±
      ‚îÇ       ‚ï±
  0.0 ‚îÇ______‚ï±
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ z
       Classe 0    ‚îÇ    Classe 1
```

### 1.2 Impl√©mentation

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import classification_report, confusion_matrix

# --- Charger les donn√©es ---
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target  # 0 = malin, 1 = b√©nin

# --- Pr√©parer ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Entra√Æner ---
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# --- Pr√©dire ---
y_pred = log_reg.predict(X_test_scaled)           # Classe pr√©dite (0 ou 1)
y_proba = log_reg.predict_proba(X_test_scaled)     # Probabilit√©s [P(0), P(1)]

print("=== Exemples de pr√©dictions ===")
for i in range(5):
    print(f"  R√©el: {y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]}, "
          f"Pr√©dit: {y_pred[i]}, "
          f"Proba(b√©nin): {y_proba[i][1]:.3f}")

# --- √âvaluer ---
print("\n=== Rapport de classification ===")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))
```

> üí° **Conseil** : "**Toujours** commencer par la r√©gression logistique comme baseline pour un probl√®me de classification. Elle est rapide, interpr√©table et souvent plus performante qu'on ne le croit."

> üí° **Conseil de pro** : "Utilisez `predict_proba()` plut√¥t que `predict()` quand c'est possible. Les probabilit√©s donnent plus d'information que la classe seule et permettent d'ajuster le seuil de d√©cision."

---

## 2. üéØ K-Nearest Neighbors (KNN)

### 2.1 Concept

KNN est l'algorithme le plus intuitif : pour classifier un point, on regarde les **K voisins les plus proches** et on vote.

```
Nouveau point (?) ‚Üí On regarde les K=5 voisins les plus proches :
  - 3 sont de classe A (‚óè)
  - 2 sont de classe B (‚ñ†)
  ‚Üí Vote majoritaire ‚Üí Classe A ‚úÖ
```

### 2.2 Impl√©mentation

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# --- Entra√Æner KNN ---
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn.fit(X_train_scaled, y_train)  # ATTENTION : il faut normaliser !
y_pred_knn = knn.predict(X_test_scaled)

print(f"Accuracy KNN (K=5) : {accuracy_score(y_test, y_pred_knn):.4f}")

# --- Trouver le meilleur K ---
import matplotlib.pyplot as plt

k_values = range(1, 31)
scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    scores.append(knn.score(X_test_scaled, y_test))

plt.figure(figsize=(10, 5))
plt.plot(k_values, scores, 'o-')
plt.xlabel('K (nombre de voisins)')
plt.ylabel('Accuracy')
plt.title('Score en fonction de K')
plt.axvline(x=k_values[np.argmax(scores)], color='red', linestyle='--',
            label=f'Meilleur K = {k_values[np.argmax(scores)]}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"Meilleur K : {k_values[np.argmax(scores)]}, Accuracy : {max(scores):.4f}")
```

### 2.3 Param√®tres importants

| Param√®tre | Options | Impact |
|-----------|---------|--------|
| **n_neighbors (K)** | Entier impair (3, 5, 7...) | K petit ‚Üí overfitting, K grand ‚Üí underfitting |
| **metric** | 'euclidean', 'manhattan', 'minkowski' | Distance utilis√©e pour trouver les voisins |
| **weights** | 'uniform', 'distance' | 'distance' donne plus de poids aux voisins proches |

> ‚ö†Ô∏è **Attention** : "KNN est **tr√®s sensible** √† l'√©chelle des features. Si une feature va de 0 √† 1000 et une autre de 0 √† 1, la premi√®re dominera le calcul de distance. **TOUJOURS normaliser** avant d'utiliser KNN."

> üí° **Conseil** : "KNN ne fonctionne pas bien en haute dimension (**curse of dimensionality**). Au-del√† de 20-30 features, les distances deviennent moins significatives. Envisagez une r√©duction de dimension (PCA) avant."

---

## 3. ‚ö° Support Vector Machine (SVM)

### 3.1 Concept

SVM cherche l'**hyperplan** qui s√©pare le mieux les classes, en maximisant la **marge** (distance) entre les classes.

```
  Classe B (‚ñ†)
       ‚ñ† ‚ñ†
      ‚ñ†  ‚ñ†          Marge maximale
     ‚ñ†    ‚ñ†       ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hyperplan optimal ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             ‚óè      ‚óè
              ‚óè ‚óè  ‚óè
               ‚óè  ‚óè
              Classe A (‚óè)
```

### 3.2 Le Kernel Trick

Quand les donn√©es ne sont pas lin√©airement s√©parables, le **kernel trick** projette les donn√©es dans un espace de dimension sup√©rieure o√π elles deviennent s√©parables.

| Kernel | Quand l'utiliser | Complexit√© |
|--------|-----------------|-----------|
| **linear** | Donn√©es lin√©airement s√©parables, beaucoup de features | Faible |
| **rbf** (d√©faut) | Cas g√©n√©ral, relations non-lin√©aires | Moyenne |
| **poly** | Relations polynomiales | √âlev√©e |

### 3.3 Impl√©mentation

```python
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# --- SVM avec kernel RBF (par d√©faut) ---
svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, probability=True)
svm.fit(X_train_scaled, y_train)
y_pred_svm = svm.predict(X_test_scaled)

print("=== SVM (kernel=rbf) ===")
print(classification_report(y_test, y_pred_svm))

# --- Comparer les kernels ---
kernels = ['linear', 'rbf', 'poly']
for kernel in kernels:
    svm = SVC(kernel=kernel, random_state=42)
    svm.fit(X_train_scaled, y_train)
    score = svm.score(X_test_scaled, y_test)
    print(f"Kernel {kernel:>8} ‚Üí Accuracy: {score:.4f}")
```

### 3.4 Param√®tres importants

| Param√®tre | R√¥le | Impact |
|-----------|------|--------|
| **C** | Force de r√©gularisation | C petit ‚Üí marge large (plus de tol√©rance), C grand ‚Üí marge √©troite (moins de tol√©rance) |
| **gamma** | Influence de chaque point | gamma grand ‚Üí chaque point a une petite zone d'influence, gamma petit ‚Üí zone large |
| **kernel** | Type de transformation | D√©finit la complexit√© de la fronti√®re de d√©cision |

> üí° **Conseil de pro** : "SVM excelle quand le **nombre de features est sup√©rieur au nombre d'√©chantillons** (nb_features > nb_samples). C'est aussi un bon choix pour les petits et moyens datasets (<10 000 √©chantillons)."

> ‚ö†Ô∏è **Attention** : "SVM ne passe pas bien √† l'√©chelle. Sur de grands datasets (>100 000 √©chantillons), pr√©f√©rez la r√©gression logistique ou les arbres de d√©cision."

---

## 4. üå≥ Arbres de D√©cision

### 4.1 Concept

Un arbre de d√©cision est une suite de **questions if/else** qui partitionnent les donn√©es jusqu'√† une pr√©diction.

```
                    [surface > 50m¬≤?]
                    /              \
                  Oui              Non
                  /                  \
       [quartier = centre?]     [√©tage > 3?]
        /            \           /         \
      Oui           Non        Oui        Non
       |              |          |           |
   Cher (A)    Moyen (B)   Moyen (B)   Pas cher (C)
```

### 4.2 Impl√©mentation

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# --- Entra√Æner ---
arbre = DecisionTreeClassifier(
    max_depth=4,           # Profondeur maximale (√©viter l'overfitting)
    min_samples_split=10,  # Minimum d'√©chantillons pour splitter
    min_samples_leaf=5,    # Minimum d'√©chantillons par feuille
    random_state=42
)
arbre.fit(X_train, y_train)  # PAS besoin de normaliser !
y_pred_arbre = arbre.predict(X_test)

print(f"Accuracy Arbre : {arbre.score(X_test, y_test):.4f}")

# --- Visualiser l'arbre ---
plt.figure(figsize=(20, 10))
plot_tree(
    arbre,
    feature_names=cancer.feature_names,
    class_names=cancer.target_names,
    filled=True,           # Colorer selon la classe
    rounded=True,
    fontsize=8
)
plt.title("Arbre de d√©cision")
plt.tight_layout()
plt.show()

# --- Importance des features ---
importances = pd.DataFrame({
    'Feature': cancer.feature_names,
    'Importance': arbre.feature_importances_
}).sort_values('Importance', ascending=False)

print("\n=== Top 10 features les plus importantes ===")
print(importances.head(10))

# Visualiser
plt.figure(figsize=(10, 6))
top_10 = importances.head(10)
plt.barh(top_10['Feature'], top_10['Importance'])
plt.xlabel('Importance')
plt.title("Importance des features (Arbre de D√©cision)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

### 4.3 Crit√®res de split

| Crit√®re | Formule | Quand l'utiliser |
|---------|---------|-----------------|
| **Gini** (d√©faut) | `1 - Œ£(p·µ¢¬≤)` | Cas g√©n√©ral, rapide |
| **Entropie** | `-Œ£(p·µ¢ * log‚ÇÇ(p·µ¢))` | Plus th√©orique, r√©sultats similaires |

### 4.4 Avantages et Inconv√©nients

| Avantages | Inconv√©nients |
|-----------|---------------|
| ‚úÖ Tr√®s interpr√©table | ‚ùå Overfitting facile |
| ‚úÖ Pas besoin de normaliser | ‚ùå Instable (petits changements ‚Üí arbre diff√©rent) |
| ‚úÖ G√®re num√©riques et cat√©gorielles | ‚ùå Biais√© vers les features avec beaucoup de valeurs |
| ‚úÖ Rapide √† entra√Æner | ‚ùå Limit√© pour les relations complexes |
| ‚úÖ Visualisable | ‚ùå Performances inf√©rieures aux ensembles |

> üí° **Conseil** : "Toujours limiter `max_depth` pour √©viter l'overfitting. Un arbre trop profond m√©morise les donn√©es au lieu d'apprendre les patterns."

> üí° **Conseil de pro** : "Les arbres de d√©cision seuls sont rarement les meilleurs mod√®les, mais ils sont la brique de base des **Random Forests** et du **Gradient Boosting** (XGBoost, LightGBM) qui sont parmi les meilleurs algorithmes de ML."

---

## 5. üìä M√âTRIQUES DE CLASSIFICATION

C'est la section la plus importante de ce chapitre. Bien comprendre les m√©triques est **essentiel** pour √©valuer correctement un classifieur.

### 5.1 Matrice de Confusion

La matrice de confusion est le point de d√©part de **toutes** les m√©triques de classification.

```
                        PR√âDICTION
                    Positif    N√©gatif
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  Positif  ‚îÇ    TP     ‚îÇ    FN    ‚îÇ   ‚Üê Vrais positifs
R√âALIT√â    ‚îÇ (Vrai     ‚îÇ (Faux    ‚îÇ
           ‚îÇ  Positif) ‚îÇ  N√©gatif)‚îÇ
           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  N√©gatif  ‚îÇ    FP     ‚îÇ    TN    ‚îÇ   ‚Üê Vrais n√©gatifs
           ‚îÇ (Faux     ‚îÇ (Vrai    ‚îÇ
           ‚îÇ  Positif) ‚îÇ  N√©gatif)‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Analogie m√©dicale** (test de d√©pistage du cancer) :

| Terme | Signification | Exemple m√©dical | Cons√©quence |
|-------|-------------|-----------------|-------------|
| **TP** (Vrai Positif) | Pr√©dit positif, est positif | Malade d√©tect√© comme malade | ‚úÖ Bon |
| **TN** (Vrai N√©gatif) | Pr√©dit n√©gatif, est n√©gatif | Sain d√©tect√© comme sain | ‚úÖ Bon |
| **FP** (Faux Positif) | Pr√©dit positif, est n√©gatif | Sain d√©tect√© comme malade | ‚ö†Ô∏è Stress, examens inutiles |
| **FN** (Faux N√©gatif) | Pr√©dit n√©gatif, est positif | Malade non d√©tect√© | ‚ùå **DANGEREUX** (cancer non trait√©) |

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Calculer la matrice de confusion
cm = confusion_matrix(y_test, y_pred)
print("Matrice de confusion :")
print(cm)

# Visualiser
fig, ax = plt.subplots(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cancer.target_names)
disp.plot(cmap='Blues', ax=ax, values_format='d')
plt.title('Matrice de Confusion')
plt.tight_layout()
plt.show()

# Extraire TP, TN, FP, FN (pour classification binaire)
tn, fp, fn, tp = cm.ravel()
print(f"\nTP (Vrais Positifs) : {tp}")
print(f"TN (Vrais N√©gatifs) : {tn}")
print(f"FP (Faux Positifs)  : {fp}")
print(f"FN (Faux N√©gatifs)  : {fn}")
```

### 5.2 Accuracy (Exactitude)

**Formule** : `Accuracy = (TP + TN) / (TP + TN + FP + FN)`

```python
from sklearn.metrics import accuracy_score

acc = accuracy_score(y_test, y_pred)
print(f"Accuracy : {acc:.4f}")
# Interpr√©tation : "X% des pr√©dictions sont correctes"
```

> ‚ö†Ô∏è **PI√àGE MAJEUR** : "Une accuracy de 95% sur un dataset 95/5 d√©s√©quilibr√© est **INUTILE**. Un mod√®le qui pr√©dit toujours la classe majoritaire obtient 95% d'accuracy sans rien avoir appris !"

```python
# D√©monstration du pi√®ge de l'accuracy
import numpy as np

# Dataset d√©s√©quilibr√© : 95% classe 0, 5% classe 1
y_desequilibre = np.array([0]*950 + [1]*50)

# Mod√®le "stupide" : pr√©dit toujours 0
y_stupide = np.zeros_like(y_desequilibre)

print(f"Accuracy du mod√®le 'stupide' : {accuracy_score(y_desequilibre, y_stupide):.2%}")
# ‚Üí 95.00% ! Mais il ne d√©tecte AUCUN cas positif !
```

### 5.3 Precision (Pr√©cision)

**Question** : "Parmi ceux que j'ai **pr√©dit positifs**, combien le sont **vraiment** ?"

**Formule** : `Precision = TP / (TP + FP)`

```python
from sklearn.metrics import precision_score

prec = precision_score(y_test, y_pred)
print(f"Precision : {prec:.4f}")
```

| Quand c'est important | Pourquoi | Exemple |
|----------------------|---------|---------|
| **FP co√ªteux** | Un faux positif a des cons√©quences graves | Filtre anti-spam : un vrai mail class√© spam = mail perdu |
| D√©cision irr√©versible | On ne peut pas revenir en arri√®re | Envoi d'une offre commerciale (co√ªteuse) |

> üí° **Conseil** : "La precision r√©pond √† la question : 'Quand mon mod√®le dit OUI, a-t-il raison ?' Si un faux positif est co√ªteux ‚Üí optimisez la precision."

### 5.4 Recall (Rappel / Sensibilit√©)

**Question** : "Parmi les **vrais positifs**, combien ai-je **trouv√©s** ?"

**Formule** : `Recall = TP / (TP + FN)`

```python
from sklearn.metrics import recall_score

rec = recall_score(y_test, y_pred)
print(f"Recall : {rec:.4f}")
```

| Quand c'est important | Pourquoi | Exemple |
|----------------------|---------|---------|
| **FN co√ªteux** | Un faux n√©gatif a des cons√©quences graves | Diagnostic cancer : rater un cancer = patient non trait√© |
| D√©tection critique | Ne pas rater les vrais cas | D√©tection de fraude bancaire |

> üí° **Conseil** : "Le recall r√©pond √† la question : 'Parmi tous les vrais cas, combien ai-je d√©tect√©s ?' Si un faux n√©gatif est dangereux ‚Üí optimisez le recall."

### 5.5 F1-Score

**Moyenne harmonique** de Precision et Recall ‚Äî un compromis entre les deux.

**Formule** : `F1 = 2 * (Precision * Recall) / (Precision + Recall)`

```python
from sklearn.metrics import f1_score

f1 = f1_score(y_test, y_pred)
print(f"F1-Score : {f1:.4f}")
```

| Propri√©t√© | D√©tail |
|-----------|--------|
| **Plage** | [0, 1] (1 = parfait) |
| **Quand l'utiliser** | Quand Precision ET Recall comptent, classes d√©s√©quilibr√©es |
| **Avantage** | P√©nalise fortement si l'un des deux est faible |
| **Moyenne harmonique** | Plus conservatrice que la moyenne arithm√©tique |

> üí° **Conseil de pro** : "Le F1-Score est souvent la **meilleure m√©trique par d√©faut** pour la classification, surtout avec des classes d√©s√©quilibr√©es. Si vous ne savez pas quelle m√©trique choisir, commencez par le F1."

### 5.6 ROC-AUC

La **courbe ROC** trace le **Taux de Vrais Positifs** (Recall) contre le **Taux de Faux Positifs** pour tous les seuils de d√©cision possibles.

**AUC** (Area Under the Curve) : l'aire sous la courbe ROC.

| AUC | Interpr√©tation |
|-----|---------------|
| 1.0 | Mod√®le parfait |
| 0.9 - 1.0 | Excellent |
| 0.8 - 0.9 | Bon |
| 0.7 - 0.8 | Acceptable |
| 0.5 | Random (aucun pouvoir pr√©dictif) |
| < 0.5 | Pire que le hasard (inverser les pr√©dictions !) |

```python
from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Calculer les probabilit√©s
y_proba = log_reg.predict_proba(X_test_scaled)[:, 1]

# AUC
auc = roc_auc_score(y_test, y_proba)
print(f"AUC-ROC : {auc:.4f}")

# Tracer la courbe ROC
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, linewidth=2, label=f'Logistique (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')
plt.xlabel('Taux de Faux Positifs (FPR)')
plt.ylabel('Taux de Vrais Positifs (TPR / Recall)')
plt.title('Courbe ROC')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.fill_between(fpr, tpr, alpha=0.1)
plt.show()
```

### 5.7 Courbe Precision-Recall

Plus informative que la ROC pour les **classes tr√®s d√©s√©quilibr√©es**.

```python
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import PrecisionRecallDisplay

# Calcul
precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

# Visualisation
plt.figure(figsize=(8, 6))
plt.plot(recall_vals, precision_vals, linewidth=2, label=f'AP = {ap:.3f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Courbe Precision-Recall')
plt.legend(loc='upper right')
plt.grid(True, alpha=0.3)
plt.fill_between(recall_vals, precision_vals, alpha=0.1)
plt.show()
```

> üí° **Conseil de pro** : "Pour les classes **tr√®s d√©s√©quilibr√©es** (>95/5), la courbe Precision-Recall est plus informative que la courbe ROC. La ROC peut donner une fausse impression de bonne performance."

### 5.8 Tableau r√©capitulatif de toutes les m√©triques

| M√©trique | Question | Formule | Sensible d√©s√©quilibre | Quand l'utiliser |
|----------|----------|---------|----------------------|-----------------|
| **Accuracy** | Combien de pr√©dictions correctes ? | (TP+TN)/Total | ‚ö†Ô∏è Tr√®s | Classes √©quilibr√©es uniquement |
| **Precision** | Quand je dis "oui", ai-je raison ? | TP/(TP+FP) | ‚úÖ Non | FP co√ªteux (spam, pub) |
| **Recall** | Ai-je trouv√© tous les positifs ? | TP/(TP+FN) | ‚úÖ Non | FN co√ªteux (cancer, fraude) |
| **F1-Score** | Compromis Precision/Recall | 2*P*R/(P+R) | ‚úÖ Non | M√©trique par d√©faut |
| **AUC-ROC** | Qualit√© globale du ranking | Aire sous ROC | Mod√©r√© | Vue d'ensemble, comparaison |
| **Average Precision** | Qualit√© du ranking (d√©s√©quilibr√©) | Aire sous PR | ‚úÖ Non | Classes tr√®s d√©s√©quilibr√©es |

> üí° **Conseil de pro** : "**TOUJOURS** choisir sa m√©trique **AVANT** de mod√©liser, en fonction du **co√ªt m√©tier** des erreurs (FP vs FN). Ne changez jamais de m√©trique en cours de route pour 'am√©liorer' artificiellement vos r√©sultats."

### 5.9 Fonction d'√©valuation compl√®te

```python
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, classification_report,
                             confusion_matrix, ConfusionMatrixDisplay)
import matplotlib.pyplot as plt

def evaluer_classifieur(y_true, y_pred, y_proba=None, nom="Mod√®le"):
    """√âvaluation compl√®te d'un classifieur binaire"""
    print(f"‚ïî{'‚ïê'*50}‚ïó")
    print(f"‚ïë  √âvaluation : {nom:^34} ‚ïë")
    print(f"‚ï†{'‚ïê'*50}‚ï£")

    # M√©triques textuelles
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"‚ïë  Accuracy   : {acc:.4f}                          ‚ïë")
    print(f"‚ïë  Precision  : {prec:.4f}                          ‚ïë")
    print(f"‚ïë  Recall     : {rec:.4f}                          ‚ïë")
    print(f"‚ïë  F1-Score   : {f1:.4f}                          ‚ïë")

    if y_proba is not None:
        auc = roc_auc_score(y_true, y_proba)
        print(f"‚ïë  AUC-ROC    : {auc:.4f}                          ‚ïë")

    print(f"‚ïö{'‚ïê'*50}‚ïù")

    # Rapport d√©taill√©
    print("\n" + classification_report(y_true, y_pred))

    # Matrice de confusion
    fig, ax = plt.subplots(figsize=(6, 5))
    cm = confusion_matrix(y_true, y_pred)
    ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues', ax=ax)
    plt.title(f'Matrice de Confusion - {nom}')
    plt.tight_layout()
    plt.show()

# Utilisation
evaluer_classifieur(y_test, y_pred, y_proba[:, 1], "R√©gression Logistique")
```

---

## 6. üéöÔ∏è Seuil de d√©cision

### 6.1 Le probl√®me du seuil par d√©faut

Par d√©faut, le seuil est 0.5 : si P(classe 1) >= 0.5, on pr√©dit classe 1. Mais ce seuil n'est pas toujours optimal.

### 6.2 Impact du seuil sur Precision et Recall

```
Seuil ‚Üë (ex: 0.8)
  ‚Üí Precision ‚Üë (on est plus s√ªr quand on dit "oui")
  ‚Üí Recall ‚Üì (on rate plus de vrais positifs)

Seuil ‚Üì (ex: 0.3)
  ‚Üí Precision ‚Üì (plus de faux positifs)
  ‚Üí Recall ‚Üë (on d√©tecte plus de vrais positifs)
```

> üí° **Conseil** : "Le seuil de 0.5 n'est pas une v√©rit√© absolue. Ajustez-le en fonction de votre probl√®me m√©tier."

### 6.3 Trouver le seuil optimal

```python
from sklearn.metrics import precision_recall_curve, f1_score
import numpy as np
import matplotlib.pyplot as plt

# Obtenir les probabilit√©s
y_proba = log_reg.predict_proba(X_test_scaled)[:, 1]

# --- M√©thode 1 : Seuil qui maximise le F1-Score ---
precisions, recalls, seuils = precision_recall_curve(y_test, y_proba)
# Le tableau de seuils a un √©l√©ment de moins que precision/recall
f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)
seuil_optimal_f1 = seuils[np.argmax(f1_scores)]
print(f"Seuil optimal (max F1) : {seuil_optimal_f1:.3f}")
print(f"F1 au seuil optimal    : {max(f1_scores):.4f}")

# --- M√©thode 2 : Youden's Index (max TPR - FPR) ---
from sklearn.metrics import roc_curve
fpr, tpr, seuils_roc = roc_curve(y_test, y_proba)
youden = tpr - fpr
seuil_optimal_youden = seuils_roc[np.argmax(youden)]
print(f"Seuil optimal (Youden)  : {seuil_optimal_youden:.3f}")

# --- Visualiser l'impact du seuil ---
seuils_test = np.arange(0.1, 0.95, 0.05)
prec_list, rec_list, f1_list = [], [], []

for s in seuils_test:
    y_pred_seuil = (y_proba >= s).astype(int)
    prec_list.append(precision_score(y_test, y_pred_seuil, zero_division=0))
    rec_list.append(recall_score(y_test, y_pred_seuil))
    f1_list.append(f1_score(y_test, y_pred_seuil))

plt.figure(figsize=(10, 6))
plt.plot(seuils_test, prec_list, 'b-', label='Precision')
plt.plot(seuils_test, rec_list, 'r-', label='Recall')
plt.plot(seuils_test, f1_list, 'g--', linewidth=2, label='F1-Score')
plt.axvline(x=seuil_optimal_f1, color='green', linestyle=':', label=f'Seuil optimal = {seuil_optimal_f1:.2f}')
plt.xlabel('Seuil de d√©cision')
plt.ylabel('Score')
plt.title('Precision, Recall et F1 en fonction du seuil')
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.show()

# --- Appliquer le seuil optimal ---
y_pred_optimal = (y_proba >= seuil_optimal_f1).astype(int)
print("\n=== Avec seuil par d√©faut (0.5) ===")
print(f"F1 = {f1_score(y_test, y_pred):.4f}")
print("\n=== Avec seuil optimal ===")
print(f"F1 = {f1_score(y_test, y_pred_optimal):.4f}")
```

> üí° **Conseil de pro** : "Dans un contexte m√©dical (d√©tection de cancer), baissez le seuil (ex: 0.3) pour maximiser le recall ‚Äî il vaut mieux faire des examens compl√©mentaires inutiles que rater un cancer. Dans un contexte de spam, montez le seuil (ex: 0.7) pour maximiser la precision ‚Äî il ne faut pas que des vrais mails finissent en spam."

---

## 7. üéØ Classification multi-classes

### 7.1 Strat√©gies

| Strat√©gie | Principe | Nombre de mod√®les |
|-----------|---------|-------------------|
| **One-vs-Rest (OvR)** | Un mod√®le par classe (classe X vs toutes les autres) | N mod√®les |
| **One-vs-One (OvO)** | Un mod√®le par paire de classes | N*(N-1)/2 mod√®les |

### 7.2 M√©triques multi-classes

```python
from sklearn.metrics import classification_report, f1_score
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Charger un dataset multi-classes
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42, stratify=iris.target
)

# Entra√Æner
log_reg_multi = LogisticRegression(max_iter=1000, random_state=42)
log_reg_multi.fit(X_train, y_train)
y_pred_multi = log_reg_multi.predict(X_test)

# Rapport d√©taill√© par classe
print("=== Rapport de classification (multi-classes) ===")
print(classification_report(y_test, y_pred_multi, target_names=iris.target_names))

# Diff√©rentes moyennes pour le F1
print("=== Moyennes F1 ===")
print(f"F1 macro    : {f1_score(y_test, y_pred_multi, average='macro'):.4f}")
print(f"F1 micro    : {f1_score(y_test, y_pred_multi, average='micro'):.4f}")
print(f"F1 weighted : {f1_score(y_test, y_pred_multi, average='weighted'):.4f}")
```

### 7.3 Comprendre les moyennes

| Moyenne | Calcul | Quand l'utiliser |
|---------|--------|-----------------|
| **macro** | Moyenne simple des F1 par classe | Toutes les classes ont la m√™me importance |
| **micro** | TP/FP/FN globaux | Donne plus de poids aux classes fr√©quentes |
| **weighted** | Moyenne pond√©r√©e par le support | Classes d√©s√©quilibr√©es |

> üí° **Conseil** : "Utilisez `average='weighted'` pour le F1-Score si les classes sont d√©s√©quilibr√©es. Utilisez `average='macro'` si toutes les classes sont aussi importantes."

---

## 8. üìà Comment am√©liorer son classifieur

### 8.1 Checklist d'am√©lioration

| √âtape | Action | Outil/M√©thode |
|-------|--------|--------------|
| 1Ô∏è‚É£ | **V√©rifier le d√©s√©quilibre** des classes | `value_counts()`, `class_weight='balanced'` |
| 2Ô∏è‚É£ | **Tester plusieurs algorithmes** | Logistique, KNN, SVM, Arbre, Random Forest |
| 3Ô∏è‚É£ | **Tuner les hyperparam√®tres** | `GridSearchCV`, `RandomizedSearchCV` |
| 4Ô∏è‚É£ | **Feature engineering** | Cr√©er de nouvelles features, s√©lectionner les meilleures |
| 5Ô∏è‚É£ | **Plus de donn√©es ?** | Courbes d'apprentissage |
| 6Ô∏è‚É£ | **Ajuster le seuil** | Courbe Precision-Recall |
| 7Ô∏è‚É£ | **Ensembles** | Random Forest, Gradient Boosting, Voting |

### 8.2 Comparaison de plusieurs algorithmes

```python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import f1_score, accuracy_score
import pandas as pd

# D√©finir les mod√®les
modeles = {
    'Logistique': LogisticRegression(max_iter=1000, random_state=42),
    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),
    'SVM (rbf)': SVC(kernel='rbf', random_state=42),
    'Arbre': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Entra√Æner et √©valuer
resultats = []
for nom, modele in modeles.items():
    # KNN et SVM n√©cessitent des donn√©es normalis√©es
    if nom in ['KNN (K=5)', 'SVM (rbf)', 'Logistique']:
        modele.fit(X_train_scaled, y_train)
        y_pred_m = modele.predict(X_test_scaled)
    else:
        modele.fit(X_train, y_train)
        y_pred_m = modele.predict(X_test)

    resultats.append({
        'Mod√®le': nom,
        'Accuracy': accuracy_score(y_test, y_pred_m),
        'F1': f1_score(y_test, y_pred_m),
        'Precision': precision_score(y_test, y_pred_m),
        'Recall': recall_score(y_test, y_pred_m)
    })

# Afficher le tableau comparatif
df_resultats = pd.DataFrame(resultats).sort_values('F1', ascending=False)
print("=== Comparaison des algorithmes ===")
print(df_resultats.to_string(index=False))
```

### 8.3 GridSearchCV pour le tuning

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Grille d'hyperparam√®tres
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearch
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='f1',         # Optimiser pour le F1
    n_jobs=-1,
    verbose=1
)
grid.fit(X_train, y_train)

print(f"Meilleurs param√®tres : {grid.best_params_}")
print(f"Meilleur F1 (CV) : {grid.best_score_:.4f}")
print(f"F1 test : {f1_score(y_test, grid.predict(X_test)):.4f}")
```

> üí° **Conseil de pro** : "Ne comparez **JAMAIS** des mod√®les avec des m√©triques diff√©rentes. Fixez **UNE** m√©trique principale (ex: F1-Score) et utilisez-la syst√©matiquement pour toutes les comparaisons."

### 8.4 Guide de choix d'algorithme

| Crit√®re | Logistique | KNN | SVM | Arbre | Random Forest |
|---------|-----------|-----|-----|-------|--------------|
| **Interpr√©tabilit√©** | ‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Rapidit√© entra√Ænement** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Rapidit√© pr√©diction** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Grands datasets** | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |
| **Normalisation n√©cessaire** | Oui | **Oui** | **Oui** | Non | Non |
| **Non-lin√©arit√©** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Premier choix ?** | Baseline | Prototypage | Petit dataset | Explicabilit√© | Performance |

---

## üéØ Points cl√©s √† retenir

1. **R√©gression logistique** = baseline de classification. Commencez **toujours** par elle
2. **KNN** : simple mais sensible √† l'√©chelle. Toujours normaliser
3. **SVM** : puissant sur petits datasets, kernel trick pour non-lin√©arit√©
4. **Arbres** : interpr√©tables mais overfittent. Toujours limiter `max_depth`
5. **Accuracy** est **trompeuse** avec des classes d√©s√©quilibr√©es
6. **Precision** quand FP co√ªteux (spam), **Recall** quand FN co√ªteux (cancer)
7. **F1-Score** = m√©trique par d√©faut recommand√©e
8. **AUC-ROC** pour la vue d'ensemble, **PR-AUC** pour les classes tr√®s d√©s√©quilibr√©es
9. **Le seuil de 0.5** n'est pas sacr√© ‚Äî ajustez-le selon le contexte m√©tier
10. **Choisir la m√©trique AVANT** de mod√©liser, en fonction du co√ªt des erreurs

---

## ‚úÖ Checklist de validation

- [ ] Je sais impl√©menter une r√©gression logistique et interpr√©ter `predict_proba`
- [ ] Je sais impl√©menter KNN et trouver le meilleur K
- [ ] Je comprends le concept de SVM et quand l'utiliser
- [ ] Je sais entra√Æner un arbre de d√©cision et le visualiser
- [ ] Je sais lire et interpr√©ter une matrice de confusion (TP, TN, FP, FN)
- [ ] Je connais la diff√©rence entre Accuracy, Precision, Recall et F1
- [ ] Je sais tracer et interpr√©ter une courbe ROC et calculer l'AUC
- [ ] Je sais tracer une courbe Precision-Recall
- [ ] Je sais ajuster le seuil de d√©cision selon le contexte m√©tier
- [ ] Je sais comparer plusieurs algorithmes avec les m√™mes m√©triques
- [ ] Je sais utiliser GridSearchCV pour tuner les hyperparam√®tres
- [ ] Je sais utiliser `classification_report` pour un rapport complet

---

**Pr√©c√©dent** : [Chapitre 4 : R√©gression ‚Äì Pr√©dire des Valeurs Continues](04-regression.md)
