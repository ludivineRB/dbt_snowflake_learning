# Chapitre 6 : Comprendre ses DonnÃ©es â€” La Vraie Vie des Data

## ğŸ¯ Objectifs

- Identifier et distinguer les diffÃ©rents types de donnÃ©es (numÃ©riques, catÃ©gorielles, temporelles, texte, boolÃ©ens)
- Mener une enquÃªte mÃ©thodique sur les valeurs manquantes, aberrantes et les corrÃ©lations trompeuses
- MaÃ®triser les stratÃ©gies d'imputation adaptÃ©es Ã  chaque situation
- RÃ©aliser une EDA (Exploratory Data Analysis) systÃ©matique avec pandas-profiling / ydata-profiling
- Appliquer un audit qualitÃ© complet sur un dataset rÃ©el

> ğŸ’¡ **Conseil** : "Ce chapitre est le plus important de votre parcours ML. Comprendre ses donnÃ©es, c'est 80% du travail d'un data scientist. Un modÃ¨le ne sera jamais meilleur que les donnÃ©es qu'on lui donne."

---

## 1. ğŸ§  Les Types de DonnÃ©es â€” Savoir ce qu'on manipule

Avant toute modÃ©lisation, il faut **connaÃ®tre la nature** de chaque variable. Un mauvais diagnostic ici entraÃ®ne des erreurs en cascade sur tout le projet.

### 1.1 Vue d'ensemble

```
Types de donnÃ©es
â”‚
â”œâ”€â”€ NumÃ©riques
â”‚   â”œâ”€â”€ Continues (prix, tempÃ©rature, poids)
â”‚   â””â”€â”€ DiscrÃ¨tes (nombre d'enfants, nombre de piÃ¨ces)
â”‚
â”œâ”€â”€ CatÃ©gorielles
â”‚   â”œâ”€â”€ Nominales (couleur, pays, profession)
â”‚   â””â”€â”€ Ordinales (niveau d'Ã©tudes, satisfaction)
â”‚
â”œâ”€â”€ Temporelles (dates, timestamps, durÃ©es)
â”‚
â”œâ”€â”€ Texte (descriptions, commentaires, noms)
â”‚
â””â”€â”€ BoolÃ©ens (oui/non, vrai/faux, 0/1)
```

### 1.2 NumÃ©riques : continues vs discrÃ¨tes

| Type | DÃ©finition | Exemples | OpÃ©rations possibles |
|------|-----------|----------|---------------------|
| **Continue** | Peut prendre n'importe quelle valeur dans un intervalle | Prix (29.99â‚¬), tempÃ©rature (36.7Â°C), taille (1.75m) | Moyenne, mÃ©diane, Ã©cart-type |
| **DiscrÃ¨te** | Valeurs entiÃ¨res, dÃ©nombrables | Nombre d'enfants (0, 1, 2...), nombre de piÃ¨ces, nombre d'achats | Comptage, mode, distribution |

```python
import pandas as pd
import numpy as np

# Identifier les types numÃ©riques
df = pd.read_csv("clients.csv")

# Variables continues vs discrÃ¨tes
colonnes_numeriques = df.select_dtypes(include=[np.number]).columns.tolist()
print(f"Colonnes numÃ©riques : {colonnes_numeriques}")

# Astuce : si le nombre de valeurs uniques est faible â†’ probablement discrÃ¨te
for col in colonnes_numeriques:
    n_unique = df[col].nunique()
    dtype = "DiscrÃ¨te" if n_unique < 20 else "Continue"
    print(f"  {col}: {n_unique} valeurs uniques â†’ {dtype}")
```

### 1.3 CatÃ©gorielles : nominales vs ordinales

| Type | DÃ©finition | Exemples | Encodage recommandÃ© |
|------|-----------|----------|---------------------|
| **Nominale** | Pas d'ordre entre les catÃ©gories | Couleur (rouge, bleu), pays (France, Espagne), profession | One-Hot Encoding |
| **Ordinale** | Ordre naturel entre les catÃ©gories | Niveau d'Ã©tudes (bac, licence, master, doctorat), satisfaction (1-5 Ã©toiles) | Ordinal Encoding |

> âš ï¸ **Attention** : "Confondre nominal et ordinal est une erreur classique. Si vous appliquez un Label Encoding sur une variable nominale, le modÃ¨le croira qu'il y a un ordre entre les catÃ©gories (ex: France=0, Espagne=1, Japon=2 â†’ le modÃ¨le pense que Japon > Espagne > France)."

```python
# Identifier les variables catÃ©gorielles
colonnes_cat = df.select_dtypes(include=['object', 'category']).columns.tolist()
print(f"Colonnes catÃ©gorielles : {colonnes_cat}")

for col in colonnes_cat:
    n_unique = df[col].nunique()
    print(f"  {col}: {n_unique} catÃ©gories â†’ {df[col].unique()[:5]}")
```

### 1.4 Temporelles

Les donnÃ©es temporelles sont souvent sous-exploitÃ©es. Elles cachent des patterns puissants.

```python
# Convertir en datetime
df['date_inscription'] = pd.to_datetime(df['date_inscription'])

# VÃ©rifier le type
print(df['date_inscription'].dtype)  # datetime64[ns]

# Extraire des informations
print(f"PÃ©riode couverte : {df['date_inscription'].min()} â†’ {df['date_inscription'].max()}")
print(f"DurÃ©e : {(df['date_inscription'].max() - df['date_inscription'].min()).days} jours")
```

### 1.5 Texte

Le texte brut n'est pas directement utilisable par les algorithmes ML. Il faudra le transformer (chapitre Feature Engineering).

```python
# AperÃ§u des colonnes texte
colonnes_texte = ['description', 'commentaire', 'nom_produit']

for col in colonnes_texte:
    if col in df.columns:
        print(f"\n--- {col} ---")
        print(f"  Longueur moyenne : {df[col].str.len().mean():.0f} caractÃ¨res")
        print(f"  Nombre de mots moyen : {df[col].str.split().str.len().mean():.0f}")
        print(f"  Exemple : {df[col].iloc[0][:80]}...")
```

### 1.6 BoolÃ©ens

```python
# Les boolÃ©ens peuvent Ãªtre sous diffÃ©rentes formes
# True/False, 0/1, Oui/Non, Y/N

# Identifier les colonnes binaires
for col in df.columns:
    if df[col].nunique() == 2:
        print(f"  {col}: {df[col].unique()} â†’ BoolÃ©en potentiel")
```

### 1.7 Diagnostic automatique avec pandas

```python
# Le diagnostic complet en une commande
print("=== Info gÃ©nÃ©rale ===")
print(df.info())

print("\n=== Types dÃ©tectÃ©s par pandas ===")
print(df.dtypes.value_counts())

print("\n=== Statistiques descriptives (numÃ©riques) ===")
print(df.describe())

print("\n=== Statistiques descriptives (catÃ©gorielles) ===")
print(df.describe(include='object'))
```

> ğŸ’¡ **Conseil** : "MÃ©fiez-vous des types dÃ©tectÃ©s automatiquement par pandas. Un code postal (75001) sera dÃ©tectÃ© comme numÃ©rique, alors que c'est une variable catÃ©gorielle. VÃ©rifiez toujours manuellement."

---

## 2. ğŸ” EnquÃªte #1 : Les Valeurs Manquantes

Les valeurs manquantes sont le **premier problÃ¨me** que vous rencontrerez sur un vrai dataset. Avant de les traiter, il faut comprendre **pourquoi** elles sont lÃ .

### 2.1 Pourquoi des donnÃ©es manquent-elles ?

| Cause | Exemple concret | Type statistique |
|-------|----------------|-----------------|
| Capteur cassÃ© | TempÃ©rature non enregistrÃ©e pendant 3h | MCAR |
| Refus de rÃ©pondre | Client qui ne donne pas son revenu | MNAR |
| Erreur de saisie | Champ oubliÃ© dans un formulaire | MCAR |
| DonnÃ©e non applicable | Nombre d'enfants pour un cÃ©libataire sans enfant | MAR |
| Fusion de bases | Colonnes diffÃ©rentes selon les sources | MAR |
| Bug informatique | Fichier corrompu, API en panne | MCAR |

### 2.2 Les trois types de manquantes (expliquÃ©s simplement)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TYPES DE VALEURS MANQUANTES                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    MCAR     â”‚         MAR            â”‚         MNAR             â”‚
â”‚  (Missing   â”‚    (Missing At         â”‚    (Missing Not          â”‚
â”‚  Completely â”‚     Random)            â”‚     At Random)           â”‚
â”‚  At Random) â”‚                        â”‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Le manque   â”‚ Le manque dÃ©pend       â”‚ Le manque dÃ©pend         â”‚
â”‚ est TOTALE- â”‚ d'AUTRES variables     â”‚ de la VALEUR ELLE-MÃŠME   â”‚
â”‚ MENT dÃ» au â”‚ observÃ©es              â”‚                          â”‚
â”‚ hasard      â”‚                        â”‚                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Capteur en  â”‚ Les jeunes rÃ©pondent   â”‚ Les hauts revenus ne     â”‚
â”‚ panne alÃ©a- â”‚ moins au sondage       â”‚ dÃ©clarent pas leur       â”‚
â”‚ toirement   â”‚ (liÃ© Ã  l'Ã¢ge)          â”‚ revenu                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Traitement: â”‚ Traitement:            â”‚ Traitement:              â”‚
â”‚ Suppression â”‚ Imputation basÃ©e sur   â”‚ Le plus DIFFICILE.       â”‚
â”‚ OK si peu   â”‚ les autres variables   â”‚ ModÃ¨le spÃ©cifique ou     â”‚
â”‚ de donnÃ©es  â”‚ (KNN, rÃ©gression)      â”‚ variable indicatrice     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Conseil** : "En pratique, il est souvent difficile de savoir si les donnÃ©es sont MCAR, MAR ou MNAR. La bonne approche : parlez aux gens qui ont collectÃ© les donnÃ©es et essayez de comprendre le processus de collecte."

### 2.3 DÃ©tection des valeurs manquantes

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Charger les donnÃ©es
df = pd.read_csv("clients_churn.csv")

# --- Ã‰tape 1 : Vue d'ensemble ---
print("=== Valeurs manquantes par colonne ===")
manquantes = df.isnull().sum()
pct_manquantes = (manquantes / len(df)) * 100

rapport = pd.DataFrame({
    'Nb manquantes': manquantes,
    '% manquantes': pct_manquantes
}).sort_values('% manquantes', ascending=False)

# Afficher uniquement les colonnes avec des manquantes
rapport_filtre = rapport[rapport['Nb manquantes'] > 0]
print(rapport_filtre)
print(f"\nTotal : {df.isnull().sum().sum()} valeurs manquantes "
      f"sur {df.size} ({df.isnull().sum().sum() / df.size * 100:.2f}%)")
```

```python
# --- Ã‰tape 2 : Visualisation ---

# Barplot des % de manquantes
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Graphique 1 : Barplot
cols_manquantes = rapport_filtre.index
axes[0].barh(cols_manquantes, rapport_filtre['% manquantes'], color='coral')
axes[0].set_xlabel('% de valeurs manquantes')
axes[0].set_title('Pourcentage de valeurs manquantes par colonne')
axes[0].axvline(x=5, color='green', linestyle='--', label='Seuil 5%')
axes[0].axvline(x=50, color='red', linestyle='--', label='Seuil 50%')
axes[0].legend()

# Graphique 2 : Heatmap des manquantes
axes[1].set_title('Pattern des valeurs manquantes')
sns.heatmap(df[cols_manquantes].isnull(), cbar=True, yticklabels=False, ax=axes[1])

plt.tight_layout()
plt.show()
```

```python
# --- Ã‰tape 3 : CorrÃ©lation entre les manquantes ---
# Est-ce que les manquantes sont liÃ©es entre elles ?

colonnes_avec_nan = df.columns[df.isnull().any()].tolist()
if len(colonnes_avec_nan) > 1:
    matrice_nan = df[colonnes_avec_nan].isnull().corr()
    plt.figure(figsize=(8, 6))
    sns.heatmap(matrice_nan, annot=True, cmap='YlOrRd', vmin=-1, vmax=1)
    plt.title("CorrÃ©lation entre les valeurs manquantes")
    plt.show()
```

### 2.4 StratÃ©gies d'imputation

| StratÃ©gie | Quand l'utiliser | Code sklearn | Avantage | InconvÃ©nient |
|-----------|-----------------|-------------|----------|-------------|
| **Suppression de lignes** | < 5% manquantes, MCAR | `df.dropna()` | Simple | Perte de donnÃ©es |
| **Suppression de colonnes** | > 50% manquantes | `df.drop(columns=[...])` | Ã‰limine le problÃ¨me | Perte d'info |
| **Moyenne** | NumÃ©rique, distribution symÃ©trique | `SimpleImputer(strategy='mean')` | Rapide | Sensible aux outliers |
| **MÃ©diane** | NumÃ©rique, outliers prÃ©sents | `SimpleImputer(strategy='median')` | Robuste | RÃ©duit la variance |
| **Mode** | CatÃ©goriel | `SimpleImputer(strategy='most_frequent')` | AdaptÃ© | Peut biaiser |
| **KNN** | Relations entre features | `KNNImputer(n_neighbors=5)` | Plus prÃ©cis | Plus lent |
| **Constante** | Signification mÃ©tier | `SimpleImputer(strategy='constant')` | Explicite | Ajoute une "catÃ©gorie" |
| **Indicatrice** | Garder l'info du manque | CrÃ©er colonne `_is_missing` | Conserve l'info | Ajoute des colonnes |

### 2.5 ImplÃ©mentation complÃ¨te

```python
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.model_selection import train_test_split

# âš ï¸ TOUJOURS splitter AVANT d'imputer
X = df.drop('churn', axis=1)
y = df['churn']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# --- Identifier les types de colonnes ---
colonnes_num = X_train.select_dtypes(include=[np.number]).columns.tolist()
colonnes_cat = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

print(f"Colonnes numÃ©riques : {colonnes_num}")
print(f"Colonnes catÃ©gorielles : {colonnes_cat}")
```

```python
# --- Imputation des numÃ©riques par la mÃ©diane ---
imputer_median = SimpleImputer(strategy='median')

# Fit sur le train UNIQUEMENT
X_train[colonnes_num] = imputer_median.fit_transform(X_train[colonnes_num])
# Transform sur le test (sans fit !)
X_test[colonnes_num] = imputer_median.transform(X_test[colonnes_num])

print("Manquantes numÃ©riques aprÃ¨s imputation :")
print(X_train[colonnes_num].isnull().sum())
```

```python
# --- Imputation des catÃ©gorielles par le mode ---
imputer_mode = SimpleImputer(strategy='most_frequent')

X_train[colonnes_cat] = imputer_mode.fit_transform(X_train[colonnes_cat])
X_test[colonnes_cat] = imputer_mode.transform(X_test[colonnes_cat])

print("Manquantes catÃ©gorielles aprÃ¨s imputation :")
print(X_train[colonnes_cat].isnull().sum())
```

```python
# --- Imputation KNN (plus sophistiquÃ©e) ---
# KNN utilise les k voisins les plus proches pour estimer la valeur manquante
imputer_knn = KNNImputer(n_neighbors=5, weights='distance')

# âš ï¸ KNNImputer ne fonctionne qu'avec des numÃ©riques
X_train_knn = pd.DataFrame(
    imputer_knn.fit_transform(X_train[colonnes_num]),
    columns=colonnes_num,
    index=X_train.index
)
X_test_knn = pd.DataFrame(
    imputer_knn.transform(X_test[colonnes_num]),
    columns=colonnes_num,
    index=X_test.index
)

print("Manquantes aprÃ¨s KNN :")
print(X_train_knn.isnull().sum())
```

```python
# --- Astuce : CrÃ©er une variable indicatrice AVANT d'imputer ---
# Utile si le fait que la donnÃ©e manque EST une information

for col in colonnes_num:
    if X_train[col].isnull().sum() > 0:
        X_train[f'{col}_manquant'] = X_train[col].isnull().astype(int)
        X_test[f'{col}_manquant'] = X_test[col].isnull().astype(int)
        print(f"  CrÃ©Ã© : {col}_manquant")
```

> âš ï¸ **Attention** : "La rÃ¨gle d'or : fit sur le train, transform sur le test. Si vous faites `fit_transform` sur tout le dataset avant le split, vous avez un **data leakage**. Le modÃ¨le 'voit' des informations du test set Ã  travers les moyennes/mÃ©dianes calculÃ©es."

> ğŸ’¡ **Conseil** : "Si le pourcentage de manquantes dÃ©passe 50%, supprimez la colonne. Entre 5% et 50%, imputez. En dessous de 5%, la suppression de lignes est acceptable. Ce sont des rÃ¨gles de base â€” adaptez Ã  votre contexte."

---

## 3. ğŸ” EnquÃªte #2 : Les Valeurs Aberrantes â€” Erreur ou Signal ?

Une valeur aberrante (outlier) est un point de donnÃ©es qui s'Ã©carte significativement du reste. La question cruciale : **est-ce une erreur ou une information prÃ©cieuse ?**

### 3.1 Exemples concrets

| Situation | Valeur | Erreur ou signal ? |
|-----------|--------|-------------------|
| Ã‚ge d'un client : 250 ans | 250 | âŒ Erreur de saisie |
| Salaire : 500 000â‚¬ | 500 000 | âœ… Signal (PDG, star du foot) |
| TempÃ©rature corporelle : 42Â°C | 42 | âœ… Signal (fiÃ¨vre grave) |
| Prix d'un produit : -50â‚¬ | -50 | âŒ Erreur (ou remboursement ?) |
| Nombre d'achats/mois : 300 | 300 | âš ï¸ Bot ? Client professionnel ? |

### 3.2 DÃ©tection par la mÃ©thode IQR

```
                   Valeurs normales
              â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
    â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€
          â”‚        â”‚             â”‚        â”‚
       borne_inf  Q1    mÃ©diane  Q3    borne_sup
          â”‚        â”‚             â”‚        â”‚
          â”‚  1.5Ã—IQR            1.5Ã—IQR  â”‚
          â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚             â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚
    â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€
 Outliers                                   Outliers
```

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- MÃ©thode IQR ---
def detecter_outliers_iqr(df, colonne):
    """
    DÃ©tecte les outliers avec la mÃ©thode IQR (Interquartile Range).
    Retourne les outliers, borne infÃ©rieure et borne supÃ©rieure.
    """
    Q1 = df[colonne].quantile(0.25)
    Q3 = df[colonne].quantile(0.75)
    IQR = Q3 - Q1
    borne_inf = Q1 - 1.5 * IQR
    borne_sup = Q3 + 1.5 * IQR

    outliers = df[(df[colonne] < borne_inf) | (df[colonne] > borne_sup)]
    return outliers, borne_inf, borne_sup

# Rapport d'outliers pour chaque colonne numÃ©rique
print("=== Rapport des valeurs aberrantes (IQR) ===\n")
for col in colonnes_num:
    outliers, b_inf, b_sup = detecter_outliers_iqr(df, col)
    pct = len(outliers) / len(df) * 100
    print(f"{col}:")
    print(f"  Bornes : [{b_inf:.2f}, {b_sup:.2f}]")
    print(f"  Outliers : {len(outliers)} ({pct:.1f}%)")
    if len(outliers) > 0:
        print(f"  Min outlier : {outliers[col].min():.2f}")
        print(f"  Max outlier : {outliers[col].max():.2f}")
    print()
```

### 3.3 DÃ©tection par Z-Score

```python
from scipy import stats

# --- MÃ©thode Z-Score ---
def detecter_outliers_zscore(df, colonne, seuil=3):
    """
    DÃ©tecte les outliers avec le Z-Score.
    Un Z-Score > 3 (ou < -3) = valeur Ã  plus de 3 Ã©carts-types de la moyenne.
    """
    z_scores = np.abs(stats.zscore(df[colonne].dropna()))
    outliers_mask = z_scores > seuil
    outliers = df[colonne].dropna()[outliers_mask]
    return outliers, z_scores

print("=== Rapport des valeurs aberrantes (Z-Score > 3) ===\n")
for col in colonnes_num:
    outliers, z_scores = detecter_outliers_zscore(df, col)
    pct = len(outliers) / len(df) * 100
    print(f"{col}: {len(outliers)} outliers ({pct:.1f}%)")
    if len(outliers) > 0:
        print(f"  Z-Score max : {z_scores.max():.2f}")
```

### 3.4 Visualisation avec boxplots

```python
# Boxplots pour toutes les colonnes numÃ©riques
n_cols = len(colonnes_num)
n_rows = (n_cols + 3) // 4
fig, axes = plt.subplots(n_rows, 4, figsize=(16, 4 * n_rows))
axes = axes.flatten()

for i, col in enumerate(colonnes_num):
    sns.boxplot(y=df[col], ax=axes[i], color='lightblue')
    outliers, b_inf, b_sup = detecter_outliers_iqr(df, col)
    axes[i].axhline(y=b_inf, color='red', linestyle='--', alpha=0.5)
    axes[i].axhline(y=b_sup, color='red', linestyle='--', alpha=0.5)
    axes[i].set_title(f'{col}\n({len(outliers)} outliers)')

# Masquer les axes inutilisÃ©s
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.suptitle("DÃ©tection des valeurs aberrantes (Boxplots + bornes IQR)", y=1.02)
plt.tight_layout()
plt.show()
```

```python
# Distribution + boxplot combinÃ©s
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

col_exemple = 'revenu'

# Histogramme
axes[0].hist(df[col_exemple].dropna(), bins=50, edgecolor='black', alpha=0.7)
axes[0].set_title(f'Distribution de {col_exemple}')
axes[0].set_xlabel(col_exemple)
axes[0].set_ylabel('FrÃ©quence')

# Boxplot horizontal
sns.boxplot(x=df[col_exemple], ax=axes[1], color='lightcoral')
axes[1].set_title(f'Boxplot de {col_exemple}')

plt.tight_layout()
plt.show()
```

### 3.5 Que faire des outliers ?

| DÃ©cision | Quand | Code |
|----------|-------|------|
| **Supprimer** | Clairement une erreur (Ã¢ge = 250) | `df = df[df['age'] < 150]` |
| **Capper (winsoriser)** | Valeur extrÃªme mais plausible | `df[col] = df[col].clip(lower=b_inf, upper=b_sup)` |
| **Transformer (log)** | Distribution trÃ¨s asymÃ©trique | `df['col_log'] = np.log1p(df['col'])` |
| **Garder** | Information lÃ©gitime (fraude, VIP) | Ne rien faire |
| **Variable indicatrice** | Garder l'information sans le bruit | `df['col_outlier'] = (df['col'] > seuil).astype(int)` |

```python
# --- Capping (winsorisation) ---
def capper_outliers(df, colonne):
    """Remplace les outliers par les bornes IQR."""
    Q1 = df[colonne].quantile(0.25)
    Q3 = df[colonne].quantile(0.75)
    IQR = Q3 - Q1
    borne_inf = Q1 - 1.5 * IQR
    borne_sup = Q3 + 1.5 * IQR

    n_avant = ((df[colonne] < borne_inf) | (df[colonne] > borne_sup)).sum()
    df[colonne] = df[colonne].clip(lower=borne_inf, upper=borne_sup)
    print(f"  {colonne}: {n_avant} outliers cappÃ©s dans [{borne_inf:.2f}, {borne_sup:.2f}]")
    return df

# Appliquer sur les colonnes choisies
for col in ['revenu', 'montant_dernier_achat']:
    if col in df.columns:
        df = capper_outliers(df, col)
```

```python
# --- Transformation logarithmique ---
# IdÃ©ale pour les distributions trÃ¨s asymÃ©triques (prix, revenus, surfaces)

col_asym = 'revenu'
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(df[col_asym].dropna(), bins=50, edgecolor='black', alpha=0.7)
axes[0].set_title(f'{col_asym} â€” Distribution originale')

df[f'{col_asym}_log'] = np.log1p(df[col_asym])
axes[1].hist(df[f'{col_asym}_log'].dropna(), bins=50, edgecolor='black', alpha=0.7, color='green')
axes[1].set_title(f'{col_asym}_log â€” AprÃ¨s transformation log')

plt.tight_layout()
plt.show()
```

> ğŸ’¡ **Conseil** : "Avant de supprimer un outlier, posez-vous la question : est-ce une erreur de saisie ou un cas rÃ©el ? Un client qui achÃ¨te pour 50 000â‚¬ est un outlier statistique mais c'est peut-Ãªtre votre client le plus important. La dÃ©tection de fraude repose justement sur les outliers."

---

## 4. ğŸ” EnquÃªte #3 : Les CorrÃ©lations Trompeuses

### 4.1 CorrÃ©lation â‰  CausalitÃ©

C'est LA rÃ¨gle d'or en data science. Deux variables peuvent Ãªtre corrÃ©lÃ©es **sans qu'aucune ne cause l'autre**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CORRÃ‰LATION  â‰   CAUSALITÃ‰                       â”‚
â”‚                                                         â”‚
â”‚  CorrÃ©lation :  A et B bougent ensemble                 â”‚
â”‚  CausalitÃ© :    A PROVOQUE B                            â”‚
â”‚                                                         â”‚
â”‚  Exemples cÃ©lÃ¨bres de corrÃ©lations absurdes :           â”‚
â”‚                                                         â”‚
â”‚  ğŸ¦ Ventes de glaces  â†”  Nombre de noyades             â”‚
â”‚     â†’ Variable cachÃ©e : la CHALEUR                      â”‚
â”‚                                                         â”‚
â”‚  ğŸ‘Ÿ Ventes de chaussures  â†”  Taux de divorce            â”‚
â”‚     â†’ Variable cachÃ©e : la TAILLE de la population      â”‚
â”‚                                                         â”‚
â”‚  ğŸ§€ Consommation de fromage  â†”  Morts par strangulation â”‚
â”‚     â†’ Pure coÃ¯ncidence statistique !                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Les piÃ¨ges classiques

| PiÃ¨ge | Explication | Exemple |
|-------|------------|---------|
| **Variable confondante** | Une 3Ã¨me variable cause les deux | Chaleur â†’ glaces ET noyades |
| **CorrÃ©lation fortuite** | CoÃ¯ncidence sur la pÃ©riode | Fromage et strangulations |
| **CausalitÃ© inversÃ©e** | B cause A, pas A cause B | Pompiers et dÃ©gÃ¢ts (plus de pompiers = plus de dÃ©gÃ¢ts ?) |
| **Biais de sÃ©lection** | DonnÃ©es non reprÃ©sentatives | Survivants d'avion â†’ conclusions biaisÃ©es |

### 4.3 Matrice de corrÃ©lation (heatmap)

```python
# Matrice de corrÃ©lation complÃ¨te
colonnes_num = df.select_dtypes(include=[np.number]).columns.tolist()
correlation_matrix = df[colonnes_num].corr()

# Heatmap
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # masquer le triangle supÃ©rieur
sns.heatmap(
    correlation_matrix,
    mask=mask,
    annot=True,
    fmt='.2f',
    cmap='coolwarm',
    center=0,
    vmin=-1, vmax=1,
    square=True,
    linewidths=0.5
)
plt.title("Matrice de corrÃ©lation (Pearson)")
plt.tight_layout()
plt.show()
```

```python
# Identifier les paires fortement corrÃ©lÃ©es
def trouver_fortes_correlations(df, seuil=0.7):
    """Trouve les paires de features avec |corrÃ©lation| > seuil."""
    corr_matrix = df.corr()
    upper_tri = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )

    paires = []
    for col in upper_tri.columns:
        for row in upper_tri.index:
            val = upper_tri.loc[row, col]
            if abs(val) > seuil:
                paires.append((col, row, round(val, 3)))

    paires.sort(key=lambda x: abs(x[2]), reverse=True)
    return paires

paires_correlees = trouver_fortes_correlations(df[colonnes_num], seuil=0.7)

print("=== Paires fortement corrÃ©lÃ©es (|r| > 0.7) ===")
for col1, col2, corr in paires_correlees:
    emoji = "ğŸ”´" if abs(corr) > 0.9 else "ğŸŸ " if abs(corr) > 0.8 else "ğŸŸ¡"
    print(f"  {emoji} {col1} â†” {col2} : r = {corr}")
```

```python
# CorrÃ©lation avec la target (variable cible)
print("\n=== CorrÃ©lation avec la target (churn) ===")
if 'churn' in df.columns:
    corr_target = df[colonnes_num].corrwith(df['churn']).sort_values(ascending=False)
    print(corr_target)

    # Visualisation
    plt.figure(figsize=(10, 6))
    corr_target.plot(kind='barh', color=['green' if x > 0 else 'red' for x in corr_target])
    plt.title("CorrÃ©lation de chaque feature avec la target (churn)")
    plt.xlabel("Coefficient de corrÃ©lation (Pearson)")
    plt.axvline(x=0, color='black', linewidth=0.5)
    plt.tight_layout()
    plt.show()
```

> âš ï¸ **Attention** : "La corrÃ©lation de Pearson ne capture que les relations **linÃ©aires**. Deux variables peuvent avoir une corrÃ©lation de 0 tout en Ã©tant fortement liÃ©es de maniÃ¨re non linÃ©aire (ex: relation en U). Utilisez la mutual information pour dÃ©tecter les relations non linÃ©aires."

```python
# --- Mutual Information : dÃ©tecte les relations non linÃ©aires ---
from sklearn.feature_selection import mutual_info_classif

if 'churn' in df.columns:
    # âš ï¸ Uniquement sur les colonnes numÃ©riques, sans NaN
    X_num_clean = df[colonnes_num].dropna()
    y_clean = df.loc[X_num_clean.index, 'churn']

    mi_scores = mutual_info_classif(X_num_clean, y_clean, random_state=42)
    mi_df = pd.DataFrame({
        'feature': colonnes_num,
        'mutual_info': mi_scores
    }).sort_values('mutual_info', ascending=False)

    print("\n=== Mutual Information avec la target ===")
    print(mi_df)
```

> ğŸ’¡ **Conseil** : "Quand vous trouvez une forte corrÃ©lation, demandez-vous toujours : est-ce que A cause B ? Est-ce que B cause A ? Ou est-ce qu'une variable cachÃ©e C cause les deux ? Ne tirez jamais de conclusions causales Ã  partir d'une simple corrÃ©lation."

---

## 5. ğŸ“Š EDA SystÃ©matique avec ydata-profiling

### 5.1 Pourquoi automatiser l'EDA ?

L'EDA manuelle est longue et on oublie souvent des vÃ©rifications. `ydata-profiling` (anciennement `pandas-profiling`) gÃ©nÃ¨re un rapport complet en **une seule ligne de code**.

```python
# Installation
# uv add ydata-profiling
```

### 5.2 GÃ©nÃ©rer un rapport complet

```python
from ydata_profiling import ProfileReport

# GÃ©nÃ©rer le rapport
profile = ProfileReport(
    df,
    title="Audit QualitÃ© â€” Dataset Clients Churn",
    explorative=True,
    correlations={
        "pearson": {"calculate": True},
        "spearman": {"calculate": True},
        "kendall": {"calculate": False},
    }
)

# Sauvegarder en HTML
profile.to_file("rapport_eda_churn.html")
print("Rapport sauvegardÃ© : rapport_eda_churn.html")

# Ou afficher dans un notebook Jupyter
# profile.to_notebook_iframe()
```

### 5.3 Ce que contient le rapport

| Section | Contenu | UtilitÃ© |
|---------|---------|--------|
| **Overview** | Nombre de lignes, colonnes, manquantes, doublons | Vue d'ensemble rapide |
| **Variables** | Distribution, stats, valeurs extrÃªmes par colonne | Analyse dÃ©taillÃ©e |
| **Interactions** | Scatter plots entre variables | Relations visuelles |
| **Correlations** | Heatmaps (Pearson, Spearman) | CorrÃ©lations |
| **Missing values** | Pattern des manquantes, matrice, heatmap | Comprendre les NaN |
| **Duplicates** | Lignes dupliquÃ©es | Nettoyage |
| **Alerts** | Avertissements automatiques (haute corrÃ©lation, constantes...) | Points d'attention |

### 5.4 EDA manuelle complÃ©mentaire

Le rapport automatique ne remplace pas le regard humain. Voici les vÃ©rifications complÃ©mentaires :

```python
# --- VÃ©rifications complÃ©mentaires ---

# 1. Doublons
print(f"Lignes dupliquÃ©es : {df.duplicated().sum()}")
print(f"Lignes dupliquÃ©es (%) : {df.duplicated().sum() / len(df) * 100:.2f}%")

# 2. Constantes (colonnes avec une seule valeur â†’ inutiles)
constantes = [col for col in df.columns if df[col].nunique() <= 1]
print(f"\nColonnes constantes (Ã  supprimer) : {constantes}")

# 3. Quasi-constantes (>95% la mÃªme valeur)
quasi_constantes = []
for col in df.columns:
    if df[col].value_counts(normalize=True).iloc[0] > 0.95:
        quasi_constantes.append(col)
print(f"Colonnes quasi-constantes (>95%) : {quasi_constantes}")

# 4. Haute cardinalitÃ© (trop de catÃ©gories)
for col in colonnes_cat:
    n = df[col].nunique()
    if n > 50:
        print(f"\nâš ï¸  {col} : {n} catÃ©gories (haute cardinalitÃ©)")

# 5. Identifiants (colonnes uniques â†’ inutiles pour le ML)
identifiants = [col for col in df.columns if df[col].nunique() == len(df)]
print(f"\nIdentifiants potentiels (Ã  exclure) : {identifiants}")
```

```python
# 6. Distribution de la target
if 'churn' in df.columns:
    print("\n=== Distribution de la target ===")
    print(df['churn'].value_counts())
    print(df['churn'].value_counts(normalize=True).apply(lambda x: f"{x:.1%}"))

    plt.figure(figsize=(6, 4))
    df['churn'].value_counts().plot(kind='bar', color=['steelblue', 'coral'])
    plt.title("Distribution de la variable cible (churn)")
    plt.xlabel("Churn")
    plt.ylabel("Nombre de clients")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()
```

> ğŸ’¡ **Conseil** : "Lancez toujours ydata-profiling en premier pour avoir une vue d'ensemble. Puis complÃ©tez avec une EDA manuelle ciblÃ©e sur les points d'alerte identifiÃ©s dans le rapport. Cette double approche vous fait gagner un temps considÃ©rable."

---

## 6. ğŸ—ï¸ Exercice Fil Rouge : Audit QualitÃ© du Dataset clients_churn.csv

### 6.1 Contexte

Vous Ãªtes data scientist dans une entreprise de tÃ©lÃ©communications. On vous confie le dataset `clients_churn.csv` et on vous demande un **audit qualitÃ© complet** avant toute modÃ©lisation.

### 6.2 Ã‰tapes de l'audit

```python
# ============================================================
# AUDIT QUALITÃ‰ â€” DATASET CLIENTS_CHURN.CSV
# ============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --- Ã‰tape 1 : Chargement et premiÃ¨re impression ---
df = pd.read_csv("clients_churn.csv")

print("=" * 60)
print("Ã‰TAPE 1 : Vue d'ensemble")
print("=" * 60)
print(f"Shape : {df.shape}")
print(f"Colonnes : {df.columns.tolist()}")
print(f"\nTypes :")
print(df.dtypes)
print(f"\nPremiÃ¨res lignes :")
print(df.head())
```

```python
# --- Ã‰tape 2 : Valeurs manquantes ---
print("\n" + "=" * 60)
print("Ã‰TAPE 2 : Valeurs manquantes")
print("=" * 60)

manquantes = df.isnull().sum()
pct = (manquantes / len(df) * 100).round(2)

rapport_manquantes = pd.DataFrame({
    'Nb_manquantes': manquantes,
    'Pct_manquantes': pct
}).sort_values('Pct_manquantes', ascending=False)

print(rapport_manquantes[rapport_manquantes['Nb_manquantes'] > 0])

# DÃ©cision pour chaque colonne
print("\n--- DÃ©cisions ---")
for col, row in rapport_manquantes.iterrows():
    if row['Pct_manquantes'] > 50:
        print(f"  âŒ {col} ({row['Pct_manquantes']}%) â†’ SUPPRIMER la colonne")
    elif row['Pct_manquantes'] > 5:
        print(f"  ğŸ”§ {col} ({row['Pct_manquantes']}%) â†’ IMPUTER")
    elif row['Pct_manquantes'] > 0:
        print(f"  âœ… {col} ({row['Pct_manquantes']}%) â†’ Imputer ou supprimer lignes")
```

```python
# --- Ã‰tape 3 : Valeurs aberrantes ---
print("\n" + "=" * 60)
print("Ã‰TAPE 3 : Valeurs aberrantes")
print("=" * 60)

colonnes_num = df.select_dtypes(include=[np.number]).columns.tolist()

for col in colonnes_num:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    b_inf = Q1 - 1.5 * IQR
    b_sup = Q3 + 1.5 * IQR
    n_outliers = ((df[col] < b_inf) | (df[col] > b_sup)).sum()
    pct_out = n_outliers / len(df) * 100

    if pct_out > 0:
        print(f"  {col}: {n_outliers} outliers ({pct_out:.1f}%) "
              f"â€” min={df[col].min():.2f}, max={df[col].max():.2f}, "
              f"bornes=[{b_inf:.2f}, {b_sup:.2f}]")
```

```python
# --- Ã‰tape 4 : CorrÃ©lations ---
print("\n" + "=" * 60)
print("Ã‰TAPE 4 : CorrÃ©lations")
print("=" * 60)

# CorrÃ©lations entre features
paires = trouver_fortes_correlations(df[colonnes_num], seuil=0.7)
if paires:
    print("Paires fortement corrÃ©lÃ©es (|r| > 0.7) :")
    for col1, col2, corr in paires:
        print(f"  {col1} â†” {col2} : r = {corr}")
else:
    print("Aucune paire avec |r| > 0.7")

# CorrÃ©lation avec la target
if 'churn' in df.columns:
    corr_target = df[colonnes_num].corrwith(df['churn']).abs().sort_values(ascending=False)
    print("\nTop 5 features corrÃ©lÃ©es avec churn :")
    print(corr_target.head())
```

```python
# --- Ã‰tape 5 : Doublons et anomalies ---
print("\n" + "=" * 60)
print("Ã‰TAPE 5 : Doublons et anomalies")
print("=" * 60)

print(f"Doublons complets : {df.duplicated().sum()}")

colonnes_cat = df.select_dtypes(include=['object']).columns.tolist()
for col in colonnes_cat:
    print(f"\n{col} ({df[col].nunique()} catÃ©gories) :")
    print(df[col].value_counts().head(10))
```

```python
# --- Ã‰tape 6 : Rapport ydata-profiling ---
from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Audit Clients Churn", explorative=True)
profile.to_file("audit_clients_churn.html")
print("\nâœ… Rapport complet sauvegardÃ© : audit_clients_churn.html")
```

### 6.3 Template de rapport d'audit

Ã€ la fin de votre audit, remplissez ce tableau rÃ©capitulatif :

| CritÃ¨re | RÃ©sultat | Action |
|---------|----------|--------|
| Nombre de lignes | ... | â€” |
| Nombre de colonnes | ... | â€” |
| % global de manquantes | ... | Imputer / Supprimer |
| Colonnes Ã  supprimer (>50% NaN) | ... | `df.drop(columns=[...])` |
| Nombre d'outliers dÃ©tectÃ©s | ... | Capper / Transformer / Garder |
| Paires corrÃ©lÃ©es (>0.8) | ... | Supprimer une des deux |
| Doublons | ... | `df.drop_duplicates()` |
| Colonnes identifiant | ... | Exclure du ML |
| Distribution target | ... | Stratifier / SMOTE si dÃ©sÃ©quilibrÃ© |

> ğŸ’¡ **Conseil** : "Cet audit qualitÃ© doit Ãªtre fait **systÃ©matiquement** sur tout nouveau dataset. CrÃ©ez-vous un template rÃ©utilisable. Avec le temps, vous dÃ©velopperez un instinct pour repÃ©rer les problÃ¨mes rapidement."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **ConnaÃ®tre ses types de donnÃ©es** : numÃ©riques (continues/discrÃ¨tes), catÃ©gorielles (nominales/ordinales), temporelles, texte, boolÃ©ens â€” chaque type requiert un traitement spÃ©cifique
2. **Les valeurs manquantes ont des causes** : MCAR (hasard pur), MAR (liÃ© Ã  d'autres variables), MNAR (liÃ© Ã  la valeur elle-mÃªme) â€” comprendre la cause guide le choix du traitement
3. **Imputer intelligemment** : mÃ©diane pour les numÃ©riques avec outliers, mode pour les catÃ©gorielles, KNN quand les features sont liÃ©es â€” et toujours fit sur le train, transform sur le test
4. **Les outliers ne sont pas toujours des erreurs** : distinguer erreur de saisie (supprimer) et signal rÃ©el (garder) est une compÃ©tence clÃ©
5. **MÃ©thode IQR et Z-Score** : deux outils complÃ©mentaires pour dÃ©tecter les valeurs aberrantes â€” les visualiser avec des boxplots avant de dÃ©cider
6. **CorrÃ©lation â‰  CausalitÃ©** : toujours chercher la variable confondante avant de conclure â€” les corrÃ©lations trompeuses sont partout
7. **ydata-profiling pour l'EDA automatique** : un rapport complet en une ligne de code â€” complÃ©tez-le par une analyse manuelle ciblÃ©e
8. **L'audit qualitÃ© est systÃ©matique** : avant toute modÃ©lisation, passez par les 6 Ã©tapes â€” manquantes, outliers, corrÃ©lations, doublons, distribution target
9. **CrÃ©er des variables indicatrices** : le fait qu'une donnÃ©e manque EST souvent une information utile â€” crÃ©ez une colonne `_manquant`
10. **80% du temps d'un data scientist** est passÃ© sur la comprÃ©hension et la prÃ©paration des donnÃ©es â€” ne nÃ©gligez jamais cette Ã©tape

---

## âœ… Checklist de validation

- [ ] Je sais distinguer les types de donnÃ©es (continu, discret, nominal, ordinal, temporel, texte, boolÃ©en)
- [ ] Je sais dÃ©tecter les valeurs manquantes et calculer leur pourcentage par colonne
- [ ] Je connais la diffÃ©rence entre MCAR, MAR et MNAR
- [ ] Je sais choisir la bonne stratÃ©gie d'imputation selon le contexte
- [ ] Je maÃ®trise SimpleImputer et KNNImputer de scikit-learn
- [ ] Je sais dÃ©tecter les outliers avec la mÃ©thode IQR et le Z-Score
- [ ] Je sais visualiser les outliers avec des boxplots
- [ ] Je sais dÃ©cider si un outlier est une erreur ou un signal
- [ ] Je comprends que corrÃ©lation â‰  causalitÃ© et je sais donner des exemples
- [ ] Je sais crÃ©er une matrice de corrÃ©lation (heatmap) et l'interprÃ©ter
- [ ] Je sais utiliser ydata-profiling pour gÃ©nÃ©rer un rapport EDA automatique
- [ ] Je sais mener un audit qualitÃ© complet sur un dataset
- [ ] Je respecte la rÃ¨gle : fit sur le train, transform sur le test

---

**PrÃ©cÃ©dent** : [Chapitre 5 : Classification](05-classification.md)

**Suivant** : [Chapitre 7 : Feature Engineering â€” L'Art de PrÃ©parer les DonnÃ©es](07-feature-engineering.md)
