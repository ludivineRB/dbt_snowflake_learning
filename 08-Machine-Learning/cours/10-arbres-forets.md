# Chapitre 10 : Arbres de DÃ©cision et ForÃªts AlÃ©atoires

## ğŸ¯ Objectifs

- Comprendre intuitivement les arbres de dÃ©cision comme un raisonnement humain
- MaÃ®triser l'algorithme CART (Gini, entropie, critÃ¨res d'arrÃªt)
- Visualiser et comprendre l'overfitting des arbres profonds
- ConnaÃ®tre les techniques d'Ã©lagage (pruning)
- Comprendre le Random Forest comme une intelligence collective
- Savoir comparer les performances selon le nombre d'arbres
- MaÃ®triser les hyperparamÃ¨tres clÃ©s du Random Forest

**Phase 3 â€” Semaine 10**

---

## 1. ğŸ§  Arbre de dÃ©cision = Reproduire un raisonnement humain

### 1.1 L'intuition

Un arbre de dÃ©cision fonctionne exactement comme un raisonnement humain : une sÃ©rie de **questions successives** qui mÃ¨nent Ã  une **dÃ©cision finale**. Chaque question porte sur une feature et divise les donnÃ©es en sous-groupes de plus en plus homogÃ¨nes.

### 1.2 Exemple quotidien : "Est-ce que je prends un parapluie ?"

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Il pleut dehors ?   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â•±         â•²
                    Oui â•±             â•² Non
                      â•±                 â•²
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ ğŸŒ‚ PARAPLUIE â”‚    â”‚ Ciel couvert ?    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â•±         â•²
                             Oui â•±             â•² Non
                               â•±                 â•²
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ PrÃ©visions pluie? â”‚   â”‚ ğŸš« PAS DE         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    PARAPLUIE       â”‚
                      â•±         â•²         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                Oui â•±             â•² Non
                  â•±                 â•²
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ ğŸŒ‚ PARAPLUIE â”‚    â”‚ ğŸš« PAS DE         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    PARAPLUIE       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Application au Machine Learning

En ML, l'arbre de dÃ©cision fait exactement pareil, mais avec des **donnÃ©es numÃ©riques** et des **seuils automatiques** :

```
Exemple : PrÃ©dire si un client va churner

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ anciennetÃ© < 6 mois ?      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â•±           â•²
                      Oui â•±               â•² Non
                        â•±                   â•²
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ nb_appels_support    â”‚  â”‚ montant_mensuel        â”‚
          â”‚ >= 3 ?               â”‚  â”‚ > 80â‚¬ ?                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â•±         â•²               â•±         â•²
          Oui â•±             â•² Non   Oui â•±             â•² Non
            â•±                 â•²       â•±                 â•²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CHURN    â”‚    â”‚ PAS      â”‚ â”‚ CHURN    â”‚    â”‚ PAS      â”‚
    â”‚ (85%)    â”‚    â”‚ CHURN    â”‚ â”‚ (60%)    â”‚    â”‚ CHURN    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (70%)    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (90%)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> ğŸ’¡ **Conseil** : "L'arbre de dÃ©cision est l'un des rares modÃ¨les de ML que vous pouvez montrer directement Ã  un non-technique. 'Si anciennetÃ© < 6 mois ET appels_support >= 3, alors le client va churner' â€” c'est limpide."

---

## 2. ğŸ“Š Construction d'un arbre (algorithme CART simplifiÃ©)

### 2.1 Questions successives (splits)

L'algorithme CART (Classification And Regression Trees) construit l'arbre en posant des questions de la forme :

```
"feature X est-elle < seuil S ?"

Exemples de splits possibles :
- "surface < 50 mÂ² ?"
- "Ã¢ge < 35 ans ?"
- "revenu < 30000â‚¬ ?"
- "contrat_mensuel == 1 ?"
```

Pour **chaque feature** et **chaque valeur possible**, l'algorithme teste le split et choisit celui qui sÃ©pare le mieux les classes.

### 2.2 Comment choisir la meilleure question ?

```
L'algorithme Ã©value TOUS les splits possibles et garde le meilleur :

DonnÃ©es initiales : [ğŸ”´ğŸ”´ğŸ”´ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µ]  (3 rouges, 5 bleus)

Split 1 : "Ã¢ge < 25 ?"
  Gauche : [ğŸ”´ğŸ”´ğŸ”µğŸ”µ]     â†’ mÃ©langÃ© (impur)
  Droite : [ğŸ”´ğŸ”µğŸ”µğŸ”µ]     â†’ mÃ©langÃ© (impur)
  â†’ Split mÃ©diocre

Split 2 : "revenu < 30k ?"
  Gauche : [ğŸ”´ğŸ”´ğŸ”´ğŸ”µ]     â†’ presque pur
  Droite : [ğŸ”µğŸ”µğŸ”µğŸ”µ]     â†’ pur !
  â†’ Bon split ! âœ…

L'algorithme choisit le split qui maximise la "puretÃ©" des sous-groupes.
```

### 2.3 Indice de Gini

L'indice de Gini mesure l'**impuretÃ©** d'un noeud. Un Gini de 0 signifie que le noeud est pur (une seule classe).

**Formule** : `Gini = 1 - Î£(piÂ²)` oÃ¹ pi est la proportion de chaque classe

#### Calcul pas Ã  pas

```
Exemple : un noeud contient 40 clients churn et 60 clients non-churn

  p(churn)     = 40/100 = 0.40
  p(non-churn) = 60/100 = 0.60

  Gini = 1 - (0.40Â² + 0.60Â²)
       = 1 - (0.16 + 0.36)
       = 1 - 0.52
       = 0.48    â†’ assez impur (mÃ©langÃ©)

Noeud pur (100% churn) :
  Gini = 1 - (1.0Â²) = 0.0    â†’ parfaitement pur âœ…

Noeud maximalement impur (50/50) :
  Gini = 1 - (0.5Â² + 0.5Â²) = 0.5   â†’ mÃ©lange maximal âŒ
```

#### Visualisation de l'indice de Gini

```
  Gini
  0.50 |         â—
       |       â—   â—
  0.40 |     â—       â—
       |   â—           â—
  0.30 | â—               â—
       |â—                 â—
  0.20 |                    â—
       |â—                     â—
  0.10 | â—                      â—
       |  â—                       â—
  0.00 |â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
       +â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ p(classe 1)
       0    0.2   0.4   0.5   0.8   1.0

  Maximum Ã  p=0.5 (50/50) â†’ plus grande incertitude
  Minimum Ã  p=0 ou p=1   â†’ certitude totale
```

```python
import numpy as np
import matplotlib.pyplot as plt

# Visualiser l'indice de Gini
p = np.linspace(0, 1, 100)
gini = 1 - p**2 - (1 - p)**2

plt.figure(figsize=(8, 5))
plt.plot(p, gini, 'b-', linewidth=2, label='Gini = 1 - pÂ² - (1-p)Â²')
plt.xlabel('p (proportion de la classe 1)', fontsize=12)
plt.ylabel('Indice de Gini', fontsize=12)
plt.title('Indice de Gini en fonction de la proportion', fontsize=14)
plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Maximum (p=0.5)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.4 Entropie (comparaison avec Gini)

L'**entropie** est une autre mesure d'impuretÃ©, issue de la thÃ©orie de l'information :

**Formule** : `Entropie = -Î£(pi * log2(pi))`

| CritÃ¨re | Gini | Entropie |
|---------|------|----------|
| **Formule** | 1 - Î£(piÂ²) | -Î£(pi * log2(pi)) |
| **Plage** | [0, 0.5] (binaire) | [0, 1] (binaire) |
| **InterprÃ©tation** | ProbabilitÃ© de mauvaise classification | QuantitÃ© d'information / dÃ©sordre |
| **Vitesse** | Plus rapide (pas de logarithme) | Plus lent |
| **Performances** | TrÃ¨s similaires Ã  l'entropie | TrÃ¨s similaires au Gini |
| **Par dÃ©faut sklearn** | Oui (`criterion='gini'`) | Non (`criterion='entropy'`) |

> ğŸ’¡ **Conseil** : "En pratique, Gini et Entropie donnent des rÃ©sultats quasiment identiques dans 95% des cas. Restez avec Gini (le dÃ©faut de sklearn) sauf raison spÃ©cifique."

### 2.5 CritÃ¨res d'arrÃªt

L'arbre pourrait continuer Ã  se diviser jusqu'Ã  ce que chaque feuille contienne un seul Ã©chantillon. Les critÃ¨res d'arrÃªt empÃªchent cela :

| CritÃ¨re | ParamÃ¨tre sklearn | Description | Valeur typique |
|---------|-------------------|-------------|----------------|
| **Profondeur maximale** | `max_depth` | Limite le nombre de niveaux | 3-20 |
| **Ã‰chantillons min pour split** | `min_samples_split` | Nombre min pour diviser un noeud | 2-20 |
| **Ã‰chantillons min par feuille** | `min_samples_leaf` | Nombre min dans une feuille | 1-10 |
| **Nombre max de feuilles** | `max_leaf_nodes` | Limite le nombre total de feuilles | None ou 10-100 |
| **RÃ©duction min d'impuretÃ©** | `min_impurity_decrease` | Split seulement si gain suffisant | 0.0 |

```python
from sklearn.tree import DecisionTreeClassifier

# Arbre avec critÃ¨res d'arrÃªt
arbre = DecisionTreeClassifier(
    max_depth=5,               # maximum 5 niveaux
    min_samples_split=10,      # au moins 10 Ã©chantillons pour diviser
    min_samples_leaf=5,        # au moins 5 Ã©chantillons par feuille
    criterion='gini',          # critÃ¨re de puretÃ©
    random_state=42
)
```

---

## 3. ğŸ” Overfitting illustrÃ© visuellement

### 3.1 Arbre profond = apprendre par cÅ“ur

Un arbre sans limite de profondeur va crÃ©er des feuilles pour **chaque observation** du dataset d'entraÃ®nement. Il apprend par cÅ“ur les donnÃ©es, y compris le bruit.

```
Arbre profond (depth=20) :                Arbre Ã©laguÃ© (depth=3) :

DonnÃ©es d'entraÃ®nement :                  DonnÃ©es d'entraÃ®nement :
  Score train = 100% ğŸ¯                     Score train = 88% ğŸ“Š
  Score test  = 72%  ğŸ˜°                     Score test  = 85% ğŸ˜Š

         â”Œâ”€â”                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
       â•±     â•²                                  â•±          â•²
     â”Œâ”€â”     â”Œâ”€â”                           â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”
    â•±   â•²   â•±   â•²                         â•±      â•²    â•±      â•²
  â”Œâ”€â” â”Œâ”€â” â”Œâ”€â” â”Œâ”€â”                      â”Œâ”€â”€â”  â”Œâ”€â”€â” â”Œâ”€â”€â”  â”Œâ”€â”€â”
  ... ... ... ...                       â”‚A â”‚  â”‚B â”‚ â”‚B â”‚  â”‚A â”‚
  (des centaines de feuilles)           â””â”€â”€â”˜  â””â”€â”€â”˜ â””â”€â”€â”˜  â””â”€â”€â”˜
  â†’ MÃ©morise chaque exemple              â†’ GÃ©nÃ©ralise les patterns
  â†’ Mauvais sur de nouvelles donnÃ©es      â†’ Bon sur de nouvelles donnÃ©es
```

### 3.2 Comparaison depth=3 vs depth=20

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# GÃ©nÃ©rer des donnÃ©es
X, y = make_classification(
    n_samples=500, n_features=10, n_informative=5,
    n_redundant=3, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Arbre profond (overfitting)
arbre_profond = DecisionTreeClassifier(max_depth=20, random_state=42)
arbre_profond.fit(X_train, y_train)

# Arbre Ã©laguÃ© (bon)
arbre_elague = DecisionTreeClassifier(max_depth=3, random_state=42)
arbre_elague.fit(X_train, y_train)

# Comparer les scores
print("=== Comparaison depth=3 vs depth=20 ===\n")
print(f"{'':>20} {'Train':>10} {'Test':>10} {'Ã‰cart':>10}")
print(f"{'â”€'*50}")

score_train_profond = arbre_profond.score(X_train, y_train)
score_test_profond = arbre_profond.score(X_test, y_test)
print(f"{'Arbre profond (20)':>20} {score_train_profond:>10.4f} "
      f"{score_test_profond:>10.4f} {score_train_profond - score_test_profond:>10.4f}")

score_train_elague = arbre_elague.score(X_train, y_train)
score_test_elague = arbre_elague.score(X_test, y_test)
print(f"{'Arbre Ã©laguÃ© (3)':>20} {score_train_elague:>10.4f} "
      f"{score_test_elague:>10.4f} {score_train_elague - score_test_elague:>10.4f}")

# Visualiser la courbe de profondeur vs score
depths = range(1, 25)
train_scores = []
test_scores = []

for d in depths:
    arbre = DecisionTreeClassifier(max_depth=d, random_state=42)
    arbre.fit(X_train, y_train)
    train_scores.append(arbre.score(X_train, y_train))
    test_scores.append(arbre.score(X_test, y_test))

plt.figure(figsize=(10, 6))
plt.plot(depths, train_scores, 'b-o', label='Score entraÃ®nement', linewidth=2)
plt.plot(depths, test_scores, 'r-o', label='Score test', linewidth=2)
plt.axvline(x=depths[np.argmax(test_scores)], color='green', linestyle='--',
            label=f'Profondeur optimale = {depths[np.argmax(test_scores)]}')
plt.xlabel('Profondeur de l\'arbre (max_depth)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Overfitting : Train vs Test selon la profondeur', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.show()
```

### 3.3 Ã‰lagage : prÃ©-pruning vs post-pruning

| Technique | Description | Comment | Avantages |
|-----------|-------------|---------|-----------|
| **PrÃ©-pruning** | Limiter la croissance PENDANT la construction | max_depth, min_samples_split, min_samples_leaf | Rapide, simple |
| **Post-pruning** | Construire l'arbre complet, puis COUPER les branches inutiles | `ccp_alpha` (cost-complexity pruning) | Explore plus de splits |

```python
# PrÃ©-pruning : limiter dÃ¨s la construction
arbre_pre = DecisionTreeClassifier(
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
arbre_pre.fit(X_train, y_train)
print(f"PrÃ©-pruning : accuracy test = {arbre_pre.score(X_test, y_test):.4f}")

# Post-pruning avec ccp_alpha (Cost Complexity Pruning)
# Ã‰tape 1 : trouver le meilleur alpha
path = DecisionTreeClassifier(random_state=42).cost_complexity_pruning_path(
    X_train, y_train
)
ccp_alphas = path.ccp_alphas

# Ã‰tape 2 : tester chaque alpha
train_scores_ccp = []
test_scores_ccp = []

for alpha in ccp_alphas:
    arbre = DecisionTreeClassifier(ccp_alpha=alpha, random_state=42)
    arbre.fit(X_train, y_train)
    train_scores_ccp.append(arbre.score(X_train, y_train))
    test_scores_ccp.append(arbre.score(X_test, y_test))

# Visualiser
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_scores_ccp, 'b-', label='Train', linewidth=2)
plt.plot(ccp_alphas, test_scores_ccp, 'r-', label='Test', linewidth=2)
plt.xlabel('Alpha (complexitÃ© du pruning)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Post-pruning : Accuracy vs alpha', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.show()

best_alpha = ccp_alphas[np.argmax(test_scores_ccp)]
print(f"Meilleur alpha : {best_alpha:.6f}")
print(f"Post-pruning : accuracy test = {max(test_scores_ccp):.4f}")
```

> ğŸ’¡ **Conseil** : "En pratique, le prÃ©-pruning (max_depth, min_samples) suffit dans la majoritÃ© des cas. Le post-pruning (ccp_alpha) est utile quand vous voulez une optimisation plus fine."

---

## 4. ğŸ“‹ Avantages et inconvÃ©nients des arbres de dÃ©cision

| Avantages | InconvÃ©nients |
|-----------|---------------|
| TrÃ¨s interprÃ©table (visualisable) | Forte tendance Ã  l'overfitting |
| Pas besoin de normalisation/scaling | Instable (petite variation des donnÃ©es â†’ arbre trÃ¨s diffÃ©rent) |
| GÃ¨re les features numÃ©riques ET catÃ©gorielles | FrontiÃ¨res de dÃ©cision uniquement parallÃ¨les aux axes |
| Feature importance native | Performance infÃ©rieure aux ensembles (RF, XGBoost) |
| Rapide Ã  entraÃ®ner et prÃ©dire | Sensible au dÃ©sÃ©quilibre des classes |
| GÃ¨re les relations non-linÃ©aires | Bias vers les features Ã  haute cardinalitÃ© |
| Pas de besoin de donnÃ©es standardisÃ©es | Un seul arbre est rarement suffisant en pratique |
| GÃ¨re les valeurs manquantes (certaines implÃ©mentations) | CrÃ©e des frontiÃ¨res en escalier |

### Pas besoin de normalisation !

```
Pourquoi les arbres n'ont pas besoin de scaling ?

Les arbres posent des questions du type "feature < seuil ?"
â†’ Seul l'ORDRE des valeurs compte, pas leur Ã©chelle

Exemple :
  "surface < 50 mÂ²" et "surface < 50000 cmÂ²" donnent le MÃŠME split
  Le rÃ©sultat est identique que les donnÃ©es soient en mÂ², cmÂ² ou piedsÂ²

C'est un avantage MAJEUR par rapport Ã  KNN et rÃ©gression logistique
qui dÃ©pendent des distances/amplitudes.
```

### Feature importance native

```python
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import matplotlib.pyplot as plt

# EntraÃ®ner un arbre
arbre = DecisionTreeClassifier(max_depth=5, random_state=42)
arbre.fit(X_train, y_train)

# Feature importance (basÃ©e sur la rÃ©duction de Gini)
feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]
importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': arbre.feature_importances_
}).sort_values('Importance', ascending=False)

print("=== Feature Importance (Arbre de DÃ©cision) ===")
print(importances)

# Visualiser
plt.figure(figsize=(10, 5))
plt.barh(importances['Feature'][::-1], importances['Importance'][::-1])
plt.xlabel('Importance (rÃ©duction de Gini)')
plt.title('Importance des features â€” Arbre de DÃ©cision')
plt.tight_layout()
plt.show()
```

### Visualiser un arbre

```python
from sklearn.tree import export_text, plot_tree

# Affichage texte
print(export_text(arbre, feature_names=feature_names, max_depth=3))

# Affichage graphique
plt.figure(figsize=(20, 10))
plot_tree(
    arbre,
    feature_names=feature_names,
    class_names=['Non-Churn', 'Churn'],
    filled=True,          # colorer selon la classe majoritaire
    rounded=True,
    max_depth=3,          # limiter l'affichage Ã  3 niveaux
    fontsize=10
)
plt.title('Arbre de DÃ©cision (3 premiers niveaux)', fontsize=16)
plt.tight_layout()
plt.show()
```

---

## 5. ğŸŒ² Random Forest = Intelligence collective

### 5.1 La mÃ©taphore du jury

```
Un seul juge (1 arbre de dÃ©cision) :
  â†’ Peut avoir des prÃ©jugÃ©s (biais)
  â†’ Peut se tromper sur des cas complexes
  â†’ Son jugement est instable

Un jury de 100 juges (Random Forest) :
  â†’ Chaque juge a un parcours diffÃ©rent (donnÃ©es bootstrap)
  â†’ Chaque juge voit des aspects diffÃ©rents (features alÃ©atoires)
  â†’ Le verdict final = vote majoritaire
  â†’ Les erreurs individuelles se compensent
  â†’ Jugement final plus juste et plus stable âœ…
```

### 5.2 Bagging expliquÃ© pas Ã  pas

```
Dataset original : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Ã‰tape 1 : CrÃ©er N Ã©chantillons bootstrap (tirage avec remise)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Ã‰chantillon 1 : [2, 5, 5, 8, 1, 9, 3, 7, 2, 6]  â† certains doublÃ©s, certains absents
  Ã‰chantillon 2 : [1, 1, 4, 6, 8, 3, 10, 7, 9, 4]
  Ã‰chantillon 3 : [3, 7, 2, 10, 5, 5, 8, 1, 6, 9]
  ...
  Ã‰chantillon N : [...]

Ã‰tape 2 : EntraÃ®ner un arbre sur chaque Ã©chantillon
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Arbre 1 â† Ã‰chantillon 1  (+ features alÃ©atoires Ã  chaque split)
  Arbre 2 â† Ã‰chantillon 2  (+ features alÃ©atoires Ã  chaque split)
  Arbre 3 â† Ã‰chantillon 3  (+ features alÃ©atoires Ã  chaque split)
  ...

Ã‰tape 3 : AgrÃ©ger les prÃ©dictions
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Nouveau client X :
    Arbre 1 prÃ©dit : CHURN
    Arbre 2 prÃ©dit : PAS CHURN
    Arbre 3 prÃ©dit : CHURN
    ...
    Arbre 100 prÃ©dit : CHURN

    Vote : 67 CHURN vs 33 PAS CHURN
    â†’ PrÃ©diction finale : CHURN (67% de confiance)
```

### 5.3 Bootstrap sampling (tirage avec remise)

```python
import numpy as np

# Illustration du bootstrap
np.random.seed(42)
donnees = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

print("Dataset original :", donnees)
print()

for i in range(3):
    bootstrap = np.random.choice(donnees, size=len(donnees), replace=True)
    absents = set(donnees) - set(bootstrap)
    pct_unique = len(set(bootstrap)) / len(donnees) * 100
    print(f"Bootstrap {i+1} : {bootstrap}")
    print(f"  Absents : {absents} ({100-pct_unique:.0f}% des donnÃ©es)")
    print(f"  Uniques : {pct_unique:.0f}%")
    print()

# En thÃ©orie, ~63.2% des donnÃ©es sont prÃ©sentes dans chaque bootstrap
# Les ~36.8% restantes sont les donnÃ©es OOB (Out-Of-Bag)
print("ThÃ©orie : ~63.2% des donnÃ©es dans chaque bootstrap")
print("â†’ les ~36.8% restantes servent de validation gratuite (OOB)")
```

### 5.4 AgrÃ©gation des votes

| Type de problÃ¨me | MÃ©thode d'agrÃ©gation | Description |
|-----------------|---------------------|-------------|
| **Classification** | Vote majoritaire | La classe la plus votÃ©e gagne |
| **Classification (proba)** | Moyenne des probabilitÃ©s | Moyenne des predict_proba de chaque arbre |
| **RÃ©gression** | Moyenne | Moyenne des prÃ©dictions de chaque arbre |

### 5.5 Out-of-bag score (OOB)

Le score OOB est une **estimation gratuite** de la performance du modÃ¨le, sans avoir besoin d'un set de validation sÃ©parÃ© :

```
Pour chaque arbre, ~36.8% des donnÃ©es n'ont PAS Ã©tÃ© utilisÃ©es (OOB).
On peut les utiliser pour Ã©valuer cet arbre.
En agrÃ©geant sur tous les arbres, on obtient le score OOB.

Avantages :
  âœ… Pas besoin de set de validation sÃ©parÃ©
  âœ… Estimation non biaisÃ©e de la performance
  âœ… Gratuit (calculÃ© pendant l'entraÃ®nement)
```

```python
from sklearn.ensemble import RandomForestClassifier

# Activer le score OOB
rf_oob = RandomForestClassifier(
    n_estimators=200,
    oob_score=True,        # activer le score OOB
    random_state=42,
    n_jobs=-1
)
rf_oob.fit(X_train, y_train)

print(f"Score OOB      : {rf_oob.oob_score_:.4f}")
print(f"Score test     : {rf_oob.score(X_test, y_test):.4f}")
print(f"â†’ OOB â‰ˆ test score (validation gratuite !)")
```

### 5.6 ImplÃ©mentation sklearn complÃ¨te

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import pandas as pd

# CrÃ©er et entraÃ®ner le Random Forest
rf = RandomForestClassifier(
    n_estimators=200,         # 200 arbres
    max_depth=15,             # profondeur max de chaque arbre
    min_samples_split=5,      # au moins 5 Ã©chantillons pour splitter
    min_samples_leaf=2,       # au moins 2 par feuille
    max_features='sqrt',      # racine carrÃ©e du nombre de features par split
    oob_score=True,           # score Out-Of-Bag
    class_weight='balanced',  # gestion des classes dÃ©sÃ©quilibrÃ©es
    random_state=42,
    n_jobs=-1                 # tous les coeurs
)

rf.fit(X_train, y_train)

# Ã‰valuation
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

print("=== Random Forest â€” RÃ©sultats ===\n")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC   : {roc_auc_score(y_test, y_proba):.4f}")
print(f"Score OOB : {rf.oob_score_:.4f}")
```

### 5.7 Feature importance avec Random Forest

```python
import pandas as pd
import matplotlib.pyplot as plt

# Importance par impuretÃ© (Gini) â€” attention aux biais !
importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

print("=== Feature Importance (Gini) ===")
print(importances.head(10))

# Visualiser
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Gini importance
top_n = importances.head(10)[::-1]
axes[0].barh(top_n['Feature'], top_n['Importance'], color='steelblue')
axes[0].set_xlabel('Importance (Gini)')
axes[0].set_title('Feature Importance â€” ImpuretÃ© (Gini)')

# Permutation importance (plus fiable)
from sklearn.inspection import permutation_importance

perm_imp = permutation_importance(rf, X_test, y_test,
                                   n_repeats=10, random_state=42,
                                   n_jobs=-1)
perm_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': perm_imp.importances_mean
}).sort_values('Importance', ascending=False)

top_perm = perm_df.head(10)[::-1]
axes[1].barh(top_perm['Feature'], top_perm['Importance'], color='coral')
axes[1].set_xlabel('Diminution du score')
axes[1].set_title('Feature Importance â€” Permutation')

plt.suptitle('Comparaison des mÃ©thodes d\'importance', fontsize=14)
plt.tight_layout()
plt.show()
```

> âš ï¸ **Attention** : "L'importance par Gini surestime les features Ã  haute cardinalitÃ©. PrÃ©fÃ©rez **toujours** la permutation importance pour des rÃ©sultats fiables."

---

## 6. ğŸ§ª TP : Comparer 1 arbre vs 10 vs 100 vs 500 arbres

### 6.1 Code complet

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import time

# GÃ©nÃ©rer un dataset
X, y = make_classification(
    n_samples=2000, n_features=20, n_informative=10,
    n_redundant=5, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Tester diffÃ©rents nombres d'arbres
n_arbres_list = [1, 5, 10, 25, 50, 100, 200, 500]
resultats = []

for n_arbres in n_arbres_list:
    start = time.time()

    if n_arbres == 1:
        # Un seul arbre de dÃ©cision
        modele = DecisionTreeClassifier(random_state=42)
    else:
        modele = RandomForestClassifier(
            n_estimators=n_arbres,
            random_state=42,
            n_jobs=-1
        )

    # Cross-validation
    scores = cross_val_score(modele, X_train, y_train, cv=5, scoring='accuracy')

    # Score test
    modele.fit(X_train, y_train)
    score_test = modele.score(X_test, y_test)

    duree = time.time() - start

    resultats.append({
        'n_arbres': n_arbres,
        'cv_mean': scores.mean(),
        'cv_std': scores.std(),
        'test_score': score_test,
        'temps': duree
    })

    print(f"n_arbres={n_arbres:>3} : "
          f"CV={scores.mean():.4f}Â±{scores.std():.4f}, "
          f"Test={score_test:.4f}, "
          f"Temps={duree:.2f}s")

# Visualiser
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Score vs nombre d'arbres
n_arbres = [r['n_arbres'] for r in resultats]
cv_means = [r['cv_mean'] for r in resultats]
cv_stds = [r['cv_std'] for r in resultats]
test_scores = [r['test_score'] for r in resultats]

axes[0].errorbar(n_arbres, cv_means, yerr=cv_stds, fmt='bo-',
                 linewidth=2, capsize=5, label='CV Score (Â±std)')
axes[0].plot(n_arbres, test_scores, 'rs-', linewidth=2, label='Test Score')
axes[0].set_xlabel('Nombre d\'arbres', fontsize=12)
axes[0].set_ylabel('Accuracy', fontsize=12)
axes[0].set_title('Performance vs Nombre d\'arbres', fontsize=14)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)
axes[0].set_xscale('log')

# Temps vs nombre d'arbres
temps = [r['temps'] for r in resultats]
axes[1].plot(n_arbres, temps, 'go-', linewidth=2)
axes[1].set_xlabel('Nombre d\'arbres', fontsize=12)
axes[1].set_ylabel('Temps (secondes)', fontsize=12)
axes[1].set_title('Temps d\'entraÃ®nement vs Nombre d\'arbres', fontsize=14)
axes[1].grid(True, alpha=0.3)
axes[1].set_xscale('log')

plt.suptitle('Impact du nombre d\'arbres sur Random Forest', fontsize=16)
plt.tight_layout()
plt.show()
```

### 6.2 Observations typiques

```
RÃ©sultats typiques :

  n_arbres=  1 : CV=0.8420Â±0.0215, Test=0.8350   â† arbre seul, instable
  n_arbres=  5 : CV=0.8780Â±0.0180, Test=0.8750   â† dÃ©jÃ  une amÃ©lioration nette
  n_arbres= 10 : CV=0.8920Â±0.0120, Test=0.8900   â† gain significatif
  n_arbres= 25 : CV=0.9040Â±0.0090, Test=0.9050   â† encore mieux
  n_arbres= 50 : CV=0.9100Â±0.0075, Test=0.9100   â† le gain ralentit
  n_arbres=100 : CV=0.9120Â±0.0060, Test=0.9150   â† gain marginal
  n_arbres=200 : CV=0.9130Â±0.0055, Test=0.9150   â† quasi-plateau
  n_arbres=500 : CV=0.9135Â±0.0050, Test=0.9175   â† gain nÃ©gligeable

Conclusions :
  âœ… Passer de 1 Ã  50 arbres : gain majeur
  âš ï¸ Passer de 50 Ã  500 : gain marginal, temps Ã—10
  â†’ 100-200 arbres est souvent le sweet spot
```

> ğŸ’¡ **Conseil** : "En production, choisissez le nombre d'arbres au-delÃ  duquel le gain de performance est nÃ©gligeable. 100-200 arbres suffisent dans 90% des cas. Au-delÃ  de 500, le temps de calcul augmente mais le score stagne."

---

## 7. ğŸ”§ HyperparamÃ¨tres importants de Random Forest

### 7.1 Les paramÃ¨tres clÃ©s

| HyperparamÃ¨tre | Description | Valeur par dÃ©faut | Plage recommandÃ©e | Impact principal |
|----------------|-------------|-------------------|-------------------|-----------------|
| `n_estimators` | Nombre d'arbres | 100 | 100-500 | Plus = mieux (mais plus lent) |
| `max_depth` | Profondeur max | None (illimitÃ©e) | 5-30 ou None | ContrÃ´le la complexitÃ© |
| `max_features` | Features par split | 'sqrt' | 'sqrt', 'log2', 0.3-0.8 | DiversitÃ© des arbres |
| `min_samples_split` | Min Ã©chantillons pour split | 2 | 2-20 | RÃ©gularisation |
| `min_samples_leaf` | Min Ã©chantillons par feuille | 1 | 1-10 | RÃ©gularisation |
| `class_weight` | Poids des classes | None | 'balanced', dict | Classes dÃ©sÃ©quilibrÃ©es |
| `oob_score` | Score Out-Of-Bag | False | True | Validation gratuite |
| `n_jobs` | ParallÃ©lisation | 1 | -1 | Vitesse |

### 7.2 Guide de tuning pas Ã  pas

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Ã‰tape 1 : Exploration large avec RandomizedSearchCV
param_distributions = {
    'n_estimators': randint(50, 500),
    'max_depth': [None, 5, 10, 15, 20, 25, 30],
    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
    param_distributions=param_distributions,
    n_iter=50,        # 50 combinaisons alÃ©atoires
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train, y_train)

print(f"Meilleurs hyperparamÃ¨tres : {random_search.best_params_}")
print(f"Meilleur score AUC-ROC (CV) : {random_search.best_score_:.4f}")

# Score final
best_rf = random_search.best_estimator_
score_final = best_rf.score(X_test, y_test)
print(f"Score test (accuracy) : {score_final:.4f}")
```

### 7.3 RÃ¨gles empiriques

```
Guide rapide de tuning Random Forest :

1. n_estimators :
   â†’ Commencer Ã  100, augmenter Ã  200-300
   â†’ Au-delÃ  de 500, le gain est nÃ©gligeable
   â†’ VÃ©rifier avec le score OOB

2. max_depth :
   â†’ None (illimitÃ©) est souvent OK pour Random Forest
   â†’ RÃ©duire (10-20) si overfitting dÃ©tectÃ©
   â†’ Plus profond = plus de variance

3. max_features :
   â†’ 'sqrt' pour la classification (dÃ©faut, bon choix)
   â†’ 0.33 ou 'log2' sont aussi de bonnes options
   â†’ Plus bas = arbres plus dÃ©corrÃ©lÃ©s = meilleur bagging

4. min_samples_split / min_samples_leaf :
   â†’ Augmenter si overfitting (5-20)
   â†’ RÃ©duire si underfitting (2-5)
   â†’ min_samples_leaf=1 est souvent OK pour RF

5. class_weight :
   â†’ 'balanced' si classes dÃ©sÃ©quilibrÃ©es
   â†’ Sinon, laisser None
```

> ğŸ’¡ **Conseil** : "Random Forest est un modÃ¨le qui fonctionne bien 'out of the box'. Les valeurs par dÃ©faut de sklearn sont souvent proches de l'optimal. Commencez sans tuning, puis affinez seulement si nÃ©cessaire."

> âš ï¸ **Attention** : "Le tuning des hyperparamÃ¨tres donne gÃ©nÃ©ralement 2-5% d'amÃ©lioration. Le feature engineering en donne 10-30%. Investissez votre temps dans les features avant le tuning !"

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Un arbre de dÃ©cision** reproduit un raisonnement humain sous forme de questions successives â€” c'est le modÃ¨le le plus interprÃ©table
2. **L'algorithme CART** choisit le meilleur split Ã  chaque noeud en maximisant la puretÃ© (Gini ou Entropie)
3. **L'indice de Gini** mesure l'impuretÃ© : 0 = pur, 0.5 = mÃ©lange maximal (binaire)
4. **Un arbre profond apprend par coeur** (overfitting) â€” utilisez les critÃ¨res d'arrÃªt (max_depth, min_samples)
5. **Les arbres n'ont pas besoin de scaling** â€” seul l'ordre des valeurs compte pour les splits
6. **Random Forest = intelligence collective** : N arbres entraÃ®nÃ©s sur des donnÃ©es bootstrap avec des features alÃ©atoires
7. **Le score OOB** est une validation gratuite â€” pas besoin de set de validation sÃ©parÃ©
8. **100-200 arbres** suffisent dans 90% des cas â€” au-delÃ , le gain est marginal
9. **La feature importance par Gini est biaisÃ©e** â€” prÃ©fÃ©rez la permutation importance
10. **Random Forest est le couteau suisse du ML** : peu de tuning, robuste, rapide, interprÃ©table

---

## âœ… Checklist de validation

- [ ] Je sais expliquer un arbre de dÃ©cision Ã  un non-technique
- [ ] Je comprends l'algorithme CART (splits, Gini, entropie)
- [ ] Je sais calculer l'indice de Gini Ã  la main
- [ ] Je sais visualiser un arbre de dÃ©cision avec sklearn
- [ ] Je comprends pourquoi un arbre profond fait de l'overfitting
- [ ] Je connais la diffÃ©rence entre prÃ©-pruning et post-pruning
- [ ] Je sais pourquoi les arbres n'ont pas besoin de normalisation
- [ ] Je comprends le principe de Random Forest (bagging + features alÃ©atoires)
- [ ] Je sais expliquer le bootstrap sampling et le score OOB
- [ ] Je sais choisir le bon nombre d'arbres (impact sur performance et temps)
- [ ] Je maÃ®trise les hyperparamÃ¨tres clÃ©s de Random Forest
- [ ] Je sais utiliser la permutation importance plutÃ´t que l'importance Gini

---

**PrÃ©cÃ©dent** : [Chapitre 9 : ModÃ¨les LinÃ©aires et Logiques](09-modeles-lineaires.md)

**Suivant** : [Chapitre 11 : Boosting â€” Les Champions de Kaggle](11-boosting.md)
