# Chapitre 9 : Mod√®les Lin√©aires et Logiques ‚Äî Les Fondamentaux

## üéØ Objectifs

- Ma√Ætriser en profondeur la r√©gression lin√©aire (simple et multiple)
- Comprendre la r√©gression logistique et ses diff√©rences avec la r√©gression lin√©aire
- Revisiter KNN avec un regard critique (avantages, limites, curse of dimensionality)
- Comprendre ce que font `.fit()` et `.predict()` en interne pour chaque mod√®le
- Savoir comparer et choisir entre ces trois algorithmes fondamentaux

**Phase 3 ‚Äî Semaine 9 ‚Äî Les Algorithmes, enfin !**

---

## 1. üß† R√©gression lin√©aire (approfondissement)

### 1.1 Rappel de la formule

La r√©gression lin√©aire mod√©lise la relation entre des features (X) et une target (y) sous la forme :

```
y = b0 + b1*x1 + b2*x2 + ... + bn*xn

O√π :
- b0 = intercept (ordonn√©e √† l'origine)
- b1, b2, ..., bn = coefficients (un par feature)
- x1, x2, ..., xn = features (variables explicatives)
```

L'algorithme cherche les valeurs de `b0, b1, ..., bn` qui **minimisent** la somme des erreurs au carr√© (m√©thode des moindres carr√©s ordinaires ‚Äî OLS).

```
Objectif : minimiser Œ£(yi - ≈∑i)¬≤
                     i=1..n

O√π ≈∑i = b0 + b1*x1i + b2*x2i + ... + bn*xni
```

### 1.2 Interpr√©tation des coefficients

Chaque coefficient repr√©sente l'**impact marginal** de la feature correspondante sur la target, **toutes les autres features √©tant constantes**.

```
Exemple : pr√©diction du prix d'un appartement

prix = 50000 + 3200 * surface + 15000 * nb_pieces - 800 * distance_metro

Interpr√©tation :
- b0 = 50 000 ‚Ç¨ ‚Üí prix de base (intercept)
- b1 = 3 200    ‚Üí chaque m¬≤ suppl√©mentaire ajoute 3 200 ‚Ç¨ au prix
- b2 = 15 000   ‚Üí chaque pi√®ce suppl√©mentaire ajoute 15 000 ‚Ç¨
- b3 = -800     ‚Üí chaque km d'√©loignement du m√©tro retire 800 ‚Ç¨
```

> üí° **Conseil** : "Pour comparer l'importance relative des coefficients, il faut d'abord **standardiser** les features. Sans standardisation, un coefficient de 3200 (en m¬≤) n'est pas comparable √† un coefficient de -800 (en km)."

### 1.3 R√©gression lin√©aire simple vs multiple

| Aspect | R√©gression simple | R√©gression multiple |
|--------|-------------------|---------------------|
| **Nombre de features** | 1 seule (y = b0 + b1*x) | Plusieurs (y = b0 + b1*x1 + ... + bn*xn) |
| **Visualisation** | Droite dans un plan 2D | Hyperplan dans un espace nD |
| **Risque de multicolin√©arit√©** | Aucun | Oui, si features corr√©l√©es |
| **Interpr√©tation** | Tr√®s intuitive | Plus nuanc√©e (effet marginal) |
| **Performance typique** | Limit√©e | Meilleure (plus d'information) |

### 1.4 Hypoth√®ses du mod√®le

La r√©gression lin√©aire repose sur **5 hypoth√®ses fondamentales**. Les conna√Ætre permet de comprendre quand le mod√®le est fiable et quand il ne l'est pas.

| Hypoth√®se | Description | V√©rification | Cons√©quence si viol√©e |
|-----------|-------------|-------------|----------------------|
| **Lin√©arit√©** | Relation lin√©aire entre X et y | Scatter plots, r√©sidus vs pr√©dictions | Mod√®le biais√©, sous-performance |
| **Homosc√©dasticit√©** | Variance constante des r√©sidus | R√©sidus vs pr√©dictions (pas de c√¥ne) | Intervalles de confiance faux |
| **Normalit√© des r√©sidus** | R√©sidus ‚âà distribution normale | QQ-Plot, test de Shapiro-Wilk | Tests statistiques non fiables |
| **Ind√©pendance des r√©sidus** | Pas de corr√©lation entre r√©sidus | Durbin-Watson test | Sous-estimation de l'incertitude |
| **Pas de multicolin√©arit√©** | Features non fortement corr√©l√©es | VIF > 5 = probl√®me | Coefficients instables, ininterpr√©tables |

```
V√©rification visuelle des hypoth√®ses :

  R√©sidus vs Pr√©dictions         R√©sidus vs Pr√©dictions
  (Bon mod√®le)                   (Probl√®me : h√©t√©rosc√©dasticit√©)

  residus                        residus
    |  .  . .  .  .                |           . .  .
    |. .  . .  . .                 |        . .  .
    |-----.----.----‚Üí ≈∑            |    . .  .
    |.  .  .  . .                  |  . .
    | .  .  . .                    | . .
                                   |.
    ‚Üí al√©atoire, centr√© sur 0      ‚Üí forme de c√¥ne = probl√®me !
```

### 1.5 Limites de la r√©gression lin√©aire

| Limite | Description | Solution |
|--------|-------------|----------|
| **Non-lin√©arit√©** | Relations courbes non capt√©es | R√©gression polynomiale, arbres, XGBoost |
| **Outliers** | Tr√®s sensible aux valeurs extr√™mes | Nettoyage des donn√©es, r√©gularisation |
| **Multicolin√©arit√©** | Features corr√©l√©es ‚Üí coefficients instables | Lasso (s√©lection), PCA, suppression manuelle |
| **Features cat√©gorielles** | Ne g√®re que le num√©rique | One-Hot Encoding, Target Encoding |
| **Interactions** | Ne capture pas les interactions entre features | Ajouter manuellement x1*x2, ou mod√®le non-lin√©aire |

### 1.6 Code complet : r√©gression lin√©aire sur house_prices

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# --- G√©n√©rer un dataset house_prices r√©aliste ---
np.random.seed(42)
n = 500

surface = np.random.uniform(20, 200, n)
nb_pieces = np.random.randint(1, 7, n)
distance_metro = np.random.uniform(0.1, 10, n)
etage = np.random.randint(0, 15, n)
annee_construction = np.random.randint(1960, 2024, n)

# Prix avec relation r√©aliste + bruit
prix = (
    50000
    + 3200 * surface
    + 15000 * nb_pieces
    - 800 * distance_metro
    + 500 * etage
    + 200 * (annee_construction - 1960)
    + np.random.normal(0, 25000, n)
)

df = pd.DataFrame({
    'surface': surface,
    'nb_pieces': nb_pieces,
    'distance_metro': distance_metro,
    'etage': etage,
    'annee_construction': annee_construction,
    'prix': prix
})

print("=== Aper√ßu du dataset ===")
print(df.head())
print(f"\nShape : {df.shape}")
print(f"\nStatistiques :\n{df.describe().round(2)}")

# --- Pr√©paration ---
X = df.drop('prix', axis=1)
y = df['prix']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Entra√Ænement ---
modele = LinearRegression()
modele.fit(X_train_scaled, y_train)

# --- Coefficients ---
print("\n=== Coefficients du mod√®le ===")
coefs = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': modele.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(coefs)
print(f"\nIntercept : {modele.intercept_:.2f} ‚Ç¨")

# --- Pr√©dictions et √©valuation ---
y_pred = modele.predict(X_test_scaled)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\n=== √âvaluation ===")
print(f"RMSE : {rmse:,.0f} ‚Ç¨")
print(f"MAE  : {mae:,.0f} ‚Ç¨")
print(f"R¬≤   : {r2:.4f}")
```

### 1.7 Visualisation des r√©sidus

```python
def analyser_residus(y_true, y_pred, titre="Analyse des r√©sidus"):
    """Analyse compl√®te des r√©sidus"""
    residus = y_true - y_pred

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # 1. R√©sidus vs Pr√©dictions
    axes[0].scatter(y_pred, residus, alpha=0.3, s=20)
    axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)
    axes[0].set_xlabel('Pr√©dictions (≈∑)')
    axes[0].set_ylabel('R√©sidus (y - ≈∑)')
    axes[0].set_title('R√©sidus vs Pr√©dictions')
    axes[0].grid(True, alpha=0.3)

    # 2. Distribution des r√©sidus
    axes[1].hist(residus, bins=30, edgecolor='black', alpha=0.7)
    axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[1].set_xlabel('R√©sidus')
    axes[1].set_ylabel('Fr√©quence')
    axes[1].set_title('Distribution des r√©sidus')

    # 3. QQ-Plot
    from scipy import stats
    stats.probplot(residus, dist="norm", plot=axes[2])
    axes[2].set_title('QQ-Plot (normalit√©)')

    plt.suptitle(titre, fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

    # Statistiques
    print(f"R√©sidus - Moyenne : {residus.mean():.2f} (devrait √™tre ‚âà 0)")
    print(f"R√©sidus - √âcart-type : {residus.std():.2f}")

analyser_residus(y_test, y_pred)
```

> üí° **Conseil** : "Tracez **toujours** les r√©sidus apr√®s une r√©gression lin√©aire. Si vous voyez une forme de courbe ‚Üí la relation n'est pas lin√©aire. Si vous voyez un c√¥ne ‚Üí h√©t√©rosc√©dasticit√©. Des r√©sidus al√©atoires centr√©s sur 0 = bon mod√®le."

### 1.8 R¬≤ et R¬≤ ajust√©

Le **R¬≤** (coefficient de d√©termination) mesure la proportion de variance expliqu√©e par le mod√®le :

| M√©trique | Formule | Interpr√©tation |
|----------|---------|----------------|
| **R¬≤** | 1 - SS_res / SS_tot | % de variance expliqu√© (0 √† 1) |
| **R¬≤ ajust√©** | 1 - (1-R¬≤)(n-1)/(n-p-1) | P√©nalise l'ajout de features inutiles |

```python
def r2_ajuste(r2, n, p):
    """
    R¬≤ ajust√©
    n = nombre d'observations
    p = nombre de features
    """
    return 1 - (1 - r2) * (n - 1) / (n - p - 1)

n = len(y_test)
p = X_test.shape[1]

print(f"R¬≤         : {r2:.4f}")
print(f"R¬≤ ajust√©  : {r2_ajuste(r2, n, p):.4f}")
```

> ‚ö†Ô∏è **Attention** : "Le R¬≤ augmente **toujours** quand on ajoute une nouvelle feature, m√™me si elle est inutile. C'est pourquoi il faut utiliser le R¬≤ ajust√© qui p√©nalise la complexit√©. Si R¬≤ ajust√© << R¬≤, vous avez trop de features."

---

## 2. üìä R√©gression logistique

### 2.1 Ce n'est PAS une r√©gression !

Malgr√© son nom trompeur, la r√©gression logistique est un algorithme de **classification**. Le nom vient du fait qu'elle utilise une **fonction logistique** (sigmo√Øde) pour transformer une r√©gression lin√©aire en probabilit√©.

```
Pourquoi ce nom trompeur ?

1. On calcule d'abord une valeur lin√©aire :   z = b0 + b1*x1 + b2*x2 + ...
2. Puis on la transforme en probabilit√© :       P(y=1) = œÉ(z) = 1 / (1 + e^(-z))

La partie "r√©gression" fait r√©f√©rence √† l'√©tape 1 (lin√©aire).
La partie "logistique" fait r√©f√©rence √† l'√©tape 2 (sigmo√Øde).
Le r√©sultat final est une CLASSIFICATION (classe 0 ou 1).
```

### 2.2 La fonction sigmo√Øde

La fonction sigmo√Øde transforme n'importe quel nombre r√©el en une valeur entre 0 et 1, ce qui en fait une probabilit√© :

```
           œÉ(z) = 1 / (1 + e^(-z))

  Probabilit√©
  P(y=1)
    1.0 |                          ___________
        |                       __/
    0.8 |                     _/
        |                   _/
    0.6 |                  /
        |                 /
    0.5 |- - - - - - - -/- - - - - - - seuil par d√©faut
        |              /
    0.4 |             /
        |           _/
    0.2 |         _/
        |      __/
    0.0 |_____/
        +-------+-------+-------+-------‚Üí  z
       -6      -3       0       3       6

  Si z >> 0 ‚Üí œÉ(z) ‚âà 1 ‚Üí classe 1
  Si z << 0 ‚Üí œÉ(z) ‚âà 0 ‚Üí classe 0
  Si z = 0  ‚Üí œÉ(z) = 0.5 ‚Üí fronti√®re de d√©cision
```

```python
import numpy as np
import matplotlib.pyplot as plt

# Visualiser la sigmo√Øde
z = np.linspace(-8, 8, 200)
sigmoid = 1 / (1 + np.exp(-z))

plt.figure(figsize=(10, 6))
plt.plot(z, sigmoid, 'b-', linewidth=2, label='œÉ(z) = 1/(1+e^(-z))')
plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Seuil = 0.5')
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
plt.fill_between(z, sigmoid, 0.5, where=(sigmoid >= 0.5),
                 alpha=0.1, color='green', label='Classe 1')
plt.fill_between(z, sigmoid, 0.5, where=(sigmoid < 0.5),
                 alpha=0.1, color='red', label='Classe 0')
plt.xlabel('z = b0 + b1*x1 + b2*x2 + ...', fontsize=12)
plt.ylabel('P(y = 1)', fontsize=12)
plt.title('La fonction sigmo√Øde', fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.ylim(-0.05, 1.05)
plt.show()
```

### 2.3 De la r√©gression lin√©aire √† la logistique

Le lien math√©matique entre les deux est clair :

```
R√©gression lin√©aire :
    y = b0 + b1*x1 + b2*x2 + ...    ‚Üí valeur continue (prix, temp√©rature...)

R√©gression logistique :
    z = b0 + b1*x1 + b2*x2 + ...    ‚Üí valeur lin√©aire (identique !)
    P(y=1) = œÉ(z) = 1 / (1 + e^(-z)) ‚Üí probabilit√© [0, 1]
    classe = 1 si P(y=1) ‚â• 0.5, sinon 0 ‚Üí classification binaire

La seule diff√©rence : on applique la sigmo√Øde √† la sortie lin√©aire.
```

### 2.4 Probabilit√©s vs classes

| Concept | Description | Exemple |
|---------|-------------|---------|
| **Probabilit√©** | P(y=1) entre 0 et 1 | P(churn) = 0.78 |
| **Classe pr√©dite** | 0 ou 1 (selon le seuil) | churn = 1 (car 0.78 > 0.5) |
| **Seuil (threshold)** | Fronti√®re de d√©cision | Par d√©faut 0.5, ajustable |

> üí° **Conseil** : "Utiliser les probabilit√©s (`.predict_proba()`) est souvent plus utile que les classes brutes (`.predict()`). Cela permet d'ajuster le seuil selon le contexte m√©tier, de prioriser les cas les plus confiants, et de calculer l'AUC-ROC."

### 2.5 Interpr√©tation des coefficients : Odds Ratio

En r√©gression logistique, les coefficients s'interpr√®tent via l'**odds ratio** :

```
Odds(y=1) = P(y=1) / P(y=0) = P(y=1) / (1 - P(y=1))

log(Odds) = b0 + b1*x1 + b2*x2 + ...    (log-odds = valeur lin√©aire)

Odds Ratio pour la feature xi = e^(bi)

Interpr√©tation :
- OR = 1   ‚Üí pas d'effet
- OR > 1   ‚Üí augmente la probabilit√© de la classe 1
- OR < 1   ‚Üí diminue la probabilit√© de la classe 1
- OR = 2.5 ‚Üí chaque unit√© de xi multiplie les odds par 2.5
```

```python
import numpy as np

# Exemple d'interpr√©tation
coefficients = {
    'nb_appels_support': 0.35,
    'anciennete_mois': -0.08,
    'montant_mensuel': 0.02,
    'contrat_mensuel': 1.20,
}

print("=== Odds Ratios (R√©gression Logistique - Churn) ===\n")
for feature, coef in coefficients.items():
    odds_ratio = np.exp(coef)
    if odds_ratio > 1:
        effet = f"multiplie les odds de churn par {odds_ratio:.2f}"
    else:
        effet = f"divise les odds de churn par {1/odds_ratio:.2f}"
    print(f"{feature:>25} : coef={coef:>6.2f} ‚Üí OR={odds_ratio:.2f} ‚Üí {effet}")
```

### 2.6 Code complet : classification binaire sur churn

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt

# --- G√©n√©rer un dataset churn r√©aliste ---
np.random.seed(42)
n = 1000

anciennete = np.random.exponential(24, n)  # mois
montant_mensuel = np.random.uniform(20, 120, n)
nb_appels_support = np.random.poisson(2, n)
contrat_mensuel = np.random.binomial(1, 0.4, n)
satisfaction = np.random.uniform(1, 10, n)

# Probabilit√© de churn bas√©e sur les features
z = (
    -2.0
    + 0.35 * nb_appels_support
    - 0.04 * anciennete
    + 0.02 * montant_mensuel
    + 1.2 * contrat_mensuel
    - 0.3 * satisfaction
)
prob_churn = 1 / (1 + np.exp(-z))
churn = (np.random.random(n) < prob_churn).astype(int)

df = pd.DataFrame({
    'anciennete': anciennete,
    'montant_mensuel': montant_mensuel,
    'nb_appels_support': nb_appels_support,
    'contrat_mensuel': contrat_mensuel,
    'satisfaction': satisfaction,
    'churn': churn
})

print(f"=== Distribution du churn ===")
print(df['churn'].value_counts(normalize=True).round(3))

# --- Pr√©paration ---
X = df.drop('churn', axis=1)
y = df['churn']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Entra√Ænement ---
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train_scaled, y_train)

# --- Coefficients et Odds Ratios ---
print("\n=== Coefficients et Odds Ratios ===")
coefs_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': log_reg.coef_[0],
    'Odds_Ratio': np.exp(log_reg.coef_[0])
}).sort_values('Coefficient', key=abs, ascending=False)
print(coefs_df.round(4))

# --- Pr√©dictions ---
y_pred = log_reg.predict(X_test_scaled)
y_proba = log_reg.predict_proba(X_test_scaled)[:, 1]

# --- √âvaluation ---
print("\n=== Rapport de classification ===")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC : {roc_auc_score(y_test, y_proba):.4f}")

# --- Courbe ROC ---
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, 'b-', linewidth=2,
         label=f'ROC (AUC = {roc_auc_score(y_test, y_proba):.3f})')
plt.plot([0, 1], [0, 1], 'r--', label='Al√©atoire')
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC ‚Äî R√©gression Logistique (Churn)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.7 predict vs predict_proba

```python
# predict() ‚Üí classe pr√©dite (0 ou 1)
y_pred = log_reg.predict(X_test_scaled)
print("predict() :", y_pred[:10])
# ‚Üí [0, 1, 0, 0, 1, 1, 0, 0, 1, 0]

# predict_proba() ‚Üí probabilit√©s pour chaque classe
y_proba = log_reg.predict_proba(X_test_scaled)
print("predict_proba() :")
print(y_proba[:5])
# ‚Üí [[0.82, 0.18],   ‚Üê 82% classe 0, 18% classe 1
#     [0.23, 0.77],   ‚Üê 23% classe 0, 77% classe 1
#     [0.91, 0.09],
#     [0.65, 0.35],
#     [0.12, 0.88]]

# R√©cup√©rer uniquement la probabilit√© de la classe positive
y_proba_positive = log_reg.predict_proba(X_test_scaled)[:, 1]
print("P(churn=1) :", y_proba_positive[:5].round(3))

# Ajuster le seuil de d√©cision
seuil = 0.3  # plus agressif pour d√©tecter le churn
y_pred_custom = (y_proba_positive >= seuil).astype(int)
print(f"\nSeuil 0.5 ‚Üí {y_pred.sum()} churns d√©tect√©s")
print(f"Seuil 0.3 ‚Üí {y_pred_custom.sum()} churns d√©tect√©s")
```

> ‚ö†Ô∏è **Attention** : "Le seuil par d√©faut de 0.5 n'est pas toujours optimal. En d√©tection de churn, on pr√©f√®re souvent un seuil plus bas (0.3-0.4) pour d√©tecter plus de churners, quitte √† avoir plus de faux positifs. Le bon seuil d√©pend du **co√ªt m√©tier** des erreurs."

---

## 3. üîç KNN revisit√©

### 3.1 Rappel du principe

K-Nearest Neighbors (KNN) est un algorithme **non param√©trique** : il ne fait aucune hypoth√®se sur la distribution des donn√©es. Pour pr√©dire, il cherche les K voisins les plus proches et fait un vote (classification) ou une moyenne (r√©gression).

```
Pr√©diction pour un nouveau point (‚≠ê) avec K=3 :

    Classe A : ‚óè
    Classe B : ‚ñ≤
    Nouveau  : ‚≠ê

        ‚óè
    ‚óè       ‚ñ≤
        ‚≠ê          ‚ñ≤
    ‚óè       ‚óè
        ‚ñ≤       ‚ñ≤

    Les 3 plus proches voisins de ‚≠ê : ‚óè ‚óè ‚ñ≤
    Vote majoritaire : 2 √ó ‚óè vs 1 √ó ‚ñ≤
    Pr√©diction : classe ‚óè (A)
```

### 3.2 Avantages et inconv√©nients

| Avantages | Inconv√©nients |
|-----------|---------------|
| Tr√®s simple √† comprendre | Lent en pr√©diction (O(n) par pr√©diction) |
| Aucune hypoth√®se sur les donn√©es | Tr√®s sensible au scaling des features |
| Non param√©trique | Curse of dimensionality (>20 features) |
| Pas de phase d'entra√Ænement | Pas de mod√®le interpr√©table |
| Fonctionne en classification ET r√©gression | Sensible au bruit et aux outliers |
| Capte des fronti√®res non lin√©aires | Mauvais avec des features cat√©gorielles |

### 3.3 La mal√©diction de la dimensionnalit√© (Curse of Dimensionality)

```
En haute dimension, les distances entre points deviennent presque identiques :

  Dimension 1 (1D) :  ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè     ‚Üí distances vari√©es, voisins clairs

  Dimension 100 (100D) :
  Toutes les distances convergent vers une m√™me valeur
  ‚Üí "tous les points sont aussi loin les uns des autres"
  ‚Üí KNN ne sait plus qui est vraiment "proche"

  R√®gle empirique :
  - KNN fonctionne bien jusqu'√† ~20 features
  - Au-del√†, les performances se d√©gradent
  - Solution : r√©duction de dimension (PCA) avant KNN
```

> ‚ö†Ô∏è **Attention** : "KNN est l'algorithme le plus affect√© par la mal√©diction de la dimensionnalit√©. Si vous avez plus de 20-30 features, privil√©giez un Random Forest ou une r√©gression logistique."

### 3.4 KNN pour la r√©gression

KNN n'est pas limit√© √† la classification ‚Äî il peut aussi faire de la r√©gression en calculant la **moyenne** (ou la moyenne pond√©r√©e) des valeurs des K voisins :

```python
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# KNN en r√©gression
knn_reg = KNeighborsRegressor(n_neighbors=5, weights='distance')
knn_reg.fit(X_train_scaled, y_train)
y_pred_knn = knn_reg.predict(X_test_scaled)

rmse = np.sqrt(mean_squared_error(y_test, y_pred_knn))
r2 = r2_score(y_test, y_pred_knn)
print(f"KNN R√©gression (K=5) : RMSE={rmse:.2f}, R¬≤={r2:.4f}")

# KNN en classification
knn_clf = KNeighborsClassifier(n_neighbors=5, weights='distance')
knn_clf.fit(X_train_scaled, y_train)
y_pred_knn_clf = knn_clf.predict(X_test_scaled)
```

### 3.5 Choisir K optimal : la m√©thode du coude (Elbow Method)

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

# Tester diff√©rentes valeurs de K
k_values = range(1, 31)
scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')
    score = cross_val_score(knn, X_train_scaled, y_train, cv=5,
                            scoring='accuracy').mean()
    scores.append(score)

# Trouver le K optimal
best_k = k_values[np.argmax(scores)]
best_score = max(scores)
print(f"Meilleur K : {best_k} (accuracy = {best_score:.4f})")

# Visualiser
plt.figure(figsize=(10, 6))
plt.plot(k_values, scores, 'bo-', linewidth=2)
plt.axvline(x=best_k, color='red', linestyle='--',
            label=f'K optimal = {best_k}')
plt.xlabel('K (nombre de voisins)', fontsize=12)
plt.ylabel('Accuracy (cross-validation)', fontsize=12)
plt.title('Choix de K ‚Äî M√©thode du coude', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.xticks(k_values)
plt.show()
```

> üí° **Conseil** : "Commencez avec K=5 ou K=‚àön (racine carr√©e du nombre d'√©chantillons). Utilisez toujours un nombre impair pour K en classification binaire afin d'√©viter les √©galit√©s."

### 3.6 L'importance cruciale du scaling pour KNN

```python
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Sans scaling
knn_no_scale = KNeighborsClassifier(n_neighbors=5)
knn_no_scale.fit(X_train, y_train)
score_no_scale = accuracy_score(y_test, knn_no_scale.predict(X_test))

# Avec scaling
knn_scaled = KNeighborsClassifier(n_neighbors=5)
knn_scaled.fit(X_train_scaled, y_train)
score_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))

print(f"KNN sans scaling : {score_no_scale:.4f}")
print(f"KNN avec scaling : {score_scaled:.4f}")
print(f"Diff√©rence       : +{score_scaled - score_no_scale:.4f}")
```

> ‚ö†Ô∏è **Attention** : "Le scaling est **obligatoire** pour KNN. Sans scaling, une feature en m√®tres (0-200) dominera une feature en km (0-10), faussant compl√®tement le calcul des distances."

---

## 4. ‚öôÔ∏è Comprendre .fit() et .predict()

### 4.1 Que se passe-t-il en interne ?

Chaque algorithme a un comportement diff√©rent lors de l'entra√Ænement (`.fit()`) et de la pr√©diction (`.predict()`) :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    R√âGRESSION LIN√âAIRE                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ .fit(X, y)                                                      ‚îÇ
‚îÇ   ‚Üí Calcule les coefficients b0, b1, ..., bn                   ‚îÇ
‚îÇ   ‚Üí M√©thode : moindres carr√©s (OLS) ou gradient descent        ‚îÇ
‚îÇ   ‚Üí Stocke : coef_, intercept_                                  ‚îÇ
‚îÇ   ‚Üí Temps : rapide (O(np¬≤) avec n samples, p features)         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ .predict(X)                                                      ‚îÇ
‚îÇ   ‚Üí Calcule y = b0 + b1*x1 + ... + bn*xn                      ‚îÇ
‚îÇ   ‚Üí Simple multiplication matrice-vecteur                        ‚îÇ
‚îÇ   ‚Üí Temps : tr√®s rapide (O(np))                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    R√âGRESSION LOGISTIQUE                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ .fit(X, y)                                                      ‚îÇ
‚îÇ   ‚Üí Optimise les coefficients par gradient descent              ‚îÇ
‚îÇ   ‚Üí Minimise la log-loss (cross-entropy)                        ‚îÇ
‚îÇ   ‚Üí Stocke : coef_, intercept_                                  ‚îÇ
‚îÇ   ‚Üí Temps : mod√©r√© (it√©ratif, d√©pend de max_iter)              ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ .predict(X)                                                      ‚îÇ
‚îÇ   ‚Üí Calcule z = b0 + b1*x1 + ... + bn*xn                      ‚îÇ
‚îÇ   ‚Üí Applique œÉ(z) pour obtenir les probabilit√©s                 ‚îÇ
‚îÇ   ‚Üí Applique le seuil (0.5) pour obtenir les classes            ‚îÇ
‚îÇ   ‚Üí Temps : tr√®s rapide                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           KNN                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ .fit(X, y)                                                      ‚îÇ
‚îÇ   ‚Üí Stocke simplement les donn√©es en m√©moire                    ‚îÇ
‚îÇ   ‚Üí Aucun calcul !                                              ‚îÇ
‚îÇ   ‚Üí Temps : quasi-instantan√©                                    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ .predict(X)                                                      ‚îÇ
‚îÇ   ‚Üí Pour CHAQUE nouveau point :                                  ‚îÇ
‚îÇ       1. Calcule la distance avec TOUS les points d'entra√Ænement‚îÇ
‚îÇ       2. Trie pour trouver les K plus proches                    ‚îÇ
‚îÇ       3. Vote majoritaire (classif) ou moyenne (r√©gression)      ‚îÇ
‚îÇ   ‚Üí Temps : LENT (O(n*d) par pr√©diction, n=training size)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Lazy Learner vs Eager Learner

| Caract√©ristique | Lazy Learner (KNN) | Eager Learner (R√©gression) |
|-----------------|-------------------|---------------------------|
| **Entra√Ænement (.fit)** | Instantan√© (juste stocker) | Calcul des param√®tres |
| **Pr√©diction (.predict)** | Lent (calcul √† chaque fois) | Rapide (appliquer la formule) |
| **M√©moire** | Stocke tout le dataset | Stocke seulement les param√®tres |
| **Adaptabilit√©** | S'adapte √† de nouvelles donn√©es | Doit r√©entra√Æner |
| **Interpr√©tabilit√©** | Aucun mod√®le explicite | Coefficients interpr√©tables |

```python
import time

# --- Comparer les temps de fit et predict ---
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

modeles = {
    'R√©gression Lin√©aire': LinearRegression(),
    'R√©gression Logistique': LogisticRegression(max_iter=1000),
    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),
}

print("=== Comparaison des temps (fit vs predict) ===\n")
for nom, modele in modeles.items():
    # Temps de fit
    start = time.time()
    modele.fit(X_train_scaled, y_train)
    temps_fit = time.time() - start

    # Temps de predict
    start = time.time()
    for _ in range(100):  # R√©p√©ter 100 fois pour mieux mesurer
        modele.predict(X_test_scaled)
    temps_predict = (time.time() - start) / 100

    print(f"{nom:>25} : fit={temps_fit*1000:.1f}ms, predict={temps_predict*1000:.2f}ms")
```

> üí° **Conseil** : "En production, le temps de pr√©diction est souvent plus important que le temps d'entra√Ænement. Un mod√®le lin√©aire qui pr√©dit en 0.01ms est pr√©f√©rable √† un KNN qui met 50ms, surtout si vous traitez des millions de requ√™tes par jour."

### 4.3 Que stocke chaque mod√®le apr√®s .fit() ?

```python
# R√©gression lin√©aire ‚Üí stocke les coefficients
lin_reg = LinearRegression()
lin_reg.fit(X_train_scaled, y_train)
print("R√©gression Lin√©aire stocke :")
print(f"  coef_      : {lin_reg.coef_}")
print(f"  intercept_ : {lin_reg.intercept_}")
print(f"  ‚Üí Total : {len(lin_reg.coef_) + 1} param√®tres\n")

# R√©gression logistique ‚Üí stocke les coefficients
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
print("R√©gression Logistique stocke :")
print(f"  coef_      : {log_reg.coef_[0]}")
print(f"  intercept_ : {log_reg.intercept_}")
print(f"  classes_   : {log_reg.classes_}")
print(f"  ‚Üí Total : {len(log_reg.coef_[0]) + 1} param√®tres\n")

# KNN ‚Üí stocke TOUT le dataset
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
print("KNN stocke :")
print(f"  _fit_X shape : {knn._fit_X.shape}")
print(f"  _y shape     : {knn._y.shape}")
print(f"  ‚Üí Total : {knn._fit_X.size + knn._y.size} valeurs en m√©moire !")
```

---

## 5. üìã Tableau comparatif des 3 mod√®les

### 5.1 Comparaison g√©n√©rale

| Crit√®re | R√©gression Lin√©aire | R√©gression Logistique | KNN |
|---------|--------------------|-----------------------|-----|
| **Type de probl√®me** | R√©gression | Classification | Les deux |
| **Sortie** | Valeur continue | Probabilit√© / Classe | Valeur / Classe |
| **Hypoth√®ses** | Lin√©arit√©, normalit√© r√©sidus | Lin√©arit√© du log-odds | Aucune |
| **Param√©trique** | Oui | Oui | Non |
| **Interpr√©tabilit√©** | Excellente (coefficients) | Bonne (odds ratios) | Faible |
| **Scaling n√©cessaire** | Non (sauf comparaison coefs) | Recommand√© | Obligatoire |
| **G√®re la non-lin√©arit√©** | Non | Non | Oui |
| **Vitesse fit** | Rapide | Mod√©r√©e | Instantan√©e |
| **Vitesse predict** | Tr√®s rapide | Tr√®s rapide | Lente |
| **Sensible aux outliers** | Tr√®s | Mod√©r√©ment | Oui |
| **G√®re les features cat√©gorielles** | Apr√®s encoding | Apr√®s encoding | Difficilement |
| **Nombre de features √©lev√©** | Correct | Bon | Mauvais (curse of dim.) |

### 5.2 Quand utiliser quoi ?

| Situation | Mod√®le recommand√© | Justification |
|-----------|-------------------|---------------|
| Pr√©dire un prix, une dur√©e | R√©gression Lin√©aire | Valeur continue, interpr√©table |
| Classifier spam/non-spam | R√©gression Logistique | Classification binaire, probabilit√©s |
| Peu de features, relations complexes | KNN | Non-param√©trique, fronti√®res non-lin√©aires |
| Besoin d'explicabilit√© m√©tier | R√©gression (Lin. ou Log.) | Coefficients interpr√©tables |
| Production √† grande √©chelle | R√©gression (Lin. ou Log.) | Pr√©diction ultra-rapide |
| Exploration rapide / baseline | KNN ou R√©gression | Simple, rapide √† impl√©menter |
| > 50 features | R√©gression Logistique | KNN souffre de la dimensionnalit√© |
| Dataset > 100k lignes | R√©gression (Lin. ou Log.) | KNN trop lent en pr√©diction |

### 5.3 Code de comparaison compl√®te

```python
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score
import pandas as pd
import time

# Comparaison sur un probl√®me de classification
modeles_clf = {
    'R√©gression Logistique': LogisticRegression(max_iter=1000, random_state=42),
    'KNN (K=3)': KNeighborsClassifier(n_neighbors=3),
    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),
    'KNN (K=11)': KNeighborsClassifier(n_neighbors=11),
}

resultats = []
for nom, modele in modeles_clf.items():
    start = time.time()
    scores = cross_val_score(modele, X_train_scaled, y_train,
                             cv=5, scoring='roc_auc')
    duree = time.time() - start

    resultats.append({
        'Mod√®le': nom,
        'AUC-ROC (CV)': f"{scores.mean():.4f} ¬± {scores.std():.4f}",
        'Temps (s)': f"{duree:.2f}",
    })

df_resultats = pd.DataFrame(resultats)
print("=== Comparaison des mod√®les ===")
print(df_resultats.to_string(index=False))
```

---

## üéØ Points cl√©s √† retenir

1. **La r√©gression lin√©aire** mod√©lise y = b0 + b1*x1 + ... ‚Äî chaque coefficient est l'impact marginal d'une feature sur la target
2. **Les 5 hypoth√®ses** de la r√©gression lin√©aire (lin√©arit√©, homosc√©dasticit√©, normalit√© des r√©sidus, ind√©pendance, pas de multicolin√©arit√©) doivent √™tre v√©rifi√©es
3. **La r√©gression logistique** n'est PAS une r√©gression ‚Äî c'est une classification qui applique la sigmo√Øde √† une combinaison lin√©aire
4. **predict_proba** est souvent plus utile que predict ‚Äî il donne les probabilit√©s et permet d'ajuster le seuil
5. **Les odds ratios** (e^coefficient) permettent d'interpr√©ter l'impact de chaque feature en r√©gression logistique
6. **KNN** est simple et non-param√©trique, mais lent en pr√©diction et sensible √† la dimensionnalit√©
7. **Le scaling est obligatoire** pour KNN (distances) et recommand√© pour la r√©gression logistique
8. **KNN est un lazy learner** : fit est instantan√© mais predict est lent (calcul des distances √† chaque fois)
9. **Les r√©gressions sont des eager learners** : fit calcule les param√®tres, predict les applique tr√®s rapidement
10. **Commencer simple** : r√©gression lin√©aire/logistique comme baseline, puis complexifier si n√©cessaire

---

## ‚úÖ Checklist de validation

- [ ] Je sais impl√©menter et interpr√©ter une r√©gression lin√©aire simple et multiple
- [ ] Je connais les 5 hypoth√®ses de la r√©gression lin√©aire et sais les v√©rifier
- [ ] Je sais analyser les r√©sidus et en tirer des conclusions
- [ ] Je comprends la diff√©rence entre R¬≤ et R¬≤ ajust√©
- [ ] Je sais que la r√©gression logistique est un mod√®le de classification, pas de r√©gression
- [ ] Je sais dessiner et expliquer la fonction sigmo√Øde
- [ ] Je sais utiliser predict_proba et ajuster le seuil de d√©cision
- [ ] Je sais interpr√©ter les coefficients via les odds ratios
- [ ] Je connais les avantages et limites de KNN (curse of dimensionality, scaling obligatoire)
- [ ] Je sais choisir K optimal avec la m√©thode du coude
- [ ] Je comprends la diff√©rence entre lazy learner (KNN) et eager learner (r√©gression)
- [ ] Je sais quel mod√®le choisir selon le contexte (taille des donn√©es, interpr√©tabilit√©, production)

---

**Pr√©c√©dent** : [Chapitre 8 : √âvaluation et M√©triques](08-evaluation-metriques.md)

**Suivant** : [Chapitre 10 : Arbres de D√©cision et For√™ts Al√©atoires](10-arbres-forets.md)
