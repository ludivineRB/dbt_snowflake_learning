# Chapitre 12 : M√©triques ‚Äî Au-del√† de l'Accuracy

## üéØ Objectifs

- Comprendre pourquoi l'accuracy est souvent **trompeuse**
- Ma√Ætriser la matrice de confusion et toutes ses d√©riv√©es
- Savoir choisir la **bonne m√©trique** selon le contexte m√©tier
- Tracer et interpr√©ter les courbes ROC et Precision-Recall
- Comprendre le log-loss et l'importance des probabilit√©s calibr√©es
- Ajuster le seuil de d√©cision pour optimiser la m√©trique choisie

> **Phase 4 - Semaine 12**

---

## 1. üß† Le pi√®ge de l'Accuracy : "Votre mod√®le a 95% de pr√©cision, mais il est nul"

### 1.1 Le sc√©nario qui fait mal

Imaginez : vous travaillez pour une banque. Votre mission : **d√©tecter les transactions frauduleuses**. Sur 10 000 transactions, seulement **100 sont frauduleuses** (1%).

Vous construisez un mod√®le... qui pr√©dit **toujours "pas de fraude"**. Z√©ro intelligence, z√©ro effort.

```
R√©sultat :
  - 9 900 transactions l√©gitimes pr√©dites "l√©gitime" ‚Üí ‚úÖ Correct
  - 100 transactions frauduleuses pr√©dites "l√©gitime" ‚Üí ‚ùå Rat√© !

  Accuracy = 9 900 / 10 000 = 99% ü§Ø
```

**99% d'accuracy** pour un mod√®le compl√®tement inutile. C'est le paradoxe de l'accuracy sur des **classes d√©s√©quilibr√©es**.

### 1.2 D√©monstration en code

```python
import numpy as np
from sklearn.metrics import accuracy_score, classification_report

# --- Simuler le sc√©nario ---
np.random.seed(42)
n_total = 10_000
n_fraudes = 100  # 1% de fraudes

# R√©alit√©
y_true = np.zeros(n_total)
y_true[:n_fraudes] = 1  # Les 100 premi√®res sont des fraudes

# Mod√®le "stupide" : pr√©dit toujours 0 (pas de fraude)
y_pred_stupide = np.zeros(n_total)

# Mod√®le "correct" : d√©tecte 80% des fraudes mais a quelques faux positifs
y_pred_correct = np.zeros(n_total)
y_pred_correct[:80] = 1   # D√©tecte 80 fraudes sur 100
y_pred_correct[200:250] = 1  # 50 faux positifs

print("=== Mod√®le STUPIDE (toujours 'pas de fraude') ===")
print(f"Accuracy : {accuracy_score(y_true, y_pred_stupide):.2%}")
print(classification_report(y_true, y_pred_stupide, target_names=['L√©gitime', 'Fraude']))

print("\n=== Mod√®le CORRECT ===")
print(f"Accuracy : {accuracy_score(y_true, y_pred_correct):.2%}")
print(classification_report(y_true, y_pred_correct, target_names=['L√©gitime', 'Fraude']))
```

```
=== Mod√®le STUPIDE ===
Accuracy : 99.00%
              precision    recall  f1-score   support
    L√©gitime       0.99      1.00      1.00      9900
      Fraude       0.00      0.00      0.00       100

=== Mod√®le CORRECT ===
Accuracy : 99.30%
              precision    recall  f1-score   support
    L√©gitime       1.00      0.99      1.00      9900
      Fraude       0.62      0.80      0.70       100
```

> ‚ö†Ô∏è **Attention** : "Le mod√®le stupide a 99% d'accuracy mais un recall de 0% sur les fraudes. Il ne d√©tecte **aucune** fraude. Ne faites **jamais** confiance √† l'accuracy seule sur des donn√©es d√©s√©quilibr√©es."

---

## 2. üî¢ La Matrice de Confusion ‚Äî La base de tout

### 2.1 Comprendre TP, TN, FP, FN

La matrice de confusion croise les **pr√©dictions** du mod√®le avec la **r√©alit√©**.

```
                          PR√âDICTION DU MOD√àLE
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Pr√©dit : 0  ‚îÇ  Pr√©dit : 1  ‚îÇ
                    ‚îÇ  (N√©gatif)   ‚îÇ  (Positif)   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ R√©alit√© : 0       ‚îÇ     TN       ‚îÇ     FP       ‚îÇ
‚îÇ (N√©gatif)         ‚îÇ Vrai N√©gatif ‚îÇ Faux Positif ‚îÇ
‚îÇ                   ‚îÇ              ‚îÇ (Type I)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ R√©alit√© : 1       ‚îÇ     FN       ‚îÇ     TP       ‚îÇ
‚îÇ (Positif)         ‚îÇ Faux N√©gatif ‚îÇ Vrai Positif ‚îÇ
‚îÇ                   ‚îÇ (Type II)    ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Analogie m√©dicale

Imaginons un test de d√©pistage pour une maladie :

| Terme | Signification | Analogie m√©dicale |
|-------|--------------|-------------------|
| **TP** (Vrai Positif) | Mod√®le dit "malade", patient **est** malade | Test d√©tecte la maladie chez un vrai malade |
| **TN** (Vrai N√©gatif) | Mod√®le dit "sain", patient **est** sain | Test n√©gatif pour un patient sain |
| **FP** (Faux Positif) | Mod√®le dit "malade", patient **est** sain | Fausse alerte ! Patient sain inqui√©t√© |
| **FN** (Faux N√©gatif) | Mod√®le dit "sain", patient **est** malade | Maladie **rat√©e** ! Danger ! |

> üí° **Conseil** : "Pour retenir : le **deuxi√®me mot** indique ce que le mod√®le a **dit**, et le **premier mot** indique s'il a **raison** (Vrai) ou **tort** (Faux)."

### 2.3 Visualisation avec sklearn

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    classification_report
)

# --- Pr√©parer les donn√©es ---
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, test_size=0.2, random_state=42, stratify=cancer.target
)

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

# --- Entra√Æner ---
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_s, y_train)
y_pred = model.predict(X_test_s)

# --- Matrice de confusion ---
cm = confusion_matrix(y_test, y_pred)
print("Matrice de confusion :")
print(cm)
print(f"\nTN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]}")

# --- Visualisation ---
fig, ax = plt.subplots(figsize=(8, 6))
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=['Malin', 'B√©nin']
)
disp.plot(cmap='Blues', ax=ax, values_format='d')
plt.title('Matrice de Confusion ‚Äî Cancer du sein')
plt.tight_layout()
plt.show()
```

---

## 3. üìä M√©triques expliqu√©es avec des exemples m√©tiers

### 3.1 Accuracy ‚Äî Quand l'utiliser (et quand NE PAS)

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
         = Nombre de bonnes pr√©dictions / Total
```

| Utiliser l'accuracy quand... | NE PAS utiliser quand... |
|------------------------------|--------------------------|
| Classes √©quilibr√©es (50/50) | Classes d√©s√©quilibr√©es (95/5) |
| Chaque erreur a le m√™me co√ªt | FP et FN ont des co√ªts diff√©rents |
| Cas simple (Iris, MNIST) | D√©tection fraude, cancer, spam |

### 3.2 Precision ‚Äî "Quand je dis fraude, j'ai raison combien de fois ?"

```
Precision = TP / (TP + FP)
          = Vrais Positifs / Total des pr√©dictions positives
```

**Question :** Parmi toutes les alertes que le mod√®le a lev√©es, **combien √©taient justifi√©es** ?

| Contexte | Pourquoi la precision compte |
|----------|------------------------------|
| **Filtre anti-spam** | Un FP = un vrai mail en spam = client furieux |
| **Recommandation** | Un FP = produit non pertinent = perte de confiance |
| **Recrutement** | Un FP = candidat non qualifi√© re√ßu en entretien = temps perdu |

### 3.3 Recall (Sensibilit√©) ‚Äî "Je d√©tecte combien de vraies fraudes ?"

```
Recall = TP / (TP + FN)
       = Vrais Positifs / Total des vrais positifs r√©els
```

**Question :** Parmi toutes les vraies fraudes, **combien le mod√®le en a-t-il d√©tect√©** ?

| Contexte | Pourquoi le recall compte |
|----------|--------------------------|
| **D√©tection cancer** | Un FN = cancer rat√© = danger de mort |
| **D√©tection fraude** | Un FN = fraude non d√©tect√©e = perte financi√®re |
| **S√©curit√© a√©rienne** | Un FN = menace non d√©tect√©e = catastrophe |

> ‚ö†Ô∏è **Attention** : "Precision et Recall sont **antagonistes**. Augmenter l'un fait g√©n√©ralement baisser l'autre. C'est le **trade-off Precision/Recall**."

### 3.4 F1-Score ‚Äî L'√©quilibre

```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

Le F1-Score est la **moyenne harmonique** de Precision et Recall. Pourquoi harmonique et pas arithm√©tique ?

```
Exemple :
  Precision = 0.90, Recall = 0.10

  Moyenne arithm√©tique = (0.90 + 0.10) / 2 = 0.50  ‚Üê Semble OK
  Moyenne harmonique   = 2 * (0.90 * 0.10) / (0.90 + 0.10) = 0.18  ‚Üê P√©nalise !
```

La moyenne harmonique **p√©nalise fortement** les d√©s√©quilibres. Un F1-Score √©lev√© n√©cessite que Precision **et** Recall soient tous les deux √©lev√©s.

### 3.5 Specificity (Taux de Vrais N√©gatifs)

```
Specificity = TN / (TN + FP)
            = Vrais N√©gatifs / Total des vrais n√©gatifs r√©els
```

**Question :** Parmi toutes les transactions l√©gitimes, combien le mod√®le en a-t-il correctement identifi√© comme l√©gitimes ?

### 3.6 Tableau r√©capitulatif

| M√©trique | Formule | Question m√©tier | Priorit√© quand... |
|----------|---------|-----------------|-------------------|
| **Accuracy** | (TP+TN) / Total | "Quel % de bonnes r√©ponses ?" | Classes √©quilibr√©es |
| **Precision** | TP / (TP+FP) | "Mes alertes sont-elles fiables ?" | FP co√ªteux |
| **Recall** | TP / (TP+FN) | "Je rate combien de cas ?" | FN co√ªteux |
| **F1** | Harmonic(P, R) | "Quel √©quilibre P/R ?" | M√©trique par d√©faut |
| **Specificity** | TN / (TN+FP) | "Les n√©gatifs sont-ils bien class√©s ?" | Beaucoup de n√©gatifs |

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, classification_report
)

# Calcul de toutes les m√©triques
print("=== M√©triques d√©taill√©es ===")
print(f"Accuracy    : {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision   : {precision_score(y_test, y_pred):.4f}")
print(f"Recall      : {recall_score(y_test, y_pred):.4f}")
print(f"F1-Score    : {f1_score(y_test, y_pred):.4f}")

# Specificity (pas directement dans sklearn)
cm = confusion_matrix(y_test, y_pred)
specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])
print(f"Specificity : {specificity:.4f}")

# Rapport complet
print("\n=== Rapport complet ===")
print(classification_report(y_test, y_pred, target_names=['Malin', 'B√©nin']))
```

---

## 4. üìà Courbe ROC et AUC

### 4.1 Courbe ROC expliqu√©e pas √† pas

La courbe ROC (Receiver Operating Characteristic) trace **TPR vs FPR** pour tous les seuils de d√©cision possibles :

```
  TPR (Recall)
  1.0 ‚îÇ        ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Mod√®le parfait (AUC = 1.0)
      ‚îÇ      ‚ï±
      ‚îÇ    ‚ï±    ‚ï±‚îÄ‚îÄ Bon mod√®le (AUC ~ 0.85)
      ‚îÇ  ‚ï±    ‚ï±
  0.5 ‚îÇ‚ï±    ‚ï±
      ‚îÇ   ‚ï±       ‚ï±‚îÄ‚îÄ Al√©atoire (AUC = 0.5)
      ‚îÇ  ‚ï±      ‚ï±
      ‚îÇ‚ï±      ‚ï±
  0.0 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FPR (1 - Specificity)
      0.0                 1.0
```

- **TPR** (True Positive Rate) = Recall = TP / (TP + FN)
- **FPR** (False Positive Rate) = FP / (FP + TN) = 1 - Specificity

### 4.2 AUC : interpr√©tation

L'**AUC** (Area Under the Curve) r√©sume la courbe ROC en un seul nombre :

| AUC | Interpr√©tation |
|-----|---------------|
| 1.0 | Mod√®le parfait |
| 0.9 - 1.0 | Excellent |
| 0.8 - 0.9 | Bon |
| 0.7 - 0.8 | Acceptable |
| 0.5 - 0.7 | M√©diocre |
| 0.5 | Al√©atoire (aucun pouvoir discriminant) |
| < 0.5 | Pire qu'al√©atoire (labels probablement invers√©s) |

> üí° **Conseil** : "L'AUC peut √™tre interpr√©t√©e comme la probabilit√© que le mod√®le assigne un score plus √©lev√© √† un positif choisi au hasard qu'√† un n√©gatif choisi au hasard."

### 4.3 Code complet avec visualisation

```python
from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Obtenir les probabilit√©s
y_proba = model.predict_proba(X_test_s)[:, 1]

# --- Calcul de la courbe ROC ---
fpr, tpr, seuils = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)

# --- Visualisation ---
fig, ax = plt.subplots(figsize=(8, 6))
RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_score).plot(ax=ax)
ax.plot([0, 1], [0, 1], 'k--', label='Al√©atoire (AUC = 0.5)')
ax.set_title(f'Courbe ROC (AUC = {auc_score:.4f})')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# --- Comparer plusieurs mod√®les ---
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

modeles = {
    'Logistique': LogisticRegression(max_iter=1000, random_state=42),
    'Arbre': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
}

fig, ax = plt.subplots(figsize=(10, 7))
for nom, mod in modeles.items():
    mod.fit(X_train_s, y_train)
    if hasattr(mod, 'predict_proba'):
        y_prob = mod.predict_proba(X_test_s)[:, 1]
    else:
        y_prob = mod.decision_function(X_test_s)
    fpr_m, tpr_m, _ = roc_curve(y_test, y_prob)
    auc_m = roc_auc_score(y_test, y_prob)
    ax.plot(fpr_m, tpr_m, label=f'{nom} (AUC = {auc_m:.3f})')

ax.plot([0, 1], [0, 1], 'k--', label='Al√©atoire')
ax.set_xlabel('Taux de Faux Positifs (FPR)')
ax.set_ylabel('Taux de Vrais Positifs (TPR)')
ax.set_title('Comparaison des courbes ROC')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## 5. üìâ Courbe Precision-Recall

### 5.1 Pourquoi elle est meilleure que ROC pour les donn√©es d√©s√©quilibr√©es

La courbe ROC peut √™tre **trompeuse** quand les classes sont tr√®s d√©s√©quilibr√©es. Un mod√®le m√©diocre peut afficher un AUC-ROC √©lev√© simplement parce que le TN est √©norme.

La courbe **Precision-Recall** est plus informative car elle se concentre uniquement sur la **classe positive** (la classe rare).

```
  Precision
  1.0 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
      ‚îÇ       ‚ï≤        Bon mod√®le
      ‚îÇ        ‚ï≤
  0.5 ‚îÇ         ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      ‚îÇ                  ‚ï≤
      ‚îÇ                   ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ Mod√®le m√©diocre
  0.0 ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Recall
      0.0                     1.0
```

### 5.2 Average Precision (AP)

L'**Average Precision** r√©sume la courbe Precision-Recall en un nombre. C'est l'aire sous la courbe PR.

```python
from sklearn.metrics import (
    precision_recall_curve, average_precision_score,
    PrecisionRecallDisplay
)

# --- Courbe Precision-Recall ---
precision, recall, seuils_pr = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

fig, ax = plt.subplots(figsize=(8, 6))
PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=ap).plot(ax=ax)
ax.set_title(f'Courbe Precision-Recall (AP = {ap:.4f})')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"Average Precision : {ap:.4f}")
```

> üí° **Conseil** : "Utilisez la courbe **ROC** quand vos classes sont relativement √©quilibr√©es. Utilisez la courbe **Precision-Recall** quand la classe positive est rare (< 10% du dataset)."

---

## 6. üìê Log-loss : La qualit√© des probabilit√©s

### 6.1 Pourquoi les probabilit√©s comptent

Tous les mod√®les ne se valent pas en termes de **calibration des probabilit√©s**. Un mod√®le qui dit "80% de chance de fraude" devrait avoir raison 80% du temps quand il dit √ßa.

Le **log-loss** (ou binary cross-entropy) mesure la qualit√© des probabilit√©s :

```
Log-loss = -1/N * Œ£ [y_i * log(p_i) + (1 - y_i) * log(1 - p_i)]
```

| Log-loss | Interpr√©tation |
|----------|---------------|
| 0 | Parfait (probabilit√©s 0 ou 1, toujours correctes) |
| < 0.3 | Tr√®s bon |
| 0.3 - 0.5 | Correct |
| 0.5 - 1.0 | M√©diocre |
| > 1.0 | Mauvais |
| 0.693 | Al√©atoire (√©quivalent √† tirer √† pile ou face) |

### 6.2 Log-loss vs Accuracy

```python
from sklearn.metrics import log_loss
import numpy as np

# Deux mod√®les avec la m√™me accuracy mais des probabilit√©s diff√©rentes
y_true = np.array([1, 1, 0, 0, 1])

# Mod√®le A : probabilit√©s confiantes et correctes
y_proba_A = np.array([0.95, 0.90, 0.10, 0.05, 0.85])

# Mod√®le B : probabilit√©s proches de 0.5 (peu confiant)
y_proba_B = np.array([0.55, 0.60, 0.45, 0.40, 0.55])

# Les deux ont la m√™me accuracy
y_pred_A = (y_proba_A >= 0.5).astype(int)
y_pred_B = (y_proba_B >= 0.5).astype(int)

print(f"Accuracy A : {(y_pred_A == y_true).mean():.2f}")
print(f"Accuracy B : {(y_pred_B == y_true).mean():.2f}")

print(f"Log-loss A : {log_loss(y_true, y_proba_A):.4f}")  # Bas = bon
print(f"Log-loss B : {log_loss(y_true, y_proba_B):.4f}")  # Plus √©lev√© = moins bon
```

> ‚ö†Ô∏è **Attention** : "Le log-loss **p√©nalise s√©v√®rement** les pr√©dictions confiantes mais fausses. Un mod√®le qui pr√©dit 0.99 pour un cas qui est en r√©alit√© 0 sera lourdement p√©nalis√©."

---

## 7. üìè M√©triques de r√©gression (rappel)

Pour les probl√®mes de r√©gression, les m√©triques principales sont :

| M√©trique | Formule | Interpr√©tation | Sensible aux outliers ? |
|----------|---------|---------------|------------------------|
| **MSE** | Œ£(y - ≈∑)¬≤ / n | Erreur moyenne au carr√© | Oui (fortement) |
| **RMSE** | ‚àöMSE | MSE dans l'unit√© originale | Oui |
| **MAE** | Œ£\|y - ≈∑\| / n | Erreur moyenne absolue | Non (robuste) |
| **R¬≤** | 1 - SS_res/SS_tot | % de variance expliqu√©e | Mod√©r√©ment |
| **MAPE** | Œ£\|y - ≈∑\|/\|y\| / n * 100 | Erreur en pourcentage | Non |

```python
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error,
    r2_score, mean_absolute_percentage_error
)
import numpy as np

y_true_reg = np.array([100, 150, 200, 250, 300])
y_pred_reg = np.array([110, 140, 210, 260, 280])

print(f"MSE  : {mean_squared_error(y_true_reg, y_pred_reg):.2f}")
print(f"RMSE : {np.sqrt(mean_squared_error(y_true_reg, y_pred_reg)):.2f}")
print(f"MAE  : {mean_absolute_error(y_true_reg, y_pred_reg):.2f}")
print(f"R¬≤   : {r2_score(y_true_reg, y_pred_reg):.4f}")
print(f"MAPE : {mean_absolute_percentage_error(y_true_reg, y_pred_reg):.2%}")
```

---

## 8. üß™ Exercice : Choisir la m√©trique selon le contexte m√©tier

Pour chaque sc√©nario, identifiez **la m√©trique principale** √† optimiser et **justifiez** votre choix.

### Sc√©nario 1 : D√©tection de cancer

Un h√¥pital d√©ploie un mod√®le de d√©pistage du cancer du sein sur des mammographies.

> **R√©ponse :** **Recall** ‚Äî Il vaut mieux envoyer des patientes saines faire des examens compl√©mentaires (FP) que rater un cancer (FN). Un FN = un cancer non d√©tect√© = danger vital.

### Sc√©nario 2 : Filtre anti-spam d'email

Gmail veut filtrer les emails spam pour les d√©placer dans le dossier spam.

> **R√©ponse :** **Precision** ‚Äî Un FP = un vrai mail important qui finit en spam = client furieux qui rate un rendez-vous, une facture, une offre d'emploi. Un FN = un spam dans la bo√Æte de r√©ception = d√©sagr√©able mais pas grave.

### Sc√©nario 3 : D√©tection de fraude bancaire

Une banque veut d√©tecter les transactions frauduleuses en temps r√©el (0.5% de fraudes).

> **R√©ponse :** **F1-Score** (avec accent sur le Recall) et courbe **Precision-Recall**. Les fraudes sont rares (classe d√©s√©quilibr√©e), le Recall est crucial (rater une fraude co√ªte cher), mais trop de faux positifs bloquent les clients (Precision aussi importante).

### Sc√©nario 4 : Reconnaissance de chiffres manuscrits (MNIST)

Reconna√Ætre les chiffres de 0 √† 9 √©crits √† la main.

> **R√©ponse :** **Accuracy** ‚Äî Les classes sont relativement √©quilibr√©es (10 classes, ~10% chacune), et chaque erreur a le m√™me co√ªt. L'accuracy est pertinente ici.

### Sc√©nario 5 : Pr√©diction de prix immobiliers

Un site immobilier pr√©dit le prix de vente d'un bien.

> **R√©ponse :** **MAPE** ou **MAE** ‚Äî C'est un probl√®me de r√©gression. La MAPE permet de comprendre l'erreur en pourcentage ("on se trompe de 8% en moyenne"). La MAE est robuste aux outliers (villas √† plusieurs millions).

---

## 9. üéöÔ∏è Seuil de d√©cision : ajuster pour optimiser

### 9.1 Le principe

Par d√©faut, sklearn utilise un seuil de **0.5** : si P(positif) >= 0.5, on pr√©dit la classe positive. Mais ce seuil est **arbitraire**.

```
Seuil ‚Üë (ex: 0.8)
  ‚Üí On est plus exigeant pour dire "positif"
  ‚Üí Precision ‚Üë (moins de faux positifs)
  ‚Üí Recall ‚Üì (on rate plus de vrais positifs)

Seuil ‚Üì (ex: 0.3)
  ‚Üí On est plus permissif
  ‚Üí Precision ‚Üì (plus de faux positifs)
  ‚Üí Recall ‚Üë (on d√©tecte plus)
```

### 9.2 Trouver le seuil optimal

```python
from sklearn.metrics import precision_recall_curve, f1_score, roc_curve
import numpy as np
import matplotlib.pyplot as plt

# Probabilit√©s
y_proba = model.predict_proba(X_test_s)[:, 1]

# --- M√©thode 1 : Maximiser le F1-Score ---
precisions, recalls, seuils = precision_recall_curve(y_test, y_proba)
f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)
seuil_optimal_f1 = seuils[np.argmax(f1_scores)]
print(f"Seuil optimal (max F1) : {seuil_optimal_f1:.3f}")

# --- M√©thode 2 : Youden's Index (max TPR - FPR) ---
fpr, tpr, seuils_roc = roc_curve(y_test, y_proba)
youden = tpr - fpr
seuil_optimal_youden = seuils_roc[np.argmax(youden)]
print(f"Seuil optimal (Youden) : {seuil_optimal_youden:.3f}")

# --- Visualiser ---
seuils_test = np.arange(0.05, 0.96, 0.01)
prec_list, rec_list, f1_list = [], [], []

for s in seuils_test:
    y_pred_s = (y_proba >= s).astype(int)
    prec_list.append(precision_score(y_test, y_pred_s, zero_division=0))
    rec_list.append(recall_score(y_test, y_pred_s))
    f1_list.append(f1_score(y_test, y_pred_s))

plt.figure(figsize=(10, 6))
plt.plot(seuils_test, prec_list, 'b-', label='Precision')
plt.plot(seuils_test, rec_list, 'r-', label='Recall')
plt.plot(seuils_test, f1_list, 'g--', linewidth=2, label='F1-Score')
plt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5, label='Seuil par d√©faut (0.5)')
plt.axvline(x=seuil_optimal_f1, color='green', linestyle=':', label=f'Seuil optimal = {seuil_optimal_f1:.2f}')
plt.xlabel('Seuil de d√©cision')
plt.ylabel('Score')
plt.title('Precision, Recall et F1 en fonction du seuil')
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# --- Appliquer ---
y_pred_default = (y_proba >= 0.5).astype(int)
y_pred_optimal = (y_proba >= seuil_optimal_f1).astype(int)

print(f"\nSeuil 0.5     ‚Üí F1 = {f1_score(y_test, y_pred_default):.4f}")
print(f"Seuil optimal ‚Üí F1 = {f1_score(y_test, y_pred_optimal):.4f}")
```

> üí° **Conseil** : "En contexte m√©dical, baissez le seuil (ex: 0.3) pour maximiser le recall. En contexte spam, montez le seuil (ex: 0.7) pour maximiser la precision. Le seuil de 0.5 n'est pas une v√©rit√© absolue."

---

## üéØ Points cl√©s √† retenir

1. **L'accuracy est trompeuse** sur des classes d√©s√©quilibr√©es ‚Äî un mod√®le trivial peut atteindre 99%
2. **La matrice de confusion** est le point de d√©part de toutes les m√©triques : TP, TN, FP, FN
3. **Precision** = fiabilit√© des alertes positives (priorit√© quand FP co√ªteux)
4. **Recall** = capacit√© √† d√©tecter tous les cas positifs (priorit√© quand FN co√ªteux)
5. **F1-Score** = moyenne harmonique qui p√©nalise les d√©s√©quilibres entre Precision et Recall
6. **AUC-ROC** donne une vue d'ensemble du pouvoir discriminant du mod√®le
7. **Courbe Precision-Recall** est pr√©f√©rable √† ROC quand les classes sont tr√®s d√©s√©quilibr√©es
8. **Log-loss** mesure la qualit√© des probabilit√©s, pas seulement des classes pr√©dites
9. **Le seuil de 0.5 est arbitraire** ‚Äî adaptez-le au contexte m√©tier
10. **Choisissez la m√©trique AVANT de mod√©liser** en fonction du co√ªt des erreurs

---

## ‚úÖ Checklist de validation

- [ ] Je comprends pourquoi l'accuracy est trompeuse sur des classes d√©s√©quilibr√©es
- [ ] Je sais lire et interpr√©ter une matrice de confusion (TP, TN, FP, FN)
- [ ] Je connais la diff√©rence entre Precision, Recall et F1-Score
- [ ] Je sais calculer toutes ces m√©triques avec sklearn
- [ ] Je sais tracer et interpr√©ter une courbe ROC et calculer l'AUC
- [ ] Je sais tracer une courbe Precision-Recall et calculer l'Average Precision
- [ ] Je comprends le log-loss et pourquoi les probabilit√©s calibr√©es comptent
- [ ] Je sais choisir la bonne m√©trique selon le contexte m√©tier
- [ ] Je sais ajuster le seuil de d√©cision pour optimiser precision ou recall
- [ ] Je connais les m√©triques de r√©gression : MSE, RMSE, MAE, R¬≤, MAPE

---

**Pr√©c√©dent** : [Chapitre 11 : R√©duction de dimensionnalit√©](11-reduction-dimensionnalite.md)

**Suivant** : [Chapitre 13 : Validation et G√©n√©ralisation](13-validation-generalisation.md)
