# Chapitre 5 : ProbabilitÃ©s pour ne plus avoir Peur de l'Incertitude

## ğŸ¯ Objectifs

- Comprendre ce qu'est une probabilitÃ© et ce qu'elle n'est PAS
- ConnaÃ®tre les distributions essentielles (normale, uniforme, binomiale, Poisson)
- MaÃ®triser la distribution normale et la rÃ¨gle 68-95-99.7
- Comprendre les intervalles de confiance et la notion de "confiance" dans les prÃ©dictions
- DÃ©couvrir la rÃ©gression logistique comme classification probabiliste
- Comprendre le thÃ©orÃ¨me de Bayes avec des exemples concrets
- Faire le lien entre probabilitÃ©s et Machine Learning

---

## 1. ğŸ§  ProbabilitÃ© â‰  prÃ©diction parfaite

### 1.1 Ce que "70% de chances" veut dire

Quand la mÃ©tÃ©o annonce **"il y a 70% de chances qu'il pleuve demain"**, cela ne signifie PAS :
- âŒ "Il va pleuvoir Ã  70% d'intensitÃ©"
- âŒ "Il va pleuvoir pendant 70% de la journÃ©e"
- âŒ "On est sÃ»r Ã  70% qu'il va pleuvoir"

Cela signifie :
- âœ… "Sur 100 jours avec des conditions mÃ©tÃ©o identiques, il pleuvrait environ 70 fois"

```
100 jours similaires :
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 10 jours de pluie
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 20
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 30
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 40
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 50
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 60
ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§ğŸŒ§  â†’ 70 jours de pluie (70%)
â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸  â†’ 80
â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸  â†’ 90
â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸â˜€ï¸  â†’ 100 (30% sans pluie)
```

> ğŸ’¡ **Conseil** : "Une probabilitÃ© est une **frÃ©quence Ã  long terme**. Dire 'P = 0.7' signifie 'si on rÃ©pÃ©tait l'expÃ©rience un grand nombre de fois, l'Ã©vÃ©nement se produirait environ 70% du temps'."

### 1.2 PropriÃ©tÃ©s fondamentales

| PropriÃ©tÃ© | Formule | Explication |
|-----------|---------|-------------|
| Toujours entre 0 et 1 | 0 â‰¤ P(A) â‰¤ 1 | 0 = impossible, 1 = certain |
| Somme = 1 | P(A) + P(non A) = 1 | Il pleut OU il ne pleut pas |
| Ã‰vÃ©nements indÃ©pendants | P(A et B) = P(A) Ã— P(B) | Si A n'influence pas B |

```python
import numpy as np

# Simulation : lancer un dÃ© 10000 fois
np.random.seed(42)
lancers = np.random.randint(1, 7, size=10000)

# ProbabilitÃ© d'obtenir un 6
p_six = np.mean(lancers == 6)
print(f"P(6) thÃ©orique : {1/6:.4f}")
print(f"P(6) simulÃ©e   : {p_six:.4f}")

# Plus on lance, plus on se rapproche de la thÃ©orie
for n in [10, 100, 1000, 10000]:
    p = np.mean(lancers[:n] == 6)
    print(f"  n={n:>5} â†’ P(6) = {p:.4f}")
```

### 1.3 FrÃ©quentiste vs bayÃ©sien (intuitivement)

| Approche | InterprÃ©tation de P = 0.7 | Analogie |
|----------|--------------------------|----------|
| **FrÃ©quentiste** | "Sur 100 rÃ©pÃ©titions, l'Ã©vÃ©nement arrive ~70 fois" | Lancer un dÃ© 1000 fois |
| **BayÃ©sien** | "Mon degrÃ© de croyance est de 70%" | "Je suis confiant Ã  70% que c'est vrai" |

```
FrÃ©quentiste :
  "La piÃ¨ce est tombÃ©e sur face 70 fois sur 100.
   Donc P(face) â‰ˆ 0.7"
  â†’ BasÃ© sur l'observation rÃ©pÃ©tÃ©e

BayÃ©sien :
  "Je crois initialement que P(face) = 0.5 (piÃ¨ce Ã©quilibrÃ©e).
   AprÃ¨s avoir vu 70 faces sur 100, je mets Ã  jour ma croyance :
   P(face) â‰ˆ 0.7"
  â†’ BasÃ© sur des croyances mises Ã  jour par les donnÃ©es
```

> ğŸ’¡ **Conseil** : "En Machine Learning, on utilise souvent les deux approches. La rÃ©gression logistique est frÃ©quentiste, tandis que les modÃ¨les bayÃ©siens mettent Ã  jour leurs 'croyances' au fur et Ã  mesure des donnÃ©es."

---

## 2. ğŸ“Š Distribution normale (la fameuse courbe en cloche)

### 2.1 Pourquoi elle est partout

La distribution normale (ou gaussienne) est la distribution la plus importante en statistique. Elle apparaÃ®t **naturellement** dans de nombreux phÃ©nomÃ¨nes :

- Taille des personnes dans une population
- Scores Ã  un examen
- Erreurs de mesure
- Temps de trajet quotidien
- Poids des bÃ©bÃ©s Ã  la naissance

**Pourquoi ?** Ã€ cause du **thÃ©orÃ¨me central limite** (TCL) :

> Quand on fait la **moyenne** de beaucoup de variables alÃ©atoires indÃ©pendantes, le rÃ©sultat suit toujours une distribution normale, **quelle que soit** la distribution d'origine.

```
ThÃ©orÃ¨me Central Limite (intuition) :

  Distribution originale       Moyenne de 2 tirages      Moyenne de 30 tirages
  (peut Ãªtre n'importe quoi)

      â–„â–„â–„â–„                         â–„â–„                          â–„â–„
      â–ˆâ–ˆâ–ˆâ–ˆ                       â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„                      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„
      â–ˆâ–ˆâ–ˆâ–ˆâ–„                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  â–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„

  â†’ Forme bizarre            â†’ Commence Ã  ressembler   â†’ Presque parfaitement
                                Ã  une cloche                une courbe en cloche !
```

### 2.2 Moyenne et Ã©cart-type : les deux paramÃ¨tres

Une distribution normale est entiÃ¨rement dÃ©finie par **deux nombres** :
- **Î¼ (mu)** : la **moyenne** (centre de la cloche)
- **Ïƒ (sigma)** : l'**Ã©cart-type** (largeur de la cloche)

```
Ïƒ petit (donnÃ©es serrÃ©es) :          Ïƒ grand (donnÃ©es Ã©talÃ©es) :

         â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„                            â–„â–„â–„â–„â–„â–„
       â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„                       â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„
     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„                   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„              â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          Î¼                                     Î¼

â†’ PrÃ©dictions trÃ¨s prÃ©cises        â†’ PrÃ©dictions plus incertaines
```

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# GÃ©nÃ©rer des donnÃ©es normales
np.random.seed(42)

# Distribution de la taille (en cm) de 1000 personnes
tailles = np.random.normal(loc=170, scale=10, size=1000)

print(f"Moyenne (Î¼) : {np.mean(tailles):.1f} cm")
print(f"Ã‰cart-type (Ïƒ) : {np.std(tailles):.1f} cm")
print(f"Min : {np.min(tailles):.1f} cm")
print(f"Max : {np.max(tailles):.1f} cm")

# Visualiser
plt.figure(figsize=(12, 6))
plt.hist(tailles, bins=40, density=True, alpha=0.7, color='steelblue',
         edgecolor='black', label='DonnÃ©es simulÃ©es')

# Superposer la courbe thÃ©orique
x = np.linspace(130, 210, 200)
plt.plot(x, stats.norm.pdf(x, 170, 10), 'r-', linewidth=2,
         label='Distribution normale thÃ©orique')

plt.xlabel("Taille (cm)")
plt.ylabel("DensitÃ© de probabilitÃ©")
plt.title("Distribution de la taille â€” Courbe en cloche")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.3 La rÃ¨gle 68-95-99.7

C'est la rÃ¨gle la plus utile des statistiques. Elle vous dit quelle proportion des donnÃ©es se trouve Ã  1, 2 ou 3 Ã©carts-types de la moyenne.

```
                        99.7% (Î¼ Â± 3Ïƒ)
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     95% (Î¼ Â± 2Ïƒ)                 â”‚
                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                 â”‚  â”‚   68% (Î¼ Â± 1Ïƒ)             â”‚  â”‚
                 â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
                 â”‚  â”‚ â”‚                    â”‚     â”‚  â”‚
                 â”‚  â”‚ â”‚     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„         â”‚     â”‚  â”‚
                 â”‚  â”‚ â”‚   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„       â”‚     â”‚  â”‚
                 â”‚  â”‚ â”‚ â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â”‚     â”‚  â”‚
                 â”‚  â”‚ â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„   â”‚     â”‚  â”‚
              â–„â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             Î¼-3Ïƒ  Î¼-2Ïƒ  Î¼-Ïƒ   Î¼   Î¼+Ïƒ  Î¼+2Ïƒ  Î¼+3Ïƒ
```

| Intervalle | % des donnÃ©es | Exemple (taille : Î¼=170, Ïƒ=10) |
|:----------:|:-------------:|-------------------------------|
| Î¼ Â± 1Ïƒ | **68%** | Entre 160 et 180 cm |
| Î¼ Â± 2Ïƒ | **95%** | Entre 150 et 190 cm |
| Î¼ Â± 3Ïƒ | **99.7%** | Entre 140 et 200 cm |

```python
# VÃ©rifier la rÃ¨gle 68-95-99.7
mu, sigma = 170, 10

dans_1_sigma = np.mean((tailles >= mu - sigma) & (tailles <= mu + sigma))
dans_2_sigma = np.mean((tailles >= mu - 2*sigma) & (tailles <= mu + 2*sigma))
dans_3_sigma = np.mean((tailles >= mu - 3*sigma) & (tailles <= mu + 3*sigma))

print(f"Dans Î¼ Â± 1Ïƒ : {dans_1_sigma:.1%} (thÃ©orique : 68.3%)")
print(f"Dans Î¼ Â± 2Ïƒ : {dans_2_sigma:.1%} (thÃ©orique : 95.4%)")
print(f"Dans Î¼ Â± 3Ïƒ : {dans_3_sigma:.1%} (thÃ©orique : 99.7%)")
```

> ğŸ’¡ **Conseil** : "La rÃ¨gle 68-95-99.7 est **extrÃªmement utile** pour dÃ©tecter les anomalies. Si une valeur est Ã  plus de 3 Ã©carts-types de la moyenne, elle est probablement un outlier (seulement 0.3% de chances d'Ãªtre 'normale')."

### 2.4 Application : dÃ©tecter des anomalies

```python
# Transactions bancaires (montant en â‚¬)
np.random.seed(42)
transactions = np.random.normal(loc=50, scale=15, size=1000)

# Ajouter quelques transactions frauduleuses
transactions = np.append(transactions, [250, 300, -50, 280])

mu = np.mean(transactions)
sigma = np.std(transactions)

# DÃ©tecter les anomalies (> 3 Ã©carts-types)
anomalies = np.abs(transactions - mu) > 3 * sigma
print(f"Transactions normales : {np.sum(~anomalies)}")
print(f"Anomalies dÃ©tectÃ©es : {np.sum(anomalies)}")
print(f"Valeurs anormales : {transactions[anomalies]}")

# Visualiser
plt.figure(figsize=(12, 5))
plt.scatter(range(len(transactions)), transactions, c=['red' if a else 'steelblue' for a in anomalies],
            alpha=0.5, s=10)
plt.axhline(y=mu + 3*sigma, color='red', linestyle='--', label=f'Î¼ + 3Ïƒ = {mu + 3*sigma:.0f}â‚¬')
plt.axhline(y=mu - 3*sigma, color='red', linestyle='--', label=f'Î¼ - 3Ïƒ = {mu - 3*sigma:.0f}â‚¬')
plt.axhline(y=mu, color='green', linestyle='-', alpha=0.5, label=f'Î¼ = {mu:.0f}â‚¬')
plt.xlabel("Transaction #")
plt.ylabel("Montant (â‚¬)")
plt.title("DÃ©tection d'anomalies avec la rÃ¨gle des 3 Ã©carts-types")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3. ğŸ“ˆ Autres distributions utiles

### 3.1 Distribution uniforme

Tous les rÃ©sultats ont la **mÃªme probabilitÃ©**.

```
Distribution uniforme (dÃ© Ã  6 faces) :

 P(x)
 1/6 â”‚  â–„â–„  â–„â–„  â–„â–„  â–„â–„  â–„â–„  â–„â–„
     â”‚  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
     â”‚  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆ
     â””â”€â”€1â”€â”€â”€2â”€â”€â”€3â”€â”€â”€4â”€â”€â”€5â”€â”€â”€6â”€â”€ x
```

```python
# Distribution uniforme
uniform = np.random.uniform(low=0, high=10, size=10000)

plt.figure(figsize=(10, 4))
plt.hist(uniform, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')
plt.title("Distribution uniforme [0, 10]")
plt.xlabel("Valeur")
plt.ylabel("DensitÃ©")
plt.grid(True, alpha=0.3)
plt.show()
```

| CaractÃ©ristique | Valeur |
|----------------|--------|
| **Quand l'utiliser** | Quand rien ne favorise un rÃ©sultat |
| **Exemples** | DÃ©, gÃ©nÃ©rateur alÃ©atoire, initialisation de poids |
| **ParamÃ¨tres** | a (min), b (max) |

### 3.2 Distribution binomiale

Nombre de **succÃ¨s** sur N essais indÃ©pendants, avec probabilitÃ© p de succÃ¨s Ã  chaque essai.

```
Distribution binomiale (n=10, p=0.5) :
"Sur 10 lancers de piÃ¨ce, combien de faces ?"

 P(x)
  0.25â”‚        â–„â–„
      â”‚      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.20â”‚    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
      â”‚  â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.10â”‚â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.00â””â”€0â”€â”€1â”€â”€2â”€â”€3â”€â”€4â”€â”€5â”€â”€6â”€â”€7â”€â”€8â”€â”€9â”€â”€10â”€ x (nb de faces)
```

```python
from scipy import stats

# Exemple : sur 100 emails, 30% sont du spam
# Combien de spams dans un lot de 20 emails ?
n, p = 20, 0.3

x = np.arange(0, 21)
probas = stats.binom.pmf(x, n, p)

plt.figure(figsize=(10, 5))
plt.bar(x, probas, color='steelblue', edgecolor='black', alpha=0.7)
plt.xlabel("Nombre de spams sur 20 emails")
plt.ylabel("ProbabilitÃ©")
plt.title(f"Distribution binomiale (n={n}, p={p})")
plt.grid(True, alpha=0.3, axis='y')
plt.show()

# Statistiques
print(f"EspÃ©rance : {n * p:.1f} spams")
print(f"P(exactement 6 spams) : {stats.binom.pmf(6, n, p):.4f}")
print(f"P(au moins 10 spams) : {1 - stats.binom.cdf(9, n, p):.4f}")
```

| CaractÃ©ristique | Valeur |
|----------------|--------|
| **Quand l'utiliser** | Compter des succÃ¨s/Ã©checs |
| **Exemples** | Nb de clics sur une pub, nb de clients qui rÃ©sitient, nb de piÃ¨ces dÃ©fectueuses |
| **ParamÃ¨tres** | n (nb essais), p (probabilitÃ© de succÃ¨s) |

### 3.3 Distribution de Poisson

Nombre d'Ã©vÃ©nements qui se produisent dans un **intervalle fixe** (temps, espace).

```
Distribution de Poisson (Î»=4) :
"Nombre de clients qui arrivent par heure"

 P(x)
  0.20â”‚     â–„â–„
      â”‚   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.15â”‚ â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.10â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  0.05â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„
      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„
  0.00â””â”€0â”€â”€1â”€â”€2â”€â”€3â”€â”€4â”€â”€5â”€â”€6â”€â”€7â”€â”€8â”€â”€9â”€â”€10â”€ x
```

```python
# Exemple : 4 clients arrivent en moyenne par heure
lambda_param = 4

x = np.arange(0, 15)
probas = stats.poisson.pmf(x, lambda_param)

plt.figure(figsize=(10, 5))
plt.bar(x, probas, color='steelblue', edgecolor='black', alpha=0.7)
plt.xlabel("Nombre de clients par heure")
plt.ylabel("ProbabilitÃ©")
plt.title(f"Distribution de Poisson (Î»={lambda_param})")
plt.grid(True, alpha=0.3, axis='y')
plt.show()

print(f"P(0 clients) : {stats.poisson.pmf(0, lambda_param):.4f}")
print(f"P(exactement 4) : {stats.poisson.pmf(4, lambda_param):.4f}")
print(f"P(plus de 8) : {1 - stats.poisson.cdf(8, lambda_param):.4f}")
```

| CaractÃ©ristique | Valeur |
|----------------|--------|
| **Quand l'utiliser** | Ã‰vÃ©nements rares dans un intervalle fixe |
| **Exemples** | Nb d'appels au support/heure, nb d'erreurs/page, nb de pannes/mois |
| **ParamÃ¨tres** | Î» (taux moyen d'occurrence) |

### 3.4 Tableau rÃ©capitulatif

| Distribution | Type | ParamÃ¨tres | Exemple ML | Forme |
|-------------|------|-----------|------------|-------|
| **Normale** | Continue | Î¼, Ïƒ | Erreurs de prÃ©diction, features | Cloche |
| **Uniforme** | Continue | a, b | Initialisation alÃ©atoire | Rectangle |
| **Binomiale** | DiscrÃ¨te | n, p | Nb de conversions sur n visiteurs | Cloche discrÃ¨te |
| **Poisson** | DiscrÃ¨te | Î» | Nb d'Ã©vÃ©nements par unitÃ© de temps | AsymÃ©trique |

---

## 4. ğŸ¯ Intervalles de confiance : quantifier l'incertitude

### 4.1 Pourquoi parler de "confiance" ?

En ML, un modÃ¨le ne donne jamais une rÃ©ponse **certaine**. Il donne une estimation avec une **marge d'erreur**.

```
PrÃ©diction sans confiance :          PrÃ©diction avec confiance :
  "Le prix est de 250 000â‚¬"           "Le prix est de 250 000â‚¬ Â± 30 000â‚¬
                                        (intervalle de confiance Ã  95%)"

  â†’ Aucune idÃ©e de la fiabilitÃ©       â†’ On sait que le vrai prix est
                                         probablement entre 220k et 280kâ‚¬
```

### 4.2 Intervalle de confiance visualisÃ©

```
                    Intervalle de confiance Ã  95%
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                              â”‚
                 â”‚          â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„              â”‚
                 â”‚        â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„            â”‚
                 â”‚      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„          â”‚
               â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               220k     250k               280k
                          â†‘
                    Estimation ponctuelle

"On est sÃ»r Ã  95% que le vrai prix est entre 220k et 280kâ‚¬"
```

```python
from scipy import stats

# Exemple : estimer le prix moyen Ã  partir d'un Ã©chantillon
np.random.seed(42)
prix_echantillon = np.random.normal(loc=250000, scale=40000, size=50)

# Calculer l'intervalle de confiance Ã  95%
moyenne = np.mean(prix_echantillon)
erreur_standard = stats.sem(prix_echantillon)  # Standard Error of the Mean
ic_95 = stats.t.interval(
    confidence=0.95,
    df=len(prix_echantillon) - 1,
    loc=moyenne,
    scale=erreur_standard
)

print(f"Moyenne de l'Ã©chantillon : {moyenne:,.0f}â‚¬")
print(f"Intervalle de confiance 95% : [{ic_95[0]:,.0f}â‚¬ ; {ic_95[1]:,.0f}â‚¬]")
print(f"Marge d'erreur : Â±{(ic_95[1] - moyenne):,.0f}â‚¬")

# Visualiser
plt.figure(figsize=(10, 4))
plt.hist(prix_echantillon, bins=20, density=True, alpha=0.7,
         color='steelblue', edgecolor='black')
plt.axvline(x=moyenne, color='red', linewidth=2, label=f'Moyenne = {moyenne:,.0f}â‚¬')
plt.axvline(x=ic_95[0], color='orange', linewidth=2, linestyle='--',
            label=f'IC 95% = [{ic_95[0]:,.0f} ; {ic_95[1]:,.0f}]â‚¬')
plt.axvline(x=ic_95[1], color='orange', linewidth=2, linestyle='--')
plt.xlabel("Prix (â‚¬)")
plt.ylabel("DensitÃ©")
plt.title("Estimation du prix moyen avec intervalle de confiance")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 4.3 Plus de donnÃ©es = plus de prÃ©cision

```python
# Impact de la taille de l'Ã©chantillon sur l'intervalle de confiance
tailles_echantillon = [10, 30, 50, 100, 500, 1000]

print(f"{'Taille':>8} {'Moyenne':>12} {'IC 95% inf':>12} {'IC 95% sup':>12} {'Largeur':>10}")
print("-" * 60)

for n in tailles_echantillon:
    ech = np.random.normal(loc=250000, scale=40000, size=n)
    moy = np.mean(ech)
    se = stats.sem(ech)
    ic = stats.t.interval(0.95, df=n-1, loc=moy, scale=se)
    largeur = ic[1] - ic[0]
    print(f"{n:>8} {moy:>12,.0f} {ic[0]:>12,.0f} {ic[1]:>12,.0f} {largeur:>10,.0f}")
```

> ğŸ’¡ **Conseil** : "Plus vous avez de donnÃ©es, plus votre intervalle de confiance est **Ã©troit** (prÃ©cis). C'est une raison fondamentale pour laquelle le ML a besoin de **beaucoup de donnÃ©es**."

---

## 5. ğŸ¤– Application : Classification probabiliste

### 5.1 PrÃ©dire non pas "oui/non" mais "probabilitÃ© de oui"

La plupart des algorithmes de classification ne prÃ©disent pas simplement une classe â€” ils prÃ©disent une **probabilitÃ©**.

```
PrÃ©diction binaire :                 PrÃ©diction probabiliste :
  "Ce client va rÃ©silier : OUI"        "Ce client a 82% de chances de rÃ©silier"

  â†’ Pas de nuance                     â†’ On peut agir en fonction du risque :
                                        - 82% â†’ Appeler en urgence !
                                        - 55% â†’ Envoyer une promotion
                                        - 20% â†’ Ne rien faire
```

> ğŸ’¡ **Conseil** : "Les probabilitÃ©s permettent de **prioriser les actions**. Un client Ã  90% de chances de rÃ©silier ne nÃ©cessite pas la mÃªme intervention qu'un client Ã  30%. C'est beaucoup plus utile qu'un simple oui/non."

### 5.2 Introduction Ã  la rÃ©gression logistique

MalgrÃ© son nom, la rÃ©gression logistique est un algorithme de **classification** (pas de rÃ©gression). Elle prÃ©dit la **probabilitÃ©** qu'un point appartienne Ã  une classe.

**Le problÃ¨me** : la rÃ©gression linÃ©aire peut donner des valeurs < 0 ou > 1, ce qui n'est pas une probabilitÃ© valide.

```
RÃ©gression linÃ©aire (mauvais pour la classification) :

  P(spam)
   1.5â”‚           /
      â”‚         /
   1.0â”‚â”€â”€â”€â”€â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€ â† dÃ©passe 1 ! (pas une proba valide)
      â”‚     /
   0.5â”‚   /
      â”‚ /
   0.0â”‚/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â† en dessous de 0 !
  -0.5â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ nb mots suspects
```

**La solution** : utiliser la **fonction sigmoÃ¯de** pour "compresser" la sortie entre 0 et 1.

### 5.3 La fonction sigmoÃ¯de

```
Ïƒ(z) = 1 / (1 + eâ»á¶»)

    P(spam)
    1.0â”‚                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       â”‚                 â•±
    0.8â”‚               â•±
       â”‚             â•±
    0.5â”‚â”€ â”€ â”€ â”€ â”€ â— â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€  â† seuil de dÃ©cision
       â”‚         â•±
    0.2â”‚       â•±
       â”‚     â•±
    0.0â”‚â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ z = ax + b
                     0

    z < 0 â†’ Ïƒ(z) < 0.5 â†’ Classe 0 (pas spam)
    z > 0 â†’ Ïƒ(z) > 0.5 â†’ Classe 1 (spam)
    z = 0 â†’ Ïƒ(z) = 0.5 â†’ Incertain (pile entre les deux)
```

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    """La fonction sigmoÃ¯de."""
    return 1 / (1 + np.exp(-z))

# Visualiser la sigmoÃ¯de
z = np.linspace(-8, 8, 200)

plt.figure(figsize=(10, 6))
plt.plot(z, sigmoid(z), 'b-', linewidth=3)
plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Seuil = 0.5')
plt.axhline(y=0, color='grey', linewidth=0.5)
plt.axhline(y=1, color='grey', linewidth=0.5)
plt.axvline(x=0, color='grey', linewidth=0.5)
plt.xlabel("z (score linÃ©aire)")
plt.ylabel("Ïƒ(z) = probabilitÃ©")
plt.title("La fonction sigmoÃ¯de transforme tout nombre en probabilitÃ© [0, 1]")
plt.legend()
plt.grid(True, alpha=0.3)

# Annotations
plt.annotate('P â‰ˆ 0\n(TrÃ¨s improbable)', xy=(-6, 0.05), fontsize=10, color='blue')
plt.annotate('P â‰ˆ 1\n(TrÃ¨s probable)', xy=(4, 0.9), fontsize=10, color='blue')
plt.annotate('P = 0.5\n(Incertain)', xy=(0.5, 0.55), fontsize=10, color='red')

plt.show()
```

### 5.4 RÃ©gression logistique avec scikit-learn

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# GÃ©nÃ©rer un dataset
X, y = make_classification(
    n_samples=500,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# EntraÃ®ner la rÃ©gression logistique
model = LogisticRegression()
model.fit(X_train, y_train)

# PrÃ©dire les classes
y_pred = model.predict(X_test)

# PrÃ©dire les PROBABILITÃ‰S
y_proba = model.predict_proba(X_test)

print("=== Premiers rÃ©sultats ===")
print(f"{'Classe prÃ©dite':>15} {'P(classe 0)':>12} {'P(classe 1)':>12} {'Vrai label':>12}")
print("-" * 55)
for i in range(10):
    print(f"{y_pred[i]:>15} {y_proba[i, 0]:>12.4f} {y_proba[i, 1]:>12.4f} {y_test[i]:>12}")

print(f"\nAccuracy : {accuracy_score(y_test, y_pred):.2%}")
print(f"\n{classification_report(y_test, y_pred)}")
```

### 5.5 Utiliser les probabilitÃ©s pour prendre des dÃ©cisions

```python
# Exemple mÃ©tier : prÃ©dire le churn
# Action diffÃ©rente selon le niveau de risque

seuils = {
    'Risque faible': (0.0, 0.3),
    'Risque moyen': (0.3, 0.6),
    'Risque Ã©levÃ©': (0.6, 0.8),
    'Risque critique': (0.8, 1.0),
}

actions = {
    'Risque faible': 'Ne rien faire',
    'Risque moyen': 'Envoyer un email promotionnel',
    'Risque Ã©levÃ©': 'Appeler le client',
    'Risque critique': 'Offre exceptionnelle + appel du manager',
}

# ProbabilitÃ©s de churn pour 5 clients
proba_churn = y_proba[:5, 1]

print(f"{'Client':>8} {'P(churn)':>10} {'Niveau':>18} {'Action':>40}")
print("-" * 80)
for i, p in enumerate(proba_churn):
    for niveau, (low, high) in seuils.items():
        if low <= p < high:
            print(f"{'Client ' + str(i+1):>8} {p:>10.2%} {niveau:>18} {actions[niveau]:>40}")
            break
```

> âš ï¸ **Attention** : "Le seuil de 0.5 n'est pas toujours optimal ! En mÃ©decine, on prÃ©fÃ¨re un seuil bas (0.3) pour ne pas rater de malades. En anti-spam, on prÃ©fÃ¨re un seuil haut (0.7) pour ne pas bloquer de vrais emails."

---

## 6. ğŸ“– Le thÃ©orÃ¨me de Bayes

### 6.1 L'intuition avec un exemple concret

**ProblÃ¨me** : Vous avez un filtre anti-spam. Un email contient le mot "gratuit". Quelle est la probabilitÃ© que ce soit du spam ?

Ce qu'on sait :
- 30% des emails sont du spam â†’ P(spam) = 0.30
- 80% des spams contiennent "gratuit" â†’ P(gratuit | spam) = 0.80
- 10% des emails lÃ©gitimes contiennent "gratuit" â†’ P(gratuit | lÃ©gitime) = 0.10

Ce qu'on cherche :
- P(spam | gratuit) = "sachant que l'email contient 'gratuit', quelle proba que ce soit du spam ?"

### 6.2 La formule de Bayes

```
                    P(B | A) Ã— P(A)
P(A | B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      P(B)

En franÃ§ais :
                    P(gratuit | spam) Ã— P(spam)
P(spam | gratuit) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                             P(gratuit)
```

### 6.3 Calcul pas Ã  pas

```
Ã‰tape 1 : Calculer P(gratuit)
  P(gratuit) = P(gratuit | spam) Ã— P(spam) + P(gratuit | lÃ©gitime) Ã— P(lÃ©gitime)
             = 0.80 Ã— 0.30 + 0.10 Ã— 0.70
             = 0.24 + 0.07
             = 0.31

Ã‰tape 2 : Appliquer Bayes
  P(spam | gratuit) = (0.80 Ã— 0.30) / 0.31
                    = 0.24 / 0.31
                    â‰ˆ 0.774

â†’ Il y a 77.4% de chances que ce soit du spam !
```

```python
# ThÃ©orÃ¨me de Bayes : exemple spam
p_spam = 0.30
p_legitime = 0.70
p_gratuit_sachant_spam = 0.80
p_gratuit_sachant_legitime = 0.10

# P(gratuit)
p_gratuit = (p_gratuit_sachant_spam * p_spam +
             p_gratuit_sachant_legitime * p_legitime)

# P(spam | gratuit)
p_spam_sachant_gratuit = (p_gratuit_sachant_spam * p_spam) / p_gratuit

print(f"P(gratuit) = {p_gratuit:.4f}")
print(f"P(spam | contient 'gratuit') = {p_spam_sachant_gratuit:.4f}")
print(f"\nâ†’ Un email contenant 'gratuit' a {p_spam_sachant_gratuit:.1%} de chances d'Ãªtre du spam")
```

### 6.4 Visualisation avec un arbre

```
                    Tous les emails (100%)
                    â•±                    â•²
               Spam (30%)           LÃ©gitime (70%)
              â•±        â•²            â•±           â•²
     "gratuit"    pas "gratuit"  "gratuit"    pas "gratuit"
       (80%)        (20%)         (10%)         (90%)
       = 24%        = 6%          = 7%          = 63%

   Parmi les emails avec "gratuit" (24% + 7% = 31%) :
   â†’ 24% sont du spam â†’ P(spam | gratuit) = 24/31 â‰ˆ 77.4%
```

### 6.5 Bayes avec plusieurs mots

```python
# Extension : Bayes naÃ¯f avec plusieurs mots
# (C'est le principe du Naive Bayes classifier !)

def bayes_spam(mots_observes, p_mots_spam, p_mots_legit, p_spam=0.3):
    """
    Classifieur bayÃ©sien naÃ¯f pour le spam.

    Args:
        mots_observes: liste de mots trouvÃ©s dans l'email
        p_mots_spam: dict {mot: P(mot | spam)}
        p_mots_legit: dict {mot: P(mot | lÃ©gitime)}
        p_spam: probabilitÃ© a priori d'Ãªtre du spam
    """
    p_legit = 1 - p_spam

    # HypothÃ¨se "naÃ¯ve" : les mots sont indÃ©pendants
    # P(mots | spam) = P(mot1 | spam) Ã— P(mot2 | spam) Ã— ...
    p_mots_si_spam = np.prod([p_mots_spam.get(m, 0.01) for m in mots_observes])
    p_mots_si_legit = np.prod([p_mots_legit.get(m, 0.01) for m in mots_observes])

    # Bayes
    p_spam_sachant_mots = (p_mots_si_spam * p_spam) / \
                          (p_mots_si_spam * p_spam + p_mots_si_legit * p_legit)

    return p_spam_sachant_mots

# ProbabilitÃ©s conditionnelles apprises des donnÃ©es
p_mots_spam = {
    'gratuit': 0.80, 'gagner': 0.60, 'urgent': 0.50,
    'cliquez': 0.70, 'offre': 0.55, 'bonjour': 0.30,
}
p_mots_legit = {
    'gratuit': 0.10, 'gagner': 0.05, 'urgent': 0.15,
    'cliquez': 0.08, 'offre': 0.12, 'bonjour': 0.80,
}

# Tester avec diffÃ©rents emails
emails = [
    ['gratuit', 'gagner', 'cliquez'],    # TrÃ¨s spam
    ['bonjour', 'offre'],                 # Ambigu
    ['bonjour'],                          # Probablement lÃ©gitime
    ['urgent', 'gratuit', 'offre', 'gagner'],  # TrÃ¨s trÃ¨s spam
]

for email in emails:
    p = bayes_spam(email, p_mots_spam, p_mots_legit)
    label = "SPAM" if p > 0.5 else "LÃ©gitime"
    print(f"Email contenant {email}")
    print(f"  â†’ P(spam) = {p:.4f} â†’ {label}\n")
```

### 6.6 Naive Bayes avec scikit-learn

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Dataset
X, y = make_classification(n_samples=1000, n_features=10,
                           n_informative=5, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)

# PrÃ©dictions et probabilitÃ©s
y_pred = nb.predict(X_test)
y_proba = nb.predict_proba(X_test)

print(f"Accuracy : {accuracy_score(y_test, y_pred):.2%}")
print(f"\nExemple de probabilitÃ©s :")
for i in range(5):
    print(f"  P(classe 0) = {y_proba[i, 0]:.4f}, P(classe 1) = {y_proba[i, 1]:.4f} â†’ PrÃ©dit {y_pred[i]}")
```

> ğŸ’¡ **Conseil** : "Le classifieur Naive Bayes est **rapide, simple et Ã©tonnamment efficace** pour la classification de texte (spam, sentiment, catÃ©gorisation). C'est souvent un excellent point de dÃ©part (baseline)."

---

## 7. ğŸ”— Lien avec le ML : maximum de vraisemblance (intuition)

### 7.1 L'idÃ©e centrale

Le **maximum de vraisemblance** (Maximum Likelihood Estimation, MLE) est le principe fondamental derriÃ¨re l'entraÃ®nement de nombreux modÃ¨les de ML.

**Question** : "Quels paramÃ¨tres du modÃ¨le rendent les donnÃ©es observÃ©es les **plus probables** ?"

```
On a observÃ© ces donnÃ©es :  [2.1, 1.8, 2.3, 1.9, 2.0]

Quelle distribution normale les a le plus probablement gÃ©nÃ©rÃ©es ?

  HypothÃ¨se 1 : Î¼=0, Ïƒ=1     HypothÃ¨se 2 : Î¼=2, Ïƒ=0.2    HypothÃ¨se 3 : Î¼=5, Ïƒ=1

      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„                       â–„â–ˆâ–ˆâ–„                         â–„â–ˆâ–ˆâ–ˆâ–ˆâ–„
    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„                   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„                     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
  â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„               â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„                 â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„
â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€
  0    â—â—â—â—â—                    2  â—â—â—â—â—                              5
  â†‘ DonnÃ©es loin              â†‘ DonnÃ©es au centre !          â†‘ DonnÃ©es loin
  du centre                    â†’ Vraisemblance HAUTE          du centre
  â†’ Vraisemblance basse                                      â†’ Vraisemblance basse

  âŸ¹ HypothÃ¨se 2 est la meilleure ! (Î¼=2, Ïƒ=0.2)
```

### 7.2 En pratique

```python
from scipy import stats
import numpy as np

# DonnÃ©es observÃ©es
donnees = np.array([2.1, 1.8, 2.3, 1.9, 2.0, 2.2, 1.7, 2.1])

# Estimer les paramÃ¨tres par maximum de vraisemblance
mu_mle = np.mean(donnees)      # La moyenne est l'estimateur MLE de Î¼
sigma_mle = np.std(donnees)    # L'Ã©cart-type est l'estimateur MLE de Ïƒ

print(f"Estimation MLE : Î¼ = {mu_mle:.3f}, Ïƒ = {sigma_mle:.3f}")

# Visualiser
x = np.linspace(0, 4, 200)

plt.figure(figsize=(10, 6))
plt.hist(donnees, bins=8, density=True, alpha=0.7, color='steelblue',
         edgecolor='black', label='DonnÃ©es observÃ©es')
plt.plot(x, stats.norm.pdf(x, mu_mle, sigma_mle), 'r-', linewidth=2,
         label=f'MLE : N({mu_mle:.2f}, {sigma_mle:.2f})')
plt.plot(x, stats.norm.pdf(x, 0, 1), 'g--', linewidth=2, alpha=0.5,
         label='N(0, 1) â€” mauvais fit')
plt.plot(x, stats.norm.pdf(x, 5, 1), 'purple', linewidth=2, alpha=0.5,
         linestyle='--', label='N(5, 1) â€” mauvais fit')

plt.xlabel("Valeur")
plt.ylabel("DensitÃ©")
plt.title("Maximum de vraisemblance : trouver la distribution qui 'explique' le mieux les donnÃ©es")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 7.3 Lien avec les modÃ¨les ML

| ModÃ¨le | Ce que le MLE optimise |
|--------|----------------------|
| **RÃ©gression linÃ©aire** | Minimiser MSE = maximiser la vraisemblance (si erreurs normales) |
| **RÃ©gression logistique** | Maximiser la vraisemblance des classes observÃ©es |
| **Naive Bayes** | Estimer P(feature \| classe) directement par comptage |
| **RÃ©seaux de neurones** | Minimiser la cross-entropy â‰ˆ maximiser la vraisemblance |

> ğŸ’¡ **Conseil** : "Quand vous minimisez la MSE en rÃ©gression, vous faites en rÃ©alitÃ© du maximum de vraisemblance en supposant que les erreurs suivent une distribution normale. C'est pour Ã§a que la normalitÃ© des rÃ©sidus est importante !"

### 7.4 VÃ©rifier la normalitÃ© des rÃ©sidus

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import fetch_california_housing
from scipy import stats

# EntraÃ®ner une rÃ©gression
housing = fetch_california_housing()
X, y = housing.data, housing.target
model = LinearRegression()
model.fit(X, y)

# Calculer les rÃ©sidus
residus = y - model.predict(X)

# Visualiser
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(residus, bins=50, density=True, alpha=0.7,
             color='steelblue', edgecolor='black')
x_norm = np.linspace(residus.min(), residus.max(), 200)
axes[0].plot(x_norm, stats.norm.pdf(x_norm, np.mean(residus), np.std(residus)),
             'r-', linewidth=2)
axes[0].set_title("Distribution des rÃ©sidus")
axes[0].set_xlabel("RÃ©sidu")

stats.probplot(residus, dist="norm", plot=axes[1])
axes[1].set_title("Q-Q Plot (normalitÃ©)")

plt.tight_layout()
plt.show()

# Test de normalitÃ©
stat, p_value = stats.shapiro(residus[:5000])
print(f"Test de Shapiro-Wilk : p-value = {p_value:.6f}")
print(f"â†’ {'RÃ©sidus normaux' if p_value > 0.05 else 'RÃ©sidus NON normaux'}")
```

> âš ï¸ **Attention** : "Si les rÃ©sidus ne sont pas normaux, les intervalles de confiance et les tests statistiques ne sont plus fiables. Dans ce cas, envisagez des transformations (log, Box-Cox) ou des modÃ¨les non-paramÃ©triques."

---

## ğŸ¯ Points clÃ©s Ã  retenir

1. **Une probabilitÃ©** est une frÃ©quence Ã  long terme (0 = impossible, 1 = certain)
2. **La distribution normale** est dÃ©finie par Î¼ (moyenne) et Ïƒ (Ã©cart-type), et apparaÃ®t partout grÃ¢ce au thÃ©orÃ¨me central limite
3. **La rÃ¨gle 68-95-99.7** : 68% des donnÃ©es sont Ã  Â±1Ïƒ, 95% Ã  Â±2Ïƒ, 99.7% Ã  Â±3Ïƒ
4. **L'intervalle de confiance** quantifie l'incertitude : plus de donnÃ©es = intervalle plus Ã©troit
5. **La rÃ©gression logistique** prÃ©dit des probabilitÃ©s (pas juste des classes) grÃ¢ce Ã  la fonction sigmoÃ¯de
6. **Les probabilitÃ©s sont plus utiles** qu'une simple prÃ©diction binaire pour prendre des dÃ©cisions mÃ©tier
7. **Le thÃ©orÃ¨me de Bayes** permet de mettre Ã  jour une croyance Ã  partir d'une nouvelle observation
8. **Le classifieur Naive Bayes** applique Bayes en supposant l'indÃ©pendance des features â€” simple et efficace
9. **Le maximum de vraisemblance** cherche les paramÃ¨tres qui rendent les donnÃ©es observÃ©es les plus probables
10. **Minimiser la MSE** en rÃ©gression = maximiser la vraisemblance sous hypothÃ¨se de normalitÃ© des erreurs

---

## âœ… Checklist de validation

- [ ] Je sais expliquer ce que signifie "70% de chances qu'il pleuve"
- [ ] Je connais la diffÃ©rence intuitive entre frÃ©quentiste et bayÃ©sien
- [ ] Je sais ce qu'est une distribution normale et ses deux paramÃ¨tres (Î¼, Ïƒ)
- [ ] Je connais la rÃ¨gle 68-95-99.7 et je sais l'utiliser pour dÃ©tecter des anomalies
- [ ] Je sais distinguer les distributions normale, uniforme, binomiale et Poisson
- [ ] Je comprends ce qu'est un intervalle de confiance et pourquoi il rÃ©trÃ©cit avec plus de donnÃ©es
- [ ] Je sais utiliser la rÃ©gression logistique avec scikit-learn
- [ ] Je comprends la fonction sigmoÃ¯de et son rÃ´le dans la rÃ©gression logistique
- [ ] Je sais appliquer le thÃ©orÃ¨me de Bayes sur un exemple simple (spam)
- [ ] Je comprends l'intuition du maximum de vraisemblance
- [ ] Je sais utiliser `predict_proba()` pour obtenir des probabilitÃ©s et non juste des classes
- [ ] Je comprends pourquoi les probabilitÃ©s sont plus utiles qu'une prÃ©diction binaire

---

**PrÃ©cÃ©dent** : [Chapitre 4 : Fonctions, Erreurs et l'Art de s'AmÃ©liorer](04-fonctions-erreurs-gradient.md)

**Suivant** : [Chapitre 6 : MÃ©thodes d'Ensemble](06-ensemble-methods.md)
