{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering : Segmentation de produits\n",
    "\n",
    "> Ce notebook est un exemple pratique clef en main pour apprendre les techniques de clustering (apprentissage non supervise) en Machine Learning.\n",
    "> Nous allons segmenter un catalogue de produits en groupes homogenes a partir de leurs caracteristiques (prix, ventes, notes, etc.).\n",
    ">\n",
    "> **Objectifs :**\n",
    "> - Comprendre les algorithmes de clustering (K-Means, DBSCAN)\n",
    "> - Determiner le nombre optimal de clusters (methode du coude, score silhouette)\n",
    "> - Visualiser les clusters en 2D avec une reduction de dimension (PCA)\n",
    "> - Profiler les clusters pour en tirer des insights business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 1 : Imports et chargement des donnees\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration graphique\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Chargement des donnees\n",
    "df = pd.read_csv(\"../data/produits_clustering.csv\")\n",
    "\n",
    "print(f\"Dataset charge avec succes : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 2 : Analyse exploratoire (EDA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMATIONS GENERALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDimensions : {df.shape}\")\n",
    "print(f\"\\nTypes des colonnes :\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALEURS MANQUANTES\")\n",
    "print(\"=\" * 60)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTIQUES DESCRIPTIVES\")\n",
    "print(\"=\" * 60)\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Repartition par categorie\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REPARTITION PAR CATEGORIE\")\n",
    "print(\"=\" * 60)\n",
    "print(df[\"categorie\"].value_counts())\n",
    "\n",
    "# Distributions des variables numeriques\n",
    "variables_num = [\"prix\", \"nb_ventes_mensuel\", \"note_moyenne\", \"nb_avis\", \"stock\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(variables_num):\n",
    "    axes[i].hist(df[col], bins=15, edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "    axes[i].set_title(f\"Distribution de {col}\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Frequence\")\n",
    "\n",
    "# Masquer le dernier subplot inutilise\n",
    "axes[5].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot par categorie\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(data=df, x=\"categorie\", y=\"prix\", ax=axes[0], palette=\"Set2\")\n",
    "axes[0].set_title(\"Prix par categorie\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "sns.boxplot(data=df, x=\"categorie\", y=\"nb_ventes_mensuel\", ax=axes[1], palette=\"Set2\")\n",
    "axes[1].set_title(\"Ventes mensuelles par categorie\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Preprocessing des donnees\n\n### Pourquoi preparer les donnees ?\n\nLe clustering regroupe les produits \"qui se ressemblent\". Mais comment mesurer la ressemblance ?\n\nL'algorithme calcule la **distance** entre les produits. Le probleme : si le prix va de 5€ a 900€ et les notes de 1 a 5, le prix va completement **ecraser** l'influence des notes (parce que les ecarts sont beaucoup plus grands).\n\n> **Analogie** : C'est comme comparer des temperatures en Celsius et en Fahrenheit. 30°C et 86°F, c'est pareil ! Mais si on met les deux dans un calcul sans convertir, on obtient n'importe quoi.\n\nLa **standardisation** remet toutes les variables sur la meme echelle (moyenne = 0, ecart-type = 1). Ainsi, chaque variable pese le meme poids dans le calcul de distance.\n\n### Etapes de preparation :\n1. **Selection** des variables numeriques pertinentes\n2. **Encodage** de la variable categorielle (transformer \"Electronique\" en nombre)\n3. **Standardisation** : centrer et reduire chaque variable"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 3 : Preprocessing (encodage + scaling)\n",
    "# ============================================================\n",
    "\n",
    "# Selection des features pour le clustering\n",
    "# On utilise les variables numeriques + la categorie encodee\n",
    "features_clustering = [\"prix\", \"nb_ventes_mensuel\", \"note_moyenne\", \"nb_avis\", \"stock\"]\n",
    "\n",
    "# Encodage de la categorie\n",
    "le = LabelEncoder()\n",
    "df[\"categorie_encoded\"] = le.fit_transform(df[\"categorie\"])\n",
    "\n",
    "# Ajout de la categorie encodee aux features\n",
    "features_avec_cat = features_clustering + [\"categorie_encoded\"]\n",
    "\n",
    "# Preparation de la matrice de features\n",
    "X = df[features_avec_cat].copy()\n",
    "\n",
    "print(\"Features utilisees pour le clustering :\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"\\nDimensions : {X.shape}\")\n",
    "\n",
    "# Standardisation (centrage-reduction)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- Donnees apres standardisation ---\")\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_avec_cat)\n",
    "print(X_scaled_df.describe().round(2))\n",
    "print(\"\\nVerification : moyennes proches de 0 et ecarts-types proches de 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Determination du nombre optimal de clusters\n\nLe premier defi du clustering : **combien de groupes** faut-il creer ? Trop peu = on melange des produits tres differents. Trop = chaque produit est presque seul dans son groupe.\n\n### Methode 1 : Le coude (Elbow Method)\n\n**Principe** : On teste K=2, K=3, K=4... et on mesure l'**inertie** (= a quel point les produits sont proches du centre de leur groupe).\n\n> **Analogie** : Imaginez que vous placez des drapeaux dans un champ pour indiquer le point de rassemblement de chaque equipe. L'inertie, c'est la distance totale que tous les joueurs doivent parcourir pour rejoindre leur drapeau. Plus on met de drapeaux, plus cette distance diminue.\n\n**Comment lire le graphique ?** L'inertie baisse toujours quand on ajoute des clusters. On cherche le **coude** : le point ou ajouter un cluster supplementaire n'ameliore plus beaucoup l'inertie. C'est comme ajouter des drapeaux qui n'aident plus personne.\n\n### Methode 2 : Le score silhouette\n\n**Principe** : Pour chaque produit, on mesure s'il est **bien place** dans son cluster.\n\n> **Analogie du restaurant** : Imaginez un client assis a une table. Le score silhouette mesure : \"Est-il plus proche des gens de SA table que des gens de la table la plus proche ?\"\n> - Score proche de **+1** : le client est bien a sa table (cluster coherent)\n> - Score proche de **0** : le client pourrait etre a l'une ou l'autre table (frontiere floue)\n> - Score proche de **-1** : le client serait mieux a une autre table (mauvais clustering)\n\n**Comment lire le graphique ?** On prend le K qui donne le **score silhouette le plus eleve**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 4 : Methode du coude + Score silhouette\n",
    "# ============================================================\n",
    "\n",
    "# Plage de K a tester\n",
    "K_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Methode du coude\n",
    "axes[0].plot(K_range, inertias, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel(\"Nombre de clusters (K)\")\n",
    "axes[0].set_ylabel(\"Inertie\")\n",
    "axes[0].set_title(\"Methode du coude (Elbow Method)\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Score silhouette\n",
    "axes[1].plot(K_range, silhouettes, \"rs-\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Nombre de clusters (K)\")\n",
    "axes[1].set_ylabel(\"Score silhouette\")\n",
    "axes[1].set_title(\"Score silhouette en fonction de K\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Identification du meilleur K par silhouette\n",
    "meilleur_k = list(K_range)[np.argmax(silhouettes)]\n",
    "axes[1].axvline(x=meilleur_k, color=\"green\", linestyle=\"--\", linewidth=2,\n",
    "                label=f\"K optimal = {meilleur_k}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n--- Resultats ---\")\n",
    "for k, s in zip(K_range, silhouettes):\n",
    "    marqueur = \" <-- OPTIMAL\" if k == meilleur_k else \"\"\n",
    "    print(f\"  K={k} : Silhouette = {s:.4f}{marqueur}\")\n",
    "\n",
    "print(f\"\\nNombre optimal de clusters retenu : K = {meilleur_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "Nous allons appliquer K-Means avec le nombre optimal de clusters identifie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 5 : K-Means avec le K optimal\n",
    "# ============================================================\n",
    "\n",
    "# Application de K-Means\n",
    "kmeans_final = KMeans(n_clusters=meilleur_k, random_state=42, n_init=10)\n",
    "df[\"cluster_kmeans\"] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"K-Means avec K={meilleur_k} clusters\")\n",
    "print(f\"Inertie finale : {kmeans_final.inertia_:.2f}\")\n",
    "print(f\"Score silhouette : {silhouette_score(X_scaled, df['cluster_kmeans']):.4f}\")\n",
    "\n",
    "# Nombre de produits par cluster\n",
    "print(\"\\n--- Repartition des produits par cluster ---\")\n",
    "for c in sorted(df[\"cluster_kmeans\"].unique()):\n",
    "    n = (df[\"cluster_kmeans\"] == c).sum()\n",
    "    print(f\"  Cluster {c} : {n} produits ({n/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Diagramme de la silhouette par echantillon\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sample_silhouette_values = silhouette_samples(X_scaled, df[\"cluster_kmeans\"])\n",
    "y_lower = 10\n",
    "\n",
    "couleurs = plt.cm.Set2(np.linspace(0, 1, meilleur_k))\n",
    "\n",
    "for i in range(meilleur_k):\n",
    "    # Valeurs silhouette des echantillons du cluster i\n",
    "    ith_cluster_values = sample_silhouette_values[df[\"cluster_kmeans\"] == i]\n",
    "    ith_cluster_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    ax.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_values,\n",
    "        facecolor=couleurs[i],\n",
    "        edgecolor=couleurs[i],\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontweight=\"bold\")\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_title(\"Diagramme silhouette par echantillon\")\n",
    "ax.set_xlabel(\"Score silhouette\")\n",
    "ax.set_ylabel(\"Echantillons par cluster\")\n",
    "ax.axvline(x=silhouette_score(X_scaled, df[\"cluster_kmeans\"]), color=\"red\", linestyle=\"--\",\n",
    "           label=f\"Moyenne = {silhouette_score(X_scaled, df['cluster_kmeans']):.3f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DBSCAN : une approche basee sur la densite\n\n### K-Means vs DBSCAN : deux philosophies\n\n**K-Means** dit : \"Je veux exactement K groupes, et chaque produit appartient a un groupe.\"\n\n**DBSCAN** dit : \"Je vais trouver les zones denses naturellement, et les produits isoles sont du bruit.\"\n\n> **Analogie de la soiree** : K-Means, c'est un animateur qui dit \"Formez 4 groupes !\". DBSCAN, c'est observer naturellement ou les gens se regroupent — et accepter que certains restent seuls.\n\n### Comment ca marche ?\n\nDBSCAN utilise 2 parametres :\n- **eps** (epsilon) : \"Quelle distance maximale entre deux points pour qu'ils soient consideres voisins ?\"\n  - Petit eps → beaucoup de petits clusters + beaucoup de bruit\n  - Grand eps → peu de gros clusters + peu de bruit\n\n- **min_samples** : \"Combien de voisins minimum pour former un cluster ?\"\n  - Petit min_samples → detecte des micro-groupes\n  - Grand min_samples → n'accepte que les gros groupes denses\n\n### Les avantages de DBSCAN\n\n| | K-Means | DBSCAN |\n|---|---------|--------|\n| Nombre de clusters | A definir a l'avance | Detecte automatiquement |\n| Forme des clusters | Spheriques uniquement | N'importe quelle forme |\n| Outliers | Force dans un cluster | Les identifie comme \"bruit\" (label = -1) |\n| Sensibilite | A l'initialisation | Aux parametres eps et min_samples |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 6 : DBSCAN\n",
    "# ============================================================\n",
    "\n",
    "# Test de plusieurs valeurs d'eps\n",
    "print(\"--- Test de differentes valeurs d'eps ---\")\n",
    "for eps in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_bruit = (labels == -1).sum()\n",
    "    sil = silhouette_score(X_scaled, labels) if n_clusters >= 2 else -1\n",
    "    print(f\"  eps={eps:.1f} : {n_clusters} clusters, {n_bruit} points de bruit, silhouette={sil:.4f}\")\n",
    "\n",
    "# Application de DBSCAN avec les parametres retenus\n",
    "eps_choisi = 2.0\n",
    "min_samples_choisi = 3\n",
    "\n",
    "dbscan_final = DBSCAN(eps=eps_choisi, min_samples=min_samples_choisi)\n",
    "df[\"cluster_dbscan\"] = dbscan_final.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(df[\"cluster_dbscan\"])) - (1 if -1 in df[\"cluster_dbscan\"].values else 0)\n",
    "n_bruit = (df[\"cluster_dbscan\"] == -1).sum()\n",
    "\n",
    "print(f\"\\n--- DBSCAN (eps={eps_choisi}, min_samples={min_samples_choisi}) ---\")\n",
    "print(f\"  Nombre de clusters : {n_clusters_dbscan}\")\n",
    "print(f\"  Points de bruit    : {n_bruit} ({n_bruit/len(df)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters_dbscan >= 2:\n",
    "    # Score silhouette (sans les points de bruit)\n",
    "    masque = df[\"cluster_dbscan\"] != -1\n",
    "    if masque.sum() > n_clusters_dbscan:\n",
    "        sil_dbscan = silhouette_score(X_scaled[masque], df.loc[masque, \"cluster_dbscan\"])\n",
    "        print(f\"  Score silhouette   : {sil_dbscan:.4f} (hors bruit)\")\n",
    "\n",
    "print(\"\\n--- Repartition des produits ---\")\n",
    "for c in sorted(df[\"cluster_dbscan\"].unique()):\n",
    "    n = (df[\"cluster_dbscan\"] == c).sum()\n",
    "    label = f\"Cluster {c}\" if c != -1 else \"Bruit (outliers)\"\n",
    "    print(f\"  {label} : {n} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualisation des clusters en 2D (PCA)\n\n### Le probleme : comment voir en 6 dimensions ?\n\nNos produits sont decrits par 6 variables (prix, ventes, notes...). C'est comme avoir 6 axes sur un graphique — impossible a dessiner !\n\n### La solution : la PCA (Analyse en Composantes Principales)\n\nLa PCA **ecrase** les 6 dimensions en 2, en conservant un maximum d'information.\n\n> **Analogie de la photo** : Imaginez une statue en 3D. Quand vous prenez une photo, vous ecrasez les 3 dimensions en 2D. Selon l'angle, la photo capture plus ou moins bien la forme de la statue. La PCA choisit automatiquement le **meilleur angle** pour conserver le maximum d'information.\n\n**La \"variance expliquee\"** vous dit quelle proportion de l'information est conservee. Par exemple, \"65% de variance expliquee\" signifie que la projection 2D capture 65% des differences entre les produits. C'est une bonne approximation mais pas parfaite.\n\n> **A retenir** : Les clusters que vous voyez sur le graphique 2D sont une **simplification**. Deux points qui semblent proches en 2D peuvent etre plus eloignes dans l'espace reel a 6 dimensions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 7 : Visualisation PCA 2D\n",
    "# ============================================================\n",
    "\n",
    "# Reduction de dimension avec PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Variance expliquee par les 2 composantes : {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"  PC1 : {pca.explained_variance_ratio_[0]*100:.1f}%\")\n",
    "print(f\"  PC2 : {pca.explained_variance_ratio_[1]*100:.1f}%\")\n",
    "\n",
    "# Visualisation K-Means vs DBSCAN\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1],\n",
    "    c=df[\"cluster_kmeans\"],\n",
    "    cmap=\"Set2\",\n",
    "    alpha=0.7,\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    "    s=60,\n",
    ")\n",
    "\n",
    "# Centroides K-Means en PCA\n",
    "centroides_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "axes[0].scatter(\n",
    "    centroides_pca[:, 0], centroides_pca[:, 1],\n",
    "    c=\"red\", marker=\"X\", s=200, edgecolors=\"black\", linewidth=2,\n",
    "    label=\"Centroides\",\n",
    ")\n",
    "\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[0].set_title(f\"K-Means (K={meilleur_k})\")\n",
    "axes[0].legend()\n",
    "\n",
    "# DBSCAN\n",
    "# Les points de bruit (-1) sont en gris\n",
    "couleurs_dbscan = df[\"cluster_dbscan\"].copy()\n",
    "masque_bruit = couleurs_dbscan == -1\n",
    "\n",
    "# Points clusters\n",
    "scatter2 = axes[1].scatter(\n",
    "    X_pca[~masque_bruit, 0], X_pca[~masque_bruit, 1],\n",
    "    c=couleurs_dbscan[~masque_bruit],\n",
    "    cmap=\"Set2\",\n",
    "    alpha=0.7,\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    "    s=60,\n",
    "    label=\"Clusters\",\n",
    ")\n",
    "\n",
    "# Points de bruit\n",
    "if masque_bruit.any():\n",
    "    axes[1].scatter(\n",
    "        X_pca[masque_bruit, 0], X_pca[masque_bruit, 1],\n",
    "        c=\"gray\", marker=\"x\", s=50, alpha=0.5,\n",
    "        label=f\"Bruit ({masque_bruit.sum()} pts)\",\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "axes[1].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "axes[1].set_title(f\"DBSCAN (eps={eps_choisi}, min_samples={min_samples_choisi})\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Contribution des features aux composantes principales\n",
    "print(\"\\n--- Contribution des features aux composantes principales ---\")\n",
    "pca_contrib = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[\"PC1\", \"PC2\"],\n",
    "    index=features_avec_cat,\n",
    ").round(3)\n",
    "print(pca_contrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profilage des clusters\n",
    "\n",
    "Pour interpreter les clusters, nous allons calculer les statistiques moyennes de chaque cluster\n",
    "et identifier les caracteristiques dominantes de chaque groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 8 : Profilage des clusters\n",
    "# ============================================================\n",
    "\n",
    "# Statistiques moyennes par cluster K-Means\n",
    "variables_profil = [\"prix\", \"nb_ventes_mensuel\", \"note_moyenne\", \"nb_avis\", \"stock\"]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  PROFILAGE DES CLUSTERS (K-Means)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "profil = df.groupby(\"cluster_kmeans\")[variables_profil].mean().round(2)\n",
    "profil[\"nb_produits\"] = df.groupby(\"cluster_kmeans\").size()\n",
    "print(profil)\n",
    "\n",
    "# Repartition des categories par cluster\n",
    "print(\"\\n--- Repartition des categories par cluster ---\")\n",
    "cat_par_cluster = pd.crosstab(df[\"cluster_kmeans\"], df[\"categorie\"], margins=True)\n",
    "print(cat_par_cluster)\n",
    "\n",
    "# Visualisation du profil moyen de chaque cluster (radar chart simplifie)\n",
    "fig, axes = plt.subplots(1, meilleur_k, figsize=(5 * meilleur_k, 5))\n",
    "if meilleur_k == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Normalisation des moyennes pour comparaison visuelle\n",
    "profil_norm = profil[variables_profil].copy()\n",
    "for col in variables_profil:\n",
    "    col_min = profil_norm[col].min()\n",
    "    col_max = profil_norm[col].max()\n",
    "    if col_max > col_min:\n",
    "        profil_norm[col] = (profil_norm[col] - col_min) / (col_max - col_min)\n",
    "    else:\n",
    "        profil_norm[col] = 0.5\n",
    "\n",
    "couleurs_cluster = plt.cm.Set2(np.linspace(0, 1, meilleur_k))\n",
    "\n",
    "for i in range(meilleur_k):\n",
    "    valeurs = profil_norm.loc[i].values\n",
    "    axes[i].barh(variables_profil, valeurs, color=couleurs_cluster[i], edgecolor=\"black\")\n",
    "    axes[i].set_xlim(0, 1.1)\n",
    "    axes[i].set_title(f\"Cluster {i} ({profil.loc[i, 'nb_produits']} produits)\", fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"Valeur normalisee\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap des profils\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(\n",
    "    profil_norm,\n",
    "    annot=profil[variables_profil].round(1).values,\n",
    "    fmt=\"\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    ax=ax,\n",
    "    linewidths=1,\n",
    "    xticklabels=variables_profil,\n",
    "    yticklabels=[f\"Cluster {i}\" for i in range(meilleur_k)],\n",
    ")\n",
    "ax.set_title(\"Profil moyen des clusters (valeurs reelles annotees)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exemples de produits par cluster\n",
    "print(\"\\n--- Exemples de produits par cluster ---\")\n",
    "for c in sorted(df[\"cluster_kmeans\"].unique()):\n",
    "    print(f\"\\n  Cluster {c} :\")\n",
    "    exemples = df[df[\"cluster_kmeans\"] == c][[\"nom_produit\", \"categorie\", \"prix\", \"nb_ventes_mensuel\"]].head(5)\n",
    "    for _, row in exemples.iterrows():\n",
    "        print(f\"    - {row['nom_produit']} ({row['categorie']}) | {row['prix']}e | {row['nb_ventes_mensuel']} ventes/mois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion et insights business\n\n### Recapitulatif methodologique\n\n| Methode | Avantages | Inconvenients |\n|---------|-----------|---------------|\n| **K-Means** | Simple, rapide, interpretable | Nombre de clusters a definir, clusters spheriques |\n| **DBSCAN** | Detection d'outliers, formes arbitraires | Parametres eps/min_samples a regler |\n\n### Interpretation des clusters\n\nL'analyse des profils moyens permet d'attribuer des labels metier aux clusters.\nPar exemple :\n\n- **Produits premium** : prix eleve, peu de ventes, note elevee\n- **Best-sellers** : prix moyen/bas, beaucoup de ventes, beaucoup d'avis\n- **Produits de niche** : prix variable, peu de ventes, peu d'avis\n- **Produits populaires abordables** : prix bas, ventes elevees, stock important\n\n### Applications metier\n\n- **Marketing** : adapter la communication par segment de produits\n- **Pricing** : ajuster les prix selon le cluster\n- **Stock** : optimiser les niveaux de stock par segment\n- **Cross-selling** : recommander des produits du meme cluster\n\n### Pour aller plus loin\n\n- Tester d'autres algorithmes (Agglomeratif, Gaussian Mixture Models)\n- Utiliser t-SNE ou UMAP pour une meilleure visualisation 2D\n- Combiner le clustering avec des donnees clients pour une segmentation croisee\n- Automatiser le choix des hyperparametres avec une recherche en grille\n\n### Lexique debutant\n\n| Terme | Definition simple |\n|-------|------------------|\n| **Clustering** | Regrouper des elements similaires sans categories predefinies |\n| **Apprentissage non supervise** | Apprendre sans etiquettes (pas de \"bonne reponse\" connue) |\n| **K-Means** | Algorithme qui cree K groupes en minimisant les distances |\n| **DBSCAN** | Algorithme qui detecte les zones denses et identifie le bruit |\n| **Inertie** | Distance totale entre chaque point et le centre de son cluster |\n| **Score silhouette** | Mesure si chaque point est bien dans son cluster (-1 a +1) |\n| **PCA** | Technique pour reduire le nombre de dimensions (pour visualiser) |\n| **Centroide** | Le \"point central\" d'un cluster (la moyenne de tous ses membres) |\n| **Outlier / Bruit** | Point isole qui n'appartient a aucun cluster |\n| **Standardisation** | Remettre toutes les variables sur la meme echelle |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cellule 9 : Resume final\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  RESUME FINAL - SEGMENTATION DES PRODUITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n  Nombre de produits analyses  : {len(df)}\")\n",
    "print(f\"  Nombre de features utilisees : {len(features_avec_cat)}\")\n",
    "print(f\"  Algorithme retenu            : K-Means\")\n",
    "print(f\"  Nombre de clusters           : {meilleur_k}\")\n",
    "print(f\"  Score silhouette             : {silhouette_score(X_scaled, df['cluster_kmeans']):.4f}\")\n",
    "\n",
    "print(f\"\\n--- Profil synthetique des clusters ---\")\n",
    "for c in sorted(df[\"cluster_kmeans\"].unique()):\n",
    "    sous_df = df[df[\"cluster_kmeans\"] == c]\n",
    "    prix_moy = sous_df[\"prix\"].mean()\n",
    "    ventes_moy = sous_df[\"nb_ventes_mensuel\"].mean()\n",
    "    note_moy = sous_df[\"note_moyenne\"].mean()\n",
    "    categories = sous_df[\"categorie\"].mode().iloc[0]\n",
    "    \n",
    "    print(f\"\\n  Cluster {c} ({len(sous_df)} produits) :\")\n",
    "    print(f\"    Prix moyen     : {prix_moy:.2f} euros\")\n",
    "    print(f\"    Ventes/mois    : {ventes_moy:.0f}\")\n",
    "    print(f\"    Note moyenne   : {note_moy:.2f}/5\")\n",
    "    print(f\"    Categorie dom. : {categories}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}