## 7. Meilleures pratiques Git pour Data Engineering

### Le fichier .gitignore

Le **.gitignore** indique Ã  Git quels fichiers ne PAS versionner.
C'est crucial en Data Engineering pour Ã©viter de commiter des donnÃ©es sensibles ou volumineuses.

```bash
# CrÃ©er un .gitignore optimisÃ© pour Data Engineering
touch .gitignore

# Ajouter des patterns
cat >> .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*.so
.Python
venv/
env/
.venv/
*.egg-info/
dist/
build/

# Jupyter Notebooks - CONFIGURATION IMPORTANTE
.ipynb_checkpoints/
# NE PAS exclure tous les *.ipynb !
# Pour nettoyer les outputs avant commit, utilisez :
# jupyter nbconvert --clear-output --inplace notebook.ipynb

# OU installez nbstripout :
# pip install nbstripout
# nbstripout --install

# Data files - NE JAMAIS VERSIONNER LES DONNÃ‰ES
*.csv
*.parquet
*.json
*.xlsx
*.xls
*.db
*.sqlite
*.sqlite3
data/
raw_data/
processed_data/
output/
*.pickle
*.pkl
*.h5
*.hdf5

# Credentials et secrets - CRITIQUE !
.env
.env.*
!.env.example
credentials.json
service-account.json
*.pem
*.key
*.p12
*.pfx
config/secrets.yaml
config/prod.yaml
.aws/credentials
.gcloud/

# Logs
*.log
logs/
airflow/logs/
*.log.*

# IDE et Ã©diteurs
.vscode/
.idea/
*.swp
*.swo
.DS_Store
Thumbs.db
*.bak

# DBT
dbt_packages/
target/
dbt_modules/
logs/
.user.yml

# Terraform
*.tfstate
*.tfstate.*
*.tfstate.backup
.terraform/
.terraform.lock.hcl
terraform.tfvars
# Mais versionner :
# !terraform.tfvars.example

# Docker
docker-compose.override.yml

# Spark
metastore_db/
spark-warehouse/
derby.log

# Airflow
airflow.db
airflow.cfg
webserver_config.py
unittests.cfg

# Kafka
.kafka/

# Great Expectations
uncommitted/

# MLflow
mlruns/
mlartifacts/

# Prefect
.prefect/
EOF
```

#### âš ï¸ Notebooks Jupyter : Configuration correcte

**NE PAS exclure tous les \*.ipynb !** Les notebooks contiennent du code qu'on veut versionner.
Le problÃ¨me, ce sont les *outputs* (rÃ©sultats d'exÃ©cution) qui peuvent Ãªtre volumineux.

**Solution recommandÃ©e :**

```bash
# Option 1 : nbstripout (RECOMMANDÃ‰)
# Installer nbstripout
pip install nbstripout

# Installer le hook Git qui nettoie automatiquement les outputs
nbstripout --install

# Maintenant, Ã  chaque commit, les outputs seront automatiquement retirÃ©s !

# Option 2 : Nettoyer manuellement avant commit
jupyter nbconvert --clear-output --inplace mon_notebook.ipynb
git add mon_notebook.ipynb
git commit -m "feat: Add data exploration notebook"

# Option 3 : Configuration Git globale avec nbstripout
# Dans .git/config ou ~/.gitconfig
git config filter.nbstripout.clean 'nbstripout'
git config filter.nbstripout.smudge cat
git config filter.nbstripout.required true

# Puis dans .gitattributes :
echo "*.ipynb filter=nbstripout" >> .gitattributes
git add .gitattributes
git commit -m "chore: Configure nbstripout for Jupyter notebooks"
```

#### ğŸš¨ Ne JAMAIS commiter

- **DonnÃ©es brutes ou transformÃ©es** : Utilisez S3, GCS, Azure Blob ou un data lake
- **Credentials et secrets** : API keys, mots de passe, tokens, certificats
- **Fichiers volumineux** : Utilisez Git LFS ou un systÃ¨me de stockage externe
- **DonnÃ©es personnelles (RGPD/GDPR)** : Respectez les rÃ©glementations
- **Fichiers binaires gÃ©nÃ©rÃ©s** : .pyc, .class, executables
- **DÃ©pendances** : node\_modules/, venv/, packages (utilisez requirements.txt)

### Versionner quoi dans un projet Data ?

#### âœ… Ã€ versionner

- Code Python/SQL des pipelines
- Configuration DBT
- Scripts ETL/ELT
- Notebooks Jupyter (**sans outputs**)
- SchÃ©mas de donnÃ©es (Avro, Protobuf)
- Documentation (README, wiki)
- Tests unitaires et d'intÃ©gration
- IaC (Terraform, CloudFormation)
- Docker et docker-compose
- DAGs Airflow/Prefect
- Fichiers de config (YAML, TOML)
- Scripts de migration de DB
- Fichiers d'exemple (.env.example)

#### âŒ Ã€ NE PAS versionner

- Datasets (.csv, .parquet, .json)
- Credentials et secrets
- Fichiers de cache
- ModÃ¨les ML entraÃ®nÃ©s (>10MB)
- Logs applicatifs
- Bases de donnÃ©es locales
- Fichiers temporaires
- Dossiers de build
- Outputs de notebooks
- Fichiers IDE personnels
- Environnements virtuels
- Artefacts de compilation

### StratÃ©gie de branches pour Data Engineering

```bash
main (production) â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â–¶
                        â†‘      â†‘      â†‘
                        â”‚      â”‚      â”‚
develop â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”´â”€â”€â—â”€â”€â”€â”´â”€â”€â—â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â–¶
            â†‘    â†‘         â†‘      â†‘
            â”‚    â”‚         â”‚      â”‚
feature/    â”‚    â”‚         â”‚      â”‚
etl-sales â”€â”€â”´â”€â”€â”€â”€â”˜         â”‚      â”‚
                            â”‚      â”‚
feature/                    â”‚      â”‚
dbt-models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

- **main** : Code dÃ©ployÃ© en production (toujours stable, protÃ©gÃ©)
- **develop** : Code en cours de dÃ©veloppement (branche d'intÃ©gration)
- **feature/\*** : Nouvelles fonctionnalitÃ©s (ex: feature/mongodb-connector)
- **fix/\*** : Corrections de bugs non urgents
- **hotfix/\*** : Corrections urgentes en production
- **release/\*** : PrÃ©paration des releases

### Messages de commit pour Data Engineering

#### Convention Conventional Commits

`<type>(<scope>): <description>`

| Type | Usage | Exemple |
| --- | --- | --- |
| feat | Nouvelle fonctionnalitÃ© | feat(etl): Add MongoDB data extractor |
| fix | Correction de bug | fix(pipeline): Handle null values in transformation |
| perf | AmÃ©lioration de performance | perf(query): Optimize PostgreSQL aggregation query |
| refactor | Refactorisation | refactor(dbt): Restructure models directory |
| docs | Documentation | docs(readme): Add setup instructions for Airflow |
| test | Ajout de tests | test(etl): Add unit tests for data validation |
| chore | Maintenance | chore(deps): Update pandas to 2.0.0 |
| ci | CI/CD | ci: Add GitHub Actions for automated tests |
| build | Build system | build: Update Docker base image to Python 3.11 |

#### âœ… Exemples de bons messages de commit

- `feat(etl): Add incremental load for customer dimension`
- `fix(dbt): Correct date partition logic in sales_daily model`
- `perf(spark): Optimize join strategy reducing runtime by 40%`
- `docs(airflow): Document DAG parameters and retry policy`
- `test(pipeline): Add integration tests for S3 to Snowflake flow`

### Git Hooks pour automatiser les vÃ©rifications

Les **hooks** sont des scripts exÃ©cutÃ©s automatiquement Ã  certains moments
(avant commit, avant push, etc.).

#### Utiliser pre-commit framework (RECOMMANDÃ‰)

```bash
# Installer pre-commit
pip install pre-commit

# CrÃ©er .pre-commit-config.yaml pour Data Engineering
cat > .pre-commit-config.yaml << 'EOF'
repos:
# Hooks standards
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-added-large-files
        args: ['--maxkb=1000']  # Bloque fichiers > 1MB
      - id: detect-private-key  # DÃ©tecte les clÃ©s privÃ©es
      - id: check-merge-conflict
      - id: mixed-line-ending

# Python formatting
  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3.11

# Python linting
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=88', '--extend-ignore=E203']

# SQL formatting
  - repo: https://github.com/sqlfluff/sqlfluff
    rev: 3.0.0
    hooks:
      - id: sqlfluff-lint
      - id: sqlfluff-fix

# Jupyter notebooks
  - repo: https://github.com/nbQA-dev/nbQA
    rev: 1.7.1
    hooks:
      - id: nbqa-black
      - id: nbqa-flake8

# Nettoyer outputs notebooks (IMPORTANT!)
  - repo: https://github.com/kynan/nbstripout
    rev: 0.6.1
    hooks:
      - id: nbstripout
        files: "\\.ipynb$"

# SÃ©curitÃ© - DÃ©tecter secrets
  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.4.0
    hooks:
      - id: detect-secrets
        args: ['--baseline', '.secrets.baseline']

# YAML linting
  - repo: https://github.com/adrienverge/yamllint
    rev: v1.33.0
    hooks:
      - id: yamllint

# Terraform
  - repo: https://github.com/antonbabenko/pre-commit-terraform
    rev: v1.86.0
    hooks:
      - id: terraform_fmt
      - id: terraform_validate
EOF

# Installer les hooks
pre-commit install

# Lancer sur tous les fichiers
pre-commit run --all-files

# Les hooks s'exÃ©cuteront automatiquement Ã  chaque commit!
```

### Git LFS pour les fichiers volumineux

**Git Large File Storage (LFS)** permet de versionner des fichiers volumineux
sans alourdir le dÃ©pÃ´t. Utile pour les petits datasets d'exemple ou modÃ¨les ML.

```bash
# Installer Git LFS
# macOS
brew install git-lfs
# Ubuntu
sudo apt-get install git-lfs
# Windows : tÃ©lÃ©charger depuis https://git-lfs.github.com

# Initialiser Git LFS dans le repo
git lfs install

# Tracker les fichiers parquet (datasets exemples)
git lfs track "*.parquet"
git lfs track "data/samples/*.csv"

# Tracker les modÃ¨les ML
git lfs track "models/*.pkl"
git lfs track "models/*.h5"
git lfs track "*.joblib"

# Le fichier .gitattributes est crÃ©Ã© automatiquement
cat .gitattributes
# *.parquet filter=lfs diff=lfs merge=lfs -text
# models/*.pkl filter=lfs diff=lfs merge=lfs -text

# Commiter normalement
git add .gitattributes
git add data/samples/example.parquet
git commit -m "feat: Add sample data with Git LFS"
git push

# Voir les fichiers LFS
git lfs ls-files

# Cloner un repo avec LFS
git clone
# Les fichiers LFS sont tÃ©lÃ©chargÃ©s automatiquement
```

#### Quand utiliser Git LFS ?

- âœ… Datasets d'exemple de petite taille (< 100MB)
- âœ… ModÃ¨les ML de rÃ©fÃ©rence
- âœ… Fichiers de documentation (PDF, images)
- âŒ Gros datasets de production (utilisez S3/GCS)
- âŒ DonnÃ©es sensibles (ne jamais versionner)

### GÃ©rer les secrets avec Git

```bash
# CrÃ©er un fichier .env.example (Ã  versionner)
cat > .env.example << 'EOF'
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=mydatabase
DB_USER=your_username
DB_PASSWORD=your_password

# AWS Configuration
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1

# API Keys
OPENAI_API_KEY=your_api_key
EOF

# Copier et remplir avec les vraies valeurs (NE PAS VERSIONNER)
cp .env.example .env

# VÃ©rifier que .env est dans .gitignore
grep ".env" .gitignore

# Utiliser des outils de gestion de secrets
# Option 1 : AWS Secrets Manager
# Option 2 : HashiCorp Vault
# Option 3 : Azure Key Vault
# Option 4 : Variables d'environnement CI/CD
```

### ğŸ’¡ Points clÃ©s Ã  retenir

- Configurez un .gitignore complet DÃˆS LE DÃ‰BUT du projet
- Utilisez nbstripout pour nettoyer automatiquement les notebooks
- NE JAMAIS commiter de credentials ou donnÃ©es sensibles
- Adoptez Conventional Commits pour des messages clairs
- Mettez en place pre-commit hooks pour automatiser les vÃ©rifications
- Utilisez Git LFS uniquement pour petits fichiers d'exemple
- Stockez les vrais datasets dans S3/GCS/Azure Blob
- Versionnez le code, pas les donnÃ©es
- Documentez votre workflow dans le README
- ProtÃ©gez la branche main avec branch protection rules

#### âœ… Partie 7 terminÃ©e !

FÃ©licitations ! Vous maÃ®trisez maintenant les meilleures pratiques Git pour le Data Engineering.
Passez Ã  la Partie 8 pour dÃ©couvrir les workflows professionnels avec Conventional Commits et Docker !

[Partie 8 : Workflows â†’](partie8.md)
[ğŸ¯ Faire les exercices](../exercices.md)