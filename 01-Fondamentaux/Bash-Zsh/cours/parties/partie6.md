## 6. Fonctions et Outils Avanc√©s

Les fonctions permettent de structurer votre code et de le rendre r√©utilisable. Cette partie couvre √©galement des outils avanc√©s essentiels en Data Engineering.

### D√©finir des fonctions

```bash
#!/bin/bash

# Syntaxe 1 (classique)
function my_function() {
    echo "Hello from function"
}

# Syntaxe 2 (pr√©f√©r√©e - POSIX)
my_function() {
    echo "Hello from function"
}

# Appeler la fonction
my_function

# Fonction avec param√®tres
greet() {
    local name=$1
    echo "Hello, $name!"
}

greet "Alice"
greet "Bob"

# Fonction avec plusieurs param√®tres
calculate_sum() {
    local a=$1
    local b=$2
    local sum=$((a + b))
    echo $sum
}

result=$(calculate_sum 10 20)
echo "Sum: $result"

# Fonction avec valeurs par d√©faut
greet_with_default() {
    local name=${1:-"Guest"}
    echo "Hello, $name!"
}

greet_with_default "Alice"  # Hello, Alice!
greet_with_default          # Hello, Guest!

# Tous les param√®tres
print_all_args() {
    echo "Number of arguments: $#"
    echo "All arguments: $@"

    local i=1
    for arg in "$@"; do
        echo "Argument $i: $arg"
        ((i++))
    done
}

print_all_args one two three
```

### Variables locales vs globales

```bash
#!/bin/bash

# Variable globale
global_var="I am global"

my_function() {
# Variable locale (seulement dans la fonction)
    local local_var="I am local"

# Modifier la variable globale
    global_var="Modified in function"

    echo "Inside function:"
    echo "  Global: $global_var"
    echo "  Local: $local_var"
}

echo "Before function:"
echo "  Global: $global_var"

my_function

echo "After function:"
echo "  Global: $global_var"
echo "  Local: $local_var"  # Vide - la variable n'existe pas ici

# Bonne pratique : toujours utiliser 'local' pour les variables de fonction
process_data() {
    local input_file=$1
    local output_file=$2
    local temp_file="/tmp/temp_$$"  # $$ = PID du script

# Traitement
    echo "Processing $input_file..."

# Les variables locales ne polluent pas l'espace global
}

# Variables en lecture seule
readonly CONFIG_FILE="config.ini"
readonly MAX_RETRIES=3

# Tentative de modification (erreur)
# CONFIG_FILE="other.ini"  # Erreur !

# Export de variables (pour les sous-shells)
export DATABASE_URL="postgresql://localhost/mydb"
export API_KEY="secret123"

# Les variables export√©es sont disponibles dans les scripts appel√©s
./another_script.sh  # Peut acc√©der √† DATABASE_URL
```

### Return vs Exit

#### Return vs Exit

`return` : Sort de la fonction et retourne √† l'appelant
`exit` : Termine compl√®tement le script

```bash
#!/bin/bash

# return : sortir de la fonction
check_file() {
    local file=$1

    if [ ! -f "$file" ]; then
        echo "File not found: $file"
        return 1  # Code d'erreur
    fi

    if [ ! -s "$file" ]; then
        echo "File is empty: $file"
        return 2
    fi

    echo "File is valid: $file"
    return 0  # Succ√®s
}

# Utiliser le code de retour
if check_file "data.csv"; then
    echo "Processing file..."
else
    echo "Cannot process file (error code: $?)"
fi

# exit : terminer le script compl√®tement
validate_config() {
    if [ ! -f "config.ini" ]; then
        echo "FATAL: config.ini not found"
        exit 1  # Termine tout le script
    fi
}

validate_config
echo "This will not execute if config.ini is missing"

# Retourner des valeurs (via echo et command substitution)
get_file_size() {
    local file=$1
    local size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
    echo $size
}

size=$(get_file_size "data.csv")
echo "File size: $size bytes"

# Fonction avec retour multiple
get_file_info() {
    local file=$1

    if [ ! -f "$file" ]; then
        return 1
    fi

    local lines=$(wc -l < "$file")
    local size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)

# Retourner plusieurs valeurs via echo
    echo "$lines $size"
}

# Capturer plusieurs valeurs
if info=$(get_file_info "data.csv"); then
    read -r lines size <<< "$info"
    echo "Lines: $lines"
    echo "Size: $size"
fi
```

### Biblioth√®que de fonctions r√©utilisables

```bash
#!/bin/bash
# Fichier: lib/utils.sh
# Biblioth√®que de fonctions r√©utilisables

# Logging avec couleurs
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $(date +'%Y-%m-%d %H:%M:%S') - $*"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $(date +'%Y-%m-%d %H:%M:%S') - $*"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $(date +'%Y-%m-%d %H:%M:%S') - $*" >&2
}

log_debug() {
    if [ "${DEBUG:-0}" -eq 1 ]; then
        echo -e "${BLUE}[DEBUG]${NC} $(date +'%Y-%m-%d %H:%M:%S') - $*"
    fi
}

# Fonction de validation
validate_file() {
    local file=$1
    local min_size=${2:-1}

    if [ ! -f "$file" ]; then
        log_error "File not found: $file"
        return 1
    fi

    if [ ! -s "$file" ]; then
        log_error "File is empty: $file"
        return 2
    fi

    local size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
    if [ $size -lt $min_size ]; then
        log_error "File too small: $file ($size bytes)"
        return 3
    fi

    return 0
}

# Fonction de retry
retry() {
    local max_attempts=$1
    shift
    local cmd="$@"
    local attempt=1

    while [ $attempt -le $max_attempts ]; do
        log_info "Attempt $attempt/$max_attempts: $cmd"

        if eval "$cmd"; then
            log_info "Success!"
            return 0
        fi

        log_warn "Attempt $attempt failed"
        ((attempt++))
        sleep 2
    done

    log_error "All attempts failed"
    return 1
}

# Cr√©er un backup
backup_file() {
    local file=$1
    local backup="${file}.backup.$(date +%Y%m%d_%H%M%S)"

    cp "$file" "$backup"
    log_info "Backup created: $backup"
}

# Utiliser dans un autre script
# source lib/utils.sh
#
# log_info "Starting process"
# validate_file "data.csv" || exit 1
# retry 3 "curl -f https://api.example.com/data"
```

### xargs - Construire et ex√©cuter des commandes

`xargs` est un outil puissant qui construit des commandes √† partir de l'entr√©e standard. Tr√®s utile pour traiter de nombreux fichiers.

```bash
# Syntaxe basique
echo "file1.txt file2.txt file3.txt" | xargs rm

# √âquivalent √† : rm file1.txt file2.txt file3.txt

# Utiliser avec find
find . -name "*.tmp" | xargs rm

# -n : nombre d'arguments par commande
echo "1 2 3 4 5 6" | xargs -n 2 echo
# Output:
# 1 2
# 3 4
# 5 6

# -I {} : remplacer un placeholder
find . -name "*.csv" | xargs -I {} echo "Processing {}"

# -P : parall√©liser (nombre de processus en parall√®le)
find . -name "*.csv" | xargs -P 4 -I {} gzip {}

# Exemples pratiques en Data Engineering

# 1. Compresser tous les CSV en parall√®le (4 processus)
find data/raw -name "*.csv" | xargs -P 4 -I {} gzip {}

# 2. Copier plusieurs fichiers
find . -name "*.csv" -mtime -1 | xargs -I {} cp {} archive/

# 3. Compter les lignes de tous les fichiers
find . -name "*.log" | xargs wc -l

# 4. Rechercher dans plusieurs fichiers
find . -name "*.csv" | xargs grep "ERROR"

# 5. T√©l√©charger plusieurs URLs
cat urls.txt | xargs -P 5 -n 1 curl -O

# 6. Ex√©cuter une fonction sur chaque fichier
process_file() {
    echo "Processing $1..."
    wc -l "$1"
}
export -f process_file

find . -name "*.csv" | xargs -I {} bash -c 'process_file "$@"' _ {}

# 7. Supprimer les fichiers vieux de plus de 30 jours
find . -name "*.log" -mtime +30 | xargs rm

# 8. Conversion de format en parall√®le
ls *.csv | xargs -P 4 -I {} sh -c 'csvtojson {} > {}.json'

# 9. Cr√©er des archives individuelles
find data -name "*.csv" | xargs -I {} tar -czf {}.tar.gz {}

# Gestion des noms avec espaces (utiliser -0 avec find -print0)
find . -name "*.txt" -print0 | xargs -0 rm

# Dry-run (afficher sans ex√©cuter)
find . -name "*.tmp" | xargs -t rm  # -t affiche la commande
```

### Traitement parall√®le avec GNU Parallel

`parallel` est un outil plus puissant que `xargs` pour ex√©cuter des t√¢ches en parall√®le.

```bash
# Installation
# macOS: brew install parallel
# Ubuntu: sudo apt install parallel

# Syntaxe basique
ls *.csv | parallel gzip

# Avec placeholder {}
ls *.csv | parallel 'echo Processing {}; wc -l {}'

# Limiter le nombre de jobs en parall√®le
ls *.csv | parallel -j 4 process_file {}

# Avec barre de progression
ls *.csv | parallel --progress gzip {}

# Dry-run (afficher sans ex√©cuter)
ls *.csv | parallel --dry-run 'echo {}'

# Exemples pratiques

# 1. Traiter tous les CSV en parall√®le
find data/raw -name "*.csv" | \
    parallel -j 8 'python process.py {} > processed/{/.}_out.csv'

# 2. T√©l√©chargement parall√®le
cat urls.txt | parallel -j 10 curl -O {}

# 3. Compression parall√®le avec progression
find . -name "*.log" | parallel --progress gzip {}

# 4. Traiter avec fonction bash
process_csv() {
    local file=$1
    echo "Processing $file"
    awk -F',' 'NR>1 {print $1,$3}' "$file" > "processed/$(basename "$file")"
}
export -f process_csv

find data -name "*.csv" | parallel process_csv

# 5. Ex√©cuter des commandes sur plusieurs serveurs
parallel -S server1,server2,server3 'hostname; uptime' ::: 1 2 3

# 6. Conversion de formats
parallel 'convert {} {.}.png' ::: *.jpg

# 7. Traitement par chunks
cat huge_file.txt | parallel --pipe -N 10000 'wc -l'

# 8. Avec substitutions avanc√©es
# {}: nom complet
# {.}: sans extension
# {/}: basename
# {//}: dirname
# {/.}: basename sans extension

parallel 'echo File: {}, Dir: {//}, Base: {/.}' ::: data/2024/sales.csv
# Output: File: data/2024/sales.csv, Dir: data/2024, Base: sales
```

### jq - Parser et manipuler du JSON

`jq` est l'outil incontournable pour travailler avec du JSON en ligne de commande. Essentiel en Data Engineering moderne.

```bash
# Installation
# macOS: brew install jq
# Ubuntu: sudo apt install jq

# Fichier JSON exemple: users.json
# {
#   "users": [
#     {"id": 1, "name": "Alice", "age": 30, "city": "Paris"},
#     {"id": 2, "name": "Bob", "age": 25, "city": "Lyon"},
#     {"id": 3, "name": "Charlie", "age": 35, "city": "Paris"}
#   ]
# }

# Pretty print
cat users.json | jq '.'

# Extraire un champ
cat users.json | jq '.users'

# Extraire un √©l√©ment d'un array
cat users.json | jq '.users[0]'

# Extraire un champ sp√©cifique
cat users.json | jq '.users[0].name'
# Output: "Alice"

# Extraire tous les noms
cat users.json | jq '.users[].name'
# Output:
# "Alice"
# "Bob"
# "Charlie"

# Cr√©er un array de noms
cat users.json | jq '[.users[].name]'
# Output: ["Alice", "Bob", "Charlie"]

# Filtrer (s√©lection)
cat users.json | jq '.users[] | select(.age > 25)'

# Filtrer par ville
cat users.json | jq '.users[] | select(.city == "Paris")'

# Transformer la structure
cat users.json | jq '.users[] | {name: .name, age: .age}'

# Compter les √©l√©ments
cat users.json | jq '.users | length'

# Trier
cat users.json | jq '.users | sort_by(.age)'

# Regrouper
cat users.json | jq 'group_by(.city)'

# Extraire uniquement certains champs
cat users.json | jq '.users[] | {name, city}'

# Calculer (somme des √¢ges)
cat users.json | jq '[.users[].age] | add'

# Moyenne des √¢ges
cat users.json | jq '[.users[].age] | add / length'

# Exemples pratiques

# 1. API REST : extraire des donn√©es
curl -s 'https://api.example.com/users' | jq '.data[].email'

# 2. Convertir JSON en CSV
cat users.json | jq -r '.users[] | [.id, .name, .age] | @csv'

# 3. Filtrer et sauvegarder
cat users.json | \
    jq '.users[] | select(.age > 25)' > filtered.json

# 4. Compter par cat√©gorie
cat orders.json | \
    jq 'group_by(.status) | map({status: .[0].status, count: length})'

# 5. Flatten nested JSON
cat nested.json | jq '.data[].items[]'

# 6. Combiner des champs
cat users.json | \
    jq '.users[] | .full_info = (.name + " - " + .city)'

# 7. Extraire et formater
cat users.json | jq -r '.users[] | "\(.name) is \(.age) years old"'

# 8. Traiter plusieurs fichiers
for file in *.json; do
    jq '.users[].email' "$file"
done

# 9. Pipeline avec jq
curl -s 'https://api.example.com/data' | \
    jq '.items[] | select(.price > 100)' | \
    jq -r '.name' | \
    sort | uniq

# 10. Cr√©er du JSON
echo '{"name": "Alice", "age": 30}' | \
    jq '. + {city: "Paris", country: "France"}'

# 11. Modifier des valeurs
cat users.json | jq '.users[].age += 1'

# 12. Supprimer des champs
cat users.json | jq 'del(.users[].id)'
```

### Process Substitution

Process substitution permet d'utiliser la sortie d'une commande comme si c'√©tait un fichier. Tr√®s puissant pour comparer ou combiner des donn√©es.

```bash
# Syntaxe : <(commande)
# La commande est ex√©cut√©e et sa sortie est disponible comme un fichier

# Comparer la sortie de deux commandes
diff <(ls dir1) <(ls dir2)

# Comparer deux listes tri√©es
diff <(sort file1.txt) <(sort file2.txt)

# Exemples pratiques

# 1. Comparer deux CSV apr√®s tri
diff <(sort data1.csv) <(sort data2.csv)

# 2. Comparer des listes d'utilisateurs sur deux serveurs
diff <(ssh server1 "cat /etc/passwd") <(ssh server2 "cat /etc/passwd")

# 3. Trouver les diff√©rences entre deux requ√™tes SQL
diff <(psql -c "SELECT * FROM users WHERE active=true") \
     <(psql -c "SELECT * FROM users WHERE created_at > '2024-01-01'")

# 4. Combiner des sources de donn√©es
paste <(cut -d',' -f1 file1.csv) <(cut -d',' -f2 file2.csv) > combined.csv

# 5. Compter les lignes uniques entre deux fichiers
comm -23 <(sort -u file1.txt) <(sort -u file2.txt)

# 6. Joindre des donn√©es de deux sources
join <(sort users.csv) <(sort orders.csv)

# 7. Comparer des APIs
diff <(curl -s api1.com/users | jq -S .) \
     <(curl -s api2.com/users | jq -S .)

# 8. Traiter plusieurs sources en parall√®le
while read -r line1 && read -r line2 <&3; do
    echo "File1: $line1, File2: $line2"
done < file1.txt 3< file2.txt

# 9. V√©rifier la coh√©rence de donn√©es
# Emails dans DB vs emails dans fichier
comm -23 \
    <(psql -t -c "SELECT email FROM users" | sort) \
    <(cut -d',' -f2 users.csv | sort)

# 10. Output substitution (inverse)
# √âcrire vers un processus
echo "data" > >(gzip > output.gz)
```

### Cas pratique complet : Pipeline ETL avanc√©

```bash
#!/usr/bin/env bash
#
# Script: etl_pipeline.sh
# Description: Pipeline ETL avanc√© avec fonctions et outils modernes
#

set -euo pipefail

# Source des fonctions utilitaires
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/lib/utils.sh"

# Configuration
readonly DATA_DIR="data"
readonly RAW_DIR="$DATA_DIR/raw"
readonly PROCESSED_DIR="$DATA_DIR/processed"
readonly ARCHIVE_DIR="$DATA_DIR/archive"
readonly PARALLEL_JOBS=4

# Cr√©er les dossiers
mkdir -p "$RAW_DIR" "$PROCESSED_DIR" "$ARCHIVE_DIR"

# Fonction d'extraction
extract_data() {
    log_info "=== EXTRACT Phase ==="

    local api_url="https://api.example.com/data"
    local output_file="$RAW_DIR/raw_data_$(date +%Y%m%d_%H%M%S).json"

    log_info "Fetching data from API..."

    if retry 3 "curl -sf '$api_url' -o '$output_file'"; then
        log_info "Data extracted successfully: $output_file"
        echo "$output_file"
        return 0
    else
        log_error "Failed to extract data"
        return 1
    fi
}

# Fonction de transformation
transform_data() {
    local input_file=$1
    local output_file="$PROCESSED_DIR/$(basename "${input_file%.json}").csv"

    log_info "=== TRANSFORM Phase ==="
    log_info "Input: $input_file"

# Valider le JSON
    if ! jq empty "$input_file" 2>/dev/null; then
        log_error "Invalid JSON file: $input_file"
        return 1
    fi

# Transformer JSON en CSV
    log_info "Converting JSON to CSV..."

    jq -r '
        ["id", "name", "email", "age", "city"],
        (.users[] | [.id, .name, .email, .age, .city])
        | @csv
    ' "$input_file" > "$output_file"

# Valider le r√©sultat
    if validate_file "$output_file" 100; then
        log_info "Data transformed successfully: $output_file"
        echo "$output_file"
        return 0
    else
        log_error "Transformation failed"
        return 1
    fi
}

# Fonction de nettoyage
clean_data() {
    local input_file=$1
    local output_file="${input_file%.csv}_clean.csv"

    log_info "Cleaning data..."

# Supprimer les lignes vides et d√©dupliquer
    sed '/^$/d' "$input_file" | \
        awk '!seen[$0]++' > "$output_file"

    local input_lines=$(wc -l < "$input_file")
    local output_lines=$(wc -l < "$output_file")
    local removed=$((input_lines - output_lines))

    log_info "Removed $removed duplicate/empty lines"
    echo "$output_file"
}

# Fonction d'enrichissement
enrich_data() {
    local input_file=$1
    local output_file="${input_file%.csv}_enriched.csv"

    log_info "Enriching data..."

# Ajouter une colonne avec timestamp
    awk -F',' -v date="$(date +%Y-%m-%d)" '
        NR==1 {print $0 ",processed_date"}
        NR>1 {print $0 "," date}
    ' "$input_file" > "$output_file"

    log_info "Data enriched: $output_file"
    echo "$output_file"
}

# Fonction de chargement
load_data() {
    local input_file=$1

    log_info "=== LOAD Phase ==="

# Simuler un chargement en base de donn√©es
    log_info "Loading data to database..."

    local records=$(tail -n +2 "$input_file" | wc -l)
    log_info "Loaded $records records"

# Archiver le fichier source
    backup_file "$input_file"
    mv "$input_file" "$ARCHIVE_DIR/"

    return 0
}

# Fonction de g√©n√©ration de rapport
generate_report() {
    local processed_files=("$@")

    log_info "=== REPORT Phase ==="

    local report_file="report_$(date +%Y%m%d_%H%M%S).txt"

    {
        echo "ETL Pipeline Report"
        echo "==================="
        echo "Date: $(date)"
        echo ""
        echo "Processed Files:"

        for file in "${processed_files[@]}"; do
            echo "  - $(basename "$file")"
            echo "    Lines: $(wc -l < "$file")"
            echo "    Size: $(du -h "$file" | cut -f1)"
        done

        echo ""
        echo "Summary:"
        echo "  Total files: ${#processed_files[@]}"
        echo "  Total records: $(find "$PROCESSED_DIR" -name "*.csv" -exec wc -l {} + | tail -1 | awk '{print $1}')"
    } > "$report_file"

    log_info "Report generated: $report_file"
}

# Pipeline principal
main() {
    log_info "Starting ETL Pipeline"
    log_info "Parallel jobs: $PARALLEL_JOBS"

    local processed_files=()

# EXTRACT
    if ! raw_file=$(extract_data); then
        log_error "Extraction failed"
        exit 1
    fi

# TRANSFORM
    if ! csv_file=$(transform_data "$raw_file"); then
        log_error "Transformation failed"
        exit 1
    fi

# CLEAN
    if ! clean_file=$(clean_data "$csv_file"); then
        log_error "Cleaning failed"
        exit 1
    fi

# ENRICH
    if ! enriched_file=$(enrich_data "$clean_file"); then
        log_error "Enrichment failed"
        exit 1
    fi

    processed_files+=("$enriched_file")

# LOAD
    if ! load_data "$enriched_file"; then
        log_error "Loading failed"
        exit 1
    fi

# REPORT
    generate_report "${processed_files[@]}"

    log_info "ETL Pipeline completed successfully!"
    exit 0
}

# Ex√©cuter
main "$@"
```

### Exercices pratiques

#### üéØ Exercice 1 : Biblioth√®que de fonctions

Cr√©ez un fichier `lib.sh` avec ces fonctions :

1. `count_lines(file)` : retourne le nombre de lignes
2. `is_csv(file)` : retourne 0 si le fichier est un CSV
3. `backup_with_date(file)` : cr√©e un backup avec timestamp

üí° Voir la solution

```bash
#!/bin/bash
# lib.sh

count_lines() {
    local file=$1
    if [ ! -f "$file" ]; then
        return 1
    fi
    wc -l < "$file"
}

is_csv() {
    local file=$1
    [[ "$file" == *.csv ]]
}

backup_with_date() {
    local file=$1
    local backup="${file}.backup.$(date +%Y%m%d_%H%M%S)"
    cp "$file" "$backup"
    echo "$backup"
}

# Utilisation :
# source lib.sh
# count_lines "data.csv"
# is_csv "file.csv" && echo "It's a CSV"
# backup_with_date "important.csv"
```

#### üéØ Exercice 2 : Processing parall√®le avec xargs

Cr√©ez un script qui :

1. Trouve tous les CSV dans un dossier
2. Compte les lignes de chacun en parall√®le (4 processus)
3. Affiche les r√©sultats tri√©s par taille

üí° Voir la solution

```bash
#!/bin/bash
set -euo pipefail

count_and_print() {
    local file=$1
    local lines=$(wc -l < "$file")
    echo "$lines $file"
}

export -f count_and_print

find . -name "*.csv" | \
    xargs -P 4 -I {} bash -c 'count_and_print "$@"' _ {} | \
    sort -rn
```

#### üí° Points cl√©s √† retenir

- Fonctions : structurer et r√©utiliser le code
- `local` : toujours utiliser pour les variables de fonction
- `return` vs `exit` : fonction vs script
- `xargs` : construire des commandes √† partir de l'entr√©e
- `parallel` : traitement parall√®le avanc√©
- `jq` : manipuler du JSON en ligne de commande
- `<(cmd)` : process substitution pour comparer

#### ‚úÖ Partie 6 termin√©e !

Vous ma√Ætrisez maintenant les fonctions et les outils avanc√©s pour cr√©er des pipelines
de donn√©es sophistiqu√©s. Passez √† la Partie 7 pour les optimisations et bonnes pratiques.

[Partie 7 : Optimisation et Bonnes Pratiques ‚Üí](partie7.md)