{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üü£ ETL avec Python\n",
    "\n",
    "**Badge:** üü£ Expert | ‚è± 60 min | üîë **Concepts cl√©s :** pipeline ETL, extract, transform, load, bonnes pratiques\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre l'architecture d'un pipeline ETL\n",
    "- Impl√©menter les 3 phases : Extract, Transform, Load\n",
    "- Valider les donn√©es avec Pydantic\n",
    "- G√©rer les erreurs et le logging\n",
    "- Appliquer les bonnes pratiques (idempotence, configuration)\n",
    "- Construire un pipeline ETL complet\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- Pandas, Pydantic, requests\n",
    "- Bases de donn√©es (SQLite, DuckDB)\n",
    "- Formats de donn√©es (CSV, Parquet)\n",
    "- Logging Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL : Extract, Transform, Load\n",
    "\n",
    "### Qu'est-ce qu'un ETL ?\n",
    "\n",
    "Un **pipeline ETL** extrait des donn√©es depuis des sources, les transforme, puis les charge dans une destination.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Extract ‚îÇ --> ‚îÇ Transform ‚îÇ --> ‚îÇ Load ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ               ‚îÇ                 ‚îÇ\n",
    "   API           Validate          Database\n",
    "   CSV           Clean             Parquet\n",
    "   DB            Enrich            S3\n",
    "```\n",
    "\n",
    "### Extract (Extraction)\n",
    "- Lire depuis des sources : API, fichiers, bases de donn√©es\n",
    "- G√©rer les erreurs de connexion\n",
    "- Pagination pour les APIs\n",
    "\n",
    "### Transform (Transformation)\n",
    "- Nettoyer : valeurs manquantes, doublons, formats\n",
    "- Valider : types, contraintes (Pydantic)\n",
    "- Enrichir : jointures, calculs, agr√©gations\n",
    "\n",
    "### Load (Chargement)\n",
    "- √âcrire dans la destination : database, data lake, warehouse\n",
    "- Gestion des conflits (upsert)\n",
    "- Partitionnement\n",
    "\n",
    "### ELT vs ETL\n",
    "\n",
    "**ETL** : Transform AVANT Load (sur le serveur local)  \n",
    "**ELT** : Load PUIS Transform (dans le data warehouse)  \n",
    "\n",
    "ELT est devenu populaire avec les data warehouses modernes (BigQuery, Snowflake) car ils ont la puissance de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup : Configuration et logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from pydantic_settings import BaseSettings\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Configuration du logging structur√©\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('ETL')\n",
    "logger.info(\"Logger ETL initialis√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration avec Pydantic Settings\n",
    "class ETLConfig(BaseSettings):\n",
    "    \"\"\"Configuration du pipeline ETL\"\"\"\n",
    "    \n",
    "    # Sources\n",
    "    source_csv_path: str = \"etl_data/sales_raw.csv\"\n",
    "    source_api_url: str = \"https://api.example.com/products\"\n",
    "    \n",
    "    # Destinations\n",
    "    output_parquet_path: str = \"etl_data/sales_clean.parquet\"\n",
    "    output_db_path: str = \"etl_data/analytics.db\"\n",
    "    \n",
    "    # Param√®tres\n",
    "    batch_size: int = 1000\n",
    "    max_retries: int = 3\n",
    "    \n",
    "    # Dead letter queue (erreurs)\n",
    "    error_log_path: str = \"etl_data/errors.jsonl\"\n",
    "    \n",
    "    class Config:\n",
    "        env_prefix = \"ETL_\"\n",
    "\n",
    "config = ETLConfig()\n",
    "logger.info(f\"Configuration charg√©e : {config.model_dump()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract : Extraction depuis sources multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es sources\n",
    "Path('etl_data').mkdir(exist_ok=True)\n",
    "\n",
    "# Source 1 : CSV avec donn√©es brutes (parfois incorrectes)\n",
    "np.random.seed(42)\n",
    "df_raw = pd.DataFrame({\n",
    "    'order_id': range(1, 1001),\n",
    "    'order_date': pd.date_range('2024-01-01', periods=1000, freq='H'),\n",
    "    'customer_id': np.random.randint(1, 200, 1000),\n",
    "    'product_id': np.random.randint(1, 50, 1000),\n",
    "    'quantity': np.random.randint(-2, 10, 1000),  # Certaines n√©gatives !\n",
    "    'unit_price': np.random.uniform(-10, 2000, 1000),  # Certaines n√©gatives !\n",
    "    'status': np.random.choice(['completed', 'pending', 'cancelled', 'invalid', None], 1000)\n",
    "})\n",
    "\n",
    "df_raw.to_csv(config.source_csv_path, index=False)\n",
    "logger.info(f\"Donn√©es source cr√©√©es : {config.source_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extrait les donn√©es d'un fichier CSV\"\"\"\n",
    "    logger.info(f\"Extract: Lecture CSV {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, parse_dates=['order_date'])\n",
    "        logger.info(f\"Extract: {len(df)} lignes extraites\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extract: Erreur lecture CSV - {e}\")\n",
    "        raise\n",
    "\n",
    "# Extraction\n",
    "df_extracted = extract_from_csv(config.source_csv_path)\n",
    "print(\"Aper√ßu des donn√©es extraites :\")\n",
    "print(df_extracted.head())\n",
    "print(f\"\\nTypes : \\n{df_extracted.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_api(api_url: str, max_retries: int = 3) -> List[dict]:\n",
    "    \"\"\"Extrait les donn√©es d'une API avec retry\"\"\"\n",
    "    logger.info(f\"Extract: Appel API {api_url}\")\n",
    "    \n",
    "    # Simulation d'une API (en r√©alit√© on utiliserait requests.get)\n",
    "    # for attempt in range(max_retries):\n",
    "    #     try:\n",
    "    #         response = requests.get(api_url, timeout=10)\n",
    "    #         response.raise_for_status()\n",
    "    #         data = response.json()\n",
    "    #         logger.info(f\"Extract: {len(data)} enregistrements de l'API\")\n",
    "    #         return data\n",
    "    #     except requests.RequestException as e:\n",
    "    #         logger.warning(f\"Extract: Tentative {attempt+1}/{max_retries} √©chou√©e - {e}\")\n",
    "    #         if attempt == max_retries - 1:\n",
    "    #             raise\n",
    "    #         time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    # Donn√©es simul√©es\n",
    "    products = [\n",
    "        {'product_id': i, 'product_name': f'Product_{i}', 'category': np.random.choice(['A', 'B', 'C'])}\n",
    "        for i in range(1, 51)\n",
    "    ]\n",
    "    logger.info(f\"Extract: {len(products)} produits simul√©s\")\n",
    "    return products\n",
    "\n",
    "# Extraction API\n",
    "products_data = extract_from_api(config.source_api_url)\n",
    "df_products = pd.DataFrame(products_data)\n",
    "print(\"\\nProduits extraits de l'API :\")\n",
    "print(df_products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform : Validation avec Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®les Pydantic pour validation\n",
    "class OrderSchema(BaseModel):\n",
    "    \"\"\"Sch√©ma de validation pour une commande\"\"\"\n",
    "    order_id: int = Field(gt=0, description=\"ID de commande positif\")\n",
    "    order_date: datetime\n",
    "    customer_id: int = Field(gt=0)\n",
    "    product_id: int = Field(gt=0)\n",
    "    quantity: int = Field(gt=0, description=\"Quantit√© strictement positive\")\n",
    "    unit_price: float = Field(gt=0, description=\"Prix strictement positif\")\n",
    "    status: str = Field(pattern=r'^(completed|pending|cancelled)$')\n",
    "    \n",
    "    @field_validator('order_date')\n",
    "    @classmethod\n",
    "    def validate_date(cls, v):\n",
    "        # Date ne doit pas √™tre dans le futur\n",
    "        if v > datetime.now():\n",
    "            raise ValueError(\"Date dans le futur\")\n",
    "        # Date pas trop ancienne (> 5 ans)\n",
    "        if v < datetime.now() - timedelta(days=365*5):\n",
    "            raise ValueError(\"Date trop ancienne\")\n",
    "        return v\n",
    "\n",
    "class ProductSchema(BaseModel):\n",
    "    product_id: int = Field(gt=0)\n",
    "    product_name: str = Field(min_length=1)\n",
    "    category: str\n",
    "\n",
    "logger.info(\"Sch√©mas de validation d√©finis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_validate_orders(df: pd.DataFrame) -> tuple[pd.DataFrame, List[dict]]:\n",
    "    \"\"\"Valide et nettoie les commandes\"\"\"\n",
    "    logger.info(f\"Transform: Validation de {len(df)} commandes\")\n",
    "    \n",
    "    valid_records = []\n",
    "    invalid_records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            # Valider avec Pydantic\n",
    "            validated = OrderSchema(**row.to_dict())\n",
    "            valid_records.append(validated.model_dump())\n",
    "        except ValidationError as e:\n",
    "            # Enregistrer l'erreur\n",
    "            error = {\n",
    "                'row_index': int(idx),\n",
    "                'data': row.to_dict(),\n",
    "                'errors': e.errors(),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            invalid_records.append(error)\n",
    "            logger.warning(f\"Transform: Ligne {idx} invalide - {e.error_count()} erreurs\")\n",
    "    \n",
    "    df_valid = pd.DataFrame(valid_records)\n",
    "    logger.info(f\"Transform: {len(df_valid)} valides, {len(invalid_records)} invalides\")\n",
    "    \n",
    "    return df_valid, invalid_records\n",
    "\n",
    "# Transformation et validation\n",
    "df_valid, errors = transform_validate_orders(df_extracted)\n",
    "\n",
    "print(f\"\\n‚úì Validation termin√©e :\")\n",
    "print(f\"  Valides : {len(df_valid)}\")\n",
    "print(f\"  Invalides : {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nPremi√®re erreur (exemple) :\")\n",
    "    print(json.dumps(errors[0], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform : Enrichissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_enrich(df_orders: pd.DataFrame, df_products: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Enrichit les commandes avec les informations produits\"\"\"\n",
    "    logger.info(f\"Transform: Enrichissement de {len(df_orders)} commandes\")\n",
    "    \n",
    "    # Jointure avec produits\n",
    "    df_enriched = df_orders.merge(\n",
    "        df_products,\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculs\n",
    "    df_enriched['total_amount'] = (df_enriched['quantity'] * df_enriched['unit_price']).round(2)\n",
    "    \n",
    "    # Colonnes d√©riv√©es\n",
    "    df_enriched['year'] = df_enriched['order_date'].dt.year\n",
    "    df_enriched['month'] = df_enriched['order_date'].dt.month\n",
    "    df_enriched['day_of_week'] = df_enriched['order_date'].dt.day_name()\n",
    "    df_enriched['is_weekend'] = df_enriched['order_date'].dt.dayofweek >= 5\n",
    "    \n",
    "    # Cat√©gories de montant\n",
    "    df_enriched['amount_category'] = pd.cut(\n",
    "        df_enriched['total_amount'],\n",
    "        bins=[0, 100, 500, 1000, float('inf')],\n",
    "        labels=['Small', 'Medium', 'Large', 'XLarge']\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Transform: Enrichissement termin√© - {len(df_enriched)} lignes\")\n",
    "    return df_enriched\n",
    "\n",
    "# Enrichissement\n",
    "df_enriched = transform_enrich(df_valid, df_products)\n",
    "\n",
    "print(\"\\n‚úì Donn√©es enrichies :\")\n",
    "print(df_enriched.head())\n",
    "print(f\"\\nColonnes : {list(df_enriched.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load : Chargement vers destinations multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_parquet(df: pd.DataFrame, output_path: str, partition_cols: Optional[List[str]] = None):\n",
    "    \"\"\"Charge les donn√©es dans un fichier Parquet\"\"\"\n",
    "    logger.info(f\"Load: √âcriture Parquet {output_path}\")\n",
    "    \n",
    "    try:\n",
    "        if partition_cols:\n",
    "            df.to_parquet(\n",
    "                output_path,\n",
    "                partition_cols=partition_cols,\n",
    "                index=False,\n",
    "                compression='snappy'\n",
    "            )\n",
    "            logger.info(f\"Load: Parquet partitionn√© par {partition_cols}\")\n",
    "        else:\n",
    "            df.to_parquet(\n",
    "                output_path,\n",
    "                index=False,\n",
    "                compression='snappy'\n",
    "            )\n",
    "        \n",
    "        file_size = Path(output_path).stat().st_size / 1024**2 if not partition_cols else 0\n",
    "        logger.info(f\"Load: {len(df)} lignes √©crites ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load: Erreur √©criture Parquet - {e}\")\n",
    "        raise\n",
    "\n",
    "# Chargement Parquet\n",
    "load_to_parquet(df_enriched, config.output_parquet_path)\n",
    "print(f\"\\n‚úì Donn√©es charg√©es : {config.output_parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_database(df: pd.DataFrame, db_path: str, table_name: str, if_exists: str = 'replace'):\n",
    "    \"\"\"Charge les donn√©es dans une base SQLite\"\"\"\n",
    "    logger.info(f\"Load: √âcriture database {db_path} table {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            df.to_sql(\n",
    "                name=table_name,\n",
    "                con=conn,\n",
    "                if_exists=if_exists,\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            # V√©rifier\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            \n",
    "            logger.info(f\"Load: {count} lignes dans {table_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load: Erreur √©criture database - {e}\")\n",
    "        raise\n",
    "\n",
    "# Chargement database\n",
    "load_to_database(df_enriched, config.output_db_path, 'orders_clean')\n",
    "print(f\"‚úì Donn√©es charg√©es : {config.output_db_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_errors_to_dlq(errors: List[dict], error_log_path: str):\n",
    "    \"\"\"Enregistre les erreurs dans une Dead Letter Queue (JSONL)\"\"\"\n",
    "    logger.info(f\"Load: √âcriture erreurs {error_log_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(error_log_path, 'a') as f:\n",
    "            for error in errors:\n",
    "                f.write(json.dumps(error, default=str) + '\\n')\n",
    "        \n",
    "        logger.info(f\"Load: {len(errors)} erreurs enregistr√©es\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load: Erreur √©criture DLQ - {e}\")\n",
    "\n",
    "# Dead Letter Queue\n",
    "if errors:\n",
    "    load_errors_to_dlq(errors, config.error_log_path)\n",
    "    print(f\"‚úì Erreurs enregistr√©es : {config.error_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline ETL complet : Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class ETLMetrics:\n",
    "    \"\"\"M√©triques du pipeline ETL\"\"\"\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    records_extracted: int\n",
    "    records_valid: int\n",
    "    records_invalid: int\n",
    "    records_loaded: int\n",
    "    \n",
    "    @property\n",
    "    def duration_seconds(self) -> float:\n",
    "        return (self.end_time - self.start_time).total_seconds()\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'start_time': self.start_time.isoformat(),\n",
    "            'end_time': self.end_time.isoformat(),\n",
    "            'duration_seconds': self.duration_seconds,\n",
    "            'records_extracted': self.records_extracted,\n",
    "            'records_valid': self.records_valid,\n",
    "            'records_invalid': self.records_invalid,\n",
    "            'records_loaded': self.records_loaded,\n",
    "            'success_rate': round(self.records_valid / self.records_extracted * 100, 2)\n",
    "        }\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"Pipeline ETL complet\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ETLConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('ETLPipeline')\n",
    "    \n",
    "    def run(self) -> ETLMetrics:\n",
    "        \"\"\"Ex√©cute le pipeline ETL complet\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        self.logger.info(\"=\"*70)\n",
    "        self.logger.info(\"D√©marrage du pipeline ETL\")\n",
    "        self.logger.info(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # 1. EXTRACT\n",
    "            self.logger.info(\"PHASE 1: EXTRACT\")\n",
    "            df_orders = extract_from_csv(self.config.source_csv_path)\n",
    "            products_data = extract_from_api(self.config.source_api_url)\n",
    "            df_products = pd.DataFrame(products_data)\n",
    "            records_extracted = len(df_orders)\n",
    "            \n",
    "            # 2. TRANSFORM\n",
    "            self.logger.info(\"PHASE 2: TRANSFORM\")\n",
    "            df_valid, errors = transform_validate_orders(df_orders)\n",
    "            df_enriched = transform_enrich(df_valid, df_products)\n",
    "            \n",
    "            # 3. LOAD\n",
    "            self.logger.info(\"PHASE 3: LOAD\")\n",
    "            load_to_parquet(df_enriched, self.config.output_parquet_path)\n",
    "            load_to_database(df_enriched, self.config.output_db_path, 'orders_clean')\n",
    "            \n",
    "            if errors:\n",
    "                load_errors_to_dlq(errors, self.config.error_log_path)\n",
    "            \n",
    "            # M√©triques\n",
    "            end_time = datetime.now()\n",
    "            metrics = ETLMetrics(\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                records_extracted=records_extracted,\n",
    "                records_valid=len(df_valid),\n",
    "                records_invalid=len(errors),\n",
    "                records_loaded=len(df_enriched)\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\"=\"*70)\n",
    "            self.logger.info(\"Pipeline ETL termin√© avec succ√®s\")\n",
    "            self.logger.info(f\"Dur√©e : {metrics.duration_seconds:.2f}s\")\n",
    "            self.logger.info(\"=\"*70)\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline ETL √©chou√© : {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "# Ex√©cuter le pipeline\n",
    "pipeline = ETLPipeline(config)\n",
    "metrics = pipeline.run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"M√âTRIQUES DU PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "for key, value in metrics.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonnes pratiques\n",
    "\n",
    "### Idempotence\n",
    "\n",
    "Un pipeline **idempotent** produit le m√™me r√©sultat s'il est ex√©cut√© plusieurs fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idempotent_load(df: pd.DataFrame, db_path: str, table_name: str, unique_key: str):\n",
    "    \"\"\"Charge avec upsert (idempotent)\"\"\"\n",
    "    logger.info(f\"Load idempotent: {table_name} avec cl√© {unique_key}\")\n",
    "    \n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        # Cr√©er table si inexistante\n",
    "        df.head(0).to_sql(table_name, conn, if_exists='append', index=False)\n",
    "        \n",
    "        # Supprimer les lignes existantes\n",
    "        existing_ids = tuple(df[unique_key].tolist())\n",
    "        if existing_ids:\n",
    "            placeholders = ','.join(['?'] * len(existing_ids))\n",
    "            conn.execute(f\"DELETE FROM {table_name} WHERE {unique_key} IN ({placeholders})\", existing_ids)\n",
    "        \n",
    "        # Ins√©rer nouvelles lignes\n",
    "        df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "        \n",
    "        logger.info(f\"Load: {len(df)} lignes upserted\")\n",
    "\n",
    "# Test idempotence\n",
    "idempotent_load(df_enriched, config.output_db_path, 'orders_idempotent', 'order_id')\n",
    "print(\"‚úì Load idempotent (peut √™tre rejou√© sans doublon)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion d'erreurs avanc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import time\n",
    "\n",
    "def retry_with_backoff(func: Callable, max_retries: int = 3, backoff_factor: int = 2):\n",
    "    \"\"\"Retry avec exponential backoff\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"√âchec apr√®s {max_retries} tentatives\")\n",
    "                raise\n",
    "            \n",
    "            wait_time = backoff_factor ** attempt\n",
    "            logger.warning(f\"Tentative {attempt+1}/{max_retries} √©chou√©e. Retry dans {wait_time}s\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def unreliable_operation():\n",
    "    # Simulation op√©ration qui √©choue parfois\n",
    "    import random\n",
    "    if random.random() < 0.3:  # 30% √©chec\n",
    "        raise Exception(\"Erreur temporaire\")\n",
    "    return \"Success\"\n",
    "\n",
    "# result = retry_with_backoff(unreliable_operation)\n",
    "print(\"‚úì Retry avec exponential backoff impl√©ment√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests unitaires (exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de test pour les transformations\n",
    "def test_transform_enrich():\n",
    "    \"\"\"Test de la fonction d'enrichissement\"\"\"\n",
    "    # Donn√©es de test\n",
    "    df_orders_test = pd.DataFrame({\n",
    "        'order_id': [1, 2],\n",
    "        'order_date': [datetime(2024, 1, 1), datetime(2024, 1, 2)],\n",
    "        'customer_id': [1, 2],\n",
    "        'product_id': [1, 2],\n",
    "        'quantity': [1, 2],\n",
    "        'unit_price': [100.0, 200.0],\n",
    "        'status': ['completed', 'completed']\n",
    "    })\n",
    "    \n",
    "    df_products_test = pd.DataFrame({\n",
    "        'product_id': [1, 2],\n",
    "        'product_name': ['Product_1', 'Product_2'],\n",
    "        'category': ['A', 'B']\n",
    "    })\n",
    "    \n",
    "    # Transformation\n",
    "    result = transform_enrich(df_orders_test, df_products_test)\n",
    "    \n",
    "    # Assertions\n",
    "    assert len(result) == 2, \"Devrait retourner 2 lignes\"\n",
    "    assert 'total_amount' in result.columns, \"Colonne total_amount manquante\"\n",
    "    assert result['total_amount'].iloc[0] == 100.0, \"Calcul incorrect\"\n",
    "    assert result['total_amount'].iloc[1] == 400.0, \"Calcul incorrect\"\n",
    "    assert 'product_name' in result.columns, \"Jointure √©chou√©e\"\n",
    "    \n",
    "    print(\"‚úì Test transform_enrich passed\")\n",
    "\n",
    "test_transform_enrich()\n",
    "print(\"\\nüí° Tests unitaires essentiels pour la fiabilit√© du pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Orchestration : Vers Airflow / Prefect\n",
    "\n",
    "Pour des pipelines production, utilisez un orchestrateur.\n",
    "\n",
    "### Apache Airflow\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'etl_pipeline',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False\n",
    ") as dag:\n",
    "    \n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract',\n",
    "        python_callable=extract_from_csv\n",
    "    )\n",
    "    \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform',\n",
    "        python_callable=transform_validate_orders\n",
    "    )\n",
    "    \n",
    "    load_task = PythonOperator(\n",
    "        task_id='load',\n",
    "        python_callable=load_to_database\n",
    "    )\n",
    "    \n",
    "    extract_task >> transform_task >> load_task\n",
    "```\n",
    "\n",
    "### Prefect (moderne)\n",
    "```python\n",
    "from prefect import flow, task\n",
    "\n",
    "@task\n",
    "def extract():\n",
    "    return extract_from_csv('data.csv')\n",
    "\n",
    "@task\n",
    "def transform(df):\n",
    "    return transform_validate_orders(df)\n",
    "\n",
    "@task\n",
    "def load(df):\n",
    "    load_to_database(df, 'db.db', 'table')\n",
    "\n",
    "@flow\n",
    "def etl_pipeline():\n",
    "    df = extract()\n",
    "    df_clean, errors = transform(df)\n",
    "    load(df_clean)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pi√®ges courants\n",
    "\n",
    "### 1. Pas de gestion d'erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Pipeline sans gestion d'erreurs\n",
    "# df = pd.read_csv('file.csv')  # Crash si fichier absent\n",
    "# df.to_sql('table', conn)  # Crash si DB inaccessible\n",
    "\n",
    "# ‚úÖ Toujours g√©rer les erreurs\n",
    "try:\n",
    "    df = pd.read_csv('file.csv')\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Fichier introuvable\")\n",
    "    # Fallback ou notification\n",
    "\n",
    "print(\"‚úì G√©rez TOUTES les erreurs possibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pas de logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Pas de logging = impossible de d√©bugger\n",
    "# df = transform(df)\n",
    "\n",
    "# ‚úÖ Logging √† chaque √©tape\n",
    "logger.info(f\"Transform: {len(df)} lignes avant\")\n",
    "# df = transform(df)\n",
    "logger.info(f\"Transform: {len(df)} lignes apr√®s\")\n",
    "\n",
    "print(\"‚úì Le logging est votre meilleur ami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Couplage fort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Couplage fort : code difficilement testable\n",
    "# def etl():\n",
    "#     df = pd.read_csv('hardcoded.csv')\n",
    "#     df.to_sql('hardcoded_table', hardcoded_conn)\n",
    "\n",
    "# ‚úÖ Injection de d√©pendances\n",
    "def etl(source: str, destination: str, conn):\n",
    "    df = pd.read_csv(source)\n",
    "    df.to_sql(destination, conn)\n",
    "\n",
    "print(\"‚úì Utilisez la configuration et l'injection de d√©pendances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-exercices\n",
    "\n",
    "### Exercice 1 : Pipeline avec logging\n",
    "\n",
    "Cr√©ez un mini pipeline qui :  \n",
    "1. Lit un CSV avec 100 lignes  \n",
    "2. Filtre les lignes o√π une colonne > seuil  \n",
    "3. √âcrit le r√©sultat en Parquet  \n",
    "4. Log chaque √©tape avec le nombre de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Validation avec Pydantic\n",
    "\n",
    "1. Cr√©ez un sch√©ma Pydantic pour un utilisateur (nom, email, age)  \n",
    "2. Cr√©ez un DataFrame avec des donn√©es valides et invalides  \n",
    "3. Validez et s√©parez valides/invalides  \n",
    "4. Enregistrez les erreurs en JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Pipeline ETL complet fil rouge\n",
    "\n",
    "Cr√©ez un pipeline ETL complet :  \n",
    "1. Extract : G√©n√©rez 1000 ventes e-commerce  \n",
    "2. Transform : Validez avec Pydantic, enrichissez avec cat√©gories  \n",
    "3. Load : Sauvez en Parquet partitionn√© par mois + SQLite  \n",
    "4. M√©triques : Affichez dur√©e, taux de succ√®s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions des exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 1\n",
    "logger_ex1 = logging.getLogger('Exercise1')\n",
    "\n",
    "# 1. Cr√©er CSV\n",
    "df_ex1 = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'value': np.random.randint(1, 100, 100)\n",
    "})\n",
    "df_ex1.to_csv('etl_data/ex1_input.csv', index=False)\n",
    "logger_ex1.info(f\"CSV cr√©√© : {len(df_ex1)} lignes\")\n",
    "\n",
    "# 2. Lire\n",
    "df_read = pd.read_csv('etl_data/ex1_input.csv')\n",
    "logger_ex1.info(f\"CSV lu : {len(df_read)} lignes\")\n",
    "\n",
    "# 3. Filtrer\n",
    "threshold = 50\n",
    "df_filtered = df_read[df_read['value'] > threshold]\n",
    "logger_ex1.info(f\"Filtrage value > {threshold} : {len(df_filtered)} lignes restantes\")\n",
    "\n",
    "# 4. √âcrire Parquet\n",
    "df_filtered.to_parquet('etl_data/ex1_output.parquet', index=False)\n",
    "logger_ex1.info(f\"Parquet √©crit : {len(df_filtered)} lignes\")\n",
    "\n",
    "print(\"\\n‚úì Exercice 1 termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 2\n",
    "from pydantic import EmailStr\n",
    "\n",
    "class UserSchema(BaseModel):\n",
    "    name: str = Field(min_length=1)\n",
    "    email: EmailStr\n",
    "    age: int = Field(ge=18, le=120)\n",
    "\n",
    "# Donn√©es mixtes\n",
    "df_users = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', '', 'Diana', 'Eve'],\n",
    "    'email': ['alice@test.com', 'invalid-email', 'bob@test.com', 'diana@test.com', 'eve@test.com'],\n",
    "    'age': [25, 30, 17, 150, 22]\n",
    "})\n",
    "\n",
    "valid_users = []\n",
    "invalid_users = []\n",
    "\n",
    "for idx, row in df_users.iterrows():\n",
    "    try:\n",
    "        validated = UserSchema(**row.to_dict())\n",
    "        valid_users.append(validated.model_dump())\n",
    "    except ValidationError as e:\n",
    "        invalid_users.append({\n",
    "            'row': int(idx),\n",
    "            'data': row.to_dict(),\n",
    "            'errors': [err['msg'] for err in e.errors()]\n",
    "        })\n",
    "\n",
    "# Sauver erreurs\n",
    "with open('etl_data/ex2_errors.jsonl', 'w') as f:\n",
    "    for error in invalid_users:\n",
    "        f.write(json.dumps(error) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úì Exercice 2 termin√© :\")\n",
    "print(f\"  Valides : {len(valid_users)}\")\n",
    "print(f\"  Invalides : {len(invalid_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 3 (pipeline complet)\n",
    "class SaleSchema(BaseModel):\n",
    "    sale_id: int = Field(gt=0)\n",
    "    date: datetime\n",
    "    product: str\n",
    "    amount: float = Field(gt=0)\n",
    "\n",
    "# 1. Extract\n",
    "df_sales_ex3 = pd.DataFrame({\n",
    "    'sale_id': range(1, 1001),\n",
    "    'date': pd.date_range('2024-01-01', periods=1000, freq='6h'),\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 1000),\n",
    "    'amount': np.random.uniform(-10, 500, 1000)  # Certaines n√©gatives\n",
    "})\n",
    "\n",
    "logger.info(f\"Extract: {len(df_sales_ex3)} ventes g√©n√©r√©es\")\n",
    "\n",
    "# 2. Transform\n",
    "valid_sales = []\n",
    "invalid_sales = []\n",
    "\n",
    "for idx, row in df_sales_ex3.iterrows():\n",
    "    try:\n",
    "        validated = SaleSchema(**row.to_dict())\n",
    "        valid_sales.append(validated.model_dump())\n",
    "    except ValidationError:\n",
    "        invalid_sales.append(row.to_dict())\n",
    "\n",
    "df_valid_ex3 = pd.DataFrame(valid_sales)\n",
    "df_valid_ex3['month'] = df_valid_ex3['date'].dt.month\n",
    "df_valid_ex3['category'] = df_valid_ex3['product'].map({'A': 'Cat1', 'B': 'Cat2', 'C': 'Cat3'})\n",
    "\n",
    "logger.info(f\"Transform: {len(df_valid_ex3)} valides, {len(invalid_sales)} invalides\")\n",
    "\n",
    "# 3. Load\n",
    "df_valid_ex3.to_parquet(\n",
    "    'etl_data/sales_ex3_partitioned',\n",
    "    partition_cols=['month'],\n",
    "    index=False\n",
    ")\n",
    "logger.info(\"Load: Parquet partitionn√© √©crit\")\n",
    "\n",
    "with sqlite3.connect('etl_data/ex3.db') as conn:\n",
    "    df_valid_ex3.to_sql('sales', conn, if_exists='replace', index=False)\n",
    "logger.info(\"Load: SQLite √©crit\")\n",
    "\n",
    "# 4. M√©triques\n",
    "success_rate = len(df_valid_ex3) / len(df_sales_ex3) * 100\n",
    "print(f\"\\n‚úì Exercice 3 termin√© :\")\n",
    "print(f\"  Extraites : {len(df_sales_ex3)}\")\n",
    "print(f\"  Valides : {len(df_valid_ex3)}\")\n",
    "print(f\"  Taux succ√®s : {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **ETL** = Extract (sources) ‚Üí Transform (validation, enrichissement) ‚Üí Load (destinations)\n",
    "2. **Pydantic** : validation de sch√©ma essentielle pour la qualit√© des donn√©es\n",
    "3. **Logging** : indispensable pour le debugging et le monitoring\n",
    "4. **Gestion d'erreurs** : Dead Letter Queue pour les donn√©es invalides\n",
    "5. **Idempotence** : pipeline peut √™tre rejou√© sans doublon\n",
    "6. **Configuration** : externalisez avec Pydantic Settings\n",
    "7. **M√©triques** : trackez dur√©e, taux de succ√®s, volum√©trie\n",
    "8. **Orchestration** : Airflow, Prefect, Dagster pour production\n",
    "\n",
    "### Checklist pipeline production\n",
    "\n",
    "- [ ] Logging structur√© √† chaque √©tape\n",
    "- [ ] Validation des donn√©es (Pydantic)\n",
    "- [ ] Gestion d'erreurs compl√®te\n",
    "- [ ] Dead Letter Queue pour donn√©es invalides\n",
    "- [ ] Idempotence (upsert)\n",
    "- [ ] Configuration externalis√©e\n",
    "- [ ] Tests unitaires\n",
    "- [ ] M√©triques (dur√©e, success rate)\n",
    "- [ ] Monitoring et alertes\n",
    "- [ ] Documentation\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- Cheatsheet : R√©capitulatif de tous les concepts\n",
    "- Approfondir : Airflow, dbt, Great Expectations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
