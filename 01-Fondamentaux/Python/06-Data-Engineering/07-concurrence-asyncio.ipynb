{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üü£ Concurrence et Asyncio\n",
    "\n",
    "**Badge:** üü£ Expert | ‚è± 60 min | üîë **Concepts cl√©s :** threading, multiprocessing, GIL, asyncio\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre la diff√©rence entre concurrence et parall√©lisme\n",
    "- Ma√Ætriser le GIL (Global Interpreter Lock) et son impact\n",
    "- Utiliser `threading` pour les t√¢ches I/O bound\n",
    "- Utiliser `multiprocessing` pour les t√¢ches CPU bound\n",
    "- D√©couvrir la programmation asynchrone avec `asyncio`\n",
    "- Choisir la bonne approche selon le cas d'usage\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- Compr√©hension des processus et threads\n",
    "- Bases de Python (fonctions, d√©corateurs)\n",
    "- Notion de latence r√©seau et I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concurrence vs Parall√©lisme\n",
    "\n",
    "### D√©finitions\n",
    "\n",
    "**Concurrence** : G√©rer plusieurs t√¢ches en m√™me temps (switching rapide)  \n",
    "- Une seule t√¢che s'ex√©cute √† un instant T\n",
    "- Utile quand les t√¢ches attendent (I/O, r√©seau)\n",
    "- Exemple : serveur web qui g√®re 1000 requ√™tes\n",
    "\n",
    "**Parall√©lisme** : Ex√©cuter plusieurs t√¢ches simultan√©ment (vraiment en parall√®le)  \n",
    "- Plusieurs t√¢ches s'ex√©cutent en m√™me temps\n",
    "- N√©cessite plusieurs CPU cores\n",
    "- Exemple : traiter 4 images en m√™me temps sur 4 cores\n",
    "\n",
    "```\n",
    "Concurrence (1 CPU):\n",
    "Task A: ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë\n",
    "Task B: ‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà\n",
    "\n",
    "Parall√©lisme (2 CPUs):\n",
    "Task A: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "Task B: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Le GIL (Global Interpreter Lock)\n",
    "\n",
    "### Qu'est-ce que le GIL ?\n",
    "\n",
    "Le **GIL** est un verrou qui emp√™che plusieurs threads Python d'ex√©cuter du bytecode Python en m√™me temps.\n",
    "\n",
    "**Cons√©quences** :\n",
    "- ‚úÖ Threading fonctionne bien pour I/O bound (attente r√©seau/disque)\n",
    "- ‚ùå Threading ne parall√©lise PAS les calculs CPU\n",
    "- ‚úÖ Multiprocessing contourne le GIL (processus s√©par√©s)\n",
    "\n",
    "**Note** : Python 3.13+ introduit une option \"free-threaded\" (pas de GIL), mais exp√©rimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "# D√©monstration du GIL\n",
    "def cpu_intensive():\n",
    "    \"\"\"Calcul intensif (CPU bound)\"\"\"\n",
    "    count = 0\n",
    "    for _ in range(10_000_000):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Version s√©quentielle\n",
    "start = time.time()\n",
    "cpu_intensive()\n",
    "cpu_intensive()\n",
    "sequential_time = time.time() - start\n",
    "print(f\"S√©quentiel : {sequential_time:.3f}s\")\n",
    "\n",
    "# Version threading (ne gagne rien √† cause du GIL)\n",
    "start = time.time()\n",
    "t1 = threading.Thread(target=cpu_intensive)\n",
    "t2 = threading.Thread(target=cpu_intensive)\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "threading_time = time.time() - start\n",
    "print(f\"Threading : {threading_time:.3f}s\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Threading est {threading_time/sequential_time:.1f}x plus lent (√† cause du GIL)\")\n",
    "print(\"Pour CPU bound, utilisez multiprocessing !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threading : Pour les t√¢ches I/O bound\n",
    "\n",
    "### Cas d'usage\n",
    "- T√©l√©chargements HTTP\n",
    "- Lecture/√©criture de fichiers\n",
    "- Requ√™tes base de donn√©es\n",
    "- Tout ce qui attend des ressources externes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Simuler un appel API lent\n",
    "def fetch_data(api_id):\n",
    "    \"\"\"Simule un appel API avec latence r√©seau\"\"\"\n",
    "    print(f\"[Thread {threading.current_thread().name}] Fetch API {api_id}...\")\n",
    "    time.sleep(random.uniform(0.5, 2))  # Simulation latence r√©seau\n",
    "    print(f\"[Thread {threading.current_thread().name}] API {api_id} done\")\n",
    "    return f\"Data from API {api_id}\"\n",
    "\n",
    "# Version s√©quentielle\n",
    "print(\"=== VERSION S√âQUENTIELLE ===\")\n",
    "start = time.time()\n",
    "results = []\n",
    "for i in range(5):\n",
    "    results.append(fetch_data(i))\n",
    "seq_time = time.time() - start\n",
    "print(f\"Temps total : {seq_time:.2f}s\\n\")\n",
    "\n",
    "# Version threading\n",
    "print(\"=== VERSION THREADING ===\")\n",
    "start = time.time()\n",
    "threads = []\n",
    "results_threaded = []\n",
    "\n",
    "for i in range(5):\n",
    "    thread = threading.Thread(target=lambda x: results_threaded.append(fetch_data(x)), args=(i,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Attendre que tous les threads se terminent\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "thread_time = time.time() - start\n",
    "print(f\"Temps total : {thread_time:.2f}s\")\n",
    "print(f\"\\n‚úì Gain : {seq_time/thread_time:.1f}x plus rapide avec threading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ThreadPoolExecutor : Interface moderne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Simuler le scraping de plusieurs URLs\n",
    "def scrape_url(url):\n",
    "    \"\"\"Simule le scraping d'une URL\"\"\"\n",
    "    time.sleep(random.uniform(0.3, 1))\n",
    "    return {\"url\": url, \"status\": 200, \"content_length\": random.randint(1000, 50000)}\n",
    "\n",
    "urls = [\n",
    "    \"https://api.example.com/products/1\",\n",
    "    \"https://api.example.com/products/2\",\n",
    "    \"https://api.example.com/products/3\",\n",
    "    \"https://api.example.com/products/4\",\n",
    "    \"https://api.example.com/products/5\",\n",
    "    \"https://api.example.com/products/6\",\n",
    "]\n",
    "\n",
    "print(\"Scraping avec ThreadPoolExecutor...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "# max_workers : nombre de threads dans le pool\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    # Soumettre toutes les t√¢ches\n",
    "    futures = {executor.submit(scrape_url, url): url for url in urls}\n",
    "    \n",
    "    # Traiter les r√©sultats au fur et √† mesure\n",
    "    for future in as_completed(futures):\n",
    "        url = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(f\"‚úì {url}: {result['content_length']} bytes\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {url}: {e}\")\n",
    "\n",
    "print(f\"\\nTemps total : {time.time() - start:.2f}s\")\n",
    "print(f\"Avec {len(urls)} URLs et 3 workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiprocessing : Pour les t√¢ches CPU bound\n",
    "\n",
    "### Cas d'usage\n",
    "- Calculs math√©matiques lourds\n",
    "- Traitement d'images\n",
    "- Machine learning (training)\n",
    "- Parsing de gros fichiers\n",
    "\n",
    "Multiprocessing cr√©e de **vrais processus** s√©par√©s, chacun avec son propre GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "def heavy_computation(n):\n",
    "    \"\"\"Calcul intensif : somme des carr√©s\"\"\"\n",
    "    print(f\"[Process {os.getpid()}] Calcul pour n={n}\")\n",
    "    result = sum(i*i for i in range(n))\n",
    "    return result\n",
    "\n",
    "# Version s√©quentielle\n",
    "print(\"=== VERSION S√âQUENTIELLE ===\")\n",
    "numbers = [5_000_000, 6_000_000, 7_000_000, 8_000_000]\n",
    "\n",
    "start = time.time()\n",
    "results_seq = [heavy_computation(n) for n in numbers]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Temps : {seq_time:.2f}s\\n\")\n",
    "\n",
    "# Version multiprocessing\n",
    "print(\"=== VERSION MULTIPROCESSING ===\")\n",
    "start = time.time()\n",
    "\n",
    "# Pool de processus\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    results_mp = pool.map(heavy_computation, numbers)\n",
    "\n",
    "mp_time = time.time() - start\n",
    "print(f\"Temps : {mp_time:.2f}s\")\n",
    "print(f\"\\n‚úì Gain : {seq_time/mp_time:.1f}x plus rapide avec multiprocessing\")\n",
    "print(f\"CPUs disponibles : {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProcessPoolExecutor : Interface moderne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "\n",
    "def process_chunk(data_chunk):\n",
    "    \"\"\"Traite un chunk de donn√©es (simule un calcul lourd)\"\"\"\n",
    "    # Calculs statistiques\n",
    "    result = {\n",
    "        'mean': np.mean(data_chunk),\n",
    "        'std': np.std(data_chunk),\n",
    "        'sum': np.sum(data_chunk),\n",
    "        'size': len(data_chunk)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Cr√©er un gros dataset\n",
    "full_data = np.random.randn(10_000_000)\n",
    "\n",
    "# Diviser en chunks\n",
    "num_chunks = 4\n",
    "chunks = np.array_split(full_data, num_chunks)\n",
    "\n",
    "print(f\"Dataset : {len(full_data):,} valeurs\")\n",
    "print(f\"Chunks : {num_chunks} de ~{len(chunks[0]):,} valeurs\\n\")\n",
    "\n",
    "# Traitement parall√®le\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(process_chunk, chunks))\n",
    "\n",
    "print(\"R√©sultats par chunk :\")\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"  Chunk {i}: mean={res['mean']:.4f}, std={res['std']:.4f}\")\n",
    "\n",
    "print(f\"\\nTemps : {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Asyncio : Programmation asynchrone\n",
    "\n",
    "### Concepts cl√©s\n",
    "\n",
    "- **async def** : d√©finit une coroutine (fonction asynchrone)\n",
    "- **await** : attend qu'une coroutine se termine\n",
    "- **Event loop** : orchestre l'ex√©cution des coroutines\n",
    "- **asyncio.gather()** : ex√©cute plusieurs coroutines en parall√®le\n",
    "\n",
    "**Avantages** :\n",
    "- Tr√®s efficace pour I/O bound (milliers de connexions)\n",
    "- Moins de overhead que threading\n",
    "- Code plus lisible (pas de callbacks)\n",
    "\n",
    "**Inconv√©nients** :\n",
    "- N√©cessite des biblioth√®ques async (aiohttp, asyncpg, etc.)\n",
    "- Courbe d'apprentissage\n",
    "- Ne r√©sout PAS les probl√®mes CPU bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# D√©finir une coroutine\n",
    "async def fetch_data_async(api_id):\n",
    "    \"\"\"Simule un appel API asynchrone\"\"\"\n",
    "    print(f\"[Coroutine] Fetch API {api_id}...\")\n",
    "    await asyncio.sleep(random.uniform(0.5, 1.5))  # await au lieu de time.sleep\n",
    "    print(f\"[Coroutine] API {api_id} done\")\n",
    "    return f\"Data from API {api_id}\"\n",
    "\n",
    "# Ex√©cuter plusieurs coroutines en parall√®le\n",
    "async def main():\n",
    "    print(\"=== VERSION ASYNCIO ===\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Cr√©er des t√¢ches\n",
    "    tasks = [fetch_data_async(i) for i in range(5)]\n",
    "    \n",
    "    # Ex√©cuter en parall√®le avec gather\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(f\"\\nTemps total : {time.time() - start:.2f}s\")\n",
    "    print(f\"R√©sultats : {len(results)} APIs fetched\")\n",
    "    return results\n",
    "\n",
    "# Ex√©cuter l'event loop\n",
    "results = await main()  # Dans Jupyter, utilisez 'await' directement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asyncio avec aiohttp : Requ√™tes HTTP asynchrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install aiohttp\n",
    "# import aiohttp\n",
    "\n",
    "# Exemple (n√©cessite aiohttp install√©)\n",
    "async def fetch_url_async(session, url):\n",
    "    \"\"\"Fetch une URL de mani√®re asynchrone\"\"\"\n",
    "    # async with session.get(url) as response:\n",
    "    #     return await response.text()\n",
    "    \n",
    "    # Simulation sans aiohttp\n",
    "    await asyncio.sleep(0.5)\n",
    "    return f\"Content from {url}\"\n",
    "\n",
    "async def download_all(urls):\n",
    "    \"\"\"T√©l√©charge plusieurs URLs en parall√®le\"\"\"\n",
    "    # async with aiohttp.ClientSession() as session:\n",
    "    #     tasks = [fetch_url_async(session, url) for url in urls]\n",
    "    #     return await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Simulation\n",
    "    tasks = [fetch_url_async(None, url) for url in urls]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Test\n",
    "urls = [\n",
    "    \"https://api.example.com/endpoint1\",\n",
    "    \"https://api.example.com/endpoint2\",\n",
    "    \"https://api.example.com/endpoint3\",\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "results = await download_all(urls)\n",
    "print(f\"‚úì {len(results)} URLs t√©l√©charg√©es en {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"\\nüí° Installer aiohttp pour de vraies requ√™tes HTTP async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison : Quand utiliser quoi ?\n",
    "\n",
    "| Type de t√¢che | Recommandation | Raison |\n",
    "|---------------|----------------|--------|\n",
    "| **I/O bound** (r√©seau, API, fichiers) | `asyncio` ou `threading` | Peu d'overhead, GIL n'est pas un probl√®me |\n",
    "| **CPU bound** (calculs, parsing) | `multiprocessing` | Contourne le GIL, utilise tous les cores |\n",
    "| **I/O + CPU mixte** | `multiprocessing` + `asyncio` | Processus parall√®les avec I/O async dans chacun |\n",
    "| **Scraping web simple** | `threading` + `requests` | Simple, efficace |\n",
    "| **Scraping web massif** | `asyncio` + `aiohttp` | Milliers de connexions simultan√©es |\n",
    "| **Data processing** | `multiprocessing` + `pandas` | Traiter des chunks en parall√®le |\n",
    "\n",
    "### R√®gle simple\n",
    "\n",
    "```python\n",
    "if tache_attend_beaucoup:  # I/O, r√©seau\n",
    "    use(asyncio)  # ou threading\n",
    "elif tache_calcule_beaucoup:  # CPU\n",
    "    use(multiprocessing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cas data engineering : Traitement parall√®le de fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "# Cr√©er plusieurs fichiers CSV √† traiter\n",
    "def create_sample_files(num_files=4):\n",
    "    \"\"\"Cr√©e des fichiers CSV de test\"\"\"\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        df = pd.DataFrame({\n",
    "            'order_id': range(i*1000, (i+1)*1000),\n",
    "            'amount': np.random.uniform(10, 1000, 1000),\n",
    "            'customer_id': np.random.randint(1, 100, 1000),\n",
    "            'date': pd.date_range('2024-01-01', periods=1000, freq='H')\n",
    "        })\n",
    "        df.to_csv(f'data/orders_{i}.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úì {num_files} fichiers cr√©√©s dans ./data/\")\n",
    "\n",
    "def process_file(filepath):\n",
    "    \"\"\"Traite un fichier CSV (calculs statistiques)\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Simule un traitement\n",
    "    summary = {\n",
    "        'file': filepath.name,\n",
    "        'rows': len(df),\n",
    "        'total_amount': df['amount'].sum(),\n",
    "        'avg_amount': df['amount'].mean(),\n",
    "        'unique_customers': df['customer_id'].nunique()\n",
    "    }\n",
    "    \n",
    "    print(f\"  Processed: {filepath.name}\")\n",
    "    return summary\n",
    "\n",
    "# Cr√©er les fichiers\n",
    "create_sample_files(4)\n",
    "\n",
    "# Traiter en parall√®le\n",
    "files = list(Path('data').glob('orders_*.csv'))\n",
    "print(f\"\\nTraitement de {len(files)} fichiers en parall√®le...\")\n",
    "\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(process_file, files))\n",
    "\n",
    "# Consolider les r√©sultats\n",
    "df_summary = pd.DataFrame(results)\n",
    "print(f\"\\n‚úì Traitement termin√© en {time.time() - start:.2f}s\")\n",
    "print(\"\\nR√©sum√© :\")\n",
    "print(df_summary)\n",
    "\n",
    "print(f\"\\nTotal : {df_summary['total_amount'].sum():.2f}‚Ç¨\")\n",
    "print(f\"Clients uniques : {df_summary['unique_customers'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pattern avanc√© : Producteur-Consommateur avec Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "# Queue thread-safe pour communication entre threads\n",
    "task_queue = queue.Queue()\n",
    "result_queue = queue.Queue()\n",
    "\n",
    "def producer(n_tasks):\n",
    "    \"\"\"Producteur : g√©n√®re des t√¢ches\"\"\"\n",
    "    print(f\"[Producer] G√©n√©ration de {n_tasks} t√¢ches\")\n",
    "    for i in range(n_tasks):\n",
    "        task_queue.put(i)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Signal de fin\n",
    "    task_queue.put(None)\n",
    "    print(\"[Producer] Termin√©\")\n",
    "\n",
    "def consumer(consumer_id):\n",
    "    \"\"\"Consommateur : traite les t√¢ches\"\"\"\n",
    "    while True:\n",
    "        task = task_queue.get()\n",
    "        \n",
    "        if task is None:\n",
    "            task_queue.put(None)  # Propager le signal aux autres consommateurs\n",
    "            break\n",
    "        \n",
    "        # Traiter la t√¢che\n",
    "        result = task * task\n",
    "        result_queue.put((task, result))\n",
    "        print(f\"[Consumer {consumer_id}] Task {task} ‚Üí {result}\")\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        task_queue.task_done()\n",
    "    \n",
    "    print(f\"[Consumer {consumer_id}] Termin√©\")\n",
    "\n",
    "# Lancer producteur et consommateurs\n",
    "producer_thread = threading.Thread(target=producer, args=(10,))\n",
    "consumer_threads = [\n",
    "    threading.Thread(target=consumer, args=(i,))\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "producer_thread.start()\n",
    "for ct in consumer_threads:\n",
    "    ct.start()\n",
    "\n",
    "# Attendre la fin\n",
    "producer_thread.join()\n",
    "for ct in consumer_threads:\n",
    "    ct.join()\n",
    "\n",
    "# R√©cup√©rer les r√©sultats\n",
    "results = []\n",
    "while not result_queue.empty():\n",
    "    results.append(result_queue.get())\n",
    "\n",
    "print(f\"\\n‚úì {len(results)} r√©sultats collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pi√®ges courants\n",
    "\n",
    "### 1. Race conditions (acc√®s concurrent √† une variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Race condition\n",
    "counter = 0\n",
    "\n",
    "def increment_unsafe():\n",
    "    global counter\n",
    "    for _ in range(100000):\n",
    "        counter += 1  # Non atomique !\n",
    "\n",
    "threads = [threading.Thread(target=increment_unsafe) for _ in range(5)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(f\"Counter (unsafe) : {counter} (devrait √™tre 500000)\")\n",
    "\n",
    "# ‚úÖ Solution : Lock\n",
    "counter_safe = 0\n",
    "lock = threading.Lock()\n",
    "\n",
    "def increment_safe():\n",
    "    global counter_safe\n",
    "    for _ in range(100000):\n",
    "        with lock:  # Section critique\n",
    "            counter_safe += 1\n",
    "\n",
    "threads = [threading.Thread(target=increment_safe) for _ in range(5)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(f\"Counter (safe) : {counter_safe} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deadlock (interblocage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Deadlock potentiel\n",
    "lock1 = threading.Lock()\n",
    "lock2 = threading.Lock()\n",
    "\n",
    "def task_a():\n",
    "    with lock1:\n",
    "        time.sleep(0.1)\n",
    "        # with lock2:  # D√©commenter pour deadlock\n",
    "        #     pass\n",
    "        pass\n",
    "\n",
    "def task_b():\n",
    "    with lock2:\n",
    "        time.sleep(0.1)\n",
    "        # with lock1:  # D√©commenter pour deadlock\n",
    "        #     pass\n",
    "        pass\n",
    "\n",
    "# ‚úÖ Solution : toujours acqu√©rir les locks dans le m√™me ordre\n",
    "print(\"‚úì Pour √©viter deadlock : ordre fixe des locks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confondre async et threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå time.sleep() dans une coroutine async bloque l'event loop\n",
    "async def bad_async():\n",
    "    # time.sleep(1)  # ‚ùå Bloque TOUT\n",
    "    await asyncio.sleep(1)  # ‚úÖ Lib√®re l'event loop\n",
    "\n",
    "print(\"‚úì Dans async : utilisez await asyncio.sleep(), pas time.sleep()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Croire que async acc√©l√®re le CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Async ne r√©sout PAS les probl√®mes CPU bound\n",
    "async def cpu_task():\n",
    "    result = sum(i*i for i in range(10_000_000))\n",
    "    return result\n",
    "\n",
    "# M√™me avec asyncio.gather, c'est s√©quentiel (GIL)\n",
    "# Pour CPU bound ‚Üí multiprocessing !\n",
    "\n",
    "print(\"‚úì Async = I/O bound uniquement\")\n",
    "print(\"‚úì CPU bound = multiprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-exercices\n",
    "\n",
    "### Exercice 1 : T√©l√©chargement parall√®le\n",
    "\n",
    "Simulez le t√©l√©chargement de 10 fichiers (sleep random 0.5-2s).  \n",
    "Comparez le temps s√©quentiel vs threading vs asyncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Traitement CPU avec multiprocessing\n",
    "\n",
    "Cr√©ez une fonction qui calcule la somme des carr√©s jusqu'√† N.  \n",
    "Traitez [1M, 2M, 3M, 4M, 5M] avec multiprocessing et comparez avec s√©quentiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Pipeline ETL parall√®le\n",
    "\n",
    "Cr√©ez 5 fichiers CSV avec des donn√©es al√©atoires.  \n",
    "Traitez-les en parall√®le avec ProcessPoolExecutor :  \n",
    "- Lisez chaque fichier  \n",
    "- Appliquez une transformation (ex: multiplier une colonne par 2)  \n",
    "- Sauvegardez dans un nouveau fichier  \n",
    "Mesurez le temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions des exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 1\n",
    "def download_file(file_id):\n",
    "    time.sleep(random.uniform(0.5, 2))\n",
    "    return f\"file_{file_id}.dat\"\n",
    "\n",
    "async def download_file_async(file_id):\n",
    "    await asyncio.sleep(random.uniform(0.5, 2))\n",
    "    return f\"file_{file_id}.dat\"\n",
    "\n",
    "file_ids = list(range(10))\n",
    "\n",
    "# S√©quentiel\n",
    "print(\"=== S√âQUENTIEL ===\")\n",
    "start = time.time()\n",
    "results_seq = [download_file(fid) for fid in file_ids]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Temps : {seq_time:.2f}s\\n\")\n",
    "\n",
    "# Threading\n",
    "print(\"=== THREADING ===\")\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results_thread = list(executor.map(download_file, file_ids))\n",
    "thread_time = time.time() - start\n",
    "print(f\"Temps : {thread_time:.2f}s\\n\")\n",
    "\n",
    "# Asyncio\n",
    "print(\"=== ASYNCIO ===\")\n",
    "start = time.time()\n",
    "results_async = await asyncio.gather(*[download_file_async(fid) for fid in file_ids])\n",
    "async_time = time.time() - start\n",
    "print(f\"Temps : {async_time:.2f}s\\n\")\n",
    "\n",
    "print(\"COMPARAISON :\")\n",
    "print(f\"  S√©quentiel : {seq_time:.2f}s (baseline)\")\n",
    "print(f\"  Threading  : {thread_time:.2f}s (gain {seq_time/thread_time:.1f}x)\")\n",
    "print(f\"  Asyncio    : {async_time:.2f}s (gain {seq_time/async_time:.1f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 2\n",
    "def sum_of_squares(n):\n",
    "    return sum(i*i for i in range(n))\n",
    "\n",
    "numbers = [1_000_000, 2_000_000, 3_000_000, 4_000_000, 5_000_000]\n",
    "\n",
    "# S√©quentiel\n",
    "print(\"=== S√âQUENTIEL ===\")\n",
    "start = time.time()\n",
    "results_seq = [sum_of_squares(n) for n in numbers]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Temps : {seq_time:.2f}s\\n\")\n",
    "\n",
    "# Multiprocessing\n",
    "print(\"=== MULTIPROCESSING ===\")\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    results_mp = list(executor.map(sum_of_squares, numbers))\n",
    "mp_time = time.time() - start\n",
    "print(f\"Temps : {mp_time:.2f}s\\n\")\n",
    "\n",
    "print(f\"‚úì Gain : {seq_time/mp_time:.1f}x plus rapide avec multiprocessing\")\n",
    "print(f\"CPU cores : {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 3\n",
    "# 1. Cr√©er fichiers\n",
    "Path('input').mkdir(exist_ok=True)\n",
    "Path('output').mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(i*100, (i+1)*100),\n",
    "        'value': np.random.randn(100),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "    })\n",
    "    df.to_csv(f'input/data_{i}.csv', index=False)\n",
    "\n",
    "print(\"‚úì 5 fichiers cr√©√©s\")\n",
    "\n",
    "# 2. Fonction de traitement\n",
    "def transform_file(input_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Transformation\n",
    "    df['value'] = df['value'] * 2\n",
    "    df['processed'] = True\n",
    "    \n",
    "    # Sauvegarder\n",
    "    output_path = Path('output') / input_path.name\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return output_path.name\n",
    "\n",
    "# 3. Traiter en parall√®le\n",
    "input_files = list(Path('input').glob('data_*.csv'))\n",
    "\n",
    "print(f\"\\nTraitement de {len(input_files)} fichiers...\")\n",
    "start = time.time()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=3) as executor:\n",
    "    output_files = list(executor.map(transform_file, input_files))\n",
    "\n",
    "print(f\"‚úì Traitement termin√© en {time.time() - start:.2f}s\")\n",
    "print(f\"Fichiers g√©n√©r√©s : {output_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **Concurrence** : g√©rer plusieurs t√¢ches (switching) vs **Parall√©lisme** : ex√©cuter simultan√©ment\n",
    "2. **GIL** : emp√™che le vrai parall√©lisme des threads Python pour CPU bound\n",
    "3. **threading** : id√©al pour I/O bound (r√©seau, fichiers, API)\n",
    "4. **multiprocessing** : obligatoire pour CPU bound (calculs, traitement)\n",
    "5. **asyncio** : tr√®s efficace pour I/O bound avec milliers de connexions\n",
    "6. **ThreadPoolExecutor / ProcessPoolExecutor** : interfaces modernes et simples\n",
    "7. **Race conditions** : utilisez des Lock pour prot√©ger les ressources partag√©es\n",
    "\n",
    "### Table de d√©cision\n",
    "\n",
    "| T√¢che | Solution | Biblioth√®que |\n",
    "|-------|----------|-------------|\n",
    "| T√©l√©chargement API | asyncio + aiohttp | aiohttp |\n",
    "| Scraping simple | threading + requests | requests |\n",
    "| Calculs lourds | multiprocessing | concurrent.futures |\n",
    "| Traitement fichiers | multiprocessing + pandas | pandas |\n",
    "| Serveur web | asyncio | FastAPI, aiohttp |\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- Notebook suivant : **Formats de donn√©es**\n",
    "- Approfondir : asyncio patterns, distributed computing (Celery, Ray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
