{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ DuckDB : SQLite pour l'analytique\n",
    "\n",
    "**Badge:** üî¥ Avanc√© | ‚è± 60 min | üîë **Concepts cl√©s :** DuckDB, SQL sur fichiers, Parquet, int√©gration Pandas\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- D√©couvrir DuckDB comme alternative analytique √† SQLite\n",
    "- Requ√™ter directement des fichiers (CSV, Parquet, JSON) sans import\n",
    "- Int√©grer DuckDB avec Pandas de mani√®re transparente\n",
    "- Utiliser des fonctions analytiques avanc√©es (WINDOW, PIVOT)\n",
    "- Exploiter les performances pour gros volumes\n",
    "- Lire depuis des sources distantes (S3)\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- SQL (SELECT, JOIN, GROUP BY)\n",
    "- Pandas pour la manipulation de donn√©es\n",
    "- Connaissance des formats CSV et Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Qu'est-ce que DuckDB ?\n",
    "\n",
    "**DuckDB** est une base de donn√©es embarqu√©e optimis√©e pour l'analytique (OLAP).\n",
    "\n",
    "### DuckDB vs SQLite\n",
    "\n",
    "| Feature | SQLite | DuckDB |\n",
    "|---------|--------|--------|\n",
    "| Type | OLTP (transactions) | OLAP (analytique) |\n",
    "| Performance | ‚úì Rapide pour INSERT/UPDATE | ‚úì‚úì‚úì Ultra-rapide pour SELECT/GROUP BY |\n",
    "| Format colonnaire | ‚ùå | ‚úÖ |\n",
    "| SQL sur fichiers | ‚ùå | ‚úÖ (CSV, Parquet, JSON) |\n",
    "| Fonctions analytiques | Basiques | Avanc√©es (WINDOW, PIVOT) |\n",
    "| Vectorisation | ‚ùå | ‚úÖ |\n",
    "| Int√©gration Pandas | Manuelle | Native |\n",
    "\n",
    "**Motto** : \"SQLite pour l'analytique\"\n",
    "\n",
    "### Cas d'usage\n",
    "- Exploration rapide de donn√©es locales\n",
    "- Prototypage avant passage √† BigQuery/Snowflake\n",
    "- Data science sur laptop\n",
    "- ETL local avec SQL\n",
    "- Alternative √† Spark pour volumes moyens (< 100 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install duckdb\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(f\"DuckDB version : {duckdb.__version__}\")\n",
    "print(\"‚úì DuckDB pr√™t √† l'emploi !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Premiers pas : SQL directement sur fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es de test\n",
    "Path('duckdb_data').mkdir(exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "df_sales = pd.DataFrame({\n",
    "    'order_id': range(1, 10001),\n",
    "    'order_date': pd.date_range('2024-01-01', periods=10000, freq='5min'),\n",
    "    'customer_id': np.random.randint(1, 1000, 10000),\n",
    "    'product_name': np.random.choice(['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Monitor'], 10000),\n",
    "    'category': np.random.choice(['Electronics', 'Accessories'], 10000),\n",
    "    'quantity': np.random.randint(1, 5, 10000),\n",
    "    'unit_price': np.random.uniform(50, 2000, 10000).round(2)\n",
    "})\n",
    "\n",
    "df_sales['total_amount'] = (df_sales['quantity'] * df_sales['unit_price']).round(2)\n",
    "\n",
    "# Sauvegarder en CSV et Parquet\n",
    "df_sales.to_csv('duckdb_data/sales.csv', index=False)\n",
    "df_sales.to_parquet('duckdb_data/sales.parquet', index=False)\n",
    "\n",
    "print(f\"‚úì {len(df_sales):,} ventes cr√©√©es\")\n",
    "print(f\"  CSV : duckdb_data/sales.csv\")\n",
    "print(f\"  Parquet : duckdb_data/sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL directement sur un fichier CSV - SANS IMPORT !\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM 'duckdb_data/sales.csv'\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Requ√™te SQL directe sur CSV :\")\n",
    "print(result)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_result = result.df()\n",
    "print(f\"\\nType de r√©sultat : {type(df_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL sur Parquet - encore plus rapide\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_name,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_order_value\n",
    "    FROM 'duckdb_data/sales.parquet'\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Analyse sur Parquet :\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Int√©gration avec Pandas : Transparent et puissant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requ√™ter directement un DataFrame Pandas\n",
    "# DuckDB voit automatiquement les DataFrames dans l'environnement\n",
    "\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        product_name,\n",
    "        COUNT(*) as sales_count,\n",
    "        SUM(total_amount) as revenue\n",
    "    FROM df_sales\n",
    "    GROUP BY category, product_name\n",
    "    ORDER BY revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Requ√™te SQL directe sur un DataFrame Pandas :\")\n",
    "print(result)\n",
    "\n",
    "# R√©cup√©rer comme DataFrame\n",
    "df_top_products = result.df()\n",
    "print(f\"\\n‚úì R√©sultat converti en DataFrame : {len(df_top_products)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntaxe alternative : query sur une relation\n",
    "df_result = duckdb.query(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', order_date) as day,\n",
    "        COUNT(*) as orders,\n",
    "        SUM(total_amount) as daily_revenue\n",
    "    FROM df_sales\n",
    "    GROUP BY day\n",
    "    ORDER BY day\n",
    "    LIMIT 7\n",
    "\"\"\").to_df()\n",
    "\n",
    "print(\"‚úì Ventes quotidiennes (7 premiers jours) :\")\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connexion persistante (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une base de donn√©es persistante (fichier)\n",
    "conn = duckdb.connect('duckdb_data/analytics.duckdb')\n",
    "\n",
    "# Cr√©er une table depuis un fichier Parquet\n",
    "conn.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE sales AS \n",
    "    SELECT * FROM 'duckdb_data/sales.parquet'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Table 'sales' cr√©√©e dans analytics.duckdb\")\n",
    "\n",
    "# Requ√™te sur la table\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT COUNT(*) as total_orders,\n",
    "           SUM(total_amount) as total_revenue\n",
    "    FROM sales\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"\\nStatistiques :\")\n",
    "print(f\"  Total commandes : {result[0]:,}\")\n",
    "print(f\"  Revenu total : {result[1]:,.2f}‚Ç¨\")\n",
    "\n",
    "# Fermer la connexion\n",
    "conn.close()\n",
    "print(\"\\n‚úì Connexion ferm√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fonctions analytiques avanc√©es\n",
    "\n",
    "### WINDOW functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de fen√™tre (WINDOW)\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_date,\n",
    "        total_amount,\n",
    "        -- Rang par montant\n",
    "        ROW_NUMBER() OVER (ORDER BY total_amount DESC) as rank_by_amount,\n",
    "        -- Cumul par client\n",
    "        SUM(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as cumulative_spend,\n",
    "        -- Moyenne mobile sur 3 commandes\n",
    "        AVG(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date \n",
    "                                ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3\n",
    "    FROM df_sales\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Fonctions de fen√™tre :\")\n",
    "print(result.df().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIVOT : Transformer lignes en colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIVOT : cr√©er une matrice cat√©gorie x produit\n",
    "result = duckdb.sql(\"\"\"\n",
    "    PIVOT (\n",
    "        SELECT category, product_name, SUM(quantity) as total_qty\n",
    "        FROM df_sales\n",
    "        GROUP BY category, product_name\n",
    "    )\n",
    "    ON product_name\n",
    "    USING SUM(total_qty)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Tableau crois√© dynamique (PIVOT) :\")\n",
    "print(result.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNPIVOT : Transformer colonnes en lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame wide\n",
    "df_wide = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Smartphone', 'Tablet'],\n",
    "    'Q1': [100, 150, 80],\n",
    "    'Q2': [120, 160, 90],\n",
    "    'Q3': [110, 170, 85],\n",
    "    'Q4': [130, 180, 95]\n",
    "})\n",
    "\n",
    "print(\"DataFrame wide :\")\n",
    "print(df_wide)\n",
    "\n",
    "# UNPIVOT : transformer en format long\n",
    "result = duckdb.sql(\"\"\"\n",
    "    UNPIVOT df_wide\n",
    "    ON Q1, Q2, Q3, Q4\n",
    "    INTO\n",
    "        NAME quarter\n",
    "        VALUE sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úì Apr√®s UNPIVOT (format long) :\")\n",
    "print(result.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agr√©gations et statistiques avanc√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques avanc√©es\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_name,\n",
    "        COUNT(*) as count,\n",
    "        AVG(total_amount) as mean,\n",
    "        STDDEV(total_amount) as std,\n",
    "        MIN(total_amount) as min,\n",
    "        QUANTILE_CONT(total_amount, 0.25) as q25,\n",
    "        MEDIAN(total_amount) as median,\n",
    "        QUANTILE_CONT(total_amount, 0.75) as q75,\n",
    "        MAX(total_amount) as max\n",
    "    FROM df_sales\n",
    "    GROUP BY product_name\n",
    "    ORDER BY mean DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Statistiques descriptives par produit :\")\n",
    "print(result.df().round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP BY avec ROLLUP (sous-totaux)\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        product_name,\n",
    "        COUNT(*) as orders,\n",
    "        SUM(total_amount) as revenue\n",
    "    FROM df_sales\n",
    "    GROUP BY ROLLUP(category, product_name)\n",
    "    ORDER BY category NULLS LAST, product_name NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì GROUP BY avec ROLLUP (totaux et sous-totaux) :\")\n",
    "print(result.df().head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Jointures et requ√™tes complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une table de clients\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': range(1, 1001),\n",
    "    'customer_name': [f'Customer_{i}' for i in range(1, 1001)],\n",
    "    'country': np.random.choice(['France', 'USA', 'UK', 'Germany'], 1000),\n",
    "    'segment': np.random.choice(['Premium', 'Standard', 'Basic'], 1000)\n",
    "})\n",
    "\n",
    "df_customers.to_parquet('duckdb_data/customers.parquet', index=False)\n",
    "print(\"‚úì Table customers cr√©√©e\")\n",
    "\n",
    "# Jointure entre fichiers\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.segment,\n",
    "        c.country,\n",
    "        COUNT(DISTINCT s.customer_id) as customers,\n",
    "        COUNT(s.order_id) as orders,\n",
    "        SUM(s.total_amount) as revenue,\n",
    "        AVG(s.total_amount) as avg_order_value\n",
    "    FROM 'duckdb_data/sales.parquet' s\n",
    "    JOIN 'duckdb_data/customers.parquet' c\n",
    "        ON s.customer_id = c.customer_id\n",
    "    GROUP BY c.segment, c.country\n",
    "    ORDER BY revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úì Analyse par segment et pays (jointure entre 2 fichiers) :\")\n",
    "print(result.df().round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export : COPY TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter le r√©sultat d'une requ√™te en Parquet\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            DATE_TRUNC('day', order_date) as day,\n",
    "            category,\n",
    "            COUNT(*) as orders,\n",
    "            SUM(total_amount) as revenue\n",
    "        FROM df_sales\n",
    "        GROUP BY day, category\n",
    "    ) TO 'duckdb_data/daily_sales.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì R√©sultat export√© : duckdb_data/daily_sales.parquet\")\n",
    "\n",
    "# Exporter en CSV\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT * FROM df_sales WHERE total_amount > 5000\n",
    "    ) TO 'duckdb_data/high_value_orders.csv' (HEADER, DELIMITER ',')\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Commandes > 5000‚Ç¨ export√©es en CSV\")\n",
    "\n",
    "# V√©rifier\n",
    "df_exported = pd.read_parquet('duckdb_data/daily_sales.parquet')\n",
    "print(f\"\\n‚úì Fichier Parquet contient {len(df_exported)} lignes\")\n",
    "print(df_exported.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance : DuckDB vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un dataset plus gros pour benchmark\n",
    "n = 500_000\n",
    "df_large = pd.DataFrame({\n",
    "    'id': range(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n),\n",
    "    'value': np.random.randn(n),\n",
    "    'amount': np.random.uniform(1, 1000, n)\n",
    "})\n",
    "\n",
    "print(f\"Dataset : {len(df_large):,} lignes\")\n",
    "print(f\"M√©moire : {df_large.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "# Benchmark 1 : GROUP BY avec agr√©gations\n",
    "print(\"Benchmark : GROUP BY avec statistiques\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pandas\n",
    "start = time.time()\n",
    "result_pandas = df_large.groupby('category').agg({\n",
    "    'value': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'amount': ['sum', 'mean']\n",
    "})\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "print(f\"Pandas : {pandas_time:.3f}s\")\n",
    "\n",
    "# DuckDB\n",
    "start = time.time()\n",
    "result_duckdb = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(value) as value_count,\n",
    "        AVG(value) as value_mean,\n",
    "        STDDEV(value) as value_std,\n",
    "        MIN(value) as value_min,\n",
    "        MAX(value) as value_max,\n",
    "        SUM(amount) as amount_sum,\n",
    "        AVG(amount) as amount_mean\n",
    "    FROM df_large\n",
    "    GROUP BY category\n",
    "\"\"\").df()\n",
    "duckdb_time = time.time() - start\n",
    "\n",
    "print(f\"DuckDB : {duckdb_time:.3f}s\")\n",
    "print(f\"\\n‚úì DuckDB est {pandas_time / duckdb_time:.1f}x plus rapide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 2 : Lecture depuis Parquet\n",
    "df_large.to_parquet('duckdb_data/large.parquet', index=False)\n",
    "\n",
    "print(\"\\nBenchmark : Lecture Parquet + filtrage\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Pandas\n",
    "start = time.time()\n",
    "df_pandas = pd.read_parquet('duckdb_data/large.parquet')\n",
    "df_filtered_pandas = df_pandas[df_pandas['amount'] > 500].groupby('category')['amount'].sum()\n",
    "pandas_time2 = time.time() - start\n",
    "\n",
    "print(f\"Pandas : {pandas_time2:.3f}s\")\n",
    "\n",
    "# DuckDB\n",
    "start = time.time()\n",
    "result_duckdb2 = duckdb.sql(\"\"\"\n",
    "    SELECT category, SUM(amount) as total\n",
    "    FROM 'duckdb_data/large.parquet'\n",
    "    WHERE amount > 500\n",
    "    GROUP BY category\n",
    "\"\"\").df()\n",
    "duckdb_time2 = time.time() - start\n",
    "\n",
    "print(f\"DuckDB : {duckdb_time2:.3f}s\")\n",
    "print(f\"\\n‚úì DuckDB est {pandas_time2 / duckdb_time2:.1f}x plus rapide\")\n",
    "print(\"\\nüí° DuckDB excelle sur grosses donn√©es et requ√™tes analytiques complexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Lecture depuis S3 (sources distantes)\n",
    "\n",
    "DuckDB peut lire directement depuis S3, Azure Blob, HTTP, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer l'extension httpfs\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL httpfs\")\n",
    "conn.execute(\"LOAD httpfs\")\n",
    "\n",
    "print(\"‚úì Extension httpfs install√©e\")\n",
    "\n",
    "# Exemple : lire un fichier public sur S3\n",
    "# Note : n√©cessite des credentials pour S3 priv√©\n",
    "\n",
    "# Configuration S3 (si n√©cessaire)\n",
    "# conn.execute(\"\"\"\n",
    "#     SET s3_region='us-east-1';\n",
    "#     SET s3_access_key_id='your_key';\n",
    "#     SET s3_secret_access_key='your_secret';\n",
    "# \"\"\")\n",
    "\n",
    "# Lecture d'un bucket S3 public (exemple)\n",
    "# result = conn.execute(\"\"\"\n",
    "#     SELECT * FROM 's3://bucket-name/path/file.parquet'\n",
    "#     LIMIT 10\n",
    "# \"\"\").df()\n",
    "\n",
    "print(\"\\nüí° Exemple de configuration S3 :\")\n",
    "print(\"\"\"\n",
    "    -- Configurer S3\n",
    "    SET s3_region='us-east-1';\n",
    "    SET s3_access_key_id='your_key';\n",
    "    SET s3_secret_access_key='your_secret';\n",
    "    \n",
    "    -- Requ√™ter directement S3\n",
    "    SELECT * FROM 's3://my-bucket/data/*.parquet'\n",
    "    WHERE date >= '2024-01-01';\n",
    "\"\"\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cas pratique : Analyse compl√®te du dataset e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline d'analyse complet avec DuckDB\n",
    "\n",
    "print(\"ANALYSE COMPL√àTE DES VENTES E-COMMERCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Vue d'ensemble\n",
    "overview = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_orders,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        COUNT(DISTINCT product_name) as unique_products,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_order_value,\n",
    "        MIN(order_date) as first_order,\n",
    "        MAX(order_date) as last_order\n",
    "    FROM df_sales\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\n1. VUE D'ENSEMBLE\")\n",
    "print(overview.T)\n",
    "\n",
    "# 2. Top clients (RFM-like)\n",
    "top_customers = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as lifetime_value,\n",
    "        AVG(total_amount) as avg_order,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATE_DIFF('day', MAX(order_date), CURRENT_DATE) as days_since_last_order\n",
    "    FROM df_sales\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY lifetime_value DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\n2. TOP 10 CLIENTS (par valeur)\")\n",
    "print(top_customers.to_string())\n",
    "\n",
    "# 3. Tendances temporelles\n",
    "trends = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('hour', order_date) as hour,\n",
    "        COUNT(*) as orders,\n",
    "        SUM(total_amount) as revenue,\n",
    "        AVG(total_amount) as avg_order\n",
    "    FROM df_sales\n",
    "    GROUP BY hour\n",
    "    ORDER BY hour\n",
    "    LIMIT 24\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\n3. TENDANCES PAR HEURE (24 premi√®res heures)\")\n",
    "print(trends.to_string())\n",
    "\n",
    "# 4. Analyse par produit avec ranking\n",
    "products = duckdb.sql(\"\"\"\n",
    "    WITH product_stats AS (\n",
    "        SELECT \n",
    "            product_name,\n",
    "            category,\n",
    "            COUNT(*) as orders,\n",
    "            SUM(quantity) as units_sold,\n",
    "            SUM(total_amount) as revenue\n",
    "        FROM df_sales\n",
    "        GROUP BY product_name, category\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        RANK() OVER (ORDER BY revenue DESC) as revenue_rank,\n",
    "        ROUND(100.0 * revenue / SUM(revenue) OVER (), 2) as revenue_pct\n",
    "    FROM product_stats\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\n4. ANALYSE PAR PRODUIT\")\n",
    "print(products.to_string())\n",
    "\n",
    "# 5. Cohort analysis (simplifi√©)\n",
    "cohort = duckdb.sql(\"\"\"\n",
    "    WITH customer_first_order AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            MIN(DATE_TRUNC('month', order_date)) as cohort_month\n",
    "        FROM df_sales\n",
    "        GROUP BY customer_id\n",
    "    )\n",
    "    SELECT \n",
    "        cfo.cohort_month,\n",
    "        COUNT(DISTINCT cfo.customer_id) as cohort_size,\n",
    "        SUM(s.total_amount) as cohort_revenue\n",
    "    FROM customer_first_order cfo\n",
    "    JOIN df_sales s ON cfo.customer_id = s.customer_id\n",
    "    GROUP BY cfo.cohort_month\n",
    "    ORDER BY cfo.cohort_month\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\n5. ANALYSE DE COHORTE (par mois d'acquisition)\")\n",
    "print(cohort.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Analyse compl√®te termin√©e avec DuckDB !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pi√®ges courants\n",
    "\n",
    "### 1. DuckDB en m√©moire par d√©faut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå DuckDB en m√©moire : donn√©es perdues apr√®s fermeture\n",
    "# conn = duckdb.connect()  # Base en m√©moire\n",
    "\n",
    "# ‚úÖ Base persistante\n",
    "conn_persist = duckdb.connect('my_analytics.duckdb')  # Fichier sur disque\n",
    "conn_persist.execute(\"CREATE TABLE IF NOT EXISTS test (id INTEGER)\")\n",
    "conn_persist.close()\n",
    "\n",
    "print(\"‚úì Utilisez un fichier .duckdb pour persistance\")\n",
    "print(\"üí° DuckDB par d√©faut = en m√©moire (comme SQLite ':memory:')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Limitations de concurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è DuckDB = un seul writer √† la fois\n",
    "print(\"‚ö†Ô∏è LIMITATIONS :\")\n",
    "print(\"  - Un seul writer √† la fois (comme SQLite)\")\n",
    "print(\"  - Pas de serveur distant (embedded database)\")\n",
    "print(\"  - Pas de r√©plication / haute disponibilit√©\")\n",
    "print(\"\\n‚úì Pour production multi-users : BigQuery, Snowflake, PostgreSQL\")\n",
    "print(\"‚úì DuckDB = exploration locale, prototypage, ETL laptop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Noms de colonnes avec espaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention aux noms de colonnes avec espaces/caract√®res sp√©ciaux\n",
    "df_spaces = pd.DataFrame({\n",
    "    'Order ID': [1, 2, 3],\n",
    "    'Total Amount': [100, 200, 300]\n",
    "})\n",
    "\n",
    "# ‚úÖ Utilisez des guillemets doubles\n",
    "result = duckdb.sql(\"\"\"\n",
    "    SELECT \"Order ID\", \"Total Amount\"\n",
    "    FROM df_spaces\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Colonnes avec espaces : utilisez des guillemets doubles\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-exercices\n",
    "\n",
    "### Exercice 1 : Requ√™te multi-fichiers\n",
    "\n",
    "1. Cr√©ez 3 fichiers Parquet avec des ventes de diff√©rents mois  \n",
    "2. Requ√™tez-les tous en une seule query avec UNION ALL  \n",
    "3. Calculez le total par mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : WINDOW functions avanc√©es\n",
    "\n",
    "√Ä partir de df_sales :  \n",
    "1. Calculez le ranking des clients par montant total d√©pens√©  \n",
    "2. Pour chaque client, calculez la diff√©rence entre sa commande actuelle et la pr√©c√©dente (LAG)  \n",
    "3. Identifiez les clients dans le top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Pipeline complet\n",
    "\n",
    "1. Lisez le fichier sales.parquet  \n",
    "2. Filtrez les commandes > 1000‚Ç¨  \n",
    "3. Joignez avec customers.parquet  \n",
    "4. Calculez le total par segment et pays  \n",
    "5. Exportez le r√©sultat en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions des exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 1\n",
    "# Cr√©er 3 fichiers mensuels\n",
    "for month in [1, 2, 3]:\n",
    "    df_month = pd.DataFrame({\n",
    "        'date': pd.date_range(f'2024-{month:02d}-01', periods=100, freq='H'),\n",
    "        'product': np.random.choice(['A', 'B', 'C'], 100),\n",
    "        'amount': np.random.uniform(10, 500, 100)\n",
    "    })\n",
    "    df_month.to_parquet(f'duckdb_data/sales_month_{month}.parquet', index=False)\n",
    "\n",
    "print(\"‚úì 3 fichiers mensuels cr√©√©s\\n\")\n",
    "\n",
    "# Requ√™te UNION ALL\n",
    "result_ex1 = duckdb.sql(\"\"\"\n",
    "    WITH all_sales AS (\n",
    "        SELECT * FROM 'duckdb_data/sales_month_1.parquet'\n",
    "        UNION ALL\n",
    "        SELECT * FROM 'duckdb_data/sales_month_2.parquet'\n",
    "        UNION ALL\n",
    "        SELECT * FROM 'duckdb_data/sales_month_3.parquet'\n",
    "    )\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', date) as month,\n",
    "        COUNT(*) as orders,\n",
    "        SUM(amount) as total\n",
    "    FROM all_sales\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Solution Exercice 1 :\")\n",
    "print(result_ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 2\n",
    "result_ex2 = duckdb.sql(\"\"\"\n",
    "    WITH customer_totals AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            SUM(total_amount) as total_spent\n",
    "        FROM df_sales\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    ranked_customers AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            total_spent,\n",
    "            RANK() OVER (ORDER BY total_spent DESC) as rank,\n",
    "            NTILE(10) OVER (ORDER BY total_spent DESC) as decile\n",
    "        FROM customer_totals\n",
    "    ),\n",
    "    order_diffs AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            order_date,\n",
    "            total_amount,\n",
    "            LAG(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_amount,\n",
    "            total_amount - LAG(total_amount) OVER (PARTITION BY customer_id ORDER BY order_date) as diff\n",
    "        FROM df_sales\n",
    "    )\n",
    "    SELECT \n",
    "        rc.customer_id,\n",
    "        rc.total_spent,\n",
    "        rc.rank,\n",
    "        rc.decile,\n",
    "        CASE WHEN rc.decile = 1 THEN 'Top 10%' ELSE 'Others' END as segment\n",
    "    FROM ranked_customers rc\n",
    "    WHERE rc.decile = 1\n",
    "    ORDER BY rc.total_spent DESC\n",
    "    LIMIT 20\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"Solution Exercice 2 (Top 10% clients) :\")\n",
    "print(result_ex2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 3\n",
    "duckdb.sql(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            c.segment,\n",
    "            c.country,\n",
    "            COUNT(*) as high_value_orders,\n",
    "            SUM(s.total_amount) as total_revenue,\n",
    "            AVG(s.total_amount) as avg_order_value\n",
    "        FROM 'duckdb_data/sales.parquet' s\n",
    "        JOIN 'duckdb_data/customers.parquet' c\n",
    "            ON s.customer_id = c.customer_id\n",
    "        WHERE s.total_amount > 1000\n",
    "        GROUP BY c.segment, c.country\n",
    "        ORDER BY total_revenue DESC\n",
    "    ) TO 'duckdb_data/high_value_by_segment.csv' (HEADER, DELIMITER ',')\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Solution Exercice 3 : Pipeline ex√©cut√©\")\n",
    "print(\"  R√©sultat export√© : duckdb_data/high_value_by_segment.csv\")\n",
    "\n",
    "# V√©rifier\n",
    "df_verify = pd.read_csv('duckdb_data/high_value_by_segment.csv')\n",
    "print(f\"\\nAper√ßu du r√©sultat ({len(df_verify)} lignes) :\")\n",
    "print(df_verify.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **DuckDB** = SQLite pour l'analytique, optimis√© OLAP\n",
    "2. **SQL sur fichiers** : requ√™tez CSV/Parquet/JSON sans import\n",
    "3. **Int√©gration Pandas** : DuckDB voit automatiquement vos DataFrames\n",
    "4. **Performance** : 5-10x plus rapide que Pandas sur requ√™tes analytiques\n",
    "5. **Fonctions avanc√©es** : WINDOW, PIVOT, UNPIVOT, ROLLUP\n",
    "6. **Export facile** : COPY TO pour Parquet, CSV, JSON\n",
    "7. **Sources distantes** : lecture depuis S3, Azure Blob, HTTP\n",
    "8. **Limitations** : embedded (pas de serveur), un seul writer\n",
    "\n",
    "### Quand utiliser DuckDB ?\n",
    "\n",
    "‚úÖ **Utilisez DuckDB pour** :  \n",
    "- Exploration de donn√©es locales  \n",
    "- Prototypage avant cloud  \n",
    "- ETL sur laptop  \n",
    "- Remplacement de Pandas pour gros volumes (< 100 GB)  \n",
    "- Requ√™tes analytiques complexes  \n",
    "\n",
    "‚ùå **N'utilisez PAS DuckDB pour** :  \n",
    "- Applications web multi-users  \n",
    "- Production √† haute concurrence  \n",
    "- Volumes > 100 GB (pr√©f√©rez Spark)  \n",
    "- Donn√©es distribu√©es  \n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- Notebook suivant : **ETL avec Python**\n",
    "- Approfondir : MotherDuck (DuckDB cloud), extensions DuckDB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
