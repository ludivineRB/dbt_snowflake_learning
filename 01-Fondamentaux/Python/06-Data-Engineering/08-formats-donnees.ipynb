{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Formats de Donn√©es\n",
    "\n",
    "**Badge:** ‚ö° Interm√©diaire | ‚è± 45 min | üîë **Concepts cl√©s :** CSV, JSON, Parquet, ligne vs colonne, partitionnement\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre les diff√©rences entre formats ligne et colonne\n",
    "- Ma√Ætriser CSV et ses limitations\n",
    "- Manipuler JSON et JSON Lines\n",
    "- D√©couvrir Parquet et ses avantages\n",
    "- Comparer les performances et tailles de fichiers\n",
    "- Appliquer le partitionnement pour organiser les donn√©es\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "- Pandas pour la manipulation de donn√©es\n",
    "- Notions de syst√®mes de fichiers\n",
    "- Bases de compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Formats ligne vs colonne : Concept fondamental\n",
    "\n",
    "### Format ligne (Row-oriented)\n",
    "\n",
    "Stocke les donn√©es ligne par ligne. Chaque ligne contient tous les champs.\n",
    "\n",
    "```\n",
    "Row 1: [id=1, name=\"Alice\", age=30, city=\"Paris\"]\n",
    "Row 2: [id=2, name=\"Bob\", age=25, city=\"Lyon\"]\n",
    "Row 3: [id=3, name=\"Charlie\", age=35, city=\"Nice\"]\n",
    "```\n",
    "\n",
    "**Avantages** : \n",
    "- Efficace pour lire des lignes enti√®res\n",
    "- Insert/Update rapides\n",
    "- Formats : CSV, JSON, JSONL\n",
    "\n",
    "**Cas d'usage** : OLTP (transactions), lecture de toutes les colonnes\n",
    "\n",
    "### Format colonne (Column-oriented)\n",
    "\n",
    "Stocke les donn√©es colonne par colonne. Toutes les valeurs d'une colonne sont regroup√©es.\n",
    "\n",
    "```\n",
    "Column id:   [1, 2, 3]\n",
    "Column name: [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "Column age:  [30, 25, 35]\n",
    "Column city: [\"Paris\", \"Lyon\", \"Nice\"]\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- Compression excellente (valeurs similaires)\n",
    "- Lecture s√©lective de colonnes (I/O r√©duit)\n",
    "- Agr√©gations rapides\n",
    "- Formats : Parquet, ORC, Arrow\n",
    "\n",
    "**Cas d'usage** : OLAP (analytique), data warehouses, Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CSV : Le format universel\n",
    "\n",
    "### Avantages\n",
    "- Lisible par les humains\n",
    "- Universel (Excel, SQL, tous les langages)\n",
    "- Simple\n",
    "\n",
    "### Inconv√©nients\n",
    "- Pas de types de donn√©es (tout est string)\n",
    "- Volumineux (texte non compress√©)\n",
    "- Lent pour gros volumes\n",
    "- Probl√®mes d'encodage\n",
    "- Pas de metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Cr√©er un dataset e-commerce\n",
    "np.random.seed(42)\n",
    "\n",
    "n_rows = 100_000\n",
    "df_sales = pd.DataFrame({\n",
    "    'order_id': range(1, n_rows + 1),\n",
    "    'order_date': pd.date_range('2024-01-01', periods=n_rows, freq='5min'),\n",
    "    'customer_id': np.random.randint(1, 10000, n_rows),\n",
    "    'product_name': np.random.choice(['Laptop', 'Smartphone', 'Tablet', 'Headphones', 'Monitor'], n_rows),\n",
    "    'category': np.random.choice(['Electronics', 'Accessories'], n_rows),\n",
    "    'quantity': np.random.randint(1, 10, n_rows),\n",
    "    'unit_price': np.random.uniform(10, 2000, n_rows).round(2),\n",
    "    'discount': np.random.uniform(0, 0.3, n_rows).round(2),\n",
    "    'total_amount': 0.0\n",
    "})\n",
    "\n",
    "# Calculer le montant total\n",
    "df_sales['total_amount'] = (df_sales['quantity'] * df_sales['unit_price'] * (1 - df_sales['discount'])).round(2)\n",
    "\n",
    "print(f\"Dataset cr√©√© : {len(df_sales):,} lignes\")\n",
    "print(f\"Colonnes : {list(df_sales.columns)}\")\n",
    "print(f\"\\nAper√ßu :\")\n",
    "print(df_sales.head())\n",
    "print(f\"\\nM√©moire : {df_sales.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder en CSV\n",
    "Path('data_formats').mkdir(exist_ok=True)\n",
    "\n",
    "csv_path = 'data_formats/sales.csv'\n",
    "\n",
    "start = time.time()\n",
    "df_sales.to_csv(csv_path, index=False)\n",
    "write_time = time.time() - start\n",
    "\n",
    "# Taille du fichier\n",
    "file_size = Path(csv_path).stat().st_size / 1024**2\n",
    "\n",
    "print(f\"‚úì CSV sauvegard√© : {csv_path}\")\n",
    "print(f\"  Temps √©criture : {write_time:.3f}s\")\n",
    "print(f\"  Taille : {file_size:.2f} MB\")\n",
    "\n",
    "# Lecture\n",
    "start = time.time()\n",
    "df_csv = pd.read_csv(csv_path, parse_dates=['order_date'])\n",
    "read_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì CSV lu : {len(df_csv):,} lignes\")\n",
    "print(f\"  Temps lecture : {read_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probl√®mes courants avec CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probl√®me 1 : Perte de types\n",
    "print(\"Types apr√®s lecture CSV :\")\n",
    "print(df_csv.dtypes)\n",
    "print(\"\\n‚ö†Ô∏è Tous les types doivent √™tre r√©inf√©r√©s ou sp√©cifi√©s manuellement\")\n",
    "\n",
    "# Probl√®me 2 : Virgules dans les donn√©es\n",
    "df_problematic = pd.DataFrame({\n",
    "    'name': ['Alice, Bob', 'Charlie\"s Store'],\n",
    "    'description': ['Product with, comma', 'Line 1\\nLine 2']\n",
    "})\n",
    "\n",
    "df_problematic.to_csv('data_formats/problematic.csv', index=False)\n",
    "print(\"\\n‚úì CSV avec caract√®res sp√©ciaux sauvegard√© (virgules, quotes, newlines)\")\n",
    "\n",
    "# Probl√®me 3 : Encodage\n",
    "df_encoding = pd.DataFrame({\n",
    "    'text': ['Caf√©', 'na√Øve', 'Âåó‰∫¨']\n",
    "})\n",
    "\n",
    "df_encoding.to_csv('data_formats/encoding.csv', index=False, encoding='utf-8')\n",
    "print(\"‚úì CSV avec encodage UTF-8 sauvegard√©\")\n",
    "print(\"‚ö†Ô∏è Sp√©cifiez toujours l'encodage : encoding='utf-8'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON : Format structur√©\n",
    "\n",
    "### Avantages\n",
    "- Structure hi√©rarchique (objets imbriqu√©s)\n",
    "- Types de donn√©es (nombres, bool√©ens, null)\n",
    "- Lisible par les humains\n",
    "- Standard web (APIs)\n",
    "\n",
    "### Inconv√©nients\n",
    "- Verbeux (r√©p√©tition des cl√©s)\n",
    "- Lent pour gros volumes\n",
    "- Pas optimis√© pour l'analytique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder en JSON\n",
    "json_path = 'data_formats/sales.json'\n",
    "\n",
    "start = time.time()\n",
    "df_sales.to_json(json_path, orient='records', date_format='iso', indent=2)\n",
    "json_write_time = time.time() - start\n",
    "\n",
    "json_size = Path(json_path).stat().st_size / 1024**2\n",
    "\n",
    "print(f\"‚úì JSON sauvegard√© : {json_path}\")\n",
    "print(f\"  Temps √©criture : {json_write_time:.3f}s\")\n",
    "print(f\"  Taille : {json_size:.2f} MB\")\n",
    "\n",
    "# Lecture\n",
    "start = time.time()\n",
    "df_json = pd.read_json(json_path)\n",
    "json_read_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì JSON lu : {len(df_json):,} lignes\")\n",
    "print(f\"  Temps lecture : {json_read_time:.3f}s\")\n",
    "\n",
    "# Aper√ßu du contenu JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    sample = f.read(500)\n",
    "    print(f\"\\nAper√ßu JSON (premiers caract√®res) :\")\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Lines (JSONL) : Une ligne = un JSON\n",
    "\n",
    "Format id√©al pour les logs et le streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Lines : un JSON par ligne (pas de tableau global)\n",
    "jsonl_path = 'data_formats/sales.jsonl'\n",
    "\n",
    "start = time.time()\n",
    "df_sales.to_json(jsonl_path, orient='records', lines=True, date_format='iso')\n",
    "jsonl_write_time = time.time() - start\n",
    "\n",
    "jsonl_size = Path(jsonl_path).stat().st_size / 1024**2\n",
    "\n",
    "print(f\"‚úì JSONL sauvegard√© : {jsonl_path}\")\n",
    "print(f\"  Temps √©criture : {jsonl_write_time:.3f}s\")\n",
    "print(f\"  Taille : {jsonl_size:.2f} MB\")\n",
    "\n",
    "# Lecture\n",
    "start = time.time()\n",
    "df_jsonl = pd.read_json(jsonl_path, lines=True)\n",
    "jsonl_read_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì JSONL lu : {len(df_jsonl):,} lignes\")\n",
    "print(f\"  Temps lecture : {jsonl_read_time:.3f}s\")\n",
    "\n",
    "# Aper√ßu JSONL\n",
    "with open(jsonl_path, 'r') as f:\n",
    "    lines = [f.readline() for _ in range(3)]\n",
    "    print(\"\\nAper√ßu JSONL (3 premi√®res lignes) :\")\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        print(f\"  Ligne {i}: {line[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parquet : Format colonnaire moderne\n",
    "\n",
    "Parquet est le format de r√©f√©rence pour le Big Data et l'analytique.\n",
    "\n",
    "### Avantages\n",
    "- **Compression** excellente (5-10x plus petit que CSV)\n",
    "- **Types de donn√©es** pr√©serv√©s\n",
    "- **Lecture s√©lective** de colonnes (column pruning)\n",
    "- **Rapide** pour l'analytique\n",
    "- **Metadata** int√©gr√©e (schema, statistiques)\n",
    "- **Compatible** Spark, Dask, BigQuery, Snowflake\n",
    "\n",
    "### Inconv√©nients\n",
    "- Binaire (pas lisible humainement)\n",
    "- N√©cessite une biblioth√®que (pyarrow, fastparquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Sauvegarder en Parquet\n",
    "parquet_path = 'data_formats/sales.parquet'\n",
    "\n",
    "start = time.time()\n",
    "df_sales.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "parquet_write_time = time.time() - start\n",
    "\n",
    "parquet_size = Path(parquet_path).stat().st_size / 1024**2\n",
    "\n",
    "print(f\"‚úì Parquet sauvegard√© : {parquet_path}\")\n",
    "print(f\"  Temps √©criture : {parquet_write_time:.3f}s\")\n",
    "print(f\"  Taille : {parquet_size:.2f} MB\")\n",
    "\n",
    "# Lecture compl√®te\n",
    "start = time.time()\n",
    "df_parquet = pd.read_parquet(parquet_path, engine='pyarrow')\n",
    "parquet_read_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Parquet lu : {len(df_parquet):,} lignes\")\n",
    "print(f\"  Temps lecture : {parquet_read_time:.3f}s\")\n",
    "print(f\"\\nTypes pr√©serv√©s :\")\n",
    "print(df_parquet.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture s√©lective de colonnes : Feature killer de Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire seulement certaines colonnes (column pruning)\n",
    "selected_columns = ['order_id', 'order_date', 'total_amount']\n",
    "\n",
    "start = time.time()\n",
    "df_partial = pd.read_parquet(parquet_path, columns=selected_columns)\n",
    "partial_read_time = time.time() - start\n",
    "\n",
    "print(f\"‚úì Lecture s√©lective : {len(df_partial):,} lignes, {len(df_partial.columns)} colonnes\")\n",
    "print(f\"  Temps : {partial_read_time:.3f}s\")\n",
    "print(f\"  Gain : {parquet_read_time / partial_read_time:.1f}x plus rapide\")\n",
    "\n",
    "print(f\"\\nM√©moire :\")\n",
    "print(f\"  Lecture compl√®te : {df_parquet.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Lecture s√©lective : {df_partial.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n‚úì Avec Parquet, on ne lit que les colonnes n√©cessaires !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata du fichier Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecter les metadata Parquet\n",
    "parquet_file = pq.ParquetFile(parquet_path)\n",
    "\n",
    "print(\"Metadata Parquet :\")\n",
    "print(f\"  Nombre de lignes : {parquet_file.metadata.num_rows:,}\")\n",
    "print(f\"  Nombre de row groups : {parquet_file.metadata.num_row_groups}\")\n",
    "print(f\"  Sch√©ma :\")\n",
    "print(parquet_file.schema)\n",
    "\n",
    "# Statistiques par colonne\n",
    "print(\"\\nStatistiques (premier row group) :\")\n",
    "rg = parquet_file.metadata.row_group(0)\n",
    "for i in range(min(3, rg.num_columns)):\n",
    "    col = rg.column(i)\n",
    "    print(f\"  {col.path_in_schema}: {col.statistics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison des formats : Performance et taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tableau comparatif\n",
    "comparison = pd.DataFrame({\n",
    "    'Format': ['CSV', 'JSON', 'JSONL', 'Parquet'],\n",
    "    'Taille (MB)': [file_size, json_size, jsonl_size, parquet_size],\n",
    "    'Ecriture (s)': [write_time, json_write_time, jsonl_write_time, parquet_write_time],\n",
    "    'Lecture (s)': [read_time, json_read_time, jsonl_read_time, parquet_read_time]\n",
    "})\n",
    "\n",
    "comparison['Ratio taille vs CSV'] = (comparison['Taille (MB)'] / file_size).round(2)\n",
    "\n",
    "print(\"COMPARAISON DES FORMATS\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Parquet est {file_size / parquet_size:.1f}x plus compact que CSV\")\n",
    "print(f\"üí° Parquet est {read_time / parquet_read_time:.1f}x plus rapide √† lire que CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphiques de comparaison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Graphique 1 : Taille\n",
    "axes[0].bar(comparison['Format'], comparison['Taille (MB)'], color=['blue', 'orange', 'green', 'red'])\n",
    "axes[0].set_ylabel('Taille (MB)')\n",
    "axes[0].set_title('Taille des fichiers')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Graphique 2 : Ecriture\n",
    "axes[1].bar(comparison['Format'], comparison['Ecriture (s)'], color=['blue', 'orange', 'green', 'red'])\n",
    "axes[1].set_ylabel('Temps (s)')\n",
    "axes[1].set_title('Temps d\\'√©criture')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Graphique 3 : Lecture\n",
    "axes[2].bar(comparison['Format'], comparison['Lecture (s)'], color=['blue', 'orange', 'green', 'red'])\n",
    "axes[2].set_ylabel('Temps (s)')\n",
    "axes[2].set_title('Temps de lecture')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Parquet gagne sur tous les tableaux pour l'analytique !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partitionnement : Organiser les donn√©es\n",
    "\n",
    "Le **partitionnement** consiste √† diviser les donn√©es en sous-dossiers selon une ou plusieurs colonnes.\n",
    "\n",
    "### Avantages\n",
    "- Lecture s√©lective (skip des partitions inutiles)\n",
    "- Parall√©lisation facile\n",
    "- Organisation logique\n",
    "\n",
    "### Pattern courant\n",
    "```\n",
    "data/\n",
    "  year=2024/\n",
    "    month=01/\n",
    "      part-0.parquet\n",
    "      part-1.parquet\n",
    "    month=02/\n",
    "      part-0.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter des colonnes de partitionnement\n",
    "df_sales['year'] = df_sales['order_date'].dt.year\n",
    "df_sales['month'] = df_sales['order_date'].dt.month\n",
    "\n",
    "print(f\"Dataset avec colonnes de partitionnement :\")\n",
    "print(df_sales[['order_date', 'year', 'month', 'total_amount']].head())\n",
    "\n",
    "# Sauvegarder avec partitionnement\n",
    "partitioned_path = 'data_formats/sales_partitioned'\n",
    "\n",
    "start = time.time()\n",
    "df_sales.to_parquet(\n",
    "    partitioned_path,\n",
    "    engine='pyarrow',\n",
    "    partition_cols=['year', 'month'],  # Partitionner par ann√©e et mois\n",
    "    index=False\n",
    ")\n",
    "print(f\"\\n‚úì Donn√©es partitionn√©es sauvegard√©es en {time.time() - start:.3f}s\")\n",
    "\n",
    "# Structure des dossiers\n",
    "import os\n",
    "print(\"\\nStructure des dossiers :\")\n",
    "for root, dirs, files in os.walk(partitioned_path):\n",
    "    level = root.replace(partitioned_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:2]:  # Limite √† 2 fichiers par dossier\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 2:\n",
    "        print(f\"{subindent}... (+{len(files)-2} fichiers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture s√©lective avec filtres sur partitions\n",
    "start = time.time()\n",
    "df_filtered = pd.read_parquet(\n",
    "    partitioned_path,\n",
    "    filters=[('year', '=', 2024), ('month', '=', 1)]  # Lire seulement janvier 2024\n",
    ")\n",
    "filter_time = time.time() - start\n",
    "\n",
    "print(f\"‚úì Lecture filtr√©e (janvier 2024) : {len(df_filtered):,} lignes\")\n",
    "print(f\"  Temps : {filter_time:.3f}s\")\n",
    "print(f\"\\nüí° Seule la partition year=2024/month=1 a √©t√© lue !\")\n",
    "\n",
    "# Lecture compl√®te pour comparaison\n",
    "start = time.time()\n",
    "df_full = pd.read_parquet(partitioned_path)\n",
    "full_time = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Lecture compl√®te : {len(df_full):,} lignes\")\n",
    "print(f\"  Temps : {full_time:.3f}s\")\n",
    "print(f\"\\n  Gain avec filtrage : {full_time / filter_time:.1f}x plus rapide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Autres formats (mention pour culture)\n",
    "\n",
    "### Avro\n",
    "- Format ligne binaire avec schema\n",
    "- Populaire dans Kafka\n",
    "- Bon pour streaming et √©volution de schema\n",
    "\n",
    "### ORC (Optimized Row Columnar)\n",
    "- Format colonnaire similaire √† Parquet\n",
    "- Optimis√© pour Hadoop/Hive\n",
    "- Compression l√©g√®rement meilleure que Parquet\n",
    "\n",
    "### Apache Arrow\n",
    "- Format en m√©moire (pas pour le stockage)\n",
    "- √âchange zero-copy entre langages (Python, R, Java)\n",
    "- Base de Parquet et de nombreux outils modernes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lien avec Spark et le Big Data\n",
    "\n",
    "Les formats colonnaires (Parquet, ORC) sont essentiels dans l'√©cosyst√®me Big Data :\n",
    "\n",
    "- **Spark** : format par d√©faut pour le stockage\n",
    "- **Hive** : tables externes en Parquet\n",
    "- **BigQuery** : import/export Parquet natif\n",
    "- **Snowflake** : lecture directe de Parquet sur S3\n",
    "- **Dask** : partitionnement automatique en Parquet\n",
    "\n",
    "**Philosophie** : m√™me format du laptop au data lake, scalabilit√© naturelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pi√®ges courants\n",
    "\n",
    "### 1. Encodage CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Oublier l'encodage\n",
    "# df.to_csv('file.csv')  # Peut causer des erreurs avec accents\n",
    "\n",
    "# ‚úÖ Toujours sp√©cifier UTF-8\n",
    "df_encoding = pd.DataFrame({'text': ['Caf√©', 'na√Øve', 'Âåó‰∫¨']})\n",
    "df_encoding.to_csv('data_formats/test_encoding.csv', index=False, encoding='utf-8')\n",
    "df_read = pd.read_csv('data_formats/test_encoding.csv', encoding='utf-8')\n",
    "print(\"‚úì Encodage UTF-8 correct\")\n",
    "print(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dates en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Dates converties en timestamps ou strings\n",
    "df_dates = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=3)\n",
    "})\n",
    "\n",
    "# Sauvegarder avec format ISO\n",
    "df_dates.to_json('data_formats/dates.json', orient='records', date_format='iso')\n",
    "\n",
    "# ‚úÖ Relire avec parse_dates si n√©cessaire\n",
    "df_dates_read = pd.read_json('data_formats/dates.json')\n",
    "print(\"Type apr√®s lecture JSON :\")\n",
    "print(df_dates_read.dtypes)\n",
    "print(\"\\n‚úì Avec Parquet, les dates sont pr√©serv√©es automatiquement !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Version et compression Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester diff√©rentes compressions\n",
    "compressions = ['snappy', 'gzip', 'brotli', 'none']\n",
    "results = []\n",
    "\n",
    "for comp in compressions:\n",
    "    path = f'data_formats/sales_{comp}.parquet'\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        df_sales.to_parquet(path, compression=comp, index=False)\n",
    "        write_t = time.time() - start\n",
    "        \n",
    "        size = Path(path).stat().st_size / 1024**2\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = pd.read_parquet(path)\n",
    "        read_t = time.time() - start\n",
    "        \n",
    "        results.append({\n",
    "            'Compression': comp,\n",
    "            'Taille (MB)': round(size, 2),\n",
    "            'Ecriture (s)': round(write_t, 3),\n",
    "            'Lecture (s)': round(read_t, 3)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {comp} non disponible\")\n",
    "\n",
    "df_compressions = pd.DataFrame(results)\n",
    "print(\"\\nComparaison des compressions Parquet :\")\n",
    "print(df_compressions.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Snappy = bon compromis (vitesse + compression)\")\n",
    "print(\"üí° Gzip = meilleure compression, plus lent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-exercices\n",
    "\n",
    "### Exercice 1 : Convertir CSV ‚Üí Parquet\n",
    "\n",
    "1. Cr√©ez un CSV de 50k lignes avec colonnes : id, name, age, city, salary  \n",
    "2. Chargez-le dans Pandas  \n",
    "3. Sauvegardez-le en Parquet avec compression snappy  \n",
    "4. Comparez les tailles et temps de lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Lecture s√©lective de colonnes\n",
    "\n",
    "√Ä partir du fichier Parquet `sales.parquet` :  \n",
    "1. Lisez uniquement les colonnes : order_id, customer_id, total_amount  \n",
    "2. Mesurez le temps et la m√©moire utilis√©e  \n",
    "3. Comparez avec une lecture compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : Partitionnement par cat√©gorie\n",
    "\n",
    "1. Cr√©ez un DataFrame avec 10k ventes, 5 cat√©gories de produits, dates sur 6 mois  \n",
    "2. Sauvegardez en Parquet partitionn√© par cat√©gorie et mois  \n",
    "3. Lisez uniquement la cat√©gorie \"Electronics\" du mois 3  \n",
    "4. V√©rifiez que seules les bonnes partitions ont √©t√© lues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions des exercices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 1\n",
    "n = 50_000\n",
    "df_ex1 = pd.DataFrame({\n",
    "    'id': range(1, n+1),\n",
    "    'name': [f'Person_{i}' for i in range(n)],\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'city': np.random.choice(['Paris', 'Lyon', 'Marseille', 'Toulouse'], n),\n",
    "    'salary': np.random.uniform(25000, 100000, n).round(2)\n",
    "})\n",
    "\n",
    "# CSV\n",
    "csv_ex1 = 'data_formats/ex1.csv'\n",
    "start = time.time()\n",
    "df_ex1.to_csv(csv_ex1, index=False)\n",
    "csv_write = time.time() - start\n",
    "csv_size = Path(csv_ex1).stat().st_size / 1024**2\n",
    "\n",
    "start = time.time()\n",
    "df_csv_ex1 = pd.read_csv(csv_ex1)\n",
    "csv_read = time.time() - start\n",
    "\n",
    "# Parquet\n",
    "parquet_ex1 = 'data_formats/ex1.parquet'\n",
    "start = time.time()\n",
    "df_ex1.to_parquet(parquet_ex1, compression='snappy', index=False)\n",
    "parquet_write = time.time() - start\n",
    "parquet_size = Path(parquet_ex1).stat().st_size / 1024**2\n",
    "\n",
    "start = time.time()\n",
    "df_parquet_ex1 = pd.read_parquet(parquet_ex1)\n",
    "parquet_read = time.time() - start\n",
    "\n",
    "print(\"COMPARAISON CSV vs PARQUET\")\n",
    "print(f\"\\nCSV :\")\n",
    "print(f\"  Taille : {csv_size:.2f} MB\")\n",
    "print(f\"  Lecture : {csv_read:.3f}s\")\n",
    "print(f\"\\nParquet :\")\n",
    "print(f\"  Taille : {parquet_size:.2f} MB (gain {csv_size/parquet_size:.1f}x)\")\n",
    "print(f\"  Lecture : {parquet_read:.3f}s (gain {csv_read/parquet_read:.1f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 2\n",
    "selected_cols = ['order_id', 'customer_id', 'total_amount']\n",
    "\n",
    "# Lecture compl√®te\n",
    "start = time.time()\n",
    "df_full_ex2 = pd.read_parquet(parquet_path)\n",
    "full_time_ex2 = time.time() - start\n",
    "full_memory = df_full_ex2.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "# Lecture s√©lective\n",
    "start = time.time()\n",
    "df_partial_ex2 = pd.read_parquet(parquet_path, columns=selected_cols)\n",
    "partial_time_ex2 = time.time() - start\n",
    "partial_memory = df_partial_ex2.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(\"LECTURE S√âLECTIVE vs COMPL√àTE\")\n",
    "print(f\"\\nCompl√®te ({len(df_full_ex2.columns)} colonnes) :\")\n",
    "print(f\"  Temps : {full_time_ex2:.3f}s\")\n",
    "print(f\"  M√©moire : {full_memory:.2f} MB\")\n",
    "print(f\"\\nS√©lective ({len(selected_cols)} colonnes) :\")\n",
    "print(f\"  Temps : {partial_time_ex2:.3f}s (gain {full_time_ex2/partial_time_ex2:.1f}x)\")\n",
    "print(f\"  M√©moire : {partial_memory:.2f} MB (√©conomie {full_memory/partial_memory:.1f}x)\")\n",
    "print(f\"\\n‚úì Ne lisez que ce dont vous avez besoin !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Exercice 3\n",
    "n_ex3 = 10_000\n",
    "categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Toys']\n",
    "\n",
    "df_ex3 = pd.DataFrame({\n",
    "    'sale_id': range(1, n_ex3+1),\n",
    "    'date': pd.date_range('2024-01-01', periods=n_ex3, freq='2h'),\n",
    "    'category': np.random.choice(categories, n_ex3),\n",
    "    'amount': np.random.uniform(10, 500, n_ex3).round(2),\n",
    "    'quantity': np.random.randint(1, 10, n_ex3)\n",
    "})\n",
    "\n",
    "df_ex3['month'] = df_ex3['date'].dt.month\n",
    "\n",
    "# Sauvegarder partitionn√©\n",
    "partition_path_ex3 = 'data_formats/sales_by_category'\n",
    "df_ex3.to_parquet(\n",
    "    partition_path_ex3,\n",
    "    partition_cols=['category', 'month'],\n",
    "    index=False\n",
    ")\n",
    "print(\"‚úì Donn√©es partitionn√©es par cat√©gorie et mois\")\n",
    "\n",
    "# Lire seulement Electronics mois 3\n",
    "start = time.time()\n",
    "df_filtered_ex3 = pd.read_parquet(\n",
    "    partition_path_ex3,\n",
    "    filters=[\n",
    "        ('category', '=', 'Electronics'),\n",
    "        ('month', '=', 3)\n",
    "    ]\n",
    ")\n",
    "filter_time_ex3 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Lecture filtr√©e : {len(df_filtered_ex3)} lignes\")\n",
    "print(f\"  Temps : {filter_time_ex3:.3f}s\")\n",
    "print(f\"\\n  Cat√©gories dans r√©sultat : {df_filtered_ex3['category'].unique()}\")\n",
    "print(f\"  Mois dans r√©sultat : {df_filtered_ex3['month'].unique()}\")\n",
    "print(\"\\n‚úì Seule la partition category=Electronics/month=3 a √©t√© lue !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©sum√©\n",
    "\n",
    "### Points cl√©s\n",
    "\n",
    "1. **Format ligne** (CSV, JSON) : bon pour transactions, insertion ligne par ligne\n",
    "2. **Format colonne** (Parquet) : excellent pour analytique, compression, lecture s√©lective\n",
    "3. **CSV** : universel mais lent, volumineux, perte de types\n",
    "4. **JSON** : structur√©, verbeux, bon pour APIs\n",
    "5. **Parquet** : format de r√©f√©rence pour Big Data, 5-10x plus compact que CSV\n",
    "6. **Lecture s√©lective** : avec Parquet, ne lisez que les colonnes n√©cessaires\n",
    "7. **Partitionnement** : organisez vos donn√©es par date/cat√©gorie pour des lectures ultra-rapides\n",
    "\n",
    "### R√®gles de d√©cision\n",
    "\n",
    "| Cas d'usage | Format recommand√© |\n",
    "|-------------|-------------------|\n",
    "| Export Excel, lisibilit√© humaine | CSV |\n",
    "| API REST, configuration | JSON |\n",
    "| Logs, streaming | JSON Lines |\n",
    "| Data lake, analytique | Parquet partitionn√© |\n",
    "| Data warehouse | Parquet ou ORC |\n",
    "| Kafka, messaging | Avro |\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- Notebook suivant : **DuckDB**\n",
    "- Approfondir : Spark, Delta Lake, Iceberg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
