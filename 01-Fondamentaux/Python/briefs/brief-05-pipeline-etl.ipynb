{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üü£ Brief 5 : Pipeline ETL complet\n",
    "\n",
    "**Badge:** üü£ Expert  \n",
    "**Dur√©e:** 3 heures  \n",
    "**Sections valid√©es:** 06-Data-Engineering + 04-Stdlib + 05-Qualit√©-Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Contexte\n",
    "\n",
    "Vous √™tes **Data Engineer** dans une fintech. Votre √©quipe doit ing√©rer des donn√©es de plusieurs sources (fichiers historiques CSV + API REST) pour alimenter un data warehouse.\n",
    "\n",
    "Le pipeline doit :\n",
    "- Extraire les donn√©es depuis plusieurs sources\n",
    "- Valider la qualit√© des donn√©es (sch√©ma, types, contraintes)\n",
    "- Transformer et enrichir les donn√©es\n",
    "- Charger dans un data warehouse (DuckDB)\n",
    "- √ätre robuste, test√©, et respecter les bonnes pratiques de qualit√©\n",
    "\n",
    "**Votre mission :** Construire un pipeline ETL professionnel avec validation Pydantic, tests, logging structur√©, et configuration par environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Objectifs du projet\n",
    "\n",
    "Construire un pipeline ETL complet qui :\n",
    "\n",
    "1. ‚úÖ **Extract** : Lire CSV + appeler une API REST\n",
    "2. ‚úÖ **Transform** : Valider avec Pydantic + nettoyer avec Pandas\n",
    "3. ‚úÖ **Load** : Exporter en Parquet + charger dans DuckDB\n",
    "4. ‚úÖ **Quality** : Logging JSON, tests pytest > 80%, pre-commit\n",
    "5. ‚úÖ **Structure** : Architecture propre, configuration centralis√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Sp√©cifications techniques\n",
    "\n",
    "### 1. Extract (Extraction)\n",
    "\n",
    "**Sources de donn√©es :**\n",
    "\n",
    "1. **Fichiers CSV historiques** :\n",
    "   - `data/raw/transactions_*.csv` : Historique des transactions\n",
    "   - Colonnes : `transaction_id`, `user_id`, `amount`, `currency`, `timestamp`, `status`\n",
    "\n",
    "2. **API REST** :\n",
    "   - URL : JSONPlaceholder (`https://jsonplaceholder.typicode.com/users`) ou cr√©er un mock\n",
    "   - R√©cup√©rer les informations utilisateurs : `id`, `name`, `email`, `city`\n",
    "\n",
    "**Exigences :**\n",
    "- Gestion d'erreurs robuste (try/except)\n",
    "- Retry automatique en cas d'√©chec API (3 tentatives)\n",
    "- Timeout sur les appels API (5 secondes)\n",
    "- Logging de toutes les op√©rations\n",
    "\n",
    "### 2. Transform (Transformation)\n",
    "\n",
    "**Validation avec Pydantic :**\n",
    "\n",
    "Cr√©er des mod√®les Pydantic pour chaque source :\n",
    "\n",
    "```python\n",
    "class Transaction(BaseModel):\n",
    "    transaction_id: str\n",
    "    user_id: int\n",
    "    amount: Decimal\n",
    "    currency: str = Field(pattern=\"^[A-Z]{3}$\")  # ISO 4217\n",
    "    timestamp: datetime\n",
    "    status: Literal[\"pending\", \"completed\", \"failed\"]\n",
    "    \n",
    "    @field_validator('amount')\n",
    "    def amount_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError('Amount must be positive')\n",
    "        return v\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    city: str\n",
    "```\n",
    "\n",
    "**Nettoyage avec Pandas :**\n",
    "- Supprimer les doublons\n",
    "- G√©rer les valeurs manquantes\n",
    "- Convertir les types de donn√©es\n",
    "- Normaliser les formats (dates, devises)\n",
    "\n",
    "**Enrichissement :**\n",
    "- Joindre transactions et users sur `user_id`\n",
    "- Calculer des m√©triques agr√©g√©es (total par utilisateur, par jour)\n",
    "- Ajouter des flags de qualit√© (is_valid, has_complete_info)\n",
    "\n",
    "### 3. Load (Chargement)\n",
    "\n",
    "**Export Parquet :**\n",
    "- Partitionner par date (`partition_cols=['year', 'month', 'day']`)\n",
    "- Compression snappy\n",
    "- Structure : `data/processed/transactions/year=2024/month=01/day=15/data.parquet`\n",
    "\n",
    "**Chargement DuckDB :**\n",
    "- Cr√©er une base DuckDB locale\n",
    "- Cr√©er les tables `transactions`, `users`, `transactions_enriched`\n",
    "- Charger les donn√©es depuis les Parquet\n",
    "- V√©rifier l'int√©grit√© (count, checksums)\n",
    "\n",
    "### 4. Qualit√© et tests\n",
    "\n",
    "**Logging structur√© (JSON) :**\n",
    "\n",
    "```python\n",
    "import structlog\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "logger.info(\"extraction_started\", source=\"csv\", file_count=5)\n",
    "logger.error(\"validation_failed\", error=str(e), record_id=tx_id)\n",
    "```\n",
    "\n",
    "**Tests pytest :**\n",
    "- Tests unitaires pour chaque fonction\n",
    "- Tests d'int√©gration pour le pipeline complet\n",
    "- Fixtures avec donn√©es de test\n",
    "- Coverage > 80%\n",
    "\n",
    "**Configuration pre-commit :**\n",
    "- ruff (linter + formatter)\n",
    "- mypy (type checker)\n",
    "- pytest (tests)\n",
    "\n",
    "**Pydantic Settings pour configuration :**\n",
    "\n",
    "```python\n",
    "class Settings(BaseSettings):\n",
    "    api_url: str\n",
    "    api_timeout: int = 5\n",
    "    api_retries: int = 3\n",
    "    db_path: str = \"data/warehouse.duckdb\"\n",
    "    log_level: str = \"INFO\"\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "```\n",
    "\n",
    "### 5. Structure du projet\n",
    "\n",
    "```\n",
    "pipeline/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ extract.py           # Extraction CSV + API\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transform.py         # Validation + transformation\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ load.py              # Export Parquet + DuckDB\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Mod√®les Pydantic\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration (Pydantic Settings)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ exceptions.py        # Exceptions custom\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ main.py              # Point d'entr√©e\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_transform.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ test_load.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ conftest.py          # Fixtures pytest\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # CSV bruts\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/           # Parquet\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ warehouse.duckdb     # Base DuckDB\n",
    "‚îú‚îÄ‚îÄ .env.example             # Exemple de configuration\n",
    "‚îú‚îÄ‚îÄ .pre-commit-config.yaml\n",
    "‚îú‚îÄ‚îÄ pyproject.toml\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Livrables\n",
    "\n",
    "### Checklist\n",
    "\n",
    "- [ ] Structure de projet respect√©e\n",
    "- [ ] `src/extract.py` : Lecture CSV + appel API avec retry\n",
    "- [ ] `src/models.py` : Mod√®les Pydantic avec validation\n",
    "- [ ] `src/transform.py` : Validation + nettoyage + enrichissement\n",
    "- [ ] `src/load.py` : Export Parquet partitionn√© + chargement DuckDB\n",
    "- [ ] `src/config.py` : Pydantic Settings\n",
    "- [ ] `src/exceptions.py` : Exceptions custom\n",
    "- [ ] `src/main.py` : Pipeline complet fonctionnel\n",
    "- [ ] `tests/` : 8+ tests avec coverage > 80%\n",
    "- [ ] `pyproject.toml` : Config ruff + pytest\n",
    "- [ ] `.pre-commit-config.yaml` : Hooks Git\n",
    "- [ ] `.env.example` : Variables d'environnement\n",
    "- [ ] Logging structur√© (JSON format)\n",
    "- [ ] Type hints partout\n",
    "- [ ] Docstrings (Google style)\n",
    "- [ ] README.md complet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Grille d'√©valuation\n",
    "\n",
    "| Comp√©tence | Crit√®res d'√©valuation | Points |\n",
    "|------------|----------------------|--------|\n",
    "| **Extract** | CSV + API, retry, gestion erreurs, logging | **/15** |\n",
    "| **Validation Pydantic** | Mod√®les complets, validators, types corrects | **/15** |\n",
    "| **Transform Pandas** | Nettoyage, enrichissement, jointures | **/15** |\n",
    "| **Load** | Parquet partitionn√© + DuckDB + v√©rifications | **/15** |\n",
    "| **Tests pytest** | 8+ tests, coverage > 80%, fixtures | **/15** |\n",
    "| **Logging structur√©** | JSON format, niveaux appropri√©s, contexte | **/10** |\n",
    "| **Configuration** | pre-commit + ruff + Pydantic Settings | **/10** |\n",
    "| **Qualit√© code** | Structure, type hints, docstrings, PEP 8 | **/5** |\n",
    "| **TOTAL** | | **/100** |\n",
    "\n",
    "### Crit√®res de r√©ussite\n",
    "\n",
    "- ‚úÖ **Acquis (‚â• 70/100)** : Pipeline complet et professionnel\n",
    "- üöß **En cours d'acquisition (50-69/100)** : Pipeline partiel\n",
    "- ‚ùå **Non acquis (< 50/100)** : Pipeline incomplet ou non fonctionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Pr√©paration : Cr√©er des donn√©es de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la structure de dossiers\n",
    "import os\n",
    "\n",
    "os.makedirs(\"pipeline/src\", exist_ok=True)\n",
    "os.makedirs(\"pipeline/tests\", exist_ok=True)\n",
    "os.makedirs(\"pipeline/data/raw\", exist_ok=True)\n",
    "os.makedirs(\"pipeline/data/processed\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Structure de dossiers cr√©√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/data/raw/transactions_20240115.csv\n",
    "transaction_id,user_id,amount,currency,timestamp,status\n",
    "TXN001,1,150.50,EUR,2024-01-15 10:30:00,completed\n",
    "TXN002,2,89.99,USD,2024-01-15 11:45:00,completed\n",
    "TXN003,1,200.00,EUR,2024-01-15 14:20:00,pending\n",
    "TXN004,3,45.75,GBP,2024-01-15 16:00:00,completed\n",
    "TXN005,2,  ,USD,2024-01-15 17:30:00,failed\n",
    "TXN006,4,300.00,EUR,2024-01-15 18:15:00,completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/data/raw/transactions_20240116.csv\n",
    "transaction_id,user_id,amount,currency,timestamp,status\n",
    "TXN007,1,75.25,EUR,2024-01-16 09:00:00,completed\n",
    "TXN008,3,120.00,GBP,2024-01-16 10:30:00,completed\n",
    "TXN009,2,50.50,USD,2024-01-16 12:00:00,pending\n",
    "TXN010,4,180.00,EUR,2024-01-16 15:45:00,completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/models.py\n",
    "\"\"\"Mod√®les Pydantic pour la validation des donn√©es.\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field, field_validator\n",
    "\n",
    "\n",
    "class Transaction(BaseModel):\n",
    "    \"\"\"Mod√®le de transaction financi√®re.\"\"\"\n",
    "    \n",
    "    transaction_id: str = Field(..., min_length=5)\n",
    "    user_id: int = Field(..., gt=0)\n",
    "    amount: Decimal = Field(..., gt=0)\n",
    "    currency: str = Field(..., pattern=r\"^[A-Z]{3}$\")\n",
    "    timestamp: datetime\n",
    "    status: Literal[\"pending\", \"completed\", \"failed\"]\n",
    "    \n",
    "    @field_validator('amount')\n",
    "    @classmethod\n",
    "    def amount_positive(cls, v: Decimal) -> Decimal:\n",
    "        \"\"\"Valide que le montant est positif.\"\"\"\n",
    "        if v <= 0:\n",
    "            raise ValueError(\"Amount must be positive\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        frozen = True  # Immutable\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    \"\"\"Mod√®le d'utilisateur.\"\"\"\n",
    "    \n",
    "    id: int = Field(..., gt=0)\n",
    "    name: str = Field(..., min_length=1)\n",
    "    email: EmailStr\n",
    "    city: str = Field(..., min_length=1)\n",
    "    \n",
    "    class Config:\n",
    "        frozen = True\n",
    "\n",
    "\n",
    "class TransactionEnriched(BaseModel):\n",
    "    \"\"\"Transaction enrichie avec informations utilisateur.\"\"\"\n",
    "    \n",
    "    transaction_id: str\n",
    "    user_id: int\n",
    "    user_name: str\n",
    "    user_email: str\n",
    "    user_city: str\n",
    "    amount: Decimal\n",
    "    currency: str\n",
    "    timestamp: datetime\n",
    "    status: str\n",
    "    year: int\n",
    "    month: int\n",
    "    day: int\n",
    "    \n",
    "    # TODO: Ajouter des computed fields si n√©cessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/config.py\n",
    "\"\"\"Configuration centralis√©e avec Pydantic Settings.\"\"\"\n",
    "\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \"\"\"Configuration de l'application.\"\"\"\n",
    "    \n",
    "    # API\n",
    "    api_url: str = \"https://jsonplaceholder.typicode.com/users\"\n",
    "    api_timeout: int = 5\n",
    "    api_retries: int = 3\n",
    "    \n",
    "    # Paths\n",
    "    raw_data_path: str = \"data/raw\"\n",
    "    processed_data_path: str = \"data/processed\"\n",
    "    db_path: str = \"data/warehouse.duckdb\"\n",
    "    \n",
    "    # Logging\n",
    "    log_level: str = \"INFO\"\n",
    "    log_format: str = \"json\"\n",
    "    \n",
    "    # Processing\n",
    "    batch_size: int = 1000\n",
    "    partition_by: list[str] = [\"year\", \"month\", \"day\"]\n",
    "    \n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\".env\",\n",
    "        env_file_encoding=\"utf-8\",\n",
    "        case_sensitive=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Instance globale\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/exceptions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/exceptions.py\n",
    "\"\"\"Exceptions personnalis√©es pour le pipeline.\"\"\"\n",
    "\n",
    "\n",
    "class PipelineError(Exception):\n",
    "    \"\"\"Exception de base pour le pipeline.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ExtractionError(PipelineError):\n",
    "    \"\"\"Lev√©e lors d'une erreur d'extraction.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ValidationError(PipelineError):\n",
    "    \"\"\"Lev√©e lors d'une erreur de validation.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class TransformationError(PipelineError):\n",
    "    \"\"\"Lev√©e lors d'une erreur de transformation.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class LoadError(PipelineError):\n",
    "    \"\"\"Lev√©e lors d'une erreur de chargement.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/extract.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/extract.py\n",
    "\"\"\"Module d'extraction des donn√©es.\"\"\"\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import structlog\n",
    "\n",
    "from .config import settings\n",
    "from .exceptions import ExtractionError\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "\n",
    "def extract_csv_files(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extrait et concat√®ne tous les fichiers CSV du dossier.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Chemin vers le dossier contenant les CSV.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame Pandas avec toutes les donn√©es.\n",
    "        \n",
    "    Raises:\n",
    "        ExtractionError: Si aucun fichier trouv√© ou erreur de lecture.\n",
    "    \"\"\"\n",
    "    logger.info(\"csv_extraction_started\", path=data_path)\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Lister tous les fichiers CSV\n",
    "    # 2. Lire chaque fichier avec pd.read_csv\n",
    "    # 3. Concat√©ner tous les DataFrames\n",
    "    # 4. G√©rer les erreurs (fichier vide, format invalide, etc.)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_api_data(url: str, timeout: int = 5, retries: int = 3) -> list[dict[str, Any]]:\n",
    "    \"\"\"Extrait des donn√©es depuis une API REST avec retry.\n",
    "    \n",
    "    Args:\n",
    "        url: URL de l'API.\n",
    "        timeout: Timeout en secondes.\n",
    "        retries: Nombre de tentatives.\n",
    "        \n",
    "    Returns:\n",
    "        Liste de dictionnaires avec les donn√©es.\n",
    "        \n",
    "    Raises:\n",
    "        ExtractionError: Si toutes les tentatives √©chouent.\n",
    "    \"\"\"\n",
    "    logger.info(\"api_extraction_started\", url=url, retries=retries)\n",
    "    \n",
    "    # TODO: Impl√©menter avec retry\n",
    "    # 1. Boucle sur le nombre de retries\n",
    "    # 2. requests.get() avec timeout\n",
    "    # 3. V√©rifier response.status_code\n",
    "    # 4. Parser le JSON\n",
    "    # 5. En cas d'erreur, attendre avant retry (exponential backoff)\n",
    "    # 6. Logger chaque tentative\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_all() -> tuple[pd.DataFrame, list[dict[str, Any]]]:\n",
    "    \"\"\"Extrait toutes les donn√©es (CSV + API).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (transactions_df, users_list).\n",
    "    \"\"\"\n",
    "    transactions_df = extract_csv_files(settings.raw_data_path)\n",
    "    users_list = extract_api_data(settings.api_url, settings.api_timeout, settings.api_retries)\n",
    "    \n",
    "    logger.info(\n",
    "        \"extraction_completed\",\n",
    "        transactions_count=len(transactions_df),\n",
    "        users_count=len(users_list),\n",
    "    )\n",
    "    \n",
    "    return transactions_df, users_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/transform.py\n",
    "\"\"\"Module de transformation des donn√©es.\"\"\"\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import structlog\n",
    "from pydantic import ValidationError\n",
    "\n",
    "from .exceptions import TransformationError, ValidationError as CustomValidationError\n",
    "from .models import Transaction, TransactionEnriched, User\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "\n",
    "def validate_transactions(df: pd.DataFrame) -> list[Transaction]:\n",
    "    \"\"\"Valide les transactions avec Pydantic.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec les transactions brutes.\n",
    "        \n",
    "    Returns:\n",
    "        Liste de transactions valid√©es.\n",
    "    \"\"\"\n",
    "    logger.info(\"validation_started\", entity=\"transactions\", count=len(df))\n",
    "    \n",
    "    valid_transactions = []\n",
    "    errors = []\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. It√©rer sur les lignes du DataFrame\n",
    "    # 2. Pour chaque ligne, essayer de cr√©er un Transaction\n",
    "    # 3. Si ValidationError, logger l'erreur et continuer\n",
    "    # 4. Si OK, ajouter √† valid_transactions\n",
    "    # 5. Logger le nombre d'erreurs\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def validate_users(users_list: list[dict[str, Any]]) -> list[User]:\n",
    "    \"\"\"Valide les utilisateurs avec Pydantic.\n",
    "    \n",
    "    Args:\n",
    "        users_list: Liste de dictionnaires avec les users.\n",
    "        \n",
    "    Returns:\n",
    "        Liste d'utilisateurs valid√©s.\n",
    "    \"\"\"\n",
    "    logger.info(\"validation_started\", entity=\"users\", count=len(users_list))\n",
    "    \n",
    "    # TODO: Impl√©menter (m√™me logique que validate_transactions)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Nettoie le DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame brut.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame nettoy√©.\n",
    "    \"\"\"\n",
    "    logger.info(\"cleaning_started\", rows_before=len(df))\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Supprimer les doublons (drop_duplicates)\n",
    "    # 2. G√©rer les valeurs manquantes\n",
    "    # 3. Convertir les types\n",
    "    # 4. Normaliser les formats\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def enrich_transactions(\n",
    "    transactions: list[Transaction], users: list[User]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Enrichit les transactions avec les informations utilisateurs.\n",
    "    \n",
    "    Args:\n",
    "        transactions: Liste de transactions valid√©es.\n",
    "        users: Liste d'utilisateurs valid√©s.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame enrichi.\n",
    "    \"\"\"\n",
    "    logger.info(\"enrichment_started\")\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Convertir les listes Pydantic en DataFrames\n",
    "    # 2. Joindre sur user_id\n",
    "    # 3. Ajouter des colonnes d√©riv√©es (year, month, day)\n",
    "    # 4. Retourner le DataFrame enrichi\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def transform_all(\n",
    "    transactions_df: pd.DataFrame, users_list: list[dict[str, Any]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Pipeline complet de transformation.\n",
    "    \n",
    "    Args:\n",
    "        transactions_df: DataFrame brut des transactions.\n",
    "        users_list: Liste brute des utilisateurs.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame enrichi et valid√©.\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    valid_transactions = validate_transactions(transactions_df)\n",
    "    valid_users = validate_users(users_list)\n",
    "    \n",
    "    # Enrichissement\n",
    "    enriched_df = enrich_transactions(valid_transactions, valid_users)\n",
    "    \n",
    "    # Nettoyage final\n",
    "    clean_df = clean_dataframe(enriched_df)\n",
    "    \n",
    "    logger.info(\"transformation_completed\", rows=len(clean_df))\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/load.py\n",
    "\"\"\"Module de chargement des donn√©es.\"\"\"\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import structlog\n",
    "\n",
    "from .config import settings\n",
    "from .exceptions import LoadError\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "\n",
    "def export_to_parquet(df: pd.DataFrame, output_path: str, partition_cols: list[str]) -> None:\n",
    "    \"\"\"Exporte le DataFrame en Parquet partitionn√©.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame √† exporter.\n",
    "        output_path: Chemin de base pour l'export.\n",
    "        partition_cols: Colonnes de partitionnement.\n",
    "    \"\"\"\n",
    "    logger.info(\"parquet_export_started\", path=output_path, partitions=partition_cols)\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Utiliser df.to_parquet() avec partition_cols\n",
    "    # 2. Sp√©cifier compression='snappy'\n",
    "    # 3. G√©rer les erreurs (permissions, espace disque)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def load_to_duckdb(df: pd.DataFrame, db_path: str, table_name: str) -> None:\n",
    "    \"\"\"Charge le DataFrame dans DuckDB.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame √† charger.\n",
    "        db_path: Chemin vers la base DuckDB.\n",
    "        table_name: Nom de la table.\n",
    "    \"\"\"\n",
    "    logger.info(\"duckdb_load_started\", table=table_name, rows=len(df))\n",
    "    \n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Se connecter √† DuckDB (duckdb.connect)\n",
    "    # 2. Cr√©er la table si elle n'existe pas\n",
    "    # 3. Ins√©rer les donn√©es (depuis DataFrame ou Parquet)\n",
    "    # 4. V√©rifier l'int√©grit√© (count)\n",
    "    # 5. Fermer la connexion\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def verify_data_integrity(db_path: str, table_name: str, expected_count: int) -> bool:\n",
    "    \"\"\"V√©rifie l'int√©grit√© des donn√©es charg√©es.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Chemin vers la base DuckDB.\n",
    "        table_name: Nom de la table.\n",
    "        expected_count: Nombre de lignes attendu.\n",
    "        \n",
    "    Returns:\n",
    "        True si int√©grit√© OK.\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©menter\n",
    "    # 1. Compter les lignes dans la table\n",
    "    # 2. Comparer avec expected_count\n",
    "    # 3. Logger le r√©sultat\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def load_all(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Pipeline complet de chargement.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame enrichi √† charger.\n",
    "    \"\"\"\n",
    "    # Export Parquet\n",
    "    export_to_parquet(\n",
    "        df, settings.processed_data_path, settings.partition_by\n",
    "    )\n",
    "    \n",
    "    # Load DuckDB\n",
    "    load_to_duckdb(df, settings.db_path, \"transactions_enriched\")\n",
    "    \n",
    "    # V√©rification\n",
    "    is_valid = verify_data_integrity(settings.db_path, \"transactions_enriched\", len(df))\n",
    "    \n",
    "    if not is_valid:\n",
    "        raise LoadError(\"Data integrity check failed\")\n",
    "    \n",
    "    logger.info(\"load_completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : src/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/src/main.py\n",
    "\"\"\"Point d'entr√©e du pipeline ETL.\"\"\"\n",
    "\n",
    "import structlog\n",
    "\n",
    "from .config import settings\n",
    "from .extract import extract_all\n",
    "from .load import load_all\n",
    "from .transform import transform_all\n",
    "\n",
    "# Configuration du logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.add_log_level,\n",
    "        structlog.processors.JSONRenderer(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "\n",
    "def run_pipeline() -> None:\n",
    "    \"\"\"Ex√©cute le pipeline ETL complet.\"\"\"\n",
    "    logger.info(\"pipeline_started\")\n",
    "    \n",
    "    try:\n",
    "        # Extract\n",
    "        transactions_df, users_list = extract_all()\n",
    "        \n",
    "        # Transform\n",
    "        enriched_df = transform_all(transactions_df, users_list)\n",
    "        \n",
    "        # Load\n",
    "        load_all(enriched_df)\n",
    "        \n",
    "        logger.info(\"pipeline_completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"pipeline_failed\", error=str(e), error_type=type(e).__name__)\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : tests/conftest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/tests/conftest.py\n",
    "\"\"\"Fixtures pytest partag√©es.\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_transactions_df():\n",
    "    \"\"\"Fixture avec des transactions de test.\"\"\"\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            \"transaction_id\": \"TXN001\",\n",
    "            \"user_id\": 1,\n",
    "            \"amount\": 150.50,\n",
    "            \"currency\": \"EUR\",\n",
    "            \"timestamp\": \"2024-01-15 10:30:00\",\n",
    "            \"status\": \"completed\",\n",
    "        },\n",
    "        {\n",
    "            \"transaction_id\": \"TXN002\",\n",
    "            \"user_id\": 2,\n",
    "            \"amount\": 89.99,\n",
    "            \"currency\": \"USD\",\n",
    "            \"timestamp\": \"2024-01-15 11:45:00\",\n",
    "            \"status\": \"completed\",\n",
    "        },\n",
    "    ])\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_users_list():\n",
    "    \"\"\"Fixture avec des utilisateurs de test.\"\"\"\n",
    "    return [\n",
    "        {\"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\", \"city\": \"Paris\"},\n",
    "        {\"id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\", \"city\": \"Lyon\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "# TODO: Ajouter d'autres fixtures utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí° Template : pyproject.toml et .env.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/pyproject.toml\n",
    "[project]\n",
    "name = \"pipeline-etl\"\n",
    "version = \"1.0.0\"\n",
    "requires-python = \">=3.11\"\n",
    "dependencies = [\n",
    "    \"pandas>=2.0.0\",\n",
    "    \"pydantic>=2.0.0\",\n",
    "    \"pydantic-settings>=2.0.0\",\n",
    "    \"requests>=2.31.0\",\n",
    "    \"duckdb>=0.9.0\",\n",
    "    \"structlog>=23.0.0\",\n",
    "    \"pytest>=7.4.0\",\n",
    "    \"pytest-cov>=4.1.0\",\n",
    "]\n",
    "\n",
    "[tool.ruff]\n",
    "line-length = 100\n",
    "target-version = \"py311\"\n",
    "\n",
    "[tool.ruff.lint]\n",
    "select = [\"E\", \"W\", \"F\", \"I\", \"B\", \"C4\", \"UP\"]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "testpaths = [\"tests\"]\n",
    "addopts = \"-v --cov=src --cov-report=term-missing --cov-report=html\"\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.11\"\n",
    "warn_return_any = true\n",
    "disallow_untyped_defs = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/.env.example\n",
    "# Configuration du pipeline ETL\n",
    "\n",
    "# API\n",
    "API_URL=https://jsonplaceholder.typicode.com/users\n",
    "API_TIMEOUT=5\n",
    "API_RETRIES=3\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_PATH=data/raw\n",
    "PROCESSED_DATA_PATH=data/processed\n",
    "DB_PATH=data/warehouse.duckdb\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL=INFO\n",
    "LOG_FORMAT=json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ √âtapes recommand√©es\n",
    "\n",
    "1. **Cr√©er la structure** : Dossiers et fichiers vides\n",
    "2. **Impl√©menter Extract** : CSV puis API (avec retry)\n",
    "3. **Impl√©menter les mod√®les Pydantic** : Transaction et User\n",
    "4. **Impl√©menter Transform** : Validation puis enrichissement\n",
    "5. **Impl√©menter Load** : Parquet puis DuckDB\n",
    "6. **Tester chaque module** : Un test √† la fois\n",
    "7. **Int√©grer le pipeline** : main.py\n",
    "8. **Ajouter le logging** : structlog partout\n",
    "9. **Configurer les hooks** : pre-commit\n",
    "10. **Documentation** : README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Ressources\n",
    "\n",
    "### Documentation\n",
    "- [Pydantic](https://docs.pydantic.dev/)\n",
    "- [Pandas](https://pandas.pydata.org/docs/)\n",
    "- [DuckDB](https://duckdb.org/docs/)\n",
    "- [structlog](https://www.structlog.org/)\n",
    "- [pytest](https://docs.pytest.org/)\n",
    "- [Parquet](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "### Concepts ETL\n",
    "- Extract : Ingestion de donn√©es multi-sources\n",
    "- Transform : Validation, nettoyage, enrichissement\n",
    "- Load : Persistence optimis√©e (Parquet, DuckDB)\n",
    "- Data quality : Validation Pydantic, v√©rifications\n",
    "- Observability : Logging structur√©, m√©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Crit√®res de validation finale\n",
    "\n",
    "Avant de soumettre, v√©rifiez que :\n",
    "\n",
    "1. ‚úÖ `python -m src.main` s'ex√©cute sans erreur\n",
    "2. ‚úÖ Les fichiers Parquet sont g√©n√©r√©s et partitionn√©s\n",
    "3. ‚úÖ La base DuckDB contient les donn√©es\n",
    "4. ‚úÖ `pytest` : tous les tests passent, coverage > 80%\n",
    "5. ‚úÖ `ruff check .` : aucune erreur\n",
    "6. ‚úÖ `mypy src/` : aucune erreur\n",
    "7. ‚úÖ Les logs sont en JSON et structur√©s\n",
    "8. ‚úÖ Les mod√®les Pydantic valident correctement\n",
    "9. ‚úÖ Le retry API fonctionne\n",
    "10. ‚úÖ Les exceptions custom sont utilis√©es\n",
    "11. ‚úÖ La configuration .env fonctionne\n",
    "12. ‚úÖ Le README explique l'installation et l'utilisation\n",
    "\n",
    "**Bon courage pour ce projet expert ! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
