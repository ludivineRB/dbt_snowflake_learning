{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# üå≥ Notebook 2 : Raisonnement et Logique Avanc√©s\n",
    "\n",
    "## Tree-of-Thoughts, Self-Consistency et Raisonnement Complexe\n",
    "\n",
    "Dans ce notebook, nous explorons les techniques de raisonnement avanc√©es qui permettent aux LLMs de r√©soudre des probl√®mes complexes en explorant plusieurs chemins de pens√©e.\n",
    "\n",
    "### üéØ Objectifs d'apprentissage :\n",
    "- Ma√Ætriser Tree-of-Thoughts (ToT) pour l'exploration multi-chemins\n",
    "- Impl√©menter Self-Consistency pour am√©liorer la fiabilit√©\n",
    "- Utiliser Step-Back Prompting pour l'abstraction\n",
    "- Cr√©er des syst√®mes de raisonnement robustes\n",
    "- √âvaluer et s√©lectionner les meilleurs chemins de pens√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üìö Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration termin√©e !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration des APIs\n",
    "import openai\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configuration des clients\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "print(\"‚úÖ Configuration termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. üå≥ Tree-of-Thoughts (ToT)\n",
    "\n",
    "### Qu'est-ce que Tree-of-Thoughts ?\n",
    "\n",
    "Tree-of-Thoughts est une technique qui permet aux LLMs d'explorer plusieurs chemins de raisonnement en parall√®le, d'√©valuer chaque branche, et de s√©lectionner le meilleur chemin.\n",
    "\n",
    "**Avantages :**\n",
    "- üéØ Exploration exhaustive des solutions\n",
    "- üìä Auto-√©valuation des chemins\n",
    "- üîÑ Possibilit√© de revenir en arri√®re\n",
    "- üí° D√©couverte de solutions cr√©atives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ThoughtNode:\n",
    "    \"\"\"Repr√©sente un n≈ìud dans l'arbre de pens√©es\"\"\"\n",
    "    content: str\n",
    "    score: float\n",
    "    depth: int\n",
    "    parent: Optional['ThoughtNode'] = None\n",
    "    children: List['ThoughtNode'] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.children is None:\n",
    "            self.children = []\n",
    "\n",
    "class TreeOfThoughts:\n",
    "    \"\"\"Impl√©mentation de Tree-of-Thoughts\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4\", branches: int = 3, depth: int = 3):\n",
    "        self.model = model\n",
    "        self.branches = branches\n",
    "        self.max_depth = depth\n",
    "        self.root = None\n",
    "    \n",
    "    def generate_thoughts(self, prompt: str, context: str = \"\") -> List[str]:\n",
    "        \"\"\"G√©n√®re plusieurs pens√©es/approches pour un probl√®me\"\"\"\n",
    "        generation_prompt = f\"\"\"\n",
    "        Probl√®me : {prompt}\n",
    "        {f'Contexte : {context}' if context else ''}\n",
    "        \n",
    "        G√©n√®re {self.branches} approches diff√©rentes pour r√©soudre ce probl√®me.\n",
    "        Pour chaque approche :\n",
    "        1. Donne un titre court\n",
    "        2. Explique la logique en 2-3 phrases\n",
    "        3. Liste les √©tapes principales\n",
    "        \n",
    "        Format : \n",
    "        Approche 1: [Titre]\n",
    "        [Explication et √©tapes]\n",
    "        ---\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": generation_prompt}],\n",
    "                temperature=0.8,\n",
    "                n=1\n",
    "            )\n",
    "            \n",
    "            thoughts_text = response.choices[0].message.content\n",
    "            thoughts = thoughts_text.split('---')\n",
    "            return [t.strip() for t in thoughts if t.strip()]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur g√©n√©ration : {e}\")\n",
    "            return []\n",
    "    \n",
    "    def evaluate_thought(self, thought: str, criteria: List[str]) -> float:\n",
    "        \"\"\"√âvalue une pens√©e selon des crit√®res\"\"\"\n",
    "        eval_prompt = f\"\"\"\n",
    "        √âvalue cette approche selon les crit√®res suivants :\n",
    "        \n",
    "        Approche : {thought}\n",
    "        \n",
    "        Crit√®res d'√©valuation :\n",
    "        {chr(10).join(f'- {c}' for c in criteria)}\n",
    "        \n",
    "        Pour chaque crit√®re, donne une note sur 10.\n",
    "        Puis calcule la moyenne pond√©r√©e.\n",
    "        \n",
    "        Format de r√©ponse :\n",
    "        - [Crit√®re 1]: X/10 - [Justification]\n",
    "        - [Crit√®re 2]: Y/10 - [Justification]\n",
    "        ...\n",
    "        Score final : Z/10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content\n",
    "            # Extraire le score final\n",
    "            if \"Score final\" in result:\n",
    "                score_text = result.split(\"Score final\")[-1]\n",
    "                score = float(score_text.split(\"/\")[0].replace(\":\", \"\").strip())\n",
    "                return score / 10.0\n",
    "            return 0.5\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur √©valuation : {e}\")\n",
    "            return 0.5\n",
    "    \n",
    "    def expand_node(self, node: ThoughtNode, problem: str) -> List[ThoughtNode]:\n",
    "        \"\"\"Expand un n≈ìud en g√©n√©rant des sous-pens√©es\"\"\"\n",
    "        if node.depth >= self.max_depth:\n",
    "            return []\n",
    "        \n",
    "        expansion_prompt = f\"\"\"\n",
    "        Probl√®me initial : {problem}\n",
    "        \n",
    "        Approche actuelle : {node.content}\n",
    "        \n",
    "        D√©veloppe cette approche en {self.branches} sous-√©tapes ou raffinements.\n",
    "        Chaque sous-√©tape doit √™tre plus sp√©cifique et actionnable.\n",
    "        \"\"\"\n",
    "        \n",
    "        sub_thoughts = self.generate_thoughts(expansion_prompt)\n",
    "        children = []\n",
    "        \n",
    "        for thought in sub_thoughts[:self.branches]:\n",
    "            child = ThoughtNode(\n",
    "                content=thought,\n",
    "                score=0,  # Sera √©valu√© plus tard\n",
    "                depth=node.depth + 1,\n",
    "                parent=node\n",
    "            )\n",
    "            children.append(child)\n",
    "            node.children.append(child)\n",
    "        \n",
    "        return children\n",
    "    \n",
    "    def solve(self, problem: str, criteria: List[str]) -> Dict:\n",
    "        \"\"\"R√©sout un probl√®me en utilisant Tree-of-Thoughts\"\"\"\n",
    "        print(f\"üå≥ D√©marrage Tree-of-Thoughts pour : {problem[:50]}...\")\n",
    "        \n",
    "        # G√©n√©rer les pens√©es initiales\n",
    "        initial_thoughts = self.generate_thoughts(problem)\n",
    "        \n",
    "        # Cr√©er le n≈ìud racine\n",
    "        self.root = ThoughtNode(content=\"Root\", score=0, depth=0)\n",
    "        \n",
    "        # Cr√©er et √©valuer les n≈ìuds de premier niveau\n",
    "        for thought in initial_thoughts[:self.branches]:\n",
    "            node = ThoughtNode(\n",
    "                content=thought,\n",
    "                score=self.evaluate_thought(thought, criteria),\n",
    "                depth=1,\n",
    "                parent=self.root\n",
    "            )\n",
    "            self.root.children.append(node)\n",
    "        \n",
    "        # S√©lectionner les meilleurs n≈ìuds pour expansion\n",
    "        nodes_to_expand = sorted(self.root.children, key=lambda x: x.score, reverse=True)[:2]\n",
    "        \n",
    "        # Expansion r√©cursive\n",
    "        for node in nodes_to_expand:\n",
    "            if node.score > 0.7:  # Seuil de qualit√©\n",
    "                children = self.expand_node(node, problem)\n",
    "                for child in children:\n",
    "                    child.score = self.evaluate_thought(child.content, criteria)\n",
    "        \n",
    "        # Trouver le meilleur chemin\n",
    "        best_path = self.find_best_path()\n",
    "        \n",
    "        return {\n",
    "            'best_path': best_path,\n",
    "            'tree_stats': self.get_tree_stats(),\n",
    "            'all_thoughts': self.get_all_thoughts()\n",
    "        }\n",
    "    \n",
    "    def find_best_path(self) -> List[ThoughtNode]:\n",
    "        \"\"\"Trouve le meilleur chemin dans l'arbre\"\"\"\n",
    "        def get_paths(node: ThoughtNode, current_path: List[ThoughtNode]) -> List[List[ThoughtNode]]:\n",
    "            if not node.children:\n",
    "                return [current_path + [node]]\n",
    "            \n",
    "            paths = []\n",
    "            for child in node.children:\n",
    "                paths.extend(get_paths(child, current_path + [node]))\n",
    "            return paths\n",
    "        \n",
    "        all_paths = get_paths(self.root, [])\n",
    "        \n",
    "        # Calculer le score moyen de chaque chemin\n",
    "        path_scores = []\n",
    "        for path in all_paths:\n",
    "            if len(path) > 1:  # Ignorer le root\n",
    "                avg_score = np.mean([n.score for n in path[1:]])\n",
    "                path_scores.append((path, avg_score))\n",
    "        \n",
    "        # Retourner le meilleur chemin\n",
    "        if path_scores:\n",
    "            best_path, _ = max(path_scores, key=lambda x: x[1])\n",
    "            return best_path[1:]  # Exclure le root\n",
    "        return []\n",
    "    \n",
    "    def get_tree_stats(self) -> Dict:\n",
    "        \"\"\"Obtient des statistiques sur l'arbre\"\"\"\n",
    "        def count_nodes(node: ThoughtNode) -> int:\n",
    "            return 1 + sum(count_nodes(child) for child in node.children)\n",
    "        \n",
    "        total_nodes = count_nodes(self.root) - 1  # Exclure root\n",
    "        all_scores = []\n",
    "        \n",
    "        def collect_scores(node: ThoughtNode):\n",
    "            if node != self.root:\n",
    "                all_scores.append(node.score)\n",
    "            for child in node.children:\n",
    "                collect_scores(child)\n",
    "        \n",
    "        collect_scores(self.root)\n",
    "        \n",
    "        return {\n",
    "            'total_nodes': total_nodes,\n",
    "            'avg_score': np.mean(all_scores) if all_scores else 0,\n",
    "            'max_score': max(all_scores) if all_scores else 0,\n",
    "            'min_score': min(all_scores) if all_scores else 0\n",
    "        }\n",
    "    \n",
    "    def get_all_thoughts(self) -> List[Dict]:\n",
    "        \"\"\"R√©cup√®re toutes les pens√©es avec leurs scores\"\"\"\n",
    "        thoughts = []\n",
    "        \n",
    "        def collect_thoughts(node: ThoughtNode, path: str = \"\"):\n",
    "            if node != self.root:\n",
    "                thoughts.append({\n",
    "                    'content': node.content,\n",
    "                    'score': node.score,\n",
    "                    'depth': node.depth,\n",
    "                    'path': path\n",
    "                })\n",
    "            \n",
    "            for i, child in enumerate(node.children):\n",
    "                new_path = f\"{path}/{i}\" if path else str(i)\n",
    "                collect_thoughts(child, new_path)\n",
    "        \n",
    "        collect_thoughts(self.root)\n",
    "        return sorted(thoughts, key=lambda x: x['score'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### üéØ Exemple pratique : R√©solution de probl√®me complexe avec ToT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ D√©marrage Tree-of-Thoughts pour : \n",
      "Une startup EdTech veut augmenter son taux de r√©t...\n",
      "Erreur g√©n√©ration : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "\n",
      "üåü Meilleur chemin de solution :\n",
      "\n",
      "üìä Statistiques de l'arbre :\n",
      "- N≈ìuds explor√©s : 0\n",
      "- Score moyen : 0.00\n",
      "- Score max : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Probl√®me complexe √† r√©soudre\n",
    "problem = \"\"\"\n",
    "Une startup EdTech veut augmenter son taux de r√©tention utilisateur de 40% √† 70% en 6 mois.\n",
    "Contraintes : budget limit√© (50k‚Ç¨), √©quipe de 5 personnes, app mobile existante.\n",
    "\"\"\"\n",
    "\n",
    "# Crit√®res d'√©valuation\n",
    "criteria = [\n",
    "    \"Faisabilit√© avec les contraintes donn√©es\",\n",
    "    \"Impact potentiel sur la r√©tention\",\n",
    "    \"Rapidit√© de mise en ≈ìuvre\",\n",
    "    \"Co√ªt-efficacit√©\",\n",
    "    \"Mesurabilit√© des r√©sultats\"\n",
    "]\n",
    "\n",
    "# Cr√©er et ex√©cuter ToT\n",
    "tot = TreeOfThoughts(model=\"gpt-3.5-turbo\", branches=3, depth=2)\n",
    "solution = tot.solve(problem, criteria)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\nüåü Meilleur chemin de solution :\")\n",
    "for i, node in enumerate(solution['best_path']):\n",
    "    print(f\"\\n{'  ' * i}Niveau {i+1} (Score: {node.score:.2f})\")\n",
    "    print(f\"{'  ' * i}{node.content[:200]}...\")\n",
    "\n",
    "print(f\"\\nüìä Statistiques de l'arbre :\")\n",
    "stats = solution['tree_stats']\n",
    "print(f\"- N≈ìuds explor√©s : {stats['total_nodes']}\")\n",
    "print(f\"- Score moyen : {stats['avg_score']:.2f}\")\n",
    "print(f\"- Score max : {stats['max_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. üîÑ Self-Consistency\n",
    "\n",
    "### Principe de Self-Consistency\n",
    "\n",
    "Self-Consistency g√©n√®re plusieurs r√©ponses ind√©pendantes √† la m√™me question, puis analyse les convergences pour identifier la r√©ponse la plus fiable.\n",
    "\n",
    "**Avantages :**\n",
    "- ‚úÖ R√©duit les erreurs al√©atoires\n",
    "- üéØ Augmente la fiabilit√©\n",
    "- üìä Permet d'identifier l'incertitude\n",
    "- üîç R√©v√®le diff√©rentes perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfConsistency:\n",
    "    \"\"\"Impl√©mentation de Self-Consistency pour am√©liorer la fiabilit√©\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", samples: int = 5):\n",
    "        self.model = model\n",
    "        self.samples = samples\n",
    "    \n",
    "    def generate_responses(self, prompt: str, variations: bool = True) -> List[str]:\n",
    "        \"\"\"G√©n√®re plusieurs r√©ponses au m√™me prompt\"\"\"\n",
    "        responses = []\n",
    "        \n",
    "        # Variations du prompt pour plus de diversit√©\n",
    "        prompts = []\n",
    "        if variations:\n",
    "            base_variations = [\n",
    "                prompt,\n",
    "                f\"R√©fl√©chis √©tape par √©tape : {prompt}\",\n",
    "                f\"En consid√©rant tous les aspects : {prompt}\",\n",
    "                f\"Analyse en profondeur : {prompt}\",\n",
    "                f\"Donne ta meilleure r√©ponse : {prompt}\"\n",
    "            ]\n",
    "            prompts = base_variations[:self.samples]\n",
    "        else:\n",
    "            prompts = [prompt] * self.samples\n",
    "        \n",
    "        # G√©n√©rer les r√©ponses en parall√®le\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = []\n",
    "            for i, p in enumerate(prompts):\n",
    "                future = executor.submit(self._generate_single, p, 0.7 + i * 0.05)\n",
    "                futures.append(future)\n",
    "            \n",
    "            for future in futures:\n",
    "                try:\n",
    "                    response = future.result(timeout=30)\n",
    "                    if response:\n",
    "                        responses.append(response)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur g√©n√©ration : {e}\")\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def _generate_single(self, prompt: str, temperature: float) -> str:\n",
    "        \"\"\"G√©n√®re une seule r√©ponse\"\"\"\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur API : {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def extract_key_elements(self, response: str) -> Dict:\n",
    "        \"\"\"Extrait les √©l√©ments cl√©s d'une r√©ponse\"\"\"\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyse cette r√©ponse et extrais :\n",
    "        1. Les points principaux (liste)\n",
    "        2. Les chiffres/donn√©es mentionn√©s\n",
    "        3. Les recommandations sp√©cifiques\n",
    "        4. Le ton/sentiment g√©n√©ral\n",
    "        \n",
    "        R√©ponse √† analyser :\n",
    "        {response}\n",
    "        \n",
    "        Format JSON requis.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": extraction_prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            # Parser le JSON\n",
    "            content = result.choices[0].message.content\n",
    "            # Extraire le JSON m√™me s'il est entour√© de texte\n",
    "            import re\n",
    "            json_match = re.search(r'\\{[^{}]*\\}', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "            return {}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur extraction : {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def analyze_consistency(self, responses: List[str]) -> Dict:\n",
    "        \"\"\"Analyse la coh√©rence entre les r√©ponses\"\"\"\n",
    "        if not responses:\n",
    "            return {'error': 'Aucune r√©ponse √† analyser'}\n",
    "        \n",
    "        # Extraire les √©l√©ments cl√©s de chaque r√©ponse\n",
    "        all_elements = []\n",
    "        for resp in responses:\n",
    "            elements = self.extract_key_elements(resp)\n",
    "            if elements:\n",
    "                all_elements.append(elements)\n",
    "        \n",
    "        # Analyser les convergences\n",
    "        consistency_prompt = f\"\"\"\n",
    "        Analyse la coh√©rence entre ces {len(responses)} r√©ponses :\n",
    "        \n",
    "        {chr(10).join([f'R√©ponse {i+1}: {r[:200]}...' for i, r in enumerate(responses)])}\n",
    "        \n",
    "        Identifie :\n",
    "        1. Points de convergence (mentionn√©s dans 3+ r√©ponses)\n",
    "        2. Points de divergence majeurs\n",
    "        3. Niveau de coh√©rence global (0-100%)\n",
    "        4. Recommandation synth√©tis√©e\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            analysis = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": consistency_prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'analysis': analysis.choices[0].message.content,\n",
    "                'num_responses': len(responses),\n",
    "                'elements_extracted': len(all_elements)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur analyse : {e}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def get_consensus(self, prompt: str, return_all: bool = False) -> Dict:\n",
    "        \"\"\"Obtient un consensus via Self-Consistency\"\"\"\n",
    "        print(f\"üîÑ G√©n√©ration de {self.samples} r√©ponses...\")\n",
    "        \n",
    "        # G√©n√©rer plusieurs r√©ponses\n",
    "        responses = self.generate_responses(prompt)\n",
    "        \n",
    "        if not responses:\n",
    "            return {'error': 'Aucune r√©ponse g√©n√©r√©e'}\n",
    "        \n",
    "        print(f\"‚úÖ {len(responses)} r√©ponses g√©n√©r√©es\")\n",
    "        \n",
    "        # Analyser la coh√©rence\n",
    "        consistency = self.analyze_consistency(responses)\n",
    "        \n",
    "        # Cr√©er une r√©ponse consensus\n",
    "        consensus_prompt = f\"\"\"\n",
    "        Bas√© sur ces {len(responses)} r√©ponses diff√©rentes au m√™me probl√®me,\n",
    "        cr√©e une r√©ponse consensus qui int√®gre les meilleurs √©l√©ments de chaque r√©ponse.\n",
    "        \n",
    "        Probl√®me original : {prompt}\n",
    "        \n",
    "        R√©ponses √† synth√©tiser :\n",
    "        {chr(10).join([f'--- R√©ponse {i+1} ---\\n{r}' for i, r in enumerate(responses)])}\n",
    "        \n",
    "        Cr√©e une r√©ponse finale optimale qui :\n",
    "        - Int√®gre les points de convergence\n",
    "        - R√©sout les contradictions\n",
    "        - Offre la solution la plus compl√®te et fiable\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            consensus_response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": consensus_prompt}],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'consensus': consensus_response.choices[0].message.content,\n",
    "                'consistency_analysis': consistency,\n",
    "                'num_responses': len(responses)\n",
    "            }\n",
    "            \n",
    "            if return_all:\n",
    "                result['all_responses'] = responses\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur consensus : {e}\")\n",
    "            return {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### üéØ Exemple : D√©cision critique avec Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ G√©n√©ration de 5 r√©ponses...\n",
      "Erreur API : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Erreur API : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Erreur API : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Erreur API : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "Erreur API : \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "\n",
      "üìä Analyse de coh√©rence :\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'consistency_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Afficher l'analyse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Analyse de coh√©rence :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconsistency_analysis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ R√©ponse consensus :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m'\u001b[39m\u001b[33mconsensus\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'consistency_analysis'"
     ]
    }
   ],
   "source": [
    "# Question n√©cessitant une haute fiabilit√©\n",
    "critical_question = \"\"\"\n",
    "Notre entreprise SaaS B2B (CRM pour PME) stagne √† 2M‚Ç¨ ARR depuis 6 mois.\n",
    "Nous h√©sitons entre 3 strat√©gies :\n",
    "1. Monter en gamme vers l'entreprise (investissement 500k‚Ç¨)\n",
    "2. Expansion internationale (investissement 300k‚Ç¨)\n",
    "3. Ajouter des features IA (investissement 400k‚Ç¨)\n",
    "\n",
    "Quelle strat√©gie recommandes-tu et pourquoi ?\n",
    "\"\"\"\n",
    "\n",
    "# Utiliser Self-Consistency\n",
    "sc = SelfConsistency(samples=5)\n",
    "result = sc.get_consensus(critical_question, return_all=True)\n",
    "\n",
    "# Afficher l'analyse\n",
    "print(\"\\nüìä Analyse de coh√©rence :\")\n",
    "print(result['consistency_analysis']['analysis'])\n",
    "\n",
    "print(\"\\n‚úÖ R√©ponse consensus :\")\n",
    "print(result['consensus'])\n",
    "\n",
    "# Optionnel : voir toutes les r√©ponses\n",
    "if 'all_responses' in result:\n",
    "    print(f\"\\nüîç Aper√ßu des {len(result['all_responses'])} r√©ponses g√©n√©r√©es :\")\n",
    "    for i, resp in enumerate(result['all_responses']):\n",
    "        print(f\"\\nR√©ponse {i+1} (extrait) : {resp[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3. üîô Step-Back Prompting\n",
    "\n",
    "### Principe du Step-Back Prompting\n",
    "\n",
    "Step-Back Prompting r√©sout les probl√®mes en prenant d'abord du recul pour comprendre les principes fondamentaux avant de s'attaquer au cas sp√©cifique.\n",
    "\n",
    "**Processus :**\n",
    "1. üéØ Identifier la question sp√©cifique\n",
    "2. üîç Formuler une question plus g√©n√©rale\n",
    "3. üìö Comprendre les principes fondamentaux\n",
    "4. üîß Appliquer au cas sp√©cifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepBackPrompting:\n",
    "    \"\"\"Impl√©mentation de Step-Back Prompting pour la r√©solution abstraite\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def generate_abstract_question(self, specific_question: str) -> str:\n",
    "        \"\"\"G√©n√®re une question plus abstraite/g√©n√©rale\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Question sp√©cifique : {specific_question}\n",
    "        \n",
    "        Reformule cette question de mani√®re plus abstraite et g√©n√©rale.\n",
    "        La question abstraite doit :\n",
    "        1. Capturer l'essence du probl√®me\n",
    "        2. √ätre applicable √† une classe plus large de situations\n",
    "        3. Permettre d'identifier les principes fondamentaux\n",
    "        \n",
    "        Exemples :\n",
    "        - Sp√©cifique : \"Comment augmenter les ventes de mon restaurant italien ?\"\n",
    "        - Abstraite : \"Quels sont les principes pour augmenter les revenus d'un business de service local ?\"\n",
    "        \n",
    "        Question abstraite :\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur : {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def get_fundamental_principles(self, abstract_question: str) -> str:\n",
    "        \"\"\"Obtient les principes fondamentaux pour la question abstraite\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Question : {abstract_question}\n",
    "        \n",
    "        Identifie et explique les principes fondamentaux qui s'appliquent.\n",
    "        Pour chaque principe :\n",
    "        1. Nom du principe\n",
    "        2. Explication claire\n",
    "        3. Pourquoi c'est important\n",
    "        4. Exemples d'application\n",
    "        \n",
    "        Vise 3-5 principes fondamentaux.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.5,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur : {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def apply_to_specific(self, principles: str, specific_question: str, context: str = \"\") -> str:\n",
    "        \"\"\"Applique les principes au cas sp√©cifique\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Principes fondamentaux identifi√©s :\n",
    "        {principles}\n",
    "        \n",
    "        Question sp√©cifique √† r√©soudre :\n",
    "        {specific_question}\n",
    "        \n",
    "        {f'Contexte additionnel : {context}' if context else ''}\n",
    "        \n",
    "        Applique ces principes pour cr√©er une solution sp√©cifique et actionnable.\n",
    "        Structure ta r√©ponse :\n",
    "        1. Rappel des principes pertinents\n",
    "        2. Application concr√®te au cas\n",
    "        3. Plan d'action d√©taill√©\n",
    "        4. M√©triques de succ√®s\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.6,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur : {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def solve_with_stepback(self, specific_question: str, context: str = \"\") -> Dict:\n",
    "        \"\"\"Pipeline complet de Step-Back Prompting\"\"\"\n",
    "        print(\"üîô D√©marrage Step-Back Prompting...\")\n",
    "        \n",
    "        # √âtape 1 : G√©n√©rer question abstraite\n",
    "        print(\"1Ô∏è‚É£ G√©n√©ration de la question abstraite...\")\n",
    "        abstract_q = self.generate_abstract_question(specific_question)\n",
    "        \n",
    "        if not abstract_q:\n",
    "            return {'error': 'Impossible de g√©n√©rer la question abstraite'}\n",
    "        \n",
    "        # √âtape 2 : Obtenir les principes\n",
    "        print(\"2Ô∏è‚É£ Identification des principes fondamentaux...\")\n",
    "        principles = self.get_fundamental_principles(abstract_q)\n",
    "        \n",
    "        if not principles:\n",
    "            return {'error': 'Impossible d\\'identifier les principes'}\n",
    "        \n",
    "        # √âtape 3 : Appliquer au cas sp√©cifique\n",
    "        print(\"3Ô∏è‚É£ Application au cas sp√©cifique...\")\n",
    "        solution = self.apply_to_specific(principles, specific_question, context)\n",
    "        \n",
    "        return {\n",
    "            'specific_question': specific_question,\n",
    "            'abstract_question': abstract_q,\n",
    "            'principles': principles,\n",
    "            'solution': solution\n",
    "        }\n",
    "    \n",
    "    def compare_with_direct(self, question: str) -> Dict:\n",
    "        \"\"\"Compare Step-Back avec approche directe\"\"\"\n",
    "        print(\"üî¨ Comparaison Step-Back vs Direct...\")\n",
    "        \n",
    "        # Approche directe\n",
    "        direct_prompt = f\"R√©ponds directement : {question}\"\n",
    "        try:\n",
    "            direct_response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": direct_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            direct_answer = direct_response.choices[0].message.content\n",
    "        except:\n",
    "            direct_answer = \"Erreur\"\n",
    "        \n",
    "        # Approche Step-Back\n",
    "        stepback_result = self.solve_with_stepback(question)\n",
    "        \n",
    "        return {\n",
    "            'direct_approach': direct_answer,\n",
    "            'stepback_approach': stepback_result\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üéØ Exemple : R√©solution complexe avec Step-Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probl√®me sp√©cifique complexe\n",
    "specific_problem = \"\"\"\n",
    "Notre plateforme de e-learning pour d√©veloppeurs a un probl√®me :\n",
    "70% des utilisateurs abandonnent apr√®s le premier cours.\n",
    "Comment am√©liorer drastiquement l'engagement ?\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "- Plateforme lanc√©e il y a 6 mois\n",
    "- 5000 utilisateurs inscrits\n",
    "- Cours de 2-4 heures en moyenne\n",
    "- Principalement du contenu vid√©o\n",
    "- Audience : d√©veloppeurs juniors √† mid-level\n",
    "\"\"\"\n",
    "\n",
    "# Utiliser Step-Back Prompting\n",
    "sb = StepBackPrompting()\n",
    "result = sb.solve_with_stepback(specific_problem, context)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\n‚ùì Question abstraite g√©n√©r√©e :\")\n",
    "print(result['abstract_question'])\n",
    "\n",
    "print(\"\\nüìö Principes fondamentaux identifi√©s :\")\n",
    "print(result['principles'][:500] + \"...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution sp√©cifique :\")\n",
    "print(result['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### üî¨ Comparaison : Step-Back vs Approche Directe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les deux approches\n",
    "comparison = sb.compare_with_direct(specific_problem)\n",
    "\n",
    "print(\"üîÑ APPROCHE DIRECTE :\")\n",
    "print(comparison['direct_approach'][:500] + \"...\")\n",
    "\n",
    "print(\"\\n\\nüîô APPROCHE STEP-BACK :\")\n",
    "if 'solution' in comparison['stepback_approach']:\n",
    "    print(comparison['stepback_approach']['solution'][:500] + \"...\")\n",
    "\n",
    "print(\"\\n\\nüìä Analyse comparative :\")\n",
    "print(\"- L'approche directe donne des solutions imm√©diates mais parfois superficielles\")\n",
    "print(\"- Step-Back identifie les causes profondes et propose des solutions syst√©miques\")\n",
    "print(\"- Step-Back est plus adapt√© aux probl√®mes complexes n√©cessitant une compr√©hension profonde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 4. üß© Combinaison des Techniques\n",
    "\n",
    "### Framework de Raisonnement Hybride\n",
    "\n",
    "Combinons ToT, Self-Consistency et Step-Back pour cr√©er un syst√®me de raisonnement ultra-robuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridReasoning:\n",
    "    \"\"\"Combine plusieurs techniques de raisonnement avanc√©es\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.tot = TreeOfThoughts(model, branches=3, depth=2)\n",
    "        self.sc = SelfConsistency(model, samples=3)\n",
    "        self.sb = StepBackPrompting(model)\n",
    "    \n",
    "    def analyze_problem_complexity(self, problem: str) -> Dict:\n",
    "        \"\"\"Analyse la complexit√© du probl√®me pour choisir la meilleure approche\"\"\"\n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyse ce probl√®me et d√©termine :\n",
    "        1. Niveau de complexit√© (1-10)\n",
    "        2. Type de probl√®me (analytique, cr√©atif, strat√©gique, technique)\n",
    "        3. Besoin de fiabilit√© (1-10)\n",
    "        4. Besoin d'exploration (1-10)\n",
    "        \n",
    "        Probl√®me : {problem}\n",
    "        \n",
    "        R√©ponds en JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            # Parser la r√©ponse\n",
    "            import re\n",
    "            content = response.choices[0].message.content\n",
    "            # Essayer d'extraire les valeurs m√™me si ce n'est pas du JSON parfait\n",
    "            \n",
    "            complexity = int(re.search(r'complexit√©.*?(\\d+)', content, re.I).group(1)) if re.search(r'complexit√©.*?(\\d+)', content, re.I) else 5\n",
    "            reliability = int(re.search(r'fiabilit√©.*?(\\d+)', content, re.I).group(1)) if re.search(r'fiabilit√©.*?(\\d+)', content, re.I) else 5\n",
    "            exploration = int(re.search(r'exploration.*?(\\d+)', content, re.I).group(1)) if re.search(r'exploration.*?(\\d+)', content, re.I) else 5\n",
    "            \n",
    "            return {\n",
    "                'complexity': complexity,\n",
    "                'reliability_need': reliability,\n",
    "                'exploration_need': exploration\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur analyse : {e}\")\n",
    "            return {'complexity': 5, 'reliability_need': 5, 'exploration_need': 5}\n",
    "    \n",
    "    def select_techniques(self, analysis: Dict) -> List[str]:\n",
    "        \"\"\"S√©lectionne les techniques appropri√©es selon l'analyse\"\"\"\n",
    "        techniques = []\n",
    "        \n",
    "        # Step-Back pour les probl√®mes complexes\n",
    "        if analysis['complexity'] >= 7:\n",
    "            techniques.append('step_back')\n",
    "        \n",
    "        # Self-Consistency pour la fiabilit√©\n",
    "        if analysis['reliability_need'] >= 7:\n",
    "            techniques.append('self_consistency')\n",
    "        \n",
    "        # Tree-of-Thoughts pour l'exploration\n",
    "        if analysis['exploration_need'] >= 6:\n",
    "            techniques.append('tree_of_thoughts')\n",
    "        \n",
    "        # Toujours au moins une technique\n",
    "        if not techniques:\n",
    "            techniques.append('self_consistency')\n",
    "        \n",
    "        return techniques\n",
    "    \n",
    "    def solve_hybrid(self, problem: str, context: str = \"\") -> Dict:\n",
    "        \"\"\"R√©sout un probl√®me en utilisant la meilleure combinaison de techniques\"\"\"\n",
    "        print(\"üß© D√©marrage du raisonnement hybride...\")\n",
    "        \n",
    "        # Analyser le probl√®me\n",
    "        print(\"üìä Analyse du probl√®me...\")\n",
    "        analysis = self.analyze_problem_complexity(problem)\n",
    "        print(f\"Complexit√© : {analysis['complexity']}/10\")\n",
    "        print(f\"Besoin fiabilit√© : {analysis['reliability_need']}/10\")\n",
    "        print(f\"Besoin exploration : {analysis['exploration_need']}/10\")\n",
    "        \n",
    "        # S√©lectionner les techniques\n",
    "        techniques = self.select_techniques(analysis)\n",
    "        print(f\"\\nüîß Techniques s√©lectionn√©es : {', '.join(techniques)}\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Appliquer Step-Back si n√©cessaire\n",
    "        if 'step_back' in techniques:\n",
    "            print(\"\\nüîô Application Step-Back...\")\n",
    "            sb_result = self.sb.solve_with_stepback(problem, context)\n",
    "            results['step_back'] = sb_result\n",
    "            \n",
    "            # Utiliser les principes pour enrichir le contexte\n",
    "            if 'principles' in sb_result:\n",
    "                context += f\"\\n\\nPrincipes identifi√©s : {sb_result['principles'][:200]}\"\n",
    "        \n",
    "        # Appliquer Tree-of-Thoughts si n√©cessaire\n",
    "        if 'tree_of_thoughts' in techniques:\n",
    "            print(\"\\nüå≥ Application Tree-of-Thoughts...\")\n",
    "            criteria = [\n",
    "                \"Faisabilit√© pratique\",\n",
    "                \"Impact potentiel\",\n",
    "                \"Innovation de l'approche\",\n",
    "                \"Risques et mitigation\"\n",
    "            ]\n",
    "            tot_result = self.tot.solve(problem + \"\\n\" + context, criteria)\n",
    "            results['tree_of_thoughts'] = tot_result\n",
    "        \n",
    "        # Appliquer Self-Consistency si n√©cessaire\n",
    "        if 'self_consistency' in techniques:\n",
    "            print(\"\\nüîÑ Application Self-Consistency...\")\n",
    "            \n",
    "            # Cr√©er un prompt enrichi avec les r√©sultats pr√©c√©dents\n",
    "            enriched_prompt = problem\n",
    "            if 'step_back' in results and 'solution' in results['step_back']:\n",
    "                enriched_prompt += f\"\\n\\nConsid√®re ces insights : {results['step_back']['solution'][:300]}...\"\n",
    "            if 'tree_of_thoughts' in results and results['tree_of_thoughts']['best_path']:\n",
    "                best_thought = results['tree_of_thoughts']['best_path'][0].content\n",
    "                enriched_prompt += f\"\\n\\nApproche prometteuse identifi√©e : {best_thought[:200]}...\"\n",
    "            \n",
    "            sc_result = self.sc.get_consensus(enriched_prompt)\n",
    "            results['self_consistency'] = sc_result\n",
    "        \n",
    "        # Synth√®se finale\n",
    "        print(\"\\nüéØ Synth√®se des r√©sultats...\")\n",
    "        final_synthesis = self.create_synthesis(problem, results, techniques)\n",
    "        \n",
    "        return {\n",
    "            'analysis': analysis,\n",
    "            'techniques_used': techniques,\n",
    "            'results': results,\n",
    "            'synthesis': final_synthesis\n",
    "        }\n",
    "    \n",
    "    def create_synthesis(self, problem: str, results: Dict, techniques: List[str]) -> str:\n",
    "        \"\"\"Cr√©e une synth√®se finale des diff√©rentes approches\"\"\"\n",
    "        synthesis_prompt = f\"\"\"\n",
    "        Probl√®me original : {problem}\n",
    "        \n",
    "        R√©sultats des diff√©rentes techniques de raisonnement :\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if 'step_back' in results and 'solution' in results['step_back']:\n",
    "            synthesis_prompt += f\"\\n**Step-Back Analysis:**\\n{results['step_back']['solution'][:400]}...\\n\"\n",
    "        \n",
    "        if 'tree_of_thoughts' in results and results['tree_of_thoughts']['best_path']:\n",
    "            best_thoughts = [n.content[:200] for n in results['tree_of_thoughts']['best_path'][:2]]\n",
    "            synthesis_prompt += f\"\\n**Tree-of-Thoughts Best Path:**\\n{chr(10).join(best_thoughts)}\\n\"\n",
    "        \n",
    "        if 'self_consistency' in results and 'consensus' in results['self_consistency']:\n",
    "            synthesis_prompt += f\"\\n**Self-Consistency Consensus:**\\n{results['self_consistency']['consensus'][:400]}...\\n\"\n",
    "        \n",
    "        synthesis_prompt += \"\"\"\n",
    "        \n",
    "        Cr√©e une synth√®se finale qui :\n",
    "        1. Int√®gre les meilleures id√©es de chaque approche\n",
    "        2. R√©sout les contradictions √©ventuelles\n",
    "        3. Propose un plan d'action clair et prioris√©\n",
    "        4. Identifie les risques et les mitigation\n",
    "        5. D√©finit des m√©triques de succ√®s\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": synthesis_prompt}],\n",
    "                temperature=0.4,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur synth√®se : {e}\")\n",
    "            return \"Erreur lors de la cr√©ation de la synth√®se\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### üöÄ Exemple ultime : Probl√®me complexe avec raisonnement hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probl√®me tr√®s complexe n√©cessitant plusieurs approches\n",
    "complex_problem = \"\"\"\n",
    "Notre entreprise FinTech (paiements B2B internationaux) fait face √† une d√©cision critique :\n",
    "- Croissance ralentie (15% vs 50% l'an dernier)\n",
    "- Nouveaux concurrents agressifs\n",
    "- R√©glementation changeante en Europe\n",
    "- Opportunit√© d'acquisition d'un concurrent plus petit (15M‚Ç¨)\n",
    "- Ou investir dans l'expansion Asie (20M‚Ç¨)\n",
    "- Budget total disponible : 25M‚Ç¨\n",
    "\n",
    "Quelle strat√©gie adopter pour retrouver une croissance forte tout en minimisant les risques ?\n",
    "\"\"\"\n",
    "\n",
    "additional_context = \"\"\"\n",
    "- 200 employ√©s actuellement\n",
    "- ARR : 30M‚Ç¨\n",
    "- Pr√©sence : Europe (80%), US (20%)\n",
    "- Technologie propri√©taire forte\n",
    "- NPS : 45 (en baisse)\n",
    "\"\"\"\n",
    "\n",
    "# R√©soudre avec approche hybride\n",
    "hybrid = HybridReasoning()\n",
    "solution = hybrid.solve_hybrid(complex_problem, additional_context)\n",
    "\n",
    "# Afficher la synth√®se finale\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ SYNTH√àSE FINALE - RECOMMANDATION STRAT√âGIQUE\")\n",
    "print(\"=\"*50)\n",
    "print(solution['synthesis'])\n",
    "\n",
    "# Statistiques sur l'analyse\n",
    "print(\"\\nüìä Statistiques de l'analyse :\")\n",
    "print(f\"- Techniques utilis√©es : {', '.join(solution['techniques_used'])}\")\n",
    "print(f\"- Complexit√© identifi√©e : {solution['analysis']['complexity']}/10\")\n",
    "if 'tree_of_thoughts' in solution['results']:\n",
    "    stats = solution['results']['tree_of_thoughts']['tree_stats']\n",
    "    print(f\"- Chemins explor√©s (ToT) : {stats['total_nodes']}\")\n",
    "if 'self_consistency' in solution['results']:\n",
    "    print(f\"- R√©ponses g√©n√©r√©es (SC) : {solution['results']['self_consistency']['num_responses']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. üìä M√©triques et √âvaluation du Raisonnement\n",
    "\n",
    "### Framework d'√©valuation des techniques de raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReasoningEvaluator:\n",
    "    \"\"\"√âvalue et compare diff√©rentes techniques de raisonnement\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.metrics = {\n",
    "            'coherence': 'Coh√©rence logique de la r√©ponse',\n",
    "            'completeness': 'Exhaustivit√© de la solution',\n",
    "            'practicality': 'Applicabilit√© pratique',\n",
    "            'innovation': 'Originalit√© et cr√©ativit√©',\n",
    "            'clarity': 'Clart√© et structure'\n",
    "        }\n",
    "    \n",
    "    def evaluate_response(self, response: str, problem: str) -> Dict[str, float]:\n",
    "        \"\"\"√âvalue une r√©ponse selon plusieurs m√©triques\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for metric, description in self.metrics.items():\n",
    "            eval_prompt = f\"\"\"\n",
    "            Probl√®me : {problem}\n",
    "            \n",
    "            R√©ponse √† √©valuer : {response}\n",
    "            \n",
    "            √âvalue cette r√©ponse sur le crit√®re : {description}\n",
    "            Donne une note de 0 √† 10 et justifie bri√®vement.\n",
    "            \n",
    "            Format : [Note]/10 - [Justification courte]\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                result = openai.ChatCompletion.create(\n",
    "                    model=self.model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n",
    "                    temperature=0.3\n",
    "                )\n",
    "                \n",
    "                # Extraire la note\n",
    "                content = result.choices[0].message.content\n",
    "                import re\n",
    "                match = re.search(r'(\\d+(\\.\\d+)?)/10', content)\n",
    "                if match:\n",
    "                    scores[metric] = float(match.group(1))\n",
    "                else:\n",
    "                    scores[metric] = 5.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                scores[metric] = 5.0\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare_techniques(self, problem: str, responses: Dict[str, str]) -> Dict:\n",
    "        \"\"\"Compare plusieurs techniques sur le m√™me probl√®me\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for technique, response in responses.items():\n",
    "            print(f\"\\nüìä √âvaluation de {technique}...\")\n",
    "            scores = self.evaluate_response(response, problem)\n",
    "            results[technique] = {\n",
    "                'scores': scores,\n",
    "                'average': np.mean(list(scores.values()))\n",
    "            }\n",
    "        \n",
    "        # Cr√©er un rapport comparatif\n",
    "        report = self.generate_comparison_report(results)\n",
    "        \n",
    "        return {\n",
    "            'detailed_scores': results,\n",
    "            'report': report,\n",
    "            'winner': max(results.items(), key=lambda x: x[1]['average'])[0]\n",
    "        }\n",
    "    \n",
    "    def generate_comparison_report(self, results: Dict) -> str:\n",
    "        \"\"\"G√©n√®re un rapport de comparaison d√©taill√©\"\"\"\n",
    "        report = \"üìä RAPPORT COMPARATIF DES TECHNIQUES\\n\"\n",
    "        report += \"=\" * 40 + \"\\n\\n\"\n",
    "        \n",
    "        # Scores par technique\n",
    "        for technique, data in results.items():\n",
    "            report += f\"\\n{technique.upper()}\\n\"\n",
    "            report += f\"Score moyen : {data['average']:.1f}/10\\n\"\n",
    "            for metric, score in data['scores'].items():\n",
    "                report += f\"  - {metric}: {score:.1f}/10\\n\"\n",
    "        \n",
    "        # Meilleure technique par m√©trique\n",
    "        report += \"\\n\\nüèÜ MEILLEURES TECHNIQUES PAR CRIT√àRE\\n\"\n",
    "        for metric in self.metrics.keys():\n",
    "            best_technique = max(results.items(), \n",
    "                               key=lambda x: x[1]['scores'].get(metric, 0))[0]\n",
    "            best_score = results[best_technique]['scores'].get(metric, 0)\n",
    "            report += f\"- {metric}: {best_technique} ({best_score:.1f}/10)\\n\"\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### üèÅ Test comparatif final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probl√®me test pour comparaison\n",
    "test_problem = \"\"\"\n",
    "Comment une PME de 50 personnes peut-elle impl√©menter une culture d'innovation \n",
    "sans perturber ses op√©rations quotidiennes ?\n",
    "\"\"\"\n",
    "\n",
    "# G√©n√©rer des r√©ponses avec diff√©rentes techniques\n",
    "print(\"üß™ G√©n√©ration des r√©ponses avec diff√©rentes techniques...\")\n",
    "\n",
    "responses = {}\n",
    "\n",
    "# 1. Approche directe\n",
    "print(\"\\n1Ô∏è‚É£ Approche directe...\")\n",
    "try:\n",
    "    direct = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": test_problem}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    responses['direct'] = direct.choices[0].message.content\n",
    "except:\n",
    "    responses['direct'] = \"Erreur\"\n",
    "\n",
    "# 2. Tree-of-Thoughts\n",
    "print(\"\\n2Ô∏è‚É£ Tree-of-Thoughts...\")\n",
    "tot_instance = TreeOfThoughts(branches=2, depth=2)\n",
    "tot_result = tot_instance.solve(test_problem, [\"Faisabilit√©\", \"Impact\", \"Co√ªt\"])\n",
    "if tot_result['best_path']:\n",
    "    responses['tree_of_thoughts'] = \"\\n\".join([n.content for n in tot_result['best_path']])\n",
    "\n",
    "# 3. Self-Consistency\n",
    "print(\"\\n3Ô∏è‚É£ Self-Consistency...\")\n",
    "sc_instance = SelfConsistency(samples=3)\n",
    "sc_result = sc_instance.get_consensus(test_problem)\n",
    "if 'consensus' in sc_result:\n",
    "    responses['self_consistency'] = sc_result['consensus']\n",
    "\n",
    "# 4. Step-Back\n",
    "print(\"\\n4Ô∏è‚É£ Step-Back Prompting...\")\n",
    "sb_instance = StepBackPrompting()\n",
    "sb_result = sb_instance.solve_with_stepback(test_problem)\n",
    "if 'solution' in sb_result:\n",
    "    responses['step_back'] = sb_result['solution']\n",
    "\n",
    "# √âvaluer et comparer\n",
    "print(\"\\nüî¨ √âvaluation comparative...\")\n",
    "evaluator = ReasoningEvaluator()\n",
    "comparison = evaluator.compare_techniques(test_problem, responses)\n",
    "\n",
    "# Afficher le rapport\n",
    "print(\"\\n\" + comparison['report'])\n",
    "print(f\"\\nüèÜ Technique gagnante : {comparison['winner'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## üìö R√©capitulatif et Bonnes Pratiques\n",
    "\n",
    "### üéØ Quand utiliser chaque technique :\n",
    "\n",
    "#### Tree-of-Thoughts (ToT)\n",
    "- ‚úÖ Probl√®mes avec multiples solutions possibles\n",
    "- ‚úÖ Besoin d'exploration cr√©ative\n",
    "- ‚úÖ D√©cisions strat√©giques complexes\n",
    "- ‚ùå √âviter pour les questions simples (overhead important)\n",
    "\n",
    "#### Self-Consistency\n",
    "- ‚úÖ Besoin de haute fiabilit√©\n",
    "- ‚úÖ Questions critiques n√©cessitant validation\n",
    "- ‚úÖ R√©duction du bruit et des erreurs\n",
    "- ‚ùå √âviter si le temps/co√ªt est critique\n",
    "\n",
    "#### Step-Back Prompting\n",
    "- ‚úÖ Probl√®mes n√©cessitant compr√©hension profonde\n",
    "- ‚úÖ Cas o√π les principes fondamentaux sont importants\n",
    "- ‚úÖ Transfert de connaissances entre domaines\n",
    "- ‚ùå √âviter pour les t√¢ches purement ex√©cutives\n",
    "\n",
    "### üí° Conseils d'impl√©mentation :\n",
    "\n",
    "1. **Commencez simple** : Testez d'abord chaque technique individuellement\n",
    "2. **Mesurez l'impact** : Utilisez des m√©triques pour valider l'am√©lioration\n",
    "3. **Adaptez au contexte** : Chaque domaine a ses sp√©cificit√©s\n",
    "4. **Optimisez les co√ªts** : Ces techniques consomment plus de tokens\n",
    "5. **Documentez** : Gardez trace des configurations qui fonctionnent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## üöÄ Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Impl√©menter votre propre variante de ToT\n",
    "\n",
    "Cr√©ez une version de Tree-of-Thoughts qui :\n",
    "- Utilise diff√©rents mod√®les pour chaque branche\n",
    "- Impl√©mente un syst√®me de vote entre branches\n",
    "- Permet le backtracking si une branche √©choue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "# Indice : Commencez par modifier la classe TreeOfThoughts\n",
    "# pour supporter multiple mod√®les"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Exercice 2 : Cr√©er un benchmark de raisonnement\n",
    "\n",
    "D√©veloppez un ensemble de probl√®mes tests pour √©valuer syst√©matiquement les diff√©rentes techniques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ez un benchmark avec diff√©rents types de probl√®mes\n",
    "benchmark_problems = [\n",
    "    {\n",
    "        'type': 'analytical',\n",
    "        'problem': 'Comment optimiser la supply chain d\\'une entreprise e-commerce ?',\n",
    "        'expected_qualities': ['structured', 'data-driven', 'actionable']\n",
    "    },\n",
    "    # Ajoutez plus de probl√®mes...\n",
    "]\n",
    "\n",
    "# Votre code pour tester syst√©matiquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## üéì Pour aller plus loin\n",
    "\n",
    "### üìö Ressources recommand√©es :\n",
    "- [Paper: Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n",
    "- [Paper: Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171)\n",
    "- [Paper: Take a Step Back: Evoking Reasoning via Abstraction](https://arxiv.org/abs/2310.06117)\n",
    "\n",
    "### üî¨ Prochaines √©tapes :\n",
    "1. Exp√©rimentez avec des combinaisons personnalis√©es\n",
    "2. Cr√©ez des pipelines de raisonnement pour votre domaine\n",
    "3. Mesurez quantitativement les am√©liorations\n",
    "4. Partagez vos d√©couvertes avec la communaut√© !\n",
    "\n",
    "---\n",
    "\n",
    "**Prochain notebook** : 03_Optimisation_Evaluation.ipynb - M√©triques et optimisation automatique des prompts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
