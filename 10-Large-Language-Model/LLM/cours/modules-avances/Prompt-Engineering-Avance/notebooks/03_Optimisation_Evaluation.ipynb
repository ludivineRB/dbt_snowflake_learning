{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ‚ö° Notebook 3 : Optimisation & √âvaluation des Prompts\n",
    "\n",
    "## M√©triques de Qualit√©, A/B Testing et Optimisation Automatique\n",
    "\n",
    "Dans ce notebook, nous explorons les techniques scientifiques pour mesurer, √©valuer et optimiser automatiquement la performance de vos prompts.\n",
    "\n",
    "### üéØ Objectifs d'apprentissage :\n",
    "- D√©finir des m√©triques objectives de qualit√© des prompts\n",
    "- Impl√©menter des syst√®mes d'A/B testing pour prompts\n",
    "- Cr√©er des pipelines d'optimisation automatique\n",
    "- Mesurer ROI et impact business des am√©liorations\n",
    "- Construire des dashboards de monitoring en temps r√©el"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## üìö Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration des APIs\n",
    "import openai\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configuration des clients\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Configuration termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. üìä M√©triques de Qualit√© des Prompts\n",
    "\n",
    "### Framework de M√©triques Comprehensive\n",
    "\n",
    "Nous allons impl√©menter un syst√®me complet de m√©triques pour √©valuer la qualit√© des prompts selon plusieurs dimensions :\n",
    "\n",
    "- **Pr√©cision** : Justesse de la r√©ponse\n",
    "- **Compl√©tude** : Exhaustivit√© du contenu\n",
    "- **Coh√©rence** : Logique interne\n",
    "- **Utilit√©** : Valeur actionnable\n",
    "- **Efficiency** : Ratio qualit√©/co√ªt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptMetrics:\n",
    "    \"\"\"Structure pour les m√©triques d'un prompt\"\"\"\n",
    "    prompt_id: str\n",
    "    prompt_text: str\n",
    "    response: str\n",
    "    timestamp: datetime\n",
    "    \n",
    "    # M√©triques de qualit√© (0-1)\n",
    "    accuracy: float = 0.0\n",
    "    completeness: float = 0.0\n",
    "    coherence: float = 0.0\n",
    "    usefulness: float = 0.0\n",
    "    clarity: float = 0.0\n",
    "    \n",
    "    # M√©triques techniques\n",
    "    response_time: float = 0.0\n",
    "    token_count: int = 0\n",
    "    cost_usd: float = 0.0\n",
    "    \n",
    "    # M√©triques business\n",
    "    user_satisfaction: Optional[float] = None\n",
    "    task_completion: bool = False\n",
    "    \n",
    "    def overall_score(self) -> float:\n",
    "        \"\"\"Score global pond√©r√©\"\"\"\n",
    "        weights = {\n",
    "            'accuracy': 0.25,\n",
    "            'completeness': 0.20,\n",
    "            'coherence': 0.20,\n",
    "            'usefulness': 0.25,\n",
    "            'clarity': 0.10\n",
    "        }\n",
    "        \n",
    "        score = sum(getattr(self, metric) * weight \n",
    "                   for metric, weight in weights.items())\n",
    "        return round(score, 3)\n",
    "    \n",
    "    def efficiency_score(self) -> float:\n",
    "        \"\"\"Ratio qualit√©/co√ªt\"\"\"\n",
    "        if self.cost_usd == 0:\n",
    "            return 0\n",
    "        return self.overall_score() / self.cost_usd\n",
    "\n",
    "class PromptEvaluator:\n",
    "    \"\"\"Syst√®me d'√©valuation automatique des prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        self.evaluations_db = \"prompt_evaluations.db\"\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialise la base de donn√©es pour stocker les √©valuations\"\"\"\n",
    "        conn = sqlite3.connect(self.evaluations_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS evaluations (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            prompt_text TEXT,\n",
    "            response TEXT,\n",
    "            timestamp DATETIME,\n",
    "            accuracy REAL,\n",
    "            completeness REAL,\n",
    "            coherence REAL,\n",
    "            usefulness REAL,\n",
    "            clarity REAL,\n",
    "            response_time REAL,\n",
    "            token_count INTEGER,\n",
    "            cost_usd REAL,\n",
    "            user_satisfaction REAL,\n",
    "            task_completion BOOLEAN,\n",
    "            overall_score REAL\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def evaluate_response_quality(self, prompt: str, response: str, context: str = \"\") -> Dict[str, float]:\n",
    "        \"\"\"√âvalue la qualit√© d'une r√©ponse selon plusieurs crit√®res\"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        √âvalue cette interaction prompt-r√©ponse selon 5 crit√®res pr√©cis :\n",
    "        \n",
    "        PROMPT : {prompt}\n",
    "        {f'CONTEXTE : {context}' if context else ''}\n",
    "        R√âPONSE : {response}\n",
    "        \n",
    "        Pour chaque crit√®re, donne une note de 0 √† 10 :\n",
    "        \n",
    "        1. ACCURACY (Pr√©cision) : La r√©ponse est-elle factuelle et correcte ?\n",
    "        2. COMPLETENESS (Compl√©tude) : La r√©ponse couvre-t-elle tous les aspects importants ?\n",
    "        3. COHERENCE (Coh√©rence) : La logique interne est-elle solide ?\n",
    "        4. USEFULNESS (Utilit√©) : La r√©ponse est-elle actionnable et pratique ?\n",
    "        5. CLARITY (Clart√©) : La r√©ponse est-elle claire et bien structur√©e ?\n",
    "        \n",
    "        Format de r√©ponse OBLIGATOIRE :\n",
    "        ACCURACY: X/10\n",
    "        COMPLETENESS: Y/10\n",
    "        COHERENCE: Z/10\n",
    "        USEFULNESS: W/10\n",
    "        CLARITY: V/10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            # Parser les scores\n",
    "            content = result.choices[0].message.content\n",
    "            scores = {}\n",
    "            \n",
    "            import re\n",
    "            for criterion in ['ACCURACY', 'COMPLETENESS', 'COHERENCE', 'USEFULNESS', 'CLARITY']:\n",
    "                pattern = f\"{criterion}:\\s*(\\d+(?:\\.\\d+)?)/10\"\n",
    "                match = re.search(pattern, content, re.IGNORECASE)\n",
    "                if match:\n",
    "                    scores[criterion.lower()] = float(match.group(1)) / 10\n",
    "                else:\n",
    "                    scores[criterion.lower()] = 0.5  # Score par d√©faut\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur √©valuation : {e}\")\n",
    "            return {k: 0.5 for k in ['accuracy', 'completeness', 'coherence', 'usefulness', 'clarity']}\n",
    "    \n",
    "    def calculate_technical_metrics(self, response: str, start_time: float, model: str) -> Dict:\n",
    "        \"\"\"Calcule les m√©triques techniques\"\"\"\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        \n",
    "        # Estimation du nombre de tokens (approximation)\n",
    "        token_count = len(response.split()) * 1.3  # Approximation\n",
    "        \n",
    "        # Estimation du co√ªt (tarifs approximatifs 2024)\n",
    "        cost_per_1k_tokens = {\n",
    "            'gpt-3.5-turbo': 0.002,\n",
    "            'gpt-4': 0.03,\n",
    "            'gpt-4-turbo': 0.01,\n",
    "            'claude-3-sonnet': 0.003,\n",
    "            'claude-3-opus': 0.015\n",
    "        }\n",
    "        \n",
    "        cost_rate = cost_per_1k_tokens.get(model, 0.002)\n",
    "        cost_usd = (token_count / 1000) * cost_rate\n",
    "        \n",
    "        return {\n",
    "            'response_time': response_time,\n",
    "            'token_count': int(token_count),\n",
    "            'cost_usd': round(cost_usd, 4)\n",
    "        }\n",
    "    \n",
    "    def evaluate_prompt_complete(self, prompt: str, context: str = \"\", \n",
    "                               model: str = None) -> PromptMetrics:\n",
    "        \"\"\"√âvaluation compl√®te d'un prompt\"\"\"\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        \n",
    "        prompt_id = str(uuid.uuid4())\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # G√©n√©rer la r√©ponse\n",
    "        try:\n",
    "            response_obj = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            response = response_obj.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur g√©n√©ration : {e}\")\n",
    "            response = \"Erreur lors de la g√©n√©ration\"\n",
    "        \n",
    "        # √âvaluer la qualit√©\n",
    "        quality_scores = self.evaluate_response_quality(prompt, response, context)\n",
    "        \n",
    "        # Calculer les m√©triques techniques\n",
    "        tech_metrics = self.calculate_technical_metrics(response, start_time, model)\n",
    "        \n",
    "        # Cr√©er l'objet m√©triques\n",
    "        metrics = PromptMetrics(\n",
    "            prompt_id=prompt_id,\n",
    "            prompt_text=prompt,\n",
    "            response=response,\n",
    "            timestamp=datetime.now(),\n",
    "            **quality_scores,\n",
    "            **tech_metrics\n",
    "        )\n",
    "        \n",
    "        # Sauvegarder en base\n",
    "        self.save_evaluation(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_evaluation(self, metrics: PromptMetrics):\n",
    "        \"\"\"Sauvegarde une √©valuation en base de donn√©es\"\"\"\n",
    "        conn = sqlite3.connect(self.evaluations_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO evaluations (\n",
    "            id, prompt_text, response, timestamp, accuracy, completeness,\n",
    "            coherence, usefulness, clarity, response_time, token_count,\n",
    "            cost_usd, user_satisfaction, task_completion, overall_score\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            metrics.prompt_id, metrics.prompt_text, metrics.response,\n",
    "            metrics.timestamp, metrics.accuracy, metrics.completeness,\n",
    "            metrics.coherence, metrics.usefulness, metrics.clarity,\n",
    "            metrics.response_time, metrics.token_count, metrics.cost_usd,\n",
    "            metrics.user_satisfaction, metrics.task_completion, \n",
    "            metrics.overall_score()\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_evaluation_history(self, limit: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"R√©cup√®re l'historique des √©valuations\"\"\"\n",
    "        conn = sqlite3.connect(self.evaluations_db)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT * FROM evaluations \n",
    "        ORDER BY timestamp DESC \n",
    "        LIMIT ?\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn, params=(limit,))\n",
    "        conn.close()\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### üéØ Exemple : √âvaluation de prompts diff√©rents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser l'√©valuateur\n",
    "evaluator = PromptEvaluator()\n",
    "\n",
    "# Tester diff√©rents prompts pour la m√™me t√¢che\n",
    "task_context = \"Cr√©ation d'une strat√©gie marketing pour une startup EdTech\"\n",
    "\n",
    "prompts_to_test = [\n",
    "    {\n",
    "        'name': 'Prompt Basic',\n",
    "        'text': \"Cr√©e une strat√©gie marketing pour notre startup EdTech.\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Prompt Structur√©',\n",
    "        'text': \"\"\"\n",
    "        En tant qu'expert marketing sp√©cialis√© EdTech :\n",
    "        \n",
    "        Cr√©e une strat√©gie marketing pour notre startup qui propose des cours de code en ligne.\n",
    "        \n",
    "        Inclus :\n",
    "        - Analyse du march√© cible\n",
    "        - 3 canaux d'acquisition prioritaires\n",
    "        - Budget et timeline sur 6 mois\n",
    "        - M√©triques de succ√®s\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'Prompt Avec Contexte',\n",
    "        'text': \"\"\"\n",
    "        CONTEXTE : Startup EdTech fran√ßaise, 6 mois d'existence, 500 utilisateurs, \n",
    "        budget marketing 50k‚Ç¨, √©quipe de 3 personnes.\n",
    "        \n",
    "        R√îLE : Tu es un Growth Hacker exp√©riment√© sp√©cialis√© dans l'EdTech B2C.\n",
    "        \n",
    "        T√ÇCHE : Con√ßois une strat√©gie d'acquisition client pour doubler notre base \n",
    "        utilisateur en 4 mois tout en maintenant un CAC < 100‚Ç¨.\n",
    "        \n",
    "        FORMAT : Plan actionnable avec timeline, budget d√©taill√© et KPIs pr√©cis.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# √âvaluer chaque prompt\n",
    "results = []\n",
    "print(\"üî¨ √âvaluation des prompts...\\n\")\n",
    "\n",
    "for i, prompt_data in enumerate(prompts_to_test):\n",
    "    print(f\"{i+1}. √âvaluation : {prompt_data['name']}\")\n",
    "    \n",
    "    metrics = evaluator.evaluate_prompt_complete(\n",
    "        prompt_data['text'], \n",
    "        task_context\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'name': prompt_data['name'],\n",
    "        'metrics': metrics\n",
    "    })\n",
    "    \n",
    "    print(f\"   Score global : {metrics.overall_score():.3f}\")\n",
    "    print(f\"   Co√ªt : ${metrics.cost_usd:.4f}\")\n",
    "    print(f\"   Temps : {metrics.response_time:.2f}s\\n\")\n",
    "\n",
    "# Comparaison des r√©sultats\n",
    "print(\"üìä COMPARAISON DES PROMPTS :\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for result in sorted(results, key=lambda x: x['metrics'].overall_score(), reverse=True):\n",
    "    m = result['metrics']\n",
    "    print(f\"\\nüèÜ {result['name']}\")\n",
    "    print(f\"   Score global : {m.overall_score():.3f}\")\n",
    "    print(f\"   Pr√©cision : {m.accuracy:.2f} | Compl√©tude : {m.completeness:.2f}\")\n",
    "    print(f\"   Coh√©rence : {m.coherence:.2f} | Utilit√© : {m.usefulness:.2f}\")\n",
    "    print(f\"   Clart√© : {m.clarity:.2f}\")\n",
    "    print(f\"   Efficience : {m.efficiency_score():.1f} (qualit√©/co√ªt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. üß™ A/B Testing pour Prompts\n",
    "\n",
    "### Framework d'A/B Testing Statistiquement Robuste\n",
    "\n",
    "Impl√©mentons un syst√®me d'A/B testing pour comparer objectivement diff√©rentes versions de prompts avec une rigueur statistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ABTestConfig:\n",
    "    \"\"\"Configuration pour un test A/B\"\"\"\n",
    "    test_name: str\n",
    "    prompt_a: str\n",
    "    prompt_b: str\n",
    "    sample_size: int = 50\n",
    "    significance_level: float = 0.05\n",
    "    primary_metric: str = 'overall_score'\n",
    "    secondary_metrics: List[str] = None\n",
    "    test_contexts: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.secondary_metrics is None:\n",
    "            self.secondary_metrics = ['accuracy', 'usefulness', 'efficiency_score']\n",
    "        if self.test_contexts is None:\n",
    "            self.test_contexts = [\"\"]\n",
    "\n",
    "@dataclass\n",
    "class ABTestResult:\n",
    "    \"\"\"R√©sultats d'un test A/B\"\"\"\n",
    "    test_name: str\n",
    "    sample_size_a: int\n",
    "    sample_size_b: int\n",
    "    \n",
    "    # Statistiques descriptives\n",
    "    mean_a: float\n",
    "    mean_b: float\n",
    "    std_a: float\n",
    "    std_b: float\n",
    "    \n",
    "    # Tests statistiques\n",
    "    p_value: float\n",
    "    is_significant: bool\n",
    "    confidence_interval: Tuple[float, float]\n",
    "    effect_size: float  # Cohen's d\n",
    "    \n",
    "    # M√©trics business\n",
    "    lift_percentage: float\n",
    "    winner: str  # 'A', 'B', ou 'No significant difference'\n",
    "    power: float  # Puissance statistique\n",
    "\n",
    "class PromptABTester:\n",
    "    \"\"\"Syst√®me d'A/B testing pour prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator: PromptEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.tests_db = \"ab_tests.db\"\n",
    "        self.init_tests_database()\n",
    "    \n",
    "    def init_tests_database(self):\n",
    "        \"\"\"Initialise la base de donn√©es pour les tests A/B\"\"\"\n",
    "        conn = sqlite3.connect(self.tests_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ab_tests (\n",
    "            test_id TEXT,\n",
    "            test_name TEXT,\n",
    "            variant TEXT,\n",
    "            prompt_text TEXT,\n",
    "            context TEXT,\n",
    "            response TEXT,\n",
    "            overall_score REAL,\n",
    "            accuracy REAL,\n",
    "            usefulness REAL,\n",
    "            efficiency_score REAL,\n",
    "            timestamp DATETIME,\n",
    "            PRIMARY KEY (test_id, variant)\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def run_ab_test(self, config: ABTestConfig) -> ABTestResult:\n",
    "        \"\"\"Ex√©cute un test A/B complet\"\"\"\n",
    "        print(f\"üß™ D√©marrage du test A/B : {config.test_name}\")\n",
    "        print(f\"üìä Taille d'√©chantillon : {config.sample_size} par variant\")\n",
    "        \n",
    "        test_id = str(uuid.uuid4())\n",
    "        results_a = []\n",
    "        results_b = []\n",
    "        \n",
    "        # G√©n√©rer les √©chantillons pour chaque variant\n",
    "        print(\"\\nüîÑ G√©n√©ration des √©chantillons...\")\n",
    "        \n",
    "        # Utiliser ThreadPoolExecutor pour parall√©liser\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            # Soumettre les t√¢ches pour variant A\n",
    "            futures_a = []\n",
    "            for i in range(config.sample_size):\n",
    "                context = random.choice(config.test_contexts)\n",
    "                future = executor.submit(self._evaluate_variant, \n",
    "                                       config.prompt_a, context, 'A', test_id, config.test_name)\n",
    "                futures_a.append(future)\n",
    "            \n",
    "            # Soumettre les t√¢ches pour variant B\n",
    "            futures_b = []\n",
    "            for i in range(config.sample_size):\n",
    "                context = random.choice(config.test_contexts)\n",
    "                future = executor.submit(self._evaluate_variant, \n",
    "                                       config.prompt_b, context, 'B', test_id, config.test_name)\n",
    "                futures_b.append(future)\n",
    "            \n",
    "            # Collecter les r√©sultats A\n",
    "            for i, future in enumerate(as_completed(futures_a)):\n",
    "                try:\n",
    "                    result = future.result(timeout=60)\n",
    "                    results_a.append(result)\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        print(f\"   Variant A : {i + 1}/{config.sample_size} termin√©s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Erreur variant A : {e}\")\n",
    "            \n",
    "            # Collecter les r√©sultats B\n",
    "            for i, future in enumerate(as_completed(futures_b)):\n",
    "                try:\n",
    "                    result = future.result(timeout=60)\n",
    "                    results_b.append(result)\n",
    "                    if (i + 1) % 10 == 0:\n",
    "                        print(f\"   Variant B : {i + 1}/{config.sample_size} termin√©s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Erreur variant B : {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Collecte termin√©e : {len(results_a)} vs {len(results_b)} √©chantillons\")\n",
    "        \n",
    "        # Analyser les r√©sultats\n",
    "        return self._analyze_results(config, results_a, results_b)\n",
    "    \n",
    "    def _evaluate_variant(self, prompt: str, context: str, variant: str, \n",
    "                         test_id: str, test_name: str) -> Dict:\n",
    "        \"\"\"√âvalue un variant et sauvegarde en base\"\"\"\n",
    "        metrics = self.evaluator.evaluate_prompt_complete(prompt, context)\n",
    "        \n",
    "        # Sauvegarder en base\n",
    "        conn = sqlite3.connect(self.tests_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO ab_tests (\n",
    "            test_id, test_name, variant, prompt_text, context, response,\n",
    "            overall_score, accuracy, usefulness, efficiency_score, timestamp\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            test_id, test_name, variant, prompt, context, metrics.response,\n",
    "            metrics.overall_score(), metrics.accuracy, metrics.usefulness,\n",
    "            metrics.efficiency_score(), datetime.now()\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            'overall_score': metrics.overall_score(),\n",
    "            'accuracy': metrics.accuracy,\n",
    "            'usefulness': metrics.usefulness,\n",
    "            'efficiency_score': metrics.efficiency_score()\n",
    "        }\n",
    "    \n",
    "    def _analyze_results(self, config: ABTestConfig, \n",
    "                        results_a: List[Dict], results_b: List[Dict]) -> ABTestResult:\n",
    "        \"\"\"Analyse statistique des r√©sultats A/B\"\"\"\n",
    "        print(\"\\nüìä Analyse statistique...\")\n",
    "        \n",
    "        # Extraire la m√©trique principale\n",
    "        values_a = [r[config.primary_metric] for r in results_a]\n",
    "        values_b = [r[config.primary_metric] for r in results_b]\n",
    "        \n",
    "        # Statistiques descriptives\n",
    "        mean_a, mean_b = np.mean(values_a), np.mean(values_b)\n",
    "        std_a, std_b = np.std(values_a, ddof=1), np.std(values_b, ddof=1)\n",
    "        \n",
    "        # Test t de Student\n",
    "        t_stat, p_value = stats.ttest_ind(values_a, values_b, equal_var=False)\n",
    "        \n",
    "        # Significance\n",
    "        is_significant = p_value < config.significance_level\n",
    "        \n",
    "        # Intervalle de confiance pour la diff√©rence\n",
    "        diff = mean_b - mean_a\n",
    "        se_diff = np.sqrt((std_a**2 / len(values_a)) + (std_b**2 / len(values_b)))\n",
    "        margin_error = stats.t.ppf(1 - config.significance_level/2, \n",
    "                                  len(values_a) + len(values_b) - 2) * se_diff\n",
    "        ci = (diff - margin_error, diff + margin_error)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(values_a)-1)*std_a**2 + (len(values_b)-1)*std_b**2) / \n",
    "                           (len(values_a) + len(values_b) - 2))\n",
    "        cohens_d = (mean_b - mean_a) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Lift percentage\n",
    "        lift = ((mean_b - mean_a) / mean_a * 100) if mean_a > 0 else 0\n",
    "        \n",
    "        # D√©terminer le gagnant\n",
    "        if not is_significant:\n",
    "            winner = \"No significant difference\"\n",
    "        else:\n",
    "            winner = \"B\" if mean_b > mean_a else \"A\"\n",
    "        \n",
    "        # Calcul de la puissance (approximation)\n",
    "        effect_size_for_power = abs(cohens_d)\n",
    "        power = self._calculate_power(len(values_a), effect_size_for_power, config.significance_level)\n",
    "        \n",
    "        return ABTestResult(\n",
    "            test_name=config.test_name,\n",
    "            sample_size_a=len(values_a),\n",
    "            sample_size_b=len(values_b),\n",
    "            mean_a=mean_a,\n",
    "            mean_b=mean_b,\n",
    "            std_a=std_a,\n",
    "            std_b=std_b,\n",
    "            p_value=p_value,\n",
    "            is_significant=is_significant,\n",
    "            confidence_interval=ci,\n",
    "            effect_size=cohens_d,\n",
    "            lift_percentage=lift,\n",
    "            winner=winner,\n",
    "            power=power\n",
    "        )\n",
    "    \n",
    "    def _calculate_power(self, sample_size: int, effect_size: float, alpha: float) -> float:\n",
    "        \"\"\"Calcule la puissance statistique (approximation)\"\"\"\n",
    "        # Calcul simplifi√© de la puissance\n",
    "        from scipy.stats import norm\n",
    "        \n",
    "        z_alpha = norm.ppf(1 - alpha/2)\n",
    "        z_beta = effect_size * np.sqrt(sample_size/2) - z_alpha\n",
    "        power = norm.cdf(z_beta)\n",
    "        \n",
    "        return max(0, min(1, power))\n",
    "    \n",
    "    def visualize_results(self, result: ABTestResult, save_path: str = None):\n",
    "        \"\"\"Visualise les r√©sultats du test A/B\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Comparaison des moyennes\n",
    "        variants = ['Variant A', 'Variant B']\n",
    "        means = [result.mean_a, result.mean_b]\n",
    "        stds = [result.std_a, result.std_b]\n",
    "        \n",
    "        bars = ax1.bar(variants, means, yerr=stds, capsize=10, \n",
    "                      color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "        ax1.set_title(f'{result.test_name}\\nComparaison des Performances')\n",
    "        ax1.set_ylabel('Score Moyen')\n",
    "        \n",
    "        # Annoter avec les valeurs\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{mean:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Distribution des scores\n",
    "        # Simuler les distributions pour visualisation\n",
    "        samples_a = np.random.normal(result.mean_a, result.std_a, 1000)\n",
    "        samples_b = np.random.normal(result.mean_b, result.std_b, 1000)\n",
    "        \n",
    "        ax2.hist(samples_a, bins=30, alpha=0.7, label='Variant A', color='lightblue')\n",
    "        ax2.hist(samples_b, bins=30, alpha=0.7, label='Variant B', color='lightcoral')\n",
    "        ax2.set_title('Distribution des Scores')\n",
    "        ax2.set_xlabel('Score')\n",
    "        ax2.set_ylabel('Fr√©quence')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # 3. Intervalle de confiance\n",
    "        diff = result.mean_b - result.mean_a\n",
    "        ci_lower, ci_upper = result.confidence_interval\n",
    "        \n",
    "        ax3.errorbar([0], [diff], yerr=[[diff - ci_lower], [ci_upper - diff]], \n",
    "                    fmt='o', markersize=10, capsize=10, capthick=2)\n",
    "        ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax3.set_title('Diff√©rence entre Variants\\n(avec Intervalle de Confiance 95%)')\n",
    "        ax3.set_ylabel('Diff√©rence (B - A)')\n",
    "        ax3.set_xlim(-0.5, 0.5)\n",
    "        ax3.set_xticks([])\n",
    "        \n",
    "        # 4. R√©sum√© statistique\n",
    "        ax4.axis('off')\n",
    "        summary_text = f\"\"\"\n",
    "        üìä R√âSULTATS DU TEST A/B\n",
    "        \n",
    "        üèÜ Gagnant : {result.winner}\n",
    "        üìà Lift : {result.lift_percentage:+.1f}%\n",
    "        \n",
    "        üìã Statistiques :\n",
    "        ‚Ä¢ p-value : {result.p_value:.4f}\n",
    "        ‚Ä¢ Significatif : {'‚úÖ Oui' if result.is_significant else '‚ùå Non'}\n",
    "        ‚Ä¢ Effect Size : {result.effect_size:.3f}\n",
    "        ‚Ä¢ Puissance : {result.power:.1%}\n",
    "        \n",
    "        üìè √âchantillons :\n",
    "        ‚Ä¢ Variant A : {result.sample_size_a}\n",
    "        ‚Ä¢ Variant B : {result.sample_size_b}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, \n",
    "                fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### üéØ Exemple : Test A/B sur des prompts de r√©sum√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du test A/B\n",
    "ab_config = ABTestConfig(\n",
    "    test_name=\"R√©sum√© d'Articles - Prompt Simple vs Structur√©\",\n",
    "    \n",
    "    prompt_a=\"\"\"R√©sume cet article en 3 points principaux.\"\"\",\n",
    "    \n",
    "    prompt_b=\"\"\"En tant qu'analyste exp√©riment√© :\n",
    "    \n",
    "    Analyse cet article et produis un r√©sum√© structur√© :\n",
    "    \n",
    "    üéØ ID√âE PRINCIPALE (1 phrase)\n",
    "    üìã POINTS CL√âS (3 √©l√©ments maximum)\n",
    "    üí° INSIGHTS ACTIONABLES (1-2 recommandations)\n",
    "    \n",
    "    Reste factuel et concis.\"\"\",\n",
    "    \n",
    "    sample_size=20,  # R√©duire pour la d√©mo\n",
    "    primary_metric='overall_score',\n",
    "    \n",
    "    test_contexts=[\n",
    "        \"Article tech sur l'IA\",\n",
    "        \"Article business sur les startups\", \n",
    "        \"Article scientifique sur le climat\",\n",
    "        \"Article √©conomique sur les march√©s\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Ex√©cuter le test A/B\n",
    "ab_tester = PromptABTester(evaluator)\n",
    "test_result = ab_tester.run_ab_test(ab_config)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß™ R√âSULTATS DU TEST A/B\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Test : {test_result.test_name}\")\n",
    "print(f\"\\nüèÜ Gagnant : {test_result.winner}\")\n",
    "print(f\"üìà Am√©lioration : {test_result.lift_percentage:+.1f}%\")\n",
    "\n",
    "print(f\"\\nüìã Statistiques d√©taill√©es :\")\n",
    "print(f\"  Variant A - Moyenne : {test_result.mean_a:.3f} (¬±{test_result.std_a:.3f})\")\n",
    "print(f\"  Variant B - Moyenne : {test_result.mean_b:.3f} (¬±{test_result.std_b:.3f})\")\n",
    "\n",
    "print(f\"\\nüî¨ Tests statistiques :\")\n",
    "print(f\"  p-value : {test_result.p_value:.4f}\")\n",
    "print(f\"  Significatif : {'‚úÖ Oui' if test_result.is_significant else '‚ùå Non (p > 0.05)'}\")\n",
    "print(f\"  Effect Size : {test_result.effect_size:.3f}\")\n",
    "print(f\"  Puissance : {test_result.power:.1%}\")\n",
    "\n",
    "print(f\"\\nüìè Intervalles de confiance (95%) :\")\n",
    "ci_lower, ci_upper = test_result.confidence_interval\n",
    "print(f\"  Diff√©rence B-A : [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "# Visualiser les r√©sultats\n",
    "ab_tester.visualize_results(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3. ü§ñ Optimisation Automatique des Prompts\n",
    "\n",
    "### Framework d'Optimisation par Algorithme G√©n√©tique\n",
    "\n",
    "Cr√©ons un syst√®me qui optimise automatiquement les prompts en utilisant des techniques d'optimisation inspir√©es de l'√©volution naturelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptGene:\n",
    "    \"\"\"Repr√©sente un 'g√®ne' dans un prompt (un √©l√©ment modifiable)\"\"\"\n",
    "    name: str\n",
    "    type: str  # 'role', 'instruction', 'format', 'example', 'constraint'\n",
    "    variations: List[str]\n",
    "    current_value: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.current_value and self.variations:\n",
    "            self.current_value = random.choice(self.variations)\n",
    "    \n",
    "    def mutate(self) -> str:\n",
    "        \"\"\"Mute le g√®ne en choisissant une nouvelle variation\"\"\"\n",
    "        self.current_value = random.choice(self.variations)\n",
    "        return self.current_value\n",
    "\n",
    "@dataclass\n",
    "class PromptChromosome:\n",
    "    \"\"\"Repr√©sente un prompt complet (chromosme) avec ses g√®nes\"\"\"\n",
    "    genes: List[PromptGene]\n",
    "    template: str\n",
    "    fitness: float = 0.0\n",
    "    generation: int = 0\n",
    "    \n",
    "    def generate_prompt(self) -> str:\n",
    "        \"\"\"G√©n√®re le prompt complet √† partir des g√®nes\"\"\"\n",
    "        prompt = self.template\n",
    "        \n",
    "        for gene in self.genes:\n",
    "            placeholder = f\"{{{gene.name}}}\"\n",
    "            prompt = prompt.replace(placeholder, gene.current_value)\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def mutate(self, mutation_rate: float = 0.3):\n",
    "        \"\"\"Applique des mutations al√©atoires\"\"\"\n",
    "        for gene in self.genes:\n",
    "            if random.random() < mutation_rate:\n",
    "                gene.mutate()\n",
    "    \n",
    "    def crossover(self, other: 'PromptChromosome') -> 'PromptChromosome':\n",
    "        \"\"\"Cr√©e un offspring par croisement avec un autre chromosome\"\"\"\n",
    "        new_genes = []\n",
    "        \n",
    "        for i, (gene1, gene2) in enumerate(zip(self.genes, other.genes)):\n",
    "            # Choisir al√©atoirement un parent pour chaque g√®ne\n",
    "            if random.random() < 0.5:\n",
    "                new_gene = PromptGene(\n",
    "                    name=gene1.name,\n",
    "                    type=gene1.type,\n",
    "                    variations=gene1.variations,\n",
    "                    current_value=gene1.current_value\n",
    "                )\n",
    "            else:\n",
    "                new_gene = PromptGene(\n",
    "                    name=gene2.name,\n",
    "                    type=gene2.type,\n",
    "                    variations=gene2.variations,\n",
    "                    current_value=gene2.current_value\n",
    "                )\n",
    "            new_genes.append(new_gene)\n",
    "        \n",
    "        return PromptChromosome(\n",
    "            genes=new_genes,\n",
    "            template=self.template,\n",
    "            generation=max(self.generation, other.generation) + 1\n",
    "        )\n",
    "\n",
    "class GeneticPromptOptimizer:\n",
    "    \"\"\"Optimiseur de prompts par algorithme g√©n√©tique\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator: PromptEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "        self.population = []\n",
    "        self.generation = 0\n",
    "        self.best_fitness_history = []\n",
    "        self.avg_fitness_history = []\n",
    "    \n",
    "    def create_initial_population(self, template: str, genes: List[PromptGene], \n",
    "                                population_size: int = 20) -> List[PromptChromosome]:\n",
    "        \"\"\"Cr√©e la population initiale\"\"\"\n",
    "        population = []\n",
    "        \n",
    "        for _ in range(population_size):\n",
    "            # Cr√©er des copies des g√®nes avec des valeurs al√©atoires\n",
    "            chromosome_genes = []\n",
    "            for gene in genes:\n",
    "                new_gene = PromptGene(\n",
    "                    name=gene.name,\n",
    "                    type=gene.type,\n",
    "                    variations=gene.variations.copy(),\n",
    "                    current_value=random.choice(gene.variations)\n",
    "                )\n",
    "                chromosome_genes.append(new_gene)\n",
    "            \n",
    "            chromosome = PromptChromosome(\n",
    "                genes=chromosome_genes,\n",
    "                template=template,\n",
    "                generation=0\n",
    "            )\n",
    "            population.append(chromosome)\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    def evaluate_fitness(self, chromosome: PromptChromosome, \n",
    "                        test_contexts: List[str], samples_per_context: int = 3) -> float:\n",
    "        \"\"\"√âvalue la fitness d'un chromosome\"\"\"\n",
    "        prompt = chromosome.generate_prompt()\n",
    "        total_score = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for context in test_contexts:\n",
    "            for _ in range(samples_per_context):\n",
    "                try:\n",
    "                    metrics = self.evaluator.evaluate_prompt_complete(prompt, context)\n",
    "                    total_score += metrics.overall_score()\n",
    "                    total_samples += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur √©valuation : {e}\")\n",
    "                    # P√©nalit√© pour les prompts qui causent des erreurs\n",
    "                    total_score += 0.1\n",
    "                    total_samples += 1\n",
    "        \n",
    "        fitness = total_score / total_samples if total_samples > 0 else 0\n",
    "        chromosome.fitness = fitness\n",
    "        return fitness\n",
    "    \n",
    "    def selection(self, population: List[PromptChromosome], \n",
    "                 selection_size: int) -> List[PromptChromosome]:\n",
    "        \"\"\"S√©lection par tournoi\"\"\"\n",
    "        selected = []\n",
    "        \n",
    "        for _ in range(selection_size):\n",
    "            # Tournoi de taille 3\n",
    "            tournament = random.sample(population, min(3, len(population)))\n",
    "            winner = max(tournament, key=lambda x: x.fitness)\n",
    "            selected.append(winner)\n",
    "        \n",
    "        return selected\n",
    "    \n",
    "    def evolve_generation(self, population: List[PromptChromosome],\n",
    "                         elite_size: int = 2, mutation_rate: float = 0.3) -> List[PromptChromosome]:\n",
    "        \"\"\"√âvolution d'une g√©n√©ration\"\"\"\n",
    "        # Trier par fitness\n",
    "        population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "        \n",
    "        # √âlitisme : garder les meilleurs\n",
    "        new_population = population[:elite_size]\n",
    "        \n",
    "        # S√©lection pour reproduction\n",
    "        parents = self.selection(population, len(population) - elite_size)\n",
    "        \n",
    "        # Reproduction et mutation\n",
    "        while len(new_population) < len(population):\n",
    "            parent1 = random.choice(parents)\n",
    "            parent2 = random.choice(parents)\n",
    "            \n",
    "            # Croisement\n",
    "            child = parent1.crossover(parent2)\n",
    "            \n",
    "            # Mutation\n",
    "            child.mutate(mutation_rate)\n",
    "            \n",
    "            new_population.append(child)\n",
    "        \n",
    "        return new_population\n",
    "    \n",
    "    def optimize(self, template: str, genes: List[PromptGene], \n",
    "                test_contexts: List[str], generations: int = 10,\n",
    "                population_size: int = 20, mutation_rate: float = 0.3) -> Dict:\n",
    "        \"\"\"Lance l'optimisation compl√®te\"\"\"\n",
    "        print(f\"üß¨ D√©marrage optimisation g√©n√©tique\")\n",
    "        print(f\"üìä Param√®tres : {generations} g√©n√©rations, {population_size} individus\")\n",
    "        \n",
    "        # Cr√©er population initiale\n",
    "        print(\"\\nüå± Cr√©ation de la population initiale...\")\n",
    "        self.population = self.create_initial_population(template, genes, population_size)\n",
    "        \n",
    "        # √âvoluer sur plusieurs g√©n√©rations\n",
    "        for gen in range(generations):\n",
    "            print(f\"\\nüîÑ G√©n√©ration {gen + 1}/{generations}\")\n",
    "            \n",
    "            # √âvaluer la fitness de chaque individu\n",
    "            print(\"   üìä √âvaluation de la fitness...\")\n",
    "            with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "                futures = []\n",
    "                for chromosome in self.population:\n",
    "                    future = executor.submit(self.evaluate_fitness, chromosome, \n",
    "                                           test_contexts, 2)  # 2 √©chantillons par contexte\n",
    "                    futures.append(future)\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        future.result(timeout=120)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      Erreur : {e}\")\n",
    "            \n",
    "            # Statistiques de la g√©n√©ration\n",
    "            fitnesses = [c.fitness for c in self.population]\n",
    "            best_fitness = max(fitnesses)\n",
    "            avg_fitness = np.mean(fitnesses)\n",
    "            \n",
    "            self.best_fitness_history.append(best_fitness)\n",
    "            self.avg_fitness_history.append(avg_fitness)\n",
    "            \n",
    "            print(f\"   üèÜ Meilleure fitness : {best_fitness:.3f}\")\n",
    "            print(f\"   üìà Fitness moyenne : {avg_fitness:.3f}\")\n",
    "            \n",
    "            # √âvolution (sauf pour la derni√®re g√©n√©ration)\n",
    "            if gen < generations - 1:\n",
    "                print(\"   üß¨ √âvolution...\")\n",
    "                self.population = self.evolve_generation(self.population, \n",
    "                                                        elite_size=2, \n",
    "                                                        mutation_rate=mutation_rate)\n",
    "        \n",
    "        # Trouver le meilleur individu final\n",
    "        best_chromosome = max(self.population, key=lambda x: x.fitness)\n",
    "        \n",
    "        return {\n",
    "            'best_chromosome': best_chromosome,\n",
    "            'best_prompt': best_chromosome.generate_prompt(),\n",
    "            'best_fitness': best_chromosome.fitness,\n",
    "            'fitness_history': {\n",
    "                'best': self.best_fitness_history,\n",
    "                'average': self.avg_fitness_history\n",
    "            },\n",
    "            'final_population': self.population\n",
    "        }\n",
    "    \n",
    "    def visualize_evolution(self, save_path: str = None):\n",
    "        \"\"\"Visualise l'√©volution de la fitness\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        generations = range(1, len(self.best_fitness_history) + 1)\n",
    "        \n",
    "        plt.plot(generations, self.best_fitness_history, \n",
    "                'o-', label='Meilleure Fitness', linewidth=2, markersize=6)\n",
    "        plt.plot(generations, self.avg_fitness_history, \n",
    "                's-', label='Fitness Moyenne', linewidth=2, markersize=4, alpha=0.7)\n",
    "        \n",
    "        plt.title('√âvolution de la Fitness par G√©n√©ration', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('G√©n√©ration')\n",
    "        plt.ylabel('Score de Fitness')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Annotations\n",
    "        best_gen = np.argmax(self.best_fitness_history)\n",
    "        best_score = self.best_fitness_history[best_gen]\n",
    "        plt.annotate(f'Max: {best_score:.3f}', \n",
    "                    xy=(best_gen + 1, best_score), \n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### üéØ Exemple : Optimisation d'un prompt de recommandation produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le template et les g√®nes pour l'optimisation\n",
    "template = \"\"\"\n",
    "{role}\n",
    "\n",
    "{task_instruction}\n",
    "\n",
    "{context_guide}\n",
    "\n",
    "{output_format}\n",
    "\n",
    "{quality_constraint}\n",
    "\"\"\"\n",
    "\n",
    "# D√©finir les g√®nes (√©l√©ments variables du prompt)\n",
    "optimization_genes = [\n",
    "    PromptGene(\n",
    "        name=\"role\",\n",
    "        type=\"role\",\n",
    "        variations=[\n",
    "            \"Tu es un conseiller en achats exp√©riment√©.\",\n",
    "            \"En tant qu'expert e-commerce sp√©cialis√© dans les recommandations personnalis√©es :\",\n",
    "            \"Agis comme un data scientist sp√©cialis√© en syst√®mes de recommandation.\",\n",
    "            \"Tu es un consultant retail avec 10 ans d'exp√©rience.\"\n",
    "        ]\n",
    "    ),\n",
    "    PromptGene(\n",
    "        name=\"task_instruction\",\n",
    "        type=\"instruction\",\n",
    "        variations=[\n",
    "            \"Recommande 3 produits adapt√©s au profil client donn√©.\",\n",
    "            \"Analyse le profil client et sugg√®re les 3 meilleurs produits correspondant √† ses besoins.\",\n",
    "            \"Bas√© sur les donn√©es client, identifie 3 produits qui maximiseront sa satisfaction.\",\n",
    "            \"Cr√©e une recommandation personnalis√©e de 3 produits en analysant les pr√©f√©rences client.\"\n",
    "        ]\n",
    "    ),\n",
    "    PromptGene(\n",
    "        name=\"context_guide\",\n",
    "        type=\"constraint\",\n",
    "        variations=[\n",
    "            \"Consid√®re le budget, les pr√©f√©rences et l'historique d'achat.\",\n",
    "            \"Prends en compte : budget disponible, style personnel, achats pr√©c√©dents, et besoins exprim√©s.\",\n",
    "            \"Analyse : contraintes budg√©taires, go√ªts personnels, saisonnalit√©, et tendances actuelles.\",\n",
    "            \"√âvalue budget, pr√©f√©rences, contexte d'usage, et rapport qualit√©-prix.\"\n",
    "        ]\n",
    "    ),\n",
    "    PromptGene(\n",
    "        name=\"output_format\",\n",
    "        type=\"format\",\n",
    "        variations=[\n",
    "            \"Pr√©sente chaque produit avec nom, prix, et raison de la recommandation.\",\n",
    "            \"Pour chaque produit : Nom | Prix | Pourquoi ce choix | Score de pertinence (/10)\",\n",
    "            \"Format : üè∑Ô∏è [Nom] - [Prix] \\nüí° [Justification] \\n‚≠ê [Score de match] /10\",\n",
    "            \"Structure : Produit + Prix + Avantages cl√©s + Niveau de recommandation (Excellent/Bon/Correct)\"\n",
    "        ]\n",
    "    ),\n",
    "    PromptGene(\n",
    "        name=\"quality_constraint\",\n",
    "        type=\"constraint\",\n",
    "        variations=[\n",
    "            \"Assure-toi que chaque recommandation est pertinente et bien justifi√©e.\",\n",
    "            \"Prioritise la pertinence et la valeur ajout√©e pour le client.\",\n",
    "            \"Vise l'excellence : chaque recommandation doit √™tre parfaitement adapt√©e.\",\n",
    "            \"Sois pr√©cis, pertinent et orient√© satisfaction client.\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Contextes de test pour l'optimisation\n",
    "test_contexts = [\n",
    "    \"Client homme, 30 ans, budget 200‚Ç¨, aime la tech et le sport\",\n",
    "    \"Cliente femme, 25 ans, budget 500‚Ç¨, passionn√©e de mode et voyage\", \n",
    "    \"Client mixte, 45 ans, budget 300‚Ç¨, recherche des cadeaux famille\",\n",
    "    \"Cliente femme, 35 ans, budget 150‚Ç¨, √©cologique et minimaliste\"\n",
    "]\n",
    "\n",
    "# Lancer l'optimisation\n",
    "optimizer = GeneticPromptOptimizer(evaluator)\n",
    "\n",
    "optimization_result = optimizer.optimize(\n",
    "    template=template,\n",
    "    genes=optimization_genes,\n",
    "    test_contexts=test_contexts,\n",
    "    generations=6,  # R√©duire pour la d√©mo\n",
    "    population_size=12,\n",
    "    mutation_rate=0.4\n",
    ")\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß¨ R√âSULTATS DE L'OPTIMISATION G√âN√âTIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüèÜ Meilleure fitness atteinte : {optimization_result['best_fitness']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚ú® PROMPT OPTIMIS√â :\")\n",
    "print(\"=\"*40)\n",
    "print(optimization_result['best_prompt'])\n",
    "print(\"=\"*40)\n",
    "\n",
    "# D√©tail des g√®nes du meilleur individu\n",
    "print(f\"\\nüß¨ Composition g√©n√©tique du meilleur prompt :\")\n",
    "for gene in optimization_result['best_chromosome'].genes:\n",
    "    print(f\"  {gene.name.upper()}: {gene.current_value}\")\n",
    "\n",
    "# Visualiser l'√©volution\n",
    "print(f\"\\nüìà √âvolution de la fitness sur {len(optimizer.best_fitness_history)} g√©n√©rations\")\n",
    "optimizer.visualize_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 4. üìà Dashboard de Monitoring en Temps R√©el\n",
    "\n",
    "### Syst√®me de Monitoring et Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMonitoringDashboard:\n",
    "    \"\"\"Dashboard de monitoring des performances des prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluator: PromptEvaluator):\n",
    "        self.evaluator = evaluator\n",
    "    \n",
    "    def generate_performance_report(self, days: int = 7) -> Dict:\n",
    "        \"\"\"G√©n√®re un rapport de performance sur les N derniers jours\"\"\"\n",
    "        # R√©cup√©rer les donn√©es\n",
    "        df = self.evaluator.get_evaluation_history(limit=1000)\n",
    "        \n",
    "        if df.empty:\n",
    "            return {'error': 'Aucune donn√©e disponible'}\n",
    "        \n",
    "        # Convertir timestamp\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Filtrer par p√©riode\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        df_period = df[df['timestamp'] >= cutoff_date]\n",
    "        \n",
    "        if df_period.empty:\n",
    "            return {'error': f'Aucune donn√©e sur les {days} derniers jours'}\n",
    "        \n",
    "        # Calculer les m√©triques\n",
    "        report = {\n",
    "            'period_days': days,\n",
    "            'total_evaluations': len(df_period),\n",
    "            'avg_overall_score': df_period['overall_score'].mean(),\n",
    "            'metrics': {\n",
    "                'accuracy': {\n",
    "                    'mean': df_period['accuracy'].mean(),\n",
    "                    'std': df_period['accuracy'].std(),\n",
    "                    'trend': self._calculate_trend(df_period, 'accuracy')\n",
    "                },\n",
    "                'usefulness': {\n",
    "                    'mean': df_period['usefulness'].mean(),\n",
    "                    'std': df_period['usefulness'].std(),\n",
    "                    'trend': self._calculate_trend(df_period, 'usefulness')\n",
    "                },\n",
    "                'coherence': {\n",
    "                    'mean': df_period['coherence'].mean(),\n",
    "                    'std': df_period['coherence'].std(),\n",
    "                    'trend': self._calculate_trend(df_period, 'coherence')\n",
    "                }\n",
    "            },\n",
    "            'performance': {\n",
    "                'avg_response_time': df_period['response_time'].mean(),\n",
    "                'total_cost': df_period['cost_usd'].sum(),\n",
    "                'avg_cost_per_request': df_period['cost_usd'].mean(),\n",
    "                'cost_efficiency': df_period['overall_score'].sum() / df_period['cost_usd'].sum() if df_period['cost_usd'].sum() > 0 else 0\n",
    "            },\n",
    "            'quality_distribution': {\n",
    "                'excellent': len(df_period[df_period['overall_score'] >= 0.8]),\n",
    "                'good': len(df_period[(df_period['overall_score'] >= 0.6) & (df_period['overall_score'] < 0.8)]),\n",
    "                'average': len(df_period[(df_period['overall_score'] >= 0.4) & (df_period['overall_score'] < 0.6)]),\n",
    "                'poor': len(df_period[df_period['overall_score'] < 0.4])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _calculate_trend(self, df: pd.DataFrame, metric: str) -> str:\n",
    "        \"\"\"Calcule la tendance d'une m√©trique\"\"\"\n",
    "        if len(df) < 10:\n",
    "            return \"insufficient_data\"\n",
    "        \n",
    "        # Diviser en deux moiti√©s\n",
    "        mid_point = len(df) // 2\n",
    "        first_half = df.iloc[:mid_point][metric].mean()\n",
    "        second_half = df.iloc[mid_point:][metric].mean()\n",
    "        \n",
    "        change = (second_half - first_half) / first_half if first_half > 0 else 0\n",
    "        \n",
    "        if change > 0.05:\n",
    "            return \"increasing\"\n",
    "        elif change < -0.05:\n",
    "            return \"decreasing\"\n",
    "        else:\n",
    "            return \"stable\"\n",
    "    \n",
    "    def create_dashboard_visualization(self, report: Dict, save_path: str = None):\n",
    "        \"\"\"Cr√©e une visualisation dashboard\"\"\"\n",
    "        if 'error' in report:\n",
    "            print(f\"‚ùå Erreur : {report['error']}\")\n",
    "            return\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. M√©triques principales (2x2 grid en haut)\n",
    "        gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1.2], hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Score global\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        score = report['avg_overall_score']\n",
    "        color = 'green' if score > 0.7 else 'orange' if score > 0.5 else 'red'\n",
    "        ax1.pie([score, 1-score], labels=['Score', ''], colors=[color, 'lightgray'], \n",
    "               startangle=90, counterclock=False)\n",
    "        ax1.set_title(f'Score Global\\n{score:.1%}', fontweight='bold')\n",
    "        \n",
    "        # Nombre d'√©valuations\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.bar(['√âvaluations'], [report['total_evaluations']], color='steelblue')\n",
    "        ax2.set_title(f'Total √âvaluations\\n{report[\"total_evaluations\"]}', fontweight='bold')\n",
    "        ax2.set_ylabel('Nombre')\n",
    "        \n",
    "        # Co√ªt total\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        cost = report['performance']['total_cost']\n",
    "        ax3.bar(['Co√ªt'], [cost], color='coral')\n",
    "        ax3.set_title(f'Co√ªt Total\\n${cost:.3f}', fontweight='bold')\n",
    "        ax3.set_ylabel('USD')\n",
    "        \n",
    "        # Efficience\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        efficiency = report['performance']['cost_efficiency']\n",
    "        ax4.bar(['Efficience'], [efficiency], color='gold')\n",
    "        ax4.set_title(f'Efficience\\n{efficiency:.1f}', fontweight='bold')\n",
    "        ax4.set_ylabel('Score/USD')\n",
    "        \n",
    "        # 2. M√©triques d√©taill√©es (barres avec tendances)\n",
    "        ax5 = fig.add_subplot(gs[1, :])\n",
    "        metrics_names = list(report['metrics'].keys())\n",
    "        metrics_values = [report['metrics'][m]['mean'] for m in metrics_names]\n",
    "        metrics_std = [report['metrics'][m]['std'] for m in metrics_names]\n",
    "        \n",
    "        bars = ax5.bar(metrics_names, metrics_values, yerr=metrics_std, \n",
    "                      capsize=5, color=['skyblue', 'lightgreen', 'plum'], alpha=0.8)\n",
    "        \n",
    "        # Ajouter les tendances\n",
    "        for i, (bar, metric) in enumerate(zip(bars, metrics_names)):\n",
    "            trend = report['metrics'][metric]['trend']\n",
    "            trend_symbol = 'üìà' if trend == 'increasing' else 'üìâ' if trend == 'decreasing' else '‚û°Ô∏è'\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    trend_symbol, ha='center', va='bottom', fontsize=16)\n",
    "        \n",
    "        ax5.set_title('M√©triques de Qualit√© (avec Tendances)', fontweight='bold', fontsize=14)\n",
    "        ax5.set_ylabel('Score (0-1)')\n",
    "        ax5.set_ylim(0, 1)\n",
    "        \n",
    "        # 3. Distribution de qualit√© (camembert)\n",
    "        ax6 = fig.add_subplot(gs[2, :2])\n",
    "        quality_dist = report['quality_distribution']\n",
    "        labels = ['Excellent (‚â•80%)', 'Bon (60-80%)', 'Moyen (40-60%)', 'Faible (<40%)']\n",
    "        values = [quality_dist['excellent'], quality_dist['good'], \n",
    "                 quality_dist['average'], quality_dist['poor']]\n",
    "        colors = ['green', 'lightgreen', 'orange', 'red']\n",
    "        \n",
    "        wedges, texts, autotexts = ax6.pie(values, labels=labels, colors=colors, \n",
    "                                          autopct='%1.1f%%', startangle=90)\n",
    "        ax6.set_title('Distribution de la Qualit√©', fontweight='bold', fontsize=14)\n",
    "        \n",
    "        # 4. M√©triques de performance\n",
    "        ax7 = fig.add_subplot(gs[2, 2:])\n",
    "        perf_metrics = ['Temps R√©ponse (s)', 'Co√ªt Moyen ($)', 'Efficience']\n",
    "        perf_values = [\n",
    "            report['performance']['avg_response_time'],\n",
    "            report['performance']['avg_cost_per_request'],\n",
    "            report['performance']['cost_efficiency']\n",
    "        ]\n",
    "        \n",
    "        # Normaliser pour visualisation\n",
    "        normalized_values = [\n",
    "            min(perf_values[0] / 2, 1),  # Cap √† 2s\n",
    "            min(perf_values[1] / 0.01, 1),  # Cap √† $0.01\n",
    "            min(perf_values[2] / 100, 1)  # Cap √† 100\n",
    "        ]\n",
    "        \n",
    "        bars = ax7.barh(perf_metrics, normalized_values, \n",
    "                       color=['lightcoral', 'lightsalmon', 'lightseagreen'])\n",
    "        \n",
    "        # Ajouter les valeurs r√©elles\n",
    "        for bar, value in zip(bars, perf_values):\n",
    "            ax7.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{value:.3f}', va='center', fontweight='bold')\n",
    "        \n",
    "        ax7.set_title('M√©triques de Performance', fontweight='bold', fontsize=14)\n",
    "        ax7.set_xlim(0, 1.2)\n",
    "        \n",
    "        # Titre g√©n√©ral\n",
    "        fig.suptitle(f'Dashboard Monitoring Prompts - {report[\"period_days\"]} derniers jours', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def generate_alert_system(self, report: Dict) -> List[str]:\n",
    "        \"\"\"G√©n√®re des alertes bas√©es sur les m√©triques\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        if 'error' in report:\n",
    "            return [f\"üö´ Erreur syst√®me : {report['error']}\"]\n",
    "        \n",
    "        # V√©rifier le score global\n",
    "        if report['avg_overall_score'] < 0.5:\n",
    "            alerts.append(\"üî¥ CRITIQUE : Score global tr√®s bas (<50%)\")\n",
    "        elif report['avg_overall_score'] < 0.7:\n",
    "            alerts.append(\"üü° ATTENTION : Score global mod√©r√© (<70%)\")\n",
    "        \n",
    "        # V√©rifier les tendances\n",
    "        for metric, data in report['metrics'].items():\n",
    "            if data['trend'] == 'decreasing':\n",
    "                alerts.append(f\"üìâ TENDANCE BAISSI√àRE : {metric} en diminution\")\n",
    "        \n",
    "        # V√©rifier les co√ªts\n",
    "        avg_cost = report['performance']['avg_cost_per_request']\n",
    "        if avg_cost > 0.05:\n",
    "            alerts.append(f\"üí∞ CO√õT √âLEV√â : ${avg_cost:.4f} par requ√™te\")\n",
    "        \n",
    "        # V√©rifier la qualit√©\n",
    "        poor_ratio = report['quality_distribution']['poor'] / report['total_evaluations']\n",
    "        if poor_ratio > 0.2:\n",
    "            alerts.append(f\"‚ö†Ô∏è QUALIT√â : {poor_ratio:.1%} de prompts de faible qualit√©\")\n",
    "        \n",
    "        # V√©rifier l'efficience\n",
    "        if report['performance']['cost_efficiency'] < 10:\n",
    "            alerts.append(\"üìä EFFICIENCE FAIBLE : Ratio qualit√©/co√ªt sous-optimal\")\n",
    "        \n",
    "        if not alerts:\n",
    "            alerts.append(\"‚úÖ Toutes les m√©triques sont dans les normes\")\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def export_report(self, report: Dict, filename: str = None) -> str:\n",
    "        \"\"\"Exporte le rapport en JSON\"\"\"\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"prompt_report_{timestamp}.json\"\n",
    "        \n",
    "        # Ajouter metadata\n",
    "        export_data = {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'report_type': 'prompt_monitoring',\n",
    "            'data': report\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### üéØ G√©n√©ration du Dashboard avec donn√©es simul√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer quelques donn√©es de test pour le dashboard\n",
    "print(\"üìä G√©n√©ration de donn√©es pour le dashboard...\")\n",
    "\n",
    "# Simuler quelques √©valuations pour avoir des donn√©es\n",
    "test_prompts = [\n",
    "    \"R√©sume cet article en 3 points.\",\n",
    "    \"Traduis ce texte en anglais professionnel.\",\n",
    "    \"√âcris un email de suivi commercial.\",\n",
    "    \"Cr√©e une liste de contr√¥le pour projet.\",\n",
    "    \"Analyse les risques de cette strat√©gie.\"\n",
    "]\n",
    "\n",
    "# √âvaluer quelques prompts pour cr√©er des donn√©es\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    try:\n",
    "        context = f\"Contexte test {i+1}\"\n",
    "        metrics = evaluator.evaluate_prompt_complete(prompt, context)\n",
    "        print(f\"  ‚úÖ Prompt {i+1} √©valu√© (score: {metrics.overall_score():.3f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erreur prompt {i+1}: {e}\")\n",
    "\n",
    "# Cr√©er le dashboard\n",
    "dashboard = PromptMonitoringDashboard(evaluator)\n",
    "\n",
    "# G√©n√©rer le rapport\n",
    "print(\"\\nüìà G√©n√©ration du rapport de performance...\")\n",
    "performance_report = dashboard.generate_performance_report(days=7)\n",
    "\n",
    "if 'error' not in performance_report:\n",
    "    # Afficher le rapport textuel\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä RAPPORT DE PERFORMANCE - 7 DERNIERS JOURS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìã R√©sum√© :\")\n",
    "    print(f\"  ‚Ä¢ Total √©valuations : {performance_report['total_evaluations']}\")\n",
    "    print(f\"  ‚Ä¢ Score global moyen : {performance_report['avg_overall_score']:.1%}\")\n",
    "    print(f\"  ‚Ä¢ Co√ªt total : ${performance_report['performance']['total_cost']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Efficience : {performance_report['performance']['cost_efficiency']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüìä M√©triques d√©taill√©es :\")\n",
    "    for metric, data in performance_report['metrics'].items():\n",
    "        trend_emoji = {'increasing': 'üìà', 'decreasing': 'üìâ', 'stable': '‚û°Ô∏è'}\n",
    "        emoji = trend_emoji.get(data['trend'], '‚ùì')\n",
    "        print(f\"  ‚Ä¢ {metric.capitalize()}: {data['mean']:.3f} (¬±{data['std']:.3f}) {emoji}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Distribution qualit√© :\")\n",
    "    dist = performance_report['quality_distribution']\n",
    "    total = performance_report['total_evaluations']\n",
    "    print(f\"  ‚Ä¢ Excellent: {dist['excellent']} ({dist['excellent']/total:.1%})\")\n",
    "    print(f\"  ‚Ä¢ Bon: {dist['good']} ({dist['good']/total:.1%})\")\n",
    "    print(f\"  ‚Ä¢ Moyen: {dist['average']} ({dist['average']/total:.1%})\")\n",
    "    print(f\"  ‚Ä¢ Faible: {dist['poor']} ({dist['poor']/total:.1%})\")\n",
    "    \n",
    "    # G√©n√©rer les alertes\n",
    "    print(f\"\\nüö® Alertes :\")\n",
    "    alerts = dashboard.generate_alert_system(performance_report)\n",
    "    for alert in alerts:\n",
    "        print(f\"  {alert}\")\n",
    "    \n",
    "    # Cr√©er la visualisation\n",
    "    print(f\"\\nüìà G√©n√©ration du dashboard visuel...\")\n",
    "    dashboard.create_dashboard_visualization(performance_report)\n",
    "    \n",
    "    # Exporter le rapport\n",
    "    export_filename = dashboard.export_report(performance_report)\n",
    "    print(f\"\\nüíæ Rapport export√© : {export_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Erreur : {performance_report['error']}\")\n",
    "    print(\"‚ÑπÔ∏è  Ex√©cutez d'abord quelques √©valuations pour g√©n√©rer des donn√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 5. üéØ Synth√®se et Bonnes Pratiques\n",
    "\n",
    "### Framework Complet d'Optimisation\n",
    "\n",
    "Voici une classe qui combine toutes les techniques vues dans ce notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensivePromptOptimizer:\n",
    "    \"\"\"Framework complet d'optimisation de prompts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = PromptEvaluator()\n",
    "        self.ab_tester = PromptABTester(self.evaluator)\n",
    "        self.genetic_optimizer = GeneticPromptOptimizer(self.evaluator)\n",
    "        self.dashboard = PromptMonitoringDashboard(self.evaluator)\n",
    "    \n",
    "    def comprehensive_optimization_pipeline(self, \n",
    "                                          base_prompt: str,\n",
    "                                          optimization_goal: str,\n",
    "                                          test_contexts: List[str]) -> Dict:\n",
    "        \"\"\"Pipeline complet d'optimisation\"\"\"\n",
    "        \n",
    "        print(\"üöÄ PIPELINE D'OPTIMISATION COMPLET\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        results = {\n",
    "            'base_prompt': base_prompt,\n",
    "            'goal': optimization_goal,\n",
    "            'stages': {}\n",
    "        }\n",
    "        \n",
    "        # Stage 1: √âvaluation initiale\n",
    "        print(\"\\nüìä Stage 1: √âvaluation Baseline\")\n",
    "        baseline_metrics = self.evaluator.evaluate_prompt_complete(base_prompt)\n",
    "        results['stages']['baseline'] = {\n",
    "            'metrics': baseline_metrics,\n",
    "            'score': baseline_metrics.overall_score()\n",
    "        }\n",
    "        print(f\"   Score baseline: {baseline_metrics.overall_score():.3f}\")\n",
    "        \n",
    "        # Stage 2: A/B Testing avec variations manuelles\n",
    "        print(\"\\nüß™ Stage 2: A/B Testing\")\n",
    "        improved_prompt = self._generate_improved_variant(base_prompt, optimization_goal)\n",
    "        \n",
    "        ab_config = ABTestConfig(\n",
    "            test_name=f\"Optimization: {optimization_goal}\",\n",
    "            prompt_a=base_prompt,\n",
    "            prompt_b=improved_prompt,\n",
    "            sample_size=15,\n",
    "            test_contexts=test_contexts\n",
    "        )\n",
    "        \n",
    "        ab_result = self.ab_tester.run_ab_test(ab_config)\n",
    "        results['stages']['ab_test'] = ab_result\n",
    "        \n",
    "        best_ab_prompt = improved_prompt if ab_result.winner == 'B' else base_prompt\n",
    "        print(f\"   Gagnant A/B: {ab_result.winner} (lift: {ab_result.lift_percentage:+.1f}%)\")\n",
    "        \n",
    "        # Stage 3: Optimisation g√©n√©tique\n",
    "        print(\"\\nüß¨ Stage 3: Optimisation G√©n√©tique\")\n",
    "        \n",
    "        # Cr√©er un template et des g√®nes bas√©s sur le meilleur prompt A/B\n",
    "        genetic_template, genetic_genes = self._create_genetic_components(best_ab_prompt)\n",
    "        \n",
    "        genetic_result = self.genetic_optimizer.optimize(\n",
    "            template=genetic_template,\n",
    "            genes=genetic_genes,\n",
    "            test_contexts=test_contexts,\n",
    "            generations=5,\n",
    "            population_size=10\n",
    "        )\n",
    "        \n",
    "        results['stages']['genetic'] = genetic_result\n",
    "        print(f\"   Score g√©n√©tique final: {genetic_result['best_fitness']:.3f}\")\n",
    "        \n",
    "        # Stage 4: Validation finale\n",
    "        print(\"\\n‚úÖ Stage 4: Validation Finale\")\n",
    "        final_prompt = genetic_result['best_prompt']\n",
    "        final_metrics = self.evaluator.evaluate_prompt_complete(final_prompt)\n",
    "        \n",
    "        results['stages']['final'] = {\n",
    "            'prompt': final_prompt,\n",
    "            'metrics': final_metrics,\n",
    "            'score': final_metrics.overall_score()\n",
    "        }\n",
    "        \n",
    "        # Calculer l'am√©lioration totale\n",
    "        total_improvement = (\n",
    "            (final_metrics.overall_score() - baseline_metrics.overall_score()) / \n",
    "            baseline_metrics.overall_score() * 100\n",
    "        )\n",
    "        \n",
    "        results['summary'] = {\n",
    "            'total_improvement': total_improvement,\n",
    "            'baseline_score': baseline_metrics.overall_score(),\n",
    "            'final_score': final_metrics.overall_score(),\n",
    "            'final_prompt': final_prompt\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüèÜ R√âSULTAT FINAL:\")\n",
    "        print(f\"   Am√©lioration totale: {total_improvement:+.1f}%\")\n",
    "        print(f\"   Score baseline: {baseline_metrics.overall_score():.3f}\")\n",
    "        print(f\"   Score final: {final_metrics.overall_score():.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_improved_variant(self, base_prompt: str, goal: str) -> str:\n",
    "        \"\"\"G√©n√®re une variante am√©lior√©e du prompt\"\"\"\n",
    "        improvement_prompt = f\"\"\"\n",
    "        Am√©liore ce prompt pour mieux atteindre l'objectif sp√©cifi√© :\n",
    "        \n",
    "        PROMPT ORIGINAL : {base_prompt}\n",
    "        \n",
    "        OBJECTIF D'AM√âLIORATION : {goal}\n",
    "        \n",
    "        Cr√©e une version am√©lior√©e qui :\n",
    "        1. Garde l'intention originale\n",
    "        2. Optimise pour l'objectif donn√©\n",
    "        3. Ajoute de la structure si n√©cessaire\n",
    "        4. Am√©liore la clart√©\n",
    "        \n",
    "        R√©ponds uniquement avec le prompt am√©lior√©.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": improvement_prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur g√©n√©ration variante : {e}\")\n",
    "            return base_prompt + \"\\n\\nSois pr√©cis et d√©taill√©.\"\n",
    "    \n",
    "    def _create_genetic_components(self, prompt: str) -> Tuple[str, List[PromptGene]]:\n",
    "        \"\"\"Cr√©e les composants g√©n√©tiques √† partir d'un prompt\"\"\"\n",
    "        # Template simple avec placeholders\n",
    "        template = \"{instruction}\\n\\n{enhancement}\\n\\n{format}\"\n",
    "        \n",
    "        # G√®nes g√©n√©riques pour am√©lioration\n",
    "        genes = [\n",
    "            PromptGene(\n",
    "                name=\"instruction\",\n",
    "                type=\"instruction\",\n",
    "                variations=[\n",
    "                    prompt,\n",
    "                    f\"En tant qu'expert, {prompt.lower()}\",\n",
    "                    f\"Analyse m√©thodiquement et {prompt.lower()}\",\n",
    "                    f\"√âtape par √©tape : {prompt.lower()}\"\n",
    "                ]\n",
    "            ),\n",
    "            PromptGene(\n",
    "                name=\"enhancement\",\n",
    "                type=\"constraint\",\n",
    "                variations=[\n",
    "                    \"\",\n",
    "                    \"Sois pr√©cis et factuel.\",\n",
    "                    \"Assure-toi que ta r√©ponse est compl√®te et bien structur√©e.\",\n",
    "                    \"Vise l'excellence et la clart√©.\"\n",
    "                ]\n",
    "            ),\n",
    "            PromptGene(\n",
    "                name=\"format\",\n",
    "                type=\"format\",\n",
    "                variations=[\n",
    "                    \"\",\n",
    "                    \"Pr√©sente ta r√©ponse de mani√®re claire et organis√©e.\",\n",
    "                    \"Structure ta r√©ponse avec des points principaux.\",\n",
    "                    \"Utilise des sections et des exemples si pertinent.\"\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return template, genes\n",
    "    \n",
    "    def generate_optimization_report(self, results: Dict) -> str:\n",
    "        \"\"\"G√©n√®re un rapport complet d'optimisation\"\"\"\n",
    "        report = \"\\n\" + \"=\"*80 + \"\\n\"\n",
    "        report += \"üéØ RAPPORT COMPLET D'OPTIMISATION DE PROMPT\\n\"\n",
    "        report += \"=\"*80 + \"\\n\"\n",
    "        \n",
    "        report += f\"\\nüìã OBJECTIF : {results['goal']}\\n\"\n",
    "        \n",
    "        report += f\"\\nüìä R√âSUM√â DES PERFORMANCES :\\n\"\n",
    "        report += f\"  ‚Ä¢ Score baseline      : {results['summary']['baseline_score']:.3f}\\n\"\n",
    "        report += f\"  ‚Ä¢ Score final         : {results['summary']['final_score']:.3f}\\n\"\n",
    "        report += f\"  ‚Ä¢ Am√©lioration totale  : {results['summary']['total_improvement']:+.1f}%\\n\"\n",
    "        \n",
    "        report += f\"\\nüß™ D√âTAIL DES STAGES :\\n\"\n",
    "        \n",
    "        if 'ab_test' in results['stages']:\n",
    "            ab = results['stages']['ab_test']\n",
    "            report += f\"  üìà A/B Test - Gagnant: {ab.winner} (Lift: {ab.lift_percentage:+.1f}%)\\n\"\n",
    "        \n",
    "        if 'genetic' in results['stages']:\n",
    "            genetic = results['stages']['genetic']\n",
    "            report += f\"  üß¨ G√©n√©tique - Score final: {genetic['best_fitness']:.3f}\\n\"\n",
    "        \n",
    "        report += f\"\\n‚ú® PROMPT FINAL OPTIMIS√â :\\n\"\n",
    "        report += \"‚îÄ\" * 40 + \"\\n\"\n",
    "        report += results['summary']['final_prompt'] + \"\\n\"\n",
    "        report += \"‚îÄ\" * 40 + \"\\n\"\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### üéØ Exemple : Pipeline complet d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation du pipeline complet\n",
    "optimizer = ComprehensivePromptOptimizer()\n",
    "\n",
    "# Prompt de base √† optimiser\n",
    "base_prompt = \"Explique ce concept technique.\"\n",
    "\n",
    "# Objectif d'optimisation\n",
    "optimization_goal = \"Maximiser la clart√© et l'utilit√© pour des non-experts\"\n",
    "\n",
    "# Contextes de test\n",
    "test_contexts = [\n",
    "    \"Concept: Intelligence Artificielle\",\n",
    "    \"Concept: Blockchain\", \n",
    "    \"Concept: Machine Learning\",\n",
    "    \"Concept: Cloud Computing\"\n",
    "]\n",
    "\n",
    "# Ex√©cuter le pipeline complet\n",
    "optimization_results = optimizer.comprehensive_optimization_pipeline(\n",
    "    base_prompt=base_prompt,\n",
    "    optimization_goal=optimization_goal,\n",
    "    test_contexts=test_contexts\n",
    ")\n",
    "\n",
    "# G√©n√©rer et afficher le rapport\n",
    "final_report = optimizer.generate_optimization_report(optimization_results)\n",
    "print(final_report)\n",
    "\n",
    "# Visualiser l'√©volution g√©n√©tique si disponible\n",
    "if 'genetic' in optimization_results['stages']:\n",
    "    print(\"\\nüìà √âvolution g√©n√©tique :\")\n",
    "    optimizer.genetic_optimizer.visualize_evolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## üìö Bonnes Pratiques et Recommandations\n",
    "\n",
    "### üéØ Guide de S√©lection des Techniques\n",
    "\n",
    "| Situation | Technique Recommand√©e | Raison |\n",
    "|-----------|----------------------|--------|\n",
    "| **Prompt unique critique** | A/B Testing simple | Validation statistique rapide |\n",
    "| **Optimisation continue** | Monitoring Dashboard | Suivi de performance |\n",
    "| **Exploration cr√©ative** | Algorithme G√©n√©tique | D√©couverte de solutions inattendues |\n",
    "| **Am√©lioration incr√©mentale** | M√©triques + A/B | Optimisation mesur√©e |\n",
    "| **D√©ploiement production** | Pipeline complet | Robustesse maximale |\n",
    "\n",
    "### üí° Conseils d'Impl√©mentation\n",
    "\n",
    "#### üîß M√©triques\n",
    "- **Commencez simple** : Utilisez 3-5 m√©triques maximum\n",
    "- **Alignez business** : Vos m√©triques doivent refl√©ter vos objectifs r√©els\n",
    "- **√âvitez la sur-optimisation** : Ne pas optimiser pour les m√©triques mais pour l'usage\n",
    "\n",
    "#### üß™ A/B Testing\n",
    "- **Taille d'√©chantillon** : Minimum 30 √©chantillons par variant\n",
    "- **Significativit√©** : Attendez p < 0.05 ET effect size > 0.2\n",
    "- **Contexte vari√©** : Testez sur plusieurs types d'usage\n",
    "\n",
    "#### üß¨ Optimisation G√©n√©tique\n",
    "- **Patience** : 10-20 g√©n√©rations minimum\n",
    "- **Diversit√©** : Maintenez de la variation dans la population\n",
    "- **Co√ªt vs B√©n√©fice** : Technique co√ªteuse, r√©serv√©e aux cas critiques\n",
    "\n",
    "#### üìä Monitoring\n",
    "- **Temps r√©el** : Int√©grez dans vos workflows existants\n",
    "- **Alertes automatiques** : Configurez des seuils d'alerte\n",
    "- **Tendances** : Surveillez l'√©volution plus que les valeurs absolues\n",
    "\n",
    "### ‚ö° Optimisations de Performance\n",
    "\n",
    "```python\n",
    "# Bonnes pratiques de performance\n",
    "best_practices = {\n",
    "    'batch_evaluation': 'Groupez les √©valuations pour r√©duire les co√ªts API',\n",
    "    'caching': 'Cachez les r√©sultats d\\'√©valuations similaires',\n",
    "    'parallel_processing': 'Utilisez ThreadPoolExecutor pour parall√©liser',\n",
    "    'smart_sampling': 'Adaptez la taille d\\'√©chantillon selon l\\'importance',\n",
    "    'early_stopping': 'Arr√™tez l\\'optimisation si convergence atteinte'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## üöÄ Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Cr√©er vos propres m√©triques m√©tier\n",
    "\n",
    "Impl√©mentez des m√©triques sp√©cifiques √† votre domaine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple pour l'e-commerce\n",
    "class ECommercePromptMetrics(PromptMetrics):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ajoutez vos m√©triques m√©tier\n",
    "        self.conversion_potential: float = 0.0\n",
    "        self.personalization_score: float = 0.0\n",
    "        self.urgency_factor: float = 0.0\n",
    "    \n",
    "    def business_score(self) -> float:\n",
    "        \"\"\"Score orient√© business pour l'e-commerce\"\"\"\n",
    "        return (\n",
    "            self.conversion_potential * 0.4 +\n",
    "            self.personalization_score * 0.3 +\n",
    "            self.urgency_factor * 0.3\n",
    "        )\n",
    "\n",
    "# Votre impl√©mentation ici\n",
    "# D√©finissez des m√©triques pour votre domaine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Exercice 2 : Syst√®me d'alertes intelligent\n",
    "\n",
    "Cr√©ez un syst√®me d'alertes qui s'adapte aux patterns de votre usage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAlertSystem:\n",
    "    def __init__(self):\n",
    "        self.baseline_metrics = {}\n",
    "        self.alert_thresholds = {}\n",
    "    \n",
    "    def learn_baseline(self, historical_data: pd.DataFrame):\n",
    "        \"\"\"Apprend les patterns normaux pour d√©finir les seuils\"\"\"\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "    \n",
    "    def generate_smart_alerts(self, current_metrics: Dict) -> List[str]:\n",
    "        \"\"\"G√©n√®re des alertes adapt√©es au contexte\"\"\"\n",
    "        # Votre impl√©mentation ici\n",
    "        pass\n",
    "\n",
    "# Impl√©mentez votre syst√®me d'alertes intelligent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## üéì Pour aller plus loin\n",
    "\n",
    "### üìö Ressources avanc√©es :\n",
    "- [A/B Testing Guide for AI Systems](https://example.com)\n",
    "- [Genetic Algorithms in NLP](https://example.com)\n",
    "- [MLOps for Prompt Engineering](https://example.com)\n",
    "\n",
    "### üî¨ Techniques avanc√©es √† explorer :\n",
    "1. **Bayesian Optimization** pour l'optimisation continue\n",
    "2. **Multi-Armed Bandits** pour l'exploration vs exploitation\n",
    "3. **Neural Architecture Search** pour l'optimisation de structure\n",
    "4. **Reinforcement Learning** pour l'apprentissage adaptatif\n",
    "\n",
    "### üöÄ Int√©grations recommand√©es :\n",
    "- **MLflow** pour le tracking des exp√©rimentations\n",
    "- **Weights & Biases** pour le monitoring avanc√©\n",
    "- **Apache Airflow** pour l'orchestration des pipelines\n",
    "- **FastAPI** pour cr√©er des APIs d'optimisation\n",
    "\n",
    "---\n",
    "\n",
    "**Prochain notebook** : 04_Systemes_Dynamiques.ipynb - Templates adaptatifs et syst√®mes auto-am√©lior√©s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}