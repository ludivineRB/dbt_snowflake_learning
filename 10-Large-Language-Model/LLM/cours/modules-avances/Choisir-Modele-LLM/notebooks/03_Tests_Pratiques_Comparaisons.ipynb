{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Tests Pratiques et Comparaisons - √âvaluation R√©elle\n",
    "\n",
    "## üéØ Objectifs\n",
    "- **Tester** les mod√®les sur de vraies t√¢ches\n",
    "- **Mesurer** la latence et le co√ªt r√©els\n",
    "- **Comparer** les performances sur vos cas d'usage\n",
    "- **Valider** les recommandations th√©oriques\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Pourquoi Tester en Pratique ?\n",
    "\n",
    "### Les Benchmarks ne Disent Pas Tout !\n",
    "- üìä **MMLU** : Excellent sur les questions acad√©miques\n",
    "- üéØ **Votre usage** : Peut-√™tre diff√©rent !\n",
    "\n",
    "### Exemple Concret :\n",
    "```\n",
    "GPT-4 : 86.4% MMLU ‚Üí \"Meilleur mod√®le\"\n",
    "Mistral 7B : 62.5% MMLU ‚Üí \"Mod√®le moyen\"\n",
    "\n",
    "MAIS sur votre t√¢che sp√©cifique :\n",
    "GPT-4 : Excellent mais 3 secondes et 30$/1M tokens\n",
    "Mistral 7B : 90% de la qualit√© en 0.8 seconde et gratuit\n",
    "\n",
    "‚Üí Mistral 7B peut √™tre MEILLEUR pour vous !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation et Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (6.1.2)\n",
      "Requirement already satisfied: numpy in /Users/guillaume/.pyenv/versions/3.13.0/lib/python3.13/site-packages (2.2.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install requests pandas matplotlib seaborn plotly numpy time aiohttp asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiohttp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maiohttp\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'aiohttp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Configuration des APIs\n",
    "\n",
    "### ‚ö†Ô∏è Important : S√©curit√© des Cl√©s API\n",
    "\n",
    "**JAMAIS** de cl√©s API directement dans le code !\n",
    "\n",
    "### üõ°Ô∏è M√©thodes S√©curis√©es :\n",
    "1. **Variables d'environnement** (recommand√©)\n",
    "2. **Fichier .env** (non versionn√©)\n",
    "3. **Saisie manuelle** (pour les tests)\n",
    "\n",
    "### üîß Setup Recommand√© :\n",
    "```bash\n",
    "# Dans votre terminal :\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "export GOOGLE_API_KEY=\"AI...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Configuration s√©curis√©e des APIs\n",
    "def setup_api_keys():\n",
    "    \"\"\"\n",
    "    Configuration s√©curis√©e des cl√©s API\n",
    "    \"\"\"\n",
    "    api_keys = {}\n",
    "    \n",
    "    # Essayer les variables d'environnement d'abord\n",
    "    api_keys['openai'] = os.getenv('OPENAI_API_KEY')\n",
    "    api_keys['anthropic'] = os.getenv('ANTHROPIC_API_KEY')\n",
    "    api_keys['google'] = os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    # Si pas trouv√©es, demander √† l'utilisateur\n",
    "    print(\"üîë Configuration des cl√©s API\")\n",
    "    print(\"‚ö†Ô∏è  Appuyez sur Entr√©e pour ignorer une API si vous n'avez pas la cl√©\")\n",
    "    \n",
    "    if not api_keys['openai']:\n",
    "        key = getpass(\"OpenAI API Key (sk-...): \")\n",
    "        api_keys['openai'] = key if key.strip() else None\n",
    "    \n",
    "    if not api_keys['anthropic']:\n",
    "        key = getpass(\"Anthropic API Key (sk-ant-...): \")\n",
    "        api_keys['anthropic'] = key if key.strip() else None\n",
    "    \n",
    "    if not api_keys['google']:\n",
    "        key = getpass(\"Google API Key (AI...): \")\n",
    "        api_keys['google'] = key if key.strip() else None\n",
    "    \n",
    "    # Afficher les APIs disponibles\n",
    "    available_apis = [k for k, v in api_keys.items() if v]\n",
    "    print(f\"\\n‚úÖ APIs configur√©es : {', '.join(available_apis)}\")\n",
    "    \n",
    "    if not available_apis:\n",
    "        print(\"‚ö†Ô∏è  Aucune API configur√©e. Nous utiliserons des donn√©es simul√©es.\")\n",
    "    \n",
    "    return api_keys\n",
    "\n",
    "# Configurer les APIs\n",
    "API_KEYS = setup_api_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Framework de Test Unifi√©\n",
    "\n",
    "### üéØ T√¢ches de Test Standardis√©es\n",
    "\n",
    "Nous allons tester les mod√®les sur 5 t√¢ches repr√©sentatives :\n",
    "\n",
    "1. **üìù R√©sum√© de texte**\n",
    "2. **üí¨ Questions-r√©ponses**\n",
    "3. **üíª G√©n√©ration de code**\n",
    "4. **üåç Traduction**\n",
    "5. **üé® Cr√©ativit√© (storytelling)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Dataset de test cr√©√© !\n",
      "Nombre de t√¢ches : 5\n",
      "  ‚Ä¢ R√©sum√© de Texte (100 tokens max)\n",
      "  ‚Ä¢ Questions-R√©ponses (150 tokens max)\n",
      "  ‚Ä¢ G√©n√©ration de Code (200 tokens max)\n",
      "  ‚Ä¢ Traduction (100 tokens max)\n",
      "  ‚Ä¢ Cr√©ativit√© (200 tokens max)\n"
     ]
    }
   ],
   "source": [
    "# Dataset de test standardis√©\n",
    "TEST_TASKS = {\n",
    "    'resume': {\n",
    "        'name': 'R√©sum√© de Texte',\n",
    "        'prompt': \"R√©sumez ce texte en 2-3 phrases claires :\",\n",
    "        'input': \"\"\"L'intelligence artificielle (IA) conna√Æt une r√©volution sans pr√©c√©dent avec l'√©mergence des grands mod√®les de langage (LLM). Ces syst√®mes, entra√Æn√©s sur d'√©normes corpus de texte, d√©montrent des capacit√©s remarquables de compr√©hension et de g√©n√©ration de langage naturel. \n",
    "        \n",
    "GPT-4, Claude, et leurs concurrents transforment d√©j√† de nombreux secteurs, de l'√©ducation √† la programmation. Cependant, ces avanc√©es soul√®vent aussi des questions importantes sur l'√©thique, l'emploi, et la v√©racit√© des informations g√©n√©r√©es. \n",
    "\n",
    "Les entreprises investissent massivement dans cette technologie, mais doivent naviguer entre opportunit√©s et risques. Le choix du bon mod√®le devient crucial pour maximiser les b√©n√©fices tout en contr√¥lant les co√ªts et les risques.\"\"\",\n",
    "        'max_tokens': 100\n",
    "    },\n",
    "    \n",
    "    'qa': {\n",
    "        'name': 'Questions-R√©ponses',\n",
    "        'prompt': \"R√©pondez pr√©cis√©ment √† cette question :\",\n",
    "        'input': \"Quels sont les 3 principaux avantages des mod√®les LLM open source par rapport aux mod√®les propri√©taires ?\",\n",
    "        'max_tokens': 150\n",
    "    },\n",
    "    \n",
    "    'code': {\n",
    "        'name': 'G√©n√©ration de Code',\n",
    "        'prompt': \"√âcrivez une fonction Python pour :\",\n",
    "        'input': \"Calculer la moyenne pond√©r√©e d'une liste de nombres avec leurs poids. La fonction doit g√©rer les cas d'erreur et retourner None si les listes ont des tailles diff√©rentes.\",\n",
    "        'max_tokens': 200\n",
    "    },\n",
    "    \n",
    "    'translation': {\n",
    "        'name': 'Traduction',\n",
    "        'prompt': \"Traduisez ce texte fran√ßais en anglais naturel :\",\n",
    "        'input': \"Bonjour, j'aimerais r√©server une table pour quatre personnes ce soir vers 20 heures. Avez-vous quelque chose de disponible ? De pr√©f√©rence pr√®s de la fen√™tre si possible.\",\n",
    "        'max_tokens': 100\n",
    "    },\n",
    "    \n",
    "    'creativity': {\n",
    "        'name': 'Cr√©ativit√©',\n",
    "        'prompt': \"√âcrivez le d√©but d'une histoire courte (2-3 paragraphes) avec ce th√®me :\",\n",
    "        'input': \"Un d√©veloppeur d√©couvre que son IA domestique a commenc√© √† √©crire de la po√©sie la nuit, en secret.\",\n",
    "        'max_tokens': 200\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Dataset de test cr√©√© !\")\n",
    "print(f\"Nombre de t√¢ches : {len(TEST_TASKS)}\")\n",
    "for task_id, task in TEST_TASKS.items():\n",
    "    print(f\"  ‚Ä¢ {task['name']} ({task['max_tokens']} tokens max)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Connecteurs API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Framework de test configur√© !\n"
     ]
    }
   ],
   "source": [
    "# Simulateur de r√©ponses si pas d'API\n",
    "class ModelSimulator:\n",
    "    \"\"\"Simule les r√©ponses des mod√®les si les APIs ne sont pas disponibles\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.responses = {\n",
    "            'resume': {\n",
    "                'gpt-4': \"Les LLM r√©volutionnent l'IA avec des capacit√©s remarquables de langage naturel. Ils transforment de nombreux secteurs mais soul√®vent des questions √©thiques. Le choix du bon mod√®le est crucial pour les entreprises.\",\n",
    "                'claude-3': \"L'IA conna√Æt une r√©volution avec les grands mod√®les de langage qui d√©montrent des capacit√©s exceptionnelles. Ces avanc√©es transforment les secteurs tout en soulevant des d√©fis √©thiques et √©conomiques. Les entreprises doivent choisir leur mod√®le avec soin.\",\n",
    "                'gemini': \"Les LLM marquent une r√©volution de l'IA avec des capacit√©s linguistiques avanc√©es qui transforment l'√©ducation et la programmation. Cependant, ils posent des d√©fis √©thiques et √©conomiques que les entreprises doivent naviguer soigneusement.\"\n",
    "            },\n",
    "            'qa': {\n",
    "                'gpt-4': \"Les 3 principaux avantages des LLM open source sont : 1) Contr√¥le total et confidentialit√© des donn√©es, 2) Co√ªts r√©duits (pas de frais par API), 3) Personnalisation compl√®te via fine-tuning.\",\n",
    "                'claude-3': \"Les mod√®les open source offrent : 1) Une confidentialit√© totale des donn√©es trait√©es localement, 2) Des co√ªts pr√©visibles sans d√©pendance aux tarifs API, 3) Une personnalisation pouss√©e adapt√©e aux besoins sp√©cifiques.\",\n",
    "                'gemini': \"Avantages cl√©s de l'open source : 1) Privacy et s√©curit√© avec traitement local, 2) √âconomies substantielles sur les volumes importants, 3) Flexibilit√© maximale pour adaptation et fine-tuning.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate(self, prompt, task_type='resume'):\n",
    "        \"\"\"Simule une g√©n√©ration\"\"\"\n",
    "        model_key = self.model_name.lower().replace('-', '').replace(' ', '')\n",
    "        \n",
    "        if 'gpt' in model_key:\n",
    "            model_key = 'gpt-4'\n",
    "        elif 'claude' in model_key:\n",
    "            model_key = 'claude-3'\n",
    "        else:\n",
    "            model_key = 'gemini'\n",
    "        \n",
    "        response = self.responses.get(task_type, {}).get(model_key, \"R√©ponse simul√©e pour cette t√¢che.\")\n",
    "        \n",
    "        # Simuler la latence\n",
    "        if 'gpt-4' in self.model_name.lower():\n",
    "            time.sleep(2)  # GPT-4 plus lent\n",
    "        elif 'mistral' in self.model_name.lower():\n",
    "            time.sleep(0.5)  # Mistral plus rapide\n",
    "        else:\n",
    "            time.sleep(1)  # Autres mod√®les\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Fonction de test unifi√©e\n",
    "async def test_model_on_task(model_name, task_id, task_data, api_keys=None):\n",
    "    \"\"\"\n",
    "    Teste un mod√®le sur une t√¢che sp√©cifique\n",
    "    \"\"\"\n",
    "    prompt = f\"{task_data['prompt']}\\n\\n{task_data['input']}\\n\\nR√©ponse:\"\n",
    "    \n",
    "    # Mesurer le temps de d√©but\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Si APIs disponibles, utiliser les vraies APIs\n",
    "        if api_keys and any(api_keys.values()):\n",
    "            # Ici on pourrait impl√©menter les vraies APIs\n",
    "            # Pour ce demo, on utilise le simulateur\n",
    "            simulator = ModelSimulator(model_name)\n",
    "            response = simulator.generate(prompt, task_id)\n",
    "        else:\n",
    "            # Utiliser le simulateur\n",
    "            simulator = ModelSimulator(model_name)\n",
    "            response = simulator.generate(prompt, task_id)\n",
    "        \n",
    "        # Calculer la latence\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Calculer le co√ªt (simulation)\n",
    "        input_tokens = len(prompt.split()) * 1.3  # Approximation\n",
    "        output_tokens = len(response.split()) * 1.3\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        \n",
    "        # Co√ªts par mod√®le ($/1M tokens)\n",
    "        costs_per_million = {\n",
    "            'GPT-4': 30.0,\n",
    "            'GPT-3.5 Turbo': 1.0,\n",
    "            'Claude 3 Opus': 15.0,\n",
    "            'Claude 3 Sonnet': 3.0,\n",
    "            'Gemini Pro': 2.5,\n",
    "            'Mistral 7B': 0.0,  # Open source\n",
    "            'Llama 2': 0.0     # Open source\n",
    "        }\n",
    "        \n",
    "        cost_per_token = costs_per_million.get(model_name, 0) / 1_000_000\n",
    "        cost = total_tokens * cost_per_token\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'task': task_id,\n",
    "            'task_name': task_data['name'],\n",
    "            'response': response,\n",
    "            'latency': latency,\n",
    "            'input_tokens': int(input_tokens),\n",
    "            'output_tokens': int(output_tokens),\n",
    "            'total_tokens': int(total_tokens),\n",
    "            'cost': cost,\n",
    "            'success': True\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'task': task_id,\n",
    "            'task_name': task_data['name'],\n",
    "            'response': f\"Erreur: {str(e)}\",\n",
    "            'latency': None,\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "            'total_tokens': 0,\n",
    "            'cost': 0,\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "print(\"üîå Framework de test configur√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Lancement des Tests\n",
    "\n",
    "### üéØ Mod√®les √† Tester\n",
    "\n",
    "Nous allons comparer 6 mod√®les repr√©sentatifs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Lancement des tests sur 6 mod√®les et 5 t√¢ches\n",
      "üìä Total de tests : 30 = 30\n",
      "\n",
      "‚è±Ô∏è  Temps estim√© : 2-3 minutes (avec simulation)\n",
      "\n",
      "üöÄ D√©marrage des tests...\n",
      "\n",
      "üìã Test du mod√®le 1/6: GPT-4\n",
      "  üß™ T√¢che 1/5: R√©sum√© de Texte... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'API_KEYS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üß™ T√¢che \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(TEST_TASKS)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_data[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m... \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Test synchrone pour la d√©mo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m test_model_on_task(model, task_id, task_data, \u001b[43mAPI_KEYS\u001b[49m)\n\u001b[32m     27\u001b[39m test_results.append(result)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'API_KEYS' is not defined"
     ]
    }
   ],
   "source": [
    "# Mod√®les √† tester\n",
    "MODELS_TO_TEST = [\n",
    "    'GPT-4',\n",
    "    'GPT-3.5 Turbo', \n",
    "    'Claude 3 Opus',\n",
    "    'Claude 3 Sonnet',\n",
    "    'Gemini Pro',\n",
    "    'Mistral 7B'\n",
    "]\n",
    "\n",
    "print(f\"üß™ Lancement des tests sur {len(MODELS_TO_TEST)} mod√®les et {len(TEST_TASKS)} t√¢ches\")\n",
    "print(f\"üìä Total de tests : {len(MODELS_TO_TEST) * len(TEST_TASKS)} = {len(MODELS_TO_TEST) * len(TEST_TASKS)}\")\n",
    "print(\"\\n‚è±Ô∏è  Temps estim√© : 2-3 minutes (avec simulation)\")\n",
    "print(\"\\nüöÄ D√©marrage des tests...\")\n",
    "\n",
    "# Lancer tous les tests\n",
    "test_results = []\n",
    "\n",
    "for i, model in enumerate(MODELS_TO_TEST, 1):\n",
    "    print(f\"\\nüìã Test du mod√®le {i}/{len(MODELS_TO_TEST)}: {model}\")\n",
    "    \n",
    "    for j, (task_id, task_data) in enumerate(TEST_TASKS.items(), 1):\n",
    "        print(f\"  üß™ T√¢che {j}/{len(TEST_TASKS)}: {task_data['name']}... \", end=\"\")\n",
    "        \n",
    "        # Test synchrone pour la d√©mo\n",
    "        result = await test_model_on_task(model, task_id, task_data, API_KEYS)\n",
    "        test_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úÖ {result['latency']:.2f}s, {result['total_tokens']} tokens\")\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur\")\n",
    "\n",
    "print(f\"\\nüéâ Tests termin√©s ! {len(test_results)} r√©sultats collect√©s.\")\n",
    "\n",
    "# Convertir en DataFrame pour l'analyse\n",
    "df_results = pd.DataFrame(test_results)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analyse des R√©sultats\n",
    "\n",
    "### ‚ö° Performance vs Latence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Analyser la latence par mod√®le\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m latency_by_model = \u001b[43mdf_results\u001b[49m[df_results[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]].groupby(\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mlatency\u001b[39m\u001b[33m'\u001b[39m].agg([\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m]).reset_index()\n\u001b[32m      3\u001b[39m latency_by_model.columns = [\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLatence_Moyenne\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLatence_Std\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Graphique de latence\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Analyser la latence par mod√®le\n",
    "latency_by_model = df_results[df_results['success']].groupby('model')['latency'].agg(['mean', 'std']).reset_index()\n",
    "latency_by_model.columns = ['Model', 'Latence_Moyenne', 'Latence_Std']\n",
    "\n",
    "# Graphique de latence\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Latence moyenne par mod√®le\n",
    "bars = ax1.bar(latency_by_model['Model'], latency_by_model['Latence_Moyenne'], \n",
    "               yerr=latency_by_model['Latence_Std'], capsize=5,\n",
    "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3'])\n",
    "\n",
    "ax1.set_title('‚ö° Latence Moyenne par Mod√®le', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Latence (secondes)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, latence in zip(bars, latency_by_model['Latence_Moyenne']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "             f'{latence:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# Latence par t√¢che\n",
    "latency_by_task = df_results[df_results['success']].groupby('task_name')['latency'].mean().reset_index()\n",
    "latency_by_task = latency_by_task.sort_values('latency')\n",
    "\n",
    "ax2.barh(latency_by_task['task_name'], latency_by_task['latency'],\n",
    "         color=['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6'])\n",
    "ax2.set_title('‚è±Ô∏è Latence Moyenne par T√¢che', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Latence (secondes)')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üèÜ Classement Vitesse (plus rapide ‚Üí plus lent):\")\n",
    "speed_ranking = latency_by_model.sort_values('Latence_Moyenne')\n",
    "for i, (_, row) in enumerate(speed_ranking.iterrows(), 1):\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â', '4Ô∏è‚É£', '5Ô∏è‚É£', '6Ô∏è‚É£'][i-1]\n",
    "    print(f\"  {medal} {row['Model']:15s}: {row['Latence_Moyenne']:.2f}s (¬±{row['Latence_Std']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Analyse des Co√ªts R√©els"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les co√ªts\n",
    "cost_analysis = df_results[df_results['success']].groupby('model').agg({\n",
    "    'cost': ['sum', 'mean'],\n",
    "    'total_tokens': ['sum', 'mean']\n",
    "}).round(6)\n",
    "\n",
    "cost_analysis.columns = ['Co√ªt_Total', 'Co√ªt_Moyen', 'Tokens_Total', 'Tokens_Moyen']\n",
    "cost_analysis = cost_analysis.reset_index()\n",
    "\n",
    "# Projeter les co√ªts sur diff√©rents volumes\n",
    "volumes = [1_000, 10_000, 100_000, 1_000_000]  # Nombre de requ√™tes par mois\n",
    "volume_labels = ['1K req/mois', '10K req/mois', '100K req/mois', '1M req/mois']\n",
    "\n",
    "# Calculer les co√ªts projet√©s\n",
    "cost_projections = []\n",
    "for _, row in cost_analysis.iterrows():\n",
    "    for vol, vol_label in zip(volumes, volume_labels):\n",
    "        monthly_cost = row['Co√ªt_Moyen'] * vol\n",
    "        cost_projections.append({\n",
    "            'Model': row['model'],\n",
    "            'Volume': vol_label,\n",
    "            'Volume_Num': vol,\n",
    "            'Co√ªt_Mensuel': monthly_cost\n",
    "        })\n",
    "\n",
    "df_cost_proj = pd.DataFrame(cost_projections)\n",
    "\n",
    "# Graphique des co√ªts projet√©s\n",
    "fig = px.bar(df_cost_proj, x='Volume', y='Co√ªt_Mensuel', color='Model',\n",
    "             title='üí∞ Projection des Co√ªts Mensuels par Volume d\\'Usage',\n",
    "             labels={'Co√ªt_Mensuel': 'Co√ªt Mensuel ($)', 'Volume': 'Volume d\\'Usage'},\n",
    "             height=500)\n",
    "\n",
    "# √âchelle logarithmique pour mieux voir les diff√©rences\n",
    "fig.update_layout(yaxis_type=\"log\")\n",
    "fig.show()\n",
    "\n",
    "# Tableau d√©taill√© des co√ªts\n",
    "print(\"üíµ Analyse D√©taill√©e des Co√ªts (pour les 5 t√¢ches de test):\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in cost_analysis.iterrows():\n",
    "    print(f\"\\nüìä {row['model']}:\")\n",
    "    print(f\"  üí∞ Co√ªt par test: ${row['Co√ªt_Moyen']:.6f}\")\n",
    "    print(f\"  üî¢ Tokens moyens: {row['Tokens_Moyen']:.0f}\")\n",
    "    print(f\"  üìà Co√ªt 1K requ√™tes/mois: ${row['Co√ªt_Moyen'] * 1000:.2f}\")\n",
    "    print(f\"  üìà Co√ªt 100K requ√™tes/mois: ${row['Co√ªt_Moyen'] * 100000:.2f}\")\n",
    "\n",
    "# Point de bascule co√ªt\n",
    "print(\"\\nüéØ Points de Bascule √âconomiques:\")\n",
    "proprietary_models = ['GPT-4', 'GPT-3.5 Turbo', 'Claude 3 Opus', 'Claude 3 Sonnet', 'Gemini Pro']\n",
    "open_source_hosting_cost = 500  # $/mois estimation h√©bergement\n",
    "\n",
    "for model in proprietary_models:\n",
    "    model_cost = cost_analysis[cost_analysis['model'] == model]['Co√ªt_Moyen'].iloc[0]\n",
    "    if model_cost > 0:\n",
    "        breakeven_requests = open_source_hosting_cost / model_cost\n",
    "        print(f\"  üìä {model}: Bascule vers open source √† {breakeven_requests:,.0f} requ√™tes/mois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ √âvaluation Qualitative\n",
    "\n",
    "### üìù Comparaison des R√©ponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'√©valuation qualitative simple\n",
    "def evaluate_response_quality(response, task_type):\n",
    "    \"\"\"\n",
    "    √âvaluation qualitative basique d'une r√©ponse\n",
    "    (Dans un vrai projet, utilisez des m√©triques plus sophistiqu√©es)\n",
    "    \"\"\"\n",
    "    if not response or response.startswith(\"Erreur\"):\n",
    "        return 0\n",
    "    \n",
    "    score = 50  # Score de base\n",
    "    \n",
    "    # Crit√®res g√©n√©raux\n",
    "    if len(response) > 20:  # R√©ponse substantielle\n",
    "        score += 20\n",
    "    \n",
    "    if '.' in response and len(response.split('.')) > 1:  # Structure\n",
    "        score += 10\n",
    "    \n",
    "    # Crit√®res sp√©cifiques par t√¢che\n",
    "    if task_type == 'code':\n",
    "        if 'def ' in response:  # Fonction d√©finie\n",
    "            score += 15\n",
    "        if 'return' in response:  # Return statement\n",
    "            score += 10\n",
    "    \n",
    "    elif task_type == 'resume':\n",
    "        words = len(response.split())\n",
    "        if 20 <= words <= 60:  # Longueur appropri√©e\n",
    "            score += 15\n",
    "    \n",
    "    elif task_type == 'translation':\n",
    "        if any(word in response.lower() for word in ['hello', 'table', 'tonight', 'available']):\n",
    "            score += 15\n",
    "    \n",
    "    return min(score, 100)  # Cap √† 100\n",
    "\n",
    "# Calculer les scores de qualit√©\n",
    "df_results['quality_score'] = df_results.apply(\n",
    "    lambda row: evaluate_response_quality(row['response'], row['task']) if row['success'] else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Analyser la qualit√© par mod√®le et t√¢che\n",
    "quality_by_model_task = df_results[df_results['success']].pivot_table(\n",
    "    index='model', \n",
    "    columns='task_name', \n",
    "    values='quality_score', \n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "# Heatmap de qualit√©\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(quality_by_model_task, \n",
    "            annot=True, \n",
    "            cmap='RdYlGn', \n",
    "            center=75,\n",
    "            fmt='.0f',\n",
    "            cbar_kws={'label': 'Score de Qualit√©'})\n",
    "plt.title('üéØ Qualit√© des R√©ponses par Mod√®le et T√¢che', fontweight='bold', fontsize=16)\n",
    "plt.ylabel('Mod√®le')\n",
    "plt.xlabel('T√¢che')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Score de qualit√© moyen par mod√®le\n",
    "quality_by_model = df_results[df_results['success']].groupby('model')['quality_score'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"üèÜ Classement Qualit√© (bas√© sur √©valuation automatique):\")\n",
    "for i, (model, score) in enumerate(quality_by_model.items(), 1):\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â', '4Ô∏è‚É£', '5Ô∏è‚É£', '6Ô∏è‚É£'][i-1]\n",
    "    print(f\"  {medal} {model:15s}: {score:.0f}/100\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note : Ces scores sont bas√©s sur une √©valuation automatique simple.\")\n",
    "print(\"   Pour une √©valuation rigoureuse, utilisez des m√©triques sp√©cialis√©es et l'√©valuation humaine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Synth√®se : Rapport Qualit√©/Prix/Vitesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un score composite\n",
    "summary_stats = df_results[df_results['success']].groupby('model').agg({\n",
    "    'quality_score': 'mean',\n",
    "    'latency': 'mean',\n",
    "    'cost': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Normaliser les m√©triques (0-100)\n",
    "summary_stats['quality_norm'] = summary_stats['quality_score']  # D√©j√† 0-100\n",
    "summary_stats['speed_norm'] = 100 - (summary_stats['latency'] / summary_stats['latency'].max() * 100)  # Inverse latence\n",
    "\n",
    "# Pour le co√ªt, diff√©rencier mod√®les payants vs gratuits\n",
    "max_cost = summary_stats[summary_stats['cost'] > 0]['cost'].max()\n",
    "summary_stats['cost_norm'] = summary_stats.apply(\n",
    "    lambda row: 100 if row['cost'] == 0 else 100 - (row['cost'] / max_cost * 80),  # Gratuit = 100, max payant = 20\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculer des scores composites pour diff√©rents profils\n",
    "profiles = {\n",
    "    'Qualit√© Max': {'quality': 0.7, 'speed': 0.1, 'cost': 0.2},\n",
    "    '√âquilibr√©': {'quality': 0.4, 'speed': 0.3, 'cost': 0.3},\n",
    "    'Budget Limit√©': {'quality': 0.2, 'speed': 0.2, 'cost': 0.6},\n",
    "    'Temps R√©el': {'quality': 0.3, 'speed': 0.6, 'cost': 0.1}\n",
    "}\n",
    "\n",
    "for profile_name, weights in profiles.items():\n",
    "    summary_stats[f'score_{profile_name.lower().replace(\" \", \"_\")}'] = (\n",
    "        summary_stats['quality_norm'] * weights['quality'] +\n",
    "        summary_stats['speed_norm'] * weights['speed'] +\n",
    "        summary_stats['cost_norm'] * weights['cost']\n",
    "    )\n",
    "\n",
    "# Graphique radar pour chaque profil\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{\"type\": \"polar\"}, {\"type\": \"polar\"}],\n",
    "           [{\"type\": \"polar\"}, {\"type\": \"polar\"}]],\n",
    "    subplot_titles=list(profiles.keys())\n",
    ")\n",
    "\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3']\n",
    "\n",
    "for i, (profile_name, _) in enumerate(profiles.items()):\n",
    "    row, col = positions[i]\n",
    "    \n",
    "    # Top 4 mod√®les pour ce profil\n",
    "    score_col = f'score_{profile_name.lower().replace(\" \", \"_\")}'\n",
    "    top_models = summary_stats.nlargest(4, score_col)\n",
    "    \n",
    "    for j, (_, model_row) in enumerate(top_models.iterrows()):\n",
    "        values = [model_row['quality_norm'], model_row['speed_norm'], model_row['cost_norm']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=values + [values[0]],\n",
    "                theta=['Qualit√©', 'Vitesse', 'Co√ªt'] + ['Qualit√©'],\n",
    "                fill='toself',\n",
    "                name=model_row['model'],\n",
    "                line_color=colors[j],\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üéØ Recommandations par Profil d'Usage (Top 4 par profil)\",\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Configurer les axes polaires\n",
    "for i in range(1, 3):\n",
    "    for j in range(1, 3):\n",
    "        fig.update_polars(\n",
    "            radialaxis=dict(visible=True, range=[0, 100]),\n",
    "            row=i, col=j\n",
    "        )\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Recommandations textuelles\n",
    "print(\"üéØ RECOMMANDATIONS FINALES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for profile_name, weights in profiles.items():\n",
    "    score_col = f'score_{profile_name.lower().replace(\" \", \"_\")}'\n",
    "    best_model = summary_stats.loc[summary_stats[score_col].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüèÜ {profile_name}:\")\n",
    "    print(f\"  Recommand√©: {best_model['model']}\")\n",
    "    print(f\"  Score global: {best_model[score_col]:.1f}/100\")\n",
    "    print(f\"  Qualit√©: {best_model['quality_norm']:.0f}/100\")\n",
    "    print(f\"  Vitesse: {best_model['speed_norm']:.0f}/100\")\n",
    "    print(f\"  Co√ªt: {best_model['cost_norm']:.0f}/100\")\n",
    "    print(f\"  Latence r√©elle: {best_model['latency']:.2f}s\")\n",
    "    print(f\"  Co√ªt r√©el/test: ${best_model['cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Exemple de R√©ponses D√©taill√©es\n",
    "\n",
    "### üîç Comparaison Qualitative sur la T√¢che \"R√©sum√©\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les r√©ponses pour la t√¢che de r√©sum√©\n",
    "resume_results = df_results[(df_results['task'] == 'resume') & (df_results['success'])]\n",
    "\n",
    "print(\"üìù COMPARAISON DES R√âSUM√âS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìñ Texte original (extrait):\")\n",
    "print(TEST_TASKS['resume']['input'][:200] + \"...\")\n",
    "print(\"\\nüéØ Consigne: R√©sumez en 2-3 phrases claires\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "for _, result in resume_results.iterrows():\n",
    "    print(f\"\\nü§ñ {result['model']}:\")\n",
    "    print(f\"‚è±Ô∏è  Latence: {result['latency']:.2f}s\")\n",
    "    print(f\"üí∞ Co√ªt: ${result['cost']:.6f}\")\n",
    "    print(f\"üìä Score: {result['quality_score']:.0f}/100\")\n",
    "    print(f\"üìù R√©ponse:\")\n",
    "    print(f\"   {result['response']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Analyse comparative\n",
    "print(\"\\nüîç ANALYSE COMPARATIVE:\")\n",
    "resume_analysis = resume_results.groupby('model').agg({\n",
    "    'latency': 'first',\n",
    "    'cost': 'first', \n",
    "    'quality_score': 'first',\n",
    "    'response': lambda x: len(x.iloc[0].split())  # Nombre de mots\n",
    "}).round(3)\n",
    "resume_analysis.columns = ['Latence (s)', 'Co√ªt ($)', 'Score Qualit√©', 'Nb Mots']\n",
    "\n",
    "print(resume_analysis.sort_values('Score Qualit√©', ascending=False))\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "fastest = resume_results.loc[resume_results['latency'].idxmin()]['model']\n",
    "cheapest = resume_results.loc[resume_results['cost'].idxmin()]['model'] \n",
    "best_quality = resume_results.loc[resume_results['quality_score'].idxmax()]['model']\n",
    "\n",
    "print(f\"  ‚ö° Plus rapide: {fastest}\")\n",
    "print(f\"  üí∞ Moins cher: {cheapest}\")\n",
    "print(f\"  üèÜ Meilleure qualit√©: {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusions et Recommandations Pratiques\n",
    "\n",
    "### ‚úÖ Points Cl√©s des Tests R√©els\n",
    "\n",
    "1. **üìä Les benchmarks ne racontent qu'une partie de l'histoire**\n",
    "   - Performance r√©elle d√©pend de votre cas d'usage\n",
    "   - Latence varie √©norm√©ment selon le mod√®le\n",
    "   - Co√ªts peuvent exploser selon le volume\n",
    "\n",
    "2. **‚ö° Vitesse vs Qualit√© : Compromis in√©vitable**\n",
    "   - Mod√®les rapides souvent \"assez bons\"\n",
    "   - Mod√®les lents excellents pour t√¢ches critiques\n",
    "   - Identifier votre seuil de tol√©rance\n",
    "\n",
    "3. **üí∞ Co√ªt r√©el ‚â† Prix annonc√©**\n",
    "   - Compter les tokens d'input ET output\n",
    "   - Volume change compl√®tement l'√©quation\n",
    "   - Mod√®les gratuits ont co√ªts d'h√©bergement\n",
    "\n",
    "4. **üéØ Pas de mod√®le parfait universel**\n",
    "   - Chaque profil d'usage a son optimal\n",
    "   - Tester sur VOS donn√©es = indispensable\n",
    "   - R√©√©valuer r√©guli√®rement (nouveaux mod√®les)\n",
    "\n",
    "### üõ†Ô∏è M√©thodologie de Test Recommand√©e\n",
    "\n",
    "1. **üìã D√©finir vos t√¢ches repr√©sentatives**\n",
    "   - 3-5 t√¢ches typiques de votre usage\n",
    "   - Avec vos donn√©es r√©elles\n",
    "   - M√©triques de succ√®s claires\n",
    "\n",
    "2. **‚öñÔ∏è Pond√©rer vos crit√®res**\n",
    "   - Qualit√© vs vitesse vs co√ªt\n",
    "   - Selon votre contexte business\n",
    "   - Avec contraintes techniques\n",
    "\n",
    "3. **üß™ Tester m√©thodiquement**\n",
    "   - 2-3 mod√®les finalistes\n",
    "   - Sur √©chantillon repr√©sentatif\n",
    "   - Mesurer latence ET qualit√©\n",
    "\n",
    "4. **üìà Projeter √† votre √©chelle**\n",
    "   - Co√ªts mensuels r√©alistes\n",
    "   - Besoins d'infrastructure\n",
    "   - Plan de mont√©e en charge\n",
    "\n",
    "### üöÄ Prochaine √âtape\n",
    "\n",
    "Dans le **notebook final**, nous allons cr√©er votre **matrice de d√©cision personnalis√©e** qui automatise tout ce processus !\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ F√©licitations ! Vous savez maintenant tester et comparer les LLM de mani√®re rigoureuse !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
