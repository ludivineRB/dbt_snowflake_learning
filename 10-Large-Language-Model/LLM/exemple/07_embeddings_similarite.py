# -*- coding: utf-8 -*-
from typing import List

import matplotlib.pyplot as plt
import numpy as np
import requests


def get_embedding(text: str, model: str = "mxbai-embed-large") -> List[float]:
    """
    GÃ©nÃ¨re l'embedding d'un texte via l'API Ollama

    Args:
        text: Le texte Ã  convertir en embedding
        model: Le modÃ¨le d'embedding Ã  utiliser

    Returns:
        Liste de nombres (vecteur) reprÃ©sentant le texte
    """
    url = "http://localhost:11434/api/embeddings"

    data = {"model": model, "prompt": text}

    try:
        response = requests.post(url, json=data)
        response.raise_for_status()
        result = response.json()
        return result["embedding"]
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration d'embedding: {e}")
        return []


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Calcule la similaritÃ© cosinus entre deux vecteurs

    RÃ©sultat entre -1 et 1 :
    - 1.0 = Identiques
    - 0.0 = Orthogonaux (pas de relation)
    - -1.0 = OpposÃ©s

    En pratique pour les embeddings : entre 0.0 et 1.0
    """
    # Convertir en arrays numpy pour les calculs
    v1 = np.array(vec1)
    v2 = np.array(vec2)

    # Formule de similaritÃ© cosinus
    dot_product = np.dot(v1, v2)
    magnitude1 = np.linalg.norm(v1)
    magnitude2 = np.linalg.norm(v2)

    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0

    return dot_product / (magnitude1 * magnitude2)


def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:
    """
    Calcule la distance euclidienne entre deux vecteurs

    Plus la distance est petite, plus les vecteurs sont similaires
    """
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    return np.linalg.norm(v1 - v2)


def demo_embeddings():
    """
    DÃ©monstration des embeddings et de la similaritÃ©
    """
    print("ğŸ”¬ === DÃ‰MONSTRATION DES EMBEDDINGS ===\n")

    # Phrases de test
    phrases = [
        "Je mange une pomme",
        "Je consomme un fruit rouge",  # Similaire Ã  la premiÃ¨re
        "Il pleut dehors",  # DiffÃ©rent
        "La pluie tombe",  # Similaire Ã  la prÃ©cÃ©dente
        "Python est un langage de programmation",  # ComplÃ¨tement diffÃ©rent
    ]

    print("ğŸ“ Phrases Ã  analyser :")
    for i, phrase in enumerate(phrases):
        print(f"  {i + 1}. {phrase}")

    print("\nğŸ”„ GÃ©nÃ©ration des embeddings...")

    # GÃ©nÃ©rer les embeddings
    embeddings = []
    for phrase in phrases:
        embedding = get_embedding(phrase)
        if embedding:
            embeddings.append(embedding)
            print(f"  âœ… Embedding gÃ©nÃ©rÃ© pour : '{phrase[:30]}...'")
        else:
            print(f"  âŒ Ã‰chec pour : '{phrase}'")
            return

    print(f"\nğŸ“Š Dimension des embeddings : {len(embeddings[0])}")

    # Calculer les similaritÃ©s
    print("\nğŸ” === MATRICE DE SIMILARITÃ‰ ===")
    print("(Plus proche de 1.0 = plus similaire)\n")

    # En-tÃªte
    print("     ", end="")
    for i in range(len(phrases)):
        print(f"  {i + 1:4}", end="")
    print()

    # Matrice
    for i in range(len(embeddings)):
        print(f"{i + 1:2}. ", end="")
        for j in range(len(embeddings)):
            sim = cosine_similarity(embeddings[i], embeddings[j])
            print(f"{sim:6.3f}", end="")
        print(f"  {phrases[i][:25]}...")

    # Analyse des rÃ©sultats
    print("\nğŸ§ === ANALYSE ===")

    # Trouver les paires les plus similaires (hors diagonale)
    max_sim = 0
    best_pair = (0, 0)

    for i in range(len(embeddings)):
        for j in range(i + 1, len(embeddings)):
            sim = cosine_similarity(embeddings[i], embeddings[j])
            if sim > max_sim:
                max_sim = sim
                best_pair = (i, j)

    print(f"ğŸ† Paire la plus similaire (score: {max_sim:.3f}) :")
    print(f"   â€¢ '{phrases[best_pair[0]]}'")
    print(f"   â€¢ '{phrases[best_pair[1]]}'")

    # Distance euclidienne pour comparaison
    print(
        f"\nğŸ“ Distance euclidienne entre ces phrases : {euclidean_distance(embeddings[best_pair[0]], embeddings[best_pair[1]]):.2f}"
    )


def semantic_search(query: str, documents: List[str], top_k: int = 3):
    """
    Recherche sÃ©mantique simple : trouve les documents les plus similaires Ã  la requÃªte

    Args:
        query: La requÃªte de recherche
        documents: Liste de documents dans lesquels chercher
        top_k: Nombre de rÃ©sultats Ã  retourner
    """
    print(f"ğŸ” Recherche pour : '{query}'\n")

    # GÃ©nÃ©rer l'embedding de la requÃªte
    query_embedding = get_embedding(query)
    if not query_embedding:
        print("âŒ Impossible de gÃ©nÃ©rer l'embedding de la requÃªte")
        return

    # GÃ©nÃ©rer les embeddings des documents
    doc_embeddings = []
    for doc in documents:
        embedding = get_embedding(doc)
        doc_embeddings.append(embedding)

    # Calculer les similaritÃ©s
    similarities = []
    for i, doc_embedding in enumerate(doc_embeddings):
        if doc_embedding:
            sim = cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, sim, documents[i]))
        else:
            similarities.append((i, 0.0, documents[i]))

    # Trier par similaritÃ© dÃ©croissante
    similarities.sort(key=lambda x: x[1], reverse=True)

    # Afficher les rÃ©sultats
    print("ğŸ“‹ RÃ©sultats (triÃ©s par pertinence) :")
    for rank, (doc_idx, similarity, document) in enumerate(similarities[:top_k], 1):
        print(f"{rank}. Score: {similarity:.3f} | {document}")

    return similarities[:top_k]


def visualize_embeddings_2d(texts: List[str], embeddings: List[List[float]]):
    """
    Visualise les embeddings en 2D en utilisant PCA
    (NÃ©cessite scikit-learn : pip install scikit-learn)
    """
    try:
        from sklearn.decomposition import PCA

        # RÃ©duire la dimensionnalitÃ© Ã  2D
        pca = PCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings)

        # CrÃ©er le graphique
        plt.figure(figsize=(12, 8))
        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.7)

        # Ajouter les labels
        for i, txt in enumerate(texts):
            plt.annotate(
                txt[:20],
                (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                xytext=(5, 5),
                textcoords="offset points",
                fontsize=9,
            )

        plt.title("Visualisation 2D des Embeddings")
        plt.xlabel("Composante 1")
        plt.ylabel("Composante 2")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

        print(f"ğŸ“Š Variance expliquÃ©e : {pca.explained_variance_ratio_.sum():.2%}")

    except ImportError:
        print(
            "âš ï¸  Pour la visualisation, installez scikit-learn : pip install scikit-learn"
        )


# ============================================================================
# 7. EXERCICES PRATIQUES
# ============================================================================


def exercice_classification():
    """
    Exercice : Classifier des phrases par thÃ¨me en utilisant les embeddings
    """
    print("ğŸ¯ === EXERCICE : CLASSIFICATION PAR THÃˆME ===\n")

    # Phrases de rÃ©fÃ©rence pour chaque thÃ¨me
    themes = {
        "cuisine": "recette de cuisine gastronomie plat dÃ©licieux",
        "sport": "football tennis course entraÃ®nement compÃ©tition",
        "technologie": "ordinateur programmation intelligence artificielle",
    }

    # Phrases Ã  classifier
    test_phrases = [
        "J'ai fait un gÃ¢teau au chocolat",
        "L'Ã©quipe a gagnÃ© le match",
        "Python est un excellent langage",
        "Cette pizza est dÃ©licieuse",
        "Il court un marathon",
        "L'IA rÃ©volutionne le monde",
    ]

    print("ğŸ·ï¸  ThÃ¨mes disponibles :")
    for theme, description in themes.items():
        print(f"   â€¢ {theme}: {description}")

    print("\nğŸ“ Phrases Ã  classifier :")
    for phrase in test_phrases:
        print(f"   â€¢ {phrase}")

    print("\nğŸ”„ Classification en cours...\n")

    # GÃ©nÃ©rer les embeddings des thÃ¨mes
    theme_embeddings = {}
    for theme, description in themes.items():
        theme_embeddings[theme] = get_embedding(description)

    # Classifier chaque phrase
    for phrase in test_phrases:
        phrase_embedding = get_embedding(phrase)

        if phrase_embedding:
            best_theme = None
            best_score = -1

            for theme, theme_embedding in theme_embeddings.items():
                if theme_embedding:
                    score = cosine_similarity(phrase_embedding, theme_embedding)
                    if score > best_score:
                        best_score = score
                        best_theme = theme

            print(f"ğŸ“Š '{phrase}' â†’ {best_theme} (score: {best_score:.3f})")


# ============================================================================
# 8. PROGRAMME PRINCIPAL
# ============================================================================

if __name__ == "__main__":
    print("ğŸš€ Module 4 : Embeddings et SimilaritÃ©\n")
    print(
        "âš ï¸  Assurez-vous qu'Ollama est dÃ©marrÃ© et que le modÃ¨le 'mxbai-embed-large' est installÃ©"
    )
    print("   Commande : ollama pull mxbai-embed-large\n")

    while True:
        print("=" * 60)
        print("MENU :")
        print("1. ğŸ”¬ DÃ©monstration des embeddings")
        print("2. ğŸ” Recherche sÃ©mantique")
        print("3. ğŸ“Š Visualisation 2D")
        print("4. ğŸ¯ Exercice classification")
        print("5. âŒ Quitter")
        print("=" * 60)

        choice = input("Votre choix (1-5) : ").strip()

        if choice == "1":
            demo_embeddings()

        elif choice == "2":
            documents = [
                "Python est un langage de programmation populaire",
                "L'intelligence artificielle transforme notre sociÃ©tÃ©",
                "La cuisine franÃ§aise est rÃ©putÃ©e dans le monde entier",
                "Le machine learning utilise des algorithmes complexes",
                "Les pandas sont des animaux adorables",
                "NumPy est une bibliothÃ¨que Python pour le calcul scientifique",
            ]
            query = input("ğŸ” Votre requÃªte de recherche : ")
            semantic_search(query, documents)

        elif choice == "3":
            phrases = [
                "Je programme en Python",
                "Python est un serpent",
                "J'aime cuisiner",
                "La recette est dÃ©licieuse",
                "Le sport c'est la santÃ©",
                "Je cours tous les matins",
            ]
            print("ğŸ“Š GÃ©nÃ©ration des embeddings pour la visualisation...")
            embeddings = [get_embedding(phrase) for phrase in phrases]
            if all(embeddings):
                visualize_embeddings_2d(phrases, embeddings)
            else:
                print("âŒ Erreur lors de la gÃ©nÃ©ration des embeddings")

        elif choice == "4":
            exercice_classification()

        elif choice == "5":
            print("ğŸ‘‹ Au revoir ! Direction le Module 5 pour dÃ©couvrir le RAG !")
            break

        else:
            print("âŒ Choix invalide")

        print("\n" + "=" * 60 + "\n")
